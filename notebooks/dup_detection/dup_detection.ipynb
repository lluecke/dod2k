{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89d2d0ad-fc2a-480e-bafa-9566812123a7",
   "metadata": {},
   "source": [
    "# Duplicate detection - step 1: find the potential duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd9d685-226f-4601-a156-5f19b1b59310",
   "metadata": {},
   "source": [
    "This notebook runs the first part of the duplicate detection algorithm on a dataframe with the following columns:\n",
    "\n",
    "- `archiveType`       (used for duplicate detection algorithm)\n",
    "- `dataSetName`\n",
    "- `datasetId`\n",
    "- `geo_meanElev`      (used for duplicate detection algorithm)\n",
    "- `geo_meanLat`       (used for duplicate detection algorithm)\n",
    "- `geo_meanLon`       (used for duplicate detection algorithm)\n",
    "- `geo_siteName`      (used for duplicate detection algorithm)\n",
    "- `interpretation_direction`\n",
    "- `interpretation_seasonality`\n",
    "- `interpretation_variable`\n",
    "- `interpretation_variableDetails`\n",
    "- `originalDataURL`\n",
    "- `originalDatabase`\n",
    "- `paleoData_notes`\n",
    "- `paleoData_proxy`   (used for duplicate detection algorithm)\n",
    "- `paleoData_units`\n",
    "- `paleoData_values`  (used for duplicate detection algorithm, test for correlation, RMSE, correlation of 1st difference, RMSE of 1st difference)\n",
    "- `paleoData_variableName`\n",
    "- `year`              (used for duplicate detection algorithm)\n",
    "- `yearUnits`\n",
    "\n",
    "The key function for duplicate detection is `find_duplicates` in `f_duplicate_search.py`\n",
    "\n",
    "The output is saved as csvs in the directory `data/DATABASENAME/dup_detection`, which are used again for step 2 (`dup_decisions.py`):\n",
    "- `pot_dup_correlations_DATABASENAME.csv`\n",
    "   - matrix of correlations between each pair      \n",
    "- `pot_dup_distances_km_DATABASENAME.csv`\n",
    "   - matrix of distances between each pair \n",
    "- `pot_dup_IDs_DATABASENAME.csv`\n",
    "   - saves the IDs of each pair\n",
    "- `pot_dup_indices_DATABASENAME.csv`\n",
    "   - saves the dataframe indices of each pair\n",
    "\n",
    "Summary figures of the potential duplicate pairs are created and the plots are saved in the same directory, following:\n",
    "duplicatenumber_ID1_ID2_index1_index2.jpg\n",
    "\n",
    "Updates:\n",
    "- 06/11/2025 by LL: Tidied up and updated for DoD2k v2.0\n",
    "- 27/11/2024 by LL: Fixed a bug in find_duplicates (in f_duplicate_search) and relaxed site criteria.\n",
    "\n",
    "27/9/2024 created by LL\n",
    "\n",
    "Author: Lucie J. Luecke"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94636e49-b791-475a-bbb2-084cf3589886",
   "metadata": {},
   "source": [
    "## Set up working environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b93344-e811-4852-890d-8886ce5974d3",
   "metadata": {},
   "source": [
    "Make sure the repo_root is added correctly, it should be: your_root_dir/dod2k\n",
    "This should be the working directory throughout this notebook (and all other notebooks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5917c6b-216b-497b-b598-1991ebc5181f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path (works from any notebook in notebooks/)\n",
    "# the repo_root should be the parent directory of the notebooks folder\n",
    "current_dir = Path().resolve()\n",
    "# Determine repo root\n",
    "if current_dir.name == 'dod2k': repo_root = current_dir\n",
    "elif current_dir.parent.name == 'dod2k': repo_root = current_dir.parent\n",
    "else: raise Exception('Please review the repo root structure (see first cell).')\n",
    "\n",
    "# Update cwd and path only if needed\n",
    "if os.getcwd() != str(repo_root):\n",
    "    os.chdir(repo_root)\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "print(f\"Repo root: {repo_root}\")\n",
    "if str(os.getcwd())==str(repo_root):\n",
    "    print(f\"Working directory matches repo root. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f68c6c3-d103-48d0-ac57-840ee98abb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from dod2k_utilities import ut_functions as utf # contains utility functions\n",
    "from dod2k_utilities import ut_duplicate_search as dup # contains utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d18104-d138-47cc-94ee-97811e2cb1ff",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfee1a8-5bcc-4372-a8cb-462bb38397bd",
   "metadata": {},
   "source": [
    "Define the dataset which needs to be screened for duplicates. Input files for the duplicate detection mechanism need to be compact dataframes (`pandas` dataframes with standardised columns and entry formatting). \n",
    "\n",
    "The function `load_compact_dataframe_from_csv` loads the dataframe from a `csv` file from `data\\DB\\`, with `DB` the name of the database. The database name (`db_name`) can be \n",
    "- `pages2k`\n",
    "- `ch2k`\n",
    "- `iso2k`\n",
    "- `sisal`\n",
    "- `fe23`\n",
    "\n",
    "for the individual databases, or \n",
    "\n",
    "- `all_merged`\n",
    "\n",
    "to load the merged database of all individual databases, or can be any user defined compact dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a26dfac-fed3-40a1-a97b-2d088fc3eaee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load dataframe\n",
    "db_name='all_merged' \n",
    "# db_name='ch2k' \n",
    "df = utf.load_compact_dataframe_from_csv(db_name)\n",
    "\n",
    "print(df.info())\n",
    "df.name = db_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5b6f82-cc36-44b2-b28b-fa498e27bf40",
   "metadata": {},
   "source": [
    "# Duplicate Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbdca46-7c33-4b73-85cb-e9077f08ae5c",
   "metadata": {},
   "source": [
    "### Find duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d50f18-c547-448f-ab4c-ece2d858ebaa",
   "metadata": {},
   "source": [
    "Now run the first part of the duplicate detection algorithm, which goes through each candidate pair and evaluates the pairs for the following criteria:\n",
    "\n",
    "- **metadata criteria**:\n",
    "  - archive types (`archiveType`) must be identical\n",
    "  - proxy types (`paleoData_proxy`) must be identical\n",
    "- **geographical criteria**:\n",
    "  - elevation (`geo_meanElev`) similar, within defined tolerance (use kwarg `elevation_tolerance`, defaults to 0)\n",
    "  - latitude and longtitude (`geo_meanLat` and `geo_meanLon`) similar, within defined tolerance in km (use kwarg `dist_tolerance_km`, defaults to 8 km)\n",
    "- **overlap criterion**:\n",
    "  - time must overlap for at least $n$ points (use kwarg `n_points_thresh` to modify, defaults to $n=10$) unless at least one of the record is shorter than `n_points_thresh` \n",
    "- **site criterion**:\n",
    "  - there must be some overlap in the site name (`geo_siteName`)\n",
    "- **correlation criteria**:\n",
    "  - correlation between the overlapping period must be greater than defined threshold (use `corr_thresh` to modify, defaults to 0.9) or correlation of first difference must be greater than defined threshold (use `corr_diff_thresh` to modify, defaults to 0.9)\n",
    "  - RMSE of overlapping period must be smaller than defined threshold (use `rmse_thresh` to modify, defaults to 0.1) or RMSE of first difference must be smaller than defined threshold (use `rmse_diff_thresh` to modify, defaults to 0.1)\n",
    "- **URL criterion**:\n",
    "  - URLs (`originalDataURL`) must be identical if both records originate from the same database (`originalDatabase` must be identical)\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    " **A potential duplicate candidate pair is flagged, if all of these criteria are satisfied OR the correlation between the candidates is particularly high (>0.98), while there is sufficient overlap (as defined by the overlap criterion).**\n",
    "\n",
    " ----\n",
    "\n",
    "The output for a database named `DB` is saved under `data/DB/dup_detection/dup_detection_candidates_DB.csv`.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e03440e-bd0d-46eb-a7e0-4fa7fc96c699",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run the find duplicate algorithm\n",
    "dup.find_duplicates_optimized(df, n_points_thresh=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ad514d-4f7d-43d3-a1ad-3cf58582b41d",
   "metadata": {},
   "source": [
    "## Plot duplicate candidate pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632f0b9c-81bf-4c71-97e0-7f4e25842215",
   "metadata": {},
   "source": [
    "*OPTIONAL*: plot the duplicate candidate pairs, which were flagged by the duplicate detection algorithm. \n",
    "The function `plot_duplicates` loads the flagged candidate pairs for a database named `DB` from csv (`data/DB/dup_detection/dup_detection_candidates_DB.csv`) and produces summary figures of the potential duplicates, which are saved in the directory `figs/DB/dup_detection/`.\n",
    "\n",
    "**Note that the same summary figures are being used for the duplicate decision process (`dup_decisions.ipynb`).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9a5c25-32c3-4226-8f7e-7029c2aaab91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dup.plot_duplicates(df, save_figures=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3df4c3-b538-423d-b637-e1bf166466fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = utf.find(f'dup_detection_candidates_{df.name}.csv',  f'data/{df.name}/dup_detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb3fae3-3f2c-469d-b91c-d250787128a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fn != []:\n",
    "    print('----------------------------------------------------')\n",
    "    print('Sucessfully finished the duplicate detection process!'.upper())\n",
    "    print('----------------------------------------------------')\n",
    "    print('Saved the detection output file in:')\n",
    "    print()\n",
    "    print('%s.'%', '.join(fn))\n",
    "    print()\n",
    "    print('You are now able to proceed to the next notebook: dup_decision.ipynb')\n",
    "else:\n",
    "    print('Final output file is missing.')\n",
    "    print()\n",
    "    print('Please re-run the notebook to complete duplicate detection process.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cfr-env)",
   "language": "python",
   "name": "cfr-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
