{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b8b9506-0168-46c3-9fae-a807607edba7",
   "metadata": {},
   "source": [
    "# Filter compact dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f57846-79af-4cbd-a827-b4f5c73c504a",
   "metadata": {},
   "source": [
    "This file reads the compact dataframes and filters for specific records (e.g. for moisture sensitive records). \n",
    "The filtered dataset is saved in a separate directory and can be loaded for further analysis or plotting etc.\n",
    "\n",
    "Author: Lucie Luecke\n",
    "\n",
    "Date produced: 21/01/2025\n",
    "\n",
    "Input: \n",
    "reads dataframe with the following keys:\n",
    "  - ```archiveType```\n",
    "  - ```dataSetName```\n",
    "  - ```datasetId```\n",
    "  - ```geo_meanElev```\n",
    "  - ```geo_meanLat```\n",
    "  - ```geo_meanLon```\n",
    "  - ```geo_siteName```\n",
    "  - ```interpretation_direction``` (new in v2.0)\n",
    "  - ```interpretation_variable```\n",
    "  - ```interpretation_variableDetail```\n",
    "  - ```interpretation_seasonality``` (new in v2.0)\n",
    "  - ```originalDataURL```\n",
    "  - ```originalDatabase```\n",
    "  - ```paleoData_notes```\n",
    "  - ```paleoData_proxy```\n",
    "  - ```paleoData_sensorSpecies```\n",
    "  - ```paleoData_units```\n",
    "  - ```paleoData_values```\n",
    "  - ```paleoData_variableName```\n",
    "  - ```year```\n",
    "  - ```yearUnits```\n",
    "  - (optional: `DuplicateDetails`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb61e97d-f071-4f7c-b376-40fa56555a94",
   "metadata": {},
   "source": [
    "## Set up working environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5763910-065a-4888-ba43-21b5e01d4ada",
   "metadata": {},
   "source": [
    "Make sure the repo_root is added correctly, it should be: your_root_dir/dod2k\n",
    "This should be the working directory throughout this notebook (and all other notebooks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79c4911b-41ea-4367-bbcc-ce112c5fbd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo root: C:\\Users\\aegir\\Documents\\PaleoCoLab\\dod2k\n",
      "Working directory matches repo root. \n"
     ]
    }
   ],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path (works from any notebook in notebooks/)\n",
    "# the repo_root should be the parent directory of the notebooks folder\n",
    "current_dir = Path().resolve()\n",
    "# Determine repo root\n",
    "if current_dir.name == 'dod2k':\n",
    "    repo_root = current_dir\n",
    "elif current_dir.parent.name == 'dod2k':\n",
    "    repo_root = current_dir.parent\n",
    "else:\n",
    "    raise Exception('Please review the repo root structure (see first cell).')\n",
    "\n",
    "# Update cwd and path only if needed\n",
    "if os.getcwd() != str(repo_root):\n",
    "    os.chdir(repo_root)\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "print(f\"Repo root: {repo_root}\")\n",
    "if str(os.getcwd())==str(repo_root):\n",
    "    print(f\"Working directory matches repo root. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cc44c46-7c88-4561-8694-1251f8c2b674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from dod2k_utilities import ut_functions as utf # contains utility functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dc0e2c-d3c6-40da-9e4f-5e015073d7c6",
   "metadata": {},
   "source": [
    "## read dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3750b68-6578-42ec-b432-275d3d3758d7",
   "metadata": {},
   "source": [
    "Read compact dataframe.\n",
    "\n",
    "{db_name} refers to the database, including e.g.\n",
    "  - database of databases:\n",
    "    - dod2k_v2.0 (dod2k: duplicate free, merged database)\n",
    "    - all_merged (NOT filtered for duplicates, only fusion of the input databases)\n",
    "  - original databases:\n",
    "    - fe23\n",
    "    - ch2k\n",
    "    - sisal\n",
    "    - pages2k\n",
    "    - iso2k\n",
    "\n",
    "All compact dataframes are saved in {repo_root}/data/{db_name} as {db_name}_compact.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e017f834-97a3-4ed4-a66d-9d57c440bfe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PAGES 2k v2.2.0' 'FE23 (Breitenmoser et al. (2014))'\n",
      " 'CoralHydro2k v1.0.1' 'Iso2k v1.1.2' 'SISAL v3' 'dod2k_composite_z']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4781 entries, 0 to 4780\n",
      "Data columns (total 22 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   archiveType                    4781 non-null   object \n",
      " 1   dataSetName                    4781 non-null   object \n",
      " 2   datasetId                      4781 non-null   object \n",
      " 3   duplicateDetails               4781 non-null   object \n",
      " 4   geo_meanElev                   4699 non-null   float32\n",
      " 5   geo_meanLat                    4781 non-null   float32\n",
      " 6   geo_meanLon                    4781 non-null   float32\n",
      " 7   geo_siteName                   4781 non-null   object \n",
      " 8   interpretation_direction       4781 non-null   object \n",
      " 9   interpretation_seasonality     4781 non-null   object \n",
      " 10  interpretation_variable        4781 non-null   object \n",
      " 11  interpretation_variableDetail  4781 non-null   object \n",
      " 12  originalDataURL                4781 non-null   object \n",
      " 13  originalDatabase               4781 non-null   object \n",
      " 14  paleoData_notes                4781 non-null   object \n",
      " 15  paleoData_proxy                4781 non-null   object \n",
      " 16  paleoData_sensorSpecies        4781 non-null   object \n",
      " 17  paleoData_units                4781 non-null   object \n",
      " 18  paleoData_values               4781 non-null   object \n",
      " 19  paleoData_variableName         4781 non-null   object \n",
      " 20  year                           4781 non-null   object \n",
      " 21  yearUnits                      4781 non-null   object \n",
      "dtypes: float32(3), object(19)\n",
      "memory usage: 765.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "db_name = 'dod2k_v2.0'\n",
    "\n",
    "df = utf.load_compact_dataframe_from_csv(db_name)\n",
    "print(df.originalDatabase.unique())\n",
    "df.name = db_name\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ccce94-a8bf-4b7f-bd14-adedda715b0f",
   "metadata": {},
   "source": [
    "## filter dataframe for specific record types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b85a217-339f-4094-b923-a00ba0dd65df",
   "metadata": {},
   "source": [
    "Here you can filter the dataframe for specific record types. Below is an example where we filter for interpretation_variable=temperature. \n",
    "\n",
    "This could be done with any column and any value (e.g. for a specific archive type, etc.)\n",
    "\n",
    "Please look at the examples below which are commented out for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a67160c-2b5a-4e8e-87cc-0f53f612bd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to filter for specific metadata, e.g. temperature or moisture records, run this:\n",
    "\n",
    "\n",
    "# ---> interpretation_variable\n",
    "# e.g.\n",
    "\n",
    "# # filter for >>moisture<< sensitive records only (also include records which are moisture and temperature sensitive)\n",
    "df_filter = df.loc[(df['interpretation_variable']=='moisture')|(df['interpretation_variable']=='temperature+moisture')]\n",
    "df_filter.name = db_name + \"_filtered_M_TM\" \n",
    "\n",
    "\n",
    "# # filter for >>exclusively moisture<< sensitive records only (without t+m)\n",
    "# df_filter = df.loc[(df['interpretation_variable']=='moisture')]\n",
    "# df_filter.name = db_name + \"_filtered_M\" \n",
    "\n",
    "\n",
    "# # filter for >>temperature<< sensitive records only (also include records which are moisture and temperature sensitive)\n",
    "# df_filter = df.loc[(df['interpretation_variable']=='temperature')|(df['interpretation_variable']=='temperature+moisture')]\n",
    "# df_filter.name = db_name + \"_filtered_T_TM\" \n",
    "\n",
    "# # filter for >>exclusively temperature<< sensitive records only (without t+m)\n",
    "# df_filter = df.loc[(df['interpretation_variable']=='temperature')]\n",
    "# df_filter.name = db_name + \"_filtered_T\" \n",
    "\n",
    "\n",
    "# ---> archiveType and paleoData_proxy\n",
    "# e.g.\n",
    "\n",
    "# # filter for specific proxy type, e.g. archiveType='speleothem' and paleoData_proxy='d18O'\n",
    "# df_filter = df.loc[(df['archiveType']=='speleothem')&(df['paleoData_proxy']=='d18O')]\n",
    "# df_filter.name = db_name + \"_filtered_speleo_d18O\" \n",
    "\n",
    "# # filter for specific proxy type, e.g. archiveType='speleothem' only\n",
    "# df_filter = df.loc[(df['archiveType']=='speleothem')]\n",
    "# df_filter.name = db_name + \"_filtered_speleothem\" \n",
    "\n",
    "\n",
    "# ---> paleoData_proxy only\n",
    "# e.g. \n",
    "\n",
    "# df_filter = df.loc[(df['paleoData_proxy']=='MXD')]\n",
    "\n",
    "# etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059d4fde-22db-4955-88f1-9a9a78ea2fda",
   "metadata": {},
   "source": [
    "IMPORTANT: the database name needs to be adjusted according to the filtering.\n",
    "\n",
    "Please add an identifier to the dataframe name which will be used for displaying and savng the data. \n",
    "\n",
    "Make sure it is different from the original db_name.\n",
    "\n",
    "As df.name is used for saving the filtered data it is crucial that it differs from the original db_name otherwise the data will get overwritten!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9b4af5f-1aba-4ade-aec6-ce76b9648741",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_13572\\1793990862.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# df needs name reassigned as it gets lost otherwise after assigning new value to df (through the filtering above)\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# for the M+T filtered example, revise df.name to _filtered_MT\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m print(df_filter.name)\n\u001b[32m      5\u001b[39m \n\u001b[32m      6\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m df_filter.name!=db_name\n",
      "\u001b[32mc:\\Users\\aegir\\.conda\\envs\\dod2k-env\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   6314\u001b[39m             \u001b[38;5;28;01mand\u001b[39;00m name \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m self._accessors\n\u001b[32m   6315\u001b[39m             \u001b[38;5;28;01mand\u001b[39;00m self._info_axis._can_hold_identifiers_and_holds_name(name)\n\u001b[32m   6316\u001b[39m         ):\n\u001b[32m   6317\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self[name]\n\u001b[32m-> \u001b[39m\u001b[32m6318\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m object.__getattribute__(self, name)\n",
      "\u001b[31mAttributeError\u001b[39m: 'DataFrame' object has no attribute 'name'"
     ]
    }
   ],
   "source": [
    "# df needs name reassigned as it gets lost otherwise after assigning new value to df (through the filtering above)\n",
    "\n",
    "# for the M+T filtered example, revise df.name to _filtered_MT\n",
    "print(df_filter.name)\n",
    "\n",
    "assert df_filter.name!=db_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc017188-c30b-4866-b2a3-a0c6b85a7dd5",
   "metadata": {},
   "source": [
    "Display the filtered dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d999ba0f-a2e4-42ec-9e30-a4644230281b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1216 entries, 3 to 4765\n",
      "Data columns (total 22 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   archiveType                    1216 non-null   object \n",
      " 1   dataSetName                    1216 non-null   object \n",
      " 2   datasetId                      1216 non-null   object \n",
      " 3   duplicateDetails               1216 non-null   object \n",
      " 4   geo_meanElev                   1189 non-null   float32\n",
      " 5   geo_meanLat                    1216 non-null   float32\n",
      " 6   geo_meanLon                    1216 non-null   float32\n",
      " 7   geo_siteName                   1216 non-null   object \n",
      " 8   interpretation_direction       1216 non-null   object \n",
      " 9   interpretation_seasonality     1216 non-null   object \n",
      " 10  interpretation_variable        1216 non-null   object \n",
      " 11  interpretation_variableDetail  1216 non-null   object \n",
      " 12  originalDataURL                1216 non-null   object \n",
      " 13  originalDatabase               1216 non-null   object \n",
      " 14  paleoData_notes                1216 non-null   object \n",
      " 15  paleoData_proxy                1216 non-null   object \n",
      " 16  paleoData_sensorSpecies        1216 non-null   object \n",
      " 17  paleoData_units                1216 non-null   object \n",
      " 18  paleoData_values               1216 non-null   object \n",
      " 19  paleoData_variableName         1216 non-null   object \n",
      " 20  year                           1216 non-null   object \n",
      " 21  yearUnits                      1216 non-null   object \n",
      "dtypes: float32(3), object(19)\n",
      "memory usage: 204.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_filter.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178f4369-79be-49eb-9e79-6f06f82f7eb5",
   "metadata": {},
   "source": [
    "## save filtered dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677dcace-d7c0-4b6d-a2a3-1cb69767c471",
   "metadata": {},
   "source": [
    "Saves the filtered dataframe in:\n",
    "\n",
    "{repo_root}/data/{df_filter.name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f447639-6704-4699-bba3-e98acb6c8a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new directory if dir does not exist\n",
    "path = '/data/'+df_filter.name\n",
    "os.makedirs(os.getcwd()+path, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765ab8e4-dbcd-4778-baa1-86e71db7b55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as pickle\n",
    "df_filter.to_pickle(f'data/{df_filter.name}/{df_filter.name}_compact.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0a2800-3fe6-416d-af95-e7be21d3abb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METADATA: datasetId, archiveType, dataSetName, duplicateDetails, geo_meanElev, geo_meanLat, geo_meanLon, geo_siteName, interpretation_direction, interpretation_seasonality, interpretation_variable, interpretation_variableDetail, originalDataURL, originalDatabase, paleoData_notes, paleoData_proxy, paleoData_sensorSpecies, paleoData_units, paleoData_variableName, yearUnits\n",
      "Saved to /home/jupyter-lluecke/dod2k/data/dod2k_v2.0_filtered_T/dod2k_v2.0_filtered_T_compact_%s.csv\n"
     ]
    }
   ],
   "source": [
    "# save csv\n",
    "utf.write_compact_dataframe_to_csv(df_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4446855-345b-4ae7-a7ce-0107cb0fdc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1216 entries, 0 to 1215\n",
      "Data columns (total 22 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   archiveType                    1216 non-null   object \n",
      " 1   dataSetName                    1216 non-null   object \n",
      " 2   datasetId                      1216 non-null   object \n",
      " 3   duplicateDetails               1216 non-null   object \n",
      " 4   geo_meanElev                   1189 non-null   float32\n",
      " 5   geo_meanLat                    1216 non-null   float32\n",
      " 6   geo_meanLon                    1216 non-null   float32\n",
      " 7   geo_siteName                   1216 non-null   object \n",
      " 8   interpretation_direction       1216 non-null   object \n",
      " 9   interpretation_seasonality     1216 non-null   object \n",
      " 10  interpretation_variable        1216 non-null   object \n",
      " 11  interpretation_variableDetail  1216 non-null   object \n",
      " 12  originalDataURL                1216 non-null   object \n",
      " 13  originalDatabase               1216 non-null   object \n",
      " 14  paleoData_notes                1216 non-null   object \n",
      " 15  paleoData_proxy                1216 non-null   object \n",
      " 16  paleoData_sensorSpecies        1216 non-null   object \n",
      " 17  paleoData_units                1216 non-null   object \n",
      " 18  paleoData_values               1216 non-null   object \n",
      " 19  paleoData_variableName         1216 non-null   object \n",
      " 20  year                           1216 non-null   object \n",
      " 21  yearUnits                      1216 non-null   object \n",
      "dtypes: float32(3), object(19)\n",
      "memory usage: 194.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# load dataframe\n",
    "utf.load_compact_dataframe_from_csv(df_filter.name).info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dod2k-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
