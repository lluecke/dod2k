{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b8b4008-054a-4980-baca-ec3aa2f8d184",
   "metadata": {},
   "source": [
    "## \n",
    "This notebook runs a duplicate detection algorithm on a dataframe with the following columns:\n",
    "- 'archiveType'       (used for duplicate detection algorithm)\n",
    "- 'climateInterpretation_variable'\n",
    "- 'dataSetName'\n",
    "- 'datasetId'\n",
    "- 'geo_meanElev'      (used for duplicate detection algorithm)\n",
    "- 'geo_meanLat'       (used for duplicate detection algorithm)\n",
    "- 'geo_meanLon'       (used for duplicate detection algorithm)\n",
    "- 'geo_siteName'      (used for duplicate detection algorithm)\n",
    "- 'originalDataURL'\n",
    "- 'originalDatabase'\n",
    "- 'paleoData_notes'\n",
    "- 'paleoData_proxy'   (used for duplicate detection algorithm)\n",
    "- 'paleoData_units'\n",
    "- 'paleoData_values'  (used for duplicate detection algorithm, test for correlation, RMSE, correlation of 1st difference, RMSE of 1st difference)\n",
    "- 'year'              (used for duplicate detection algorithm)\n",
    "- 'yearUnits'\n",
    "\n",
    "The key function for duplicate detection is find_duplicates in f_duplicate_search.py\n",
    "\n",
    "The output is saved as csvs in the directory dup_detection/DATABASENAME:\n",
    "- pot_dup_correlations_DATABASENAME.csv          \n",
    "- pot_dup_distances_km_DATABASENAME.csv          \n",
    "- pot_dup_IDs_DATABASENAME.csv                   (saves the IDs of each pair)\n",
    "- pot_dup_indices_DATABASENAME.csv               (saves the dataframe indices of each pair)\n",
    "\n",
    "Summary figures of the potential duplicate pairs are created and the plots are saved in the same directory, following:\n",
    "duplicatenumber_ID1_ID2_index1_index2.jpg\n",
    "\n",
    "27/11/2024: Fixed a bug in find_duplicates (in f_duplicate_search) and relaxed site criteria.\n",
    "27/9/2024 v0: Notebook written by Lucie J. Luecke \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bac31e0-9da1-44ab-9976-127a9bb30f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed1a8a9-03b3-48c2-bdee-9e8daf2ce17c",
   "metadata": {},
   "source": [
    "# Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4234ef93-7155-468e-97b9-140d8233f728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import gzip\n",
    "import os\n",
    "import pandas as pd\n",
    "# import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature \n",
    "from matplotlib.gridspec import GridSpec as GS\n",
    "from copy import deepcopy as dc\n",
    "import functions as f\n",
    "import geopy\n",
    "import datetime\n",
    "import f_duplicate_search as dupdet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a650a1b-4dc0-427b-b1db-2de3d4181919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n"
     ]
    }
   ],
   "source": [
    "# choose working directory\n",
    "wdir = os.curdir\n",
    "os.chdir(wdir)\n",
    "print(wdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d18104-d138-47cc-94ee-97811e2cb1ff",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a26dfac-fed3-40a1-a97b-2d088fc3eaee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4517 entries, 0 to 4516\n",
      "Data columns (total 19 columns):\n",
      " #   Column                                Non-Null Count  Dtype  \n",
      "---  ------                                --------------  -----  \n",
      " 0   archiveType                           4517 non-null   object \n",
      " 1   climateInterpretation_variable        4517 non-null   object \n",
      " 2   climateInterpretation_variableDetail  4517 non-null   object \n",
      " 3   dataSetName                           4517 non-null   object \n",
      " 4   datasetId                             4517 non-null   object \n",
      " 5   duplicateDetails                      4517 non-null   object \n",
      " 6   geo_meanElev                          4434 non-null   float32\n",
      " 7   geo_meanLat                           4517 non-null   float32\n",
      " 8   geo_meanLon                           4517 non-null   float32\n",
      " 9   geo_siteName                          4517 non-null   object \n",
      " 10  originalDataURL                       4517 non-null   object \n",
      " 11  originalDatabase                      4517 non-null   object \n",
      " 12  paleoData_notes                       4517 non-null   object \n",
      " 13  paleoData_proxy                       4517 non-null   object \n",
      " 14  paleoData_sensorSpecies               4517 non-null   object \n",
      " 15  paleoData_units                       4517 non-null   object \n",
      " 16  paleoData_values                      4517 non-null   object \n",
      " 17  year                                  4517 non-null   object \n",
      " 18  yearUnits                             4517 non-null   object \n",
      "dtypes: float32(3), object(16)\n",
      "memory usage: 617.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# read dataframe \n",
    "\n",
    "# db_name = 'dod2k'\n",
    "db_name = 'dod2k_dupfree'\n",
    "# db_name = 'ch2k'\n",
    "# db_name = 'fe23'\n",
    "# db_name = 'iso2k'\n",
    "# db_name = 'pages2k'\n",
    "# db_name = 'sisal'\n",
    "\n",
    "\n",
    "# load dataframe\n",
    "df = f.load_compact_dataframe_from_csv(db_name)\n",
    "# databasedir    = '%s/%s_compact.pkl'%(db_name, db_name)\n",
    "# df = pd.read_pickle(databasedir)\n",
    "\n",
    "print(df.info())\n",
    "df.name = db_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d80aff5-2706-4bb2-a856-adc2fd88d38f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [1944.0, 1945.0, 1946.0, 1947.0, 1948.0, 1949....\n",
       "1       [1950.0, 1951.0, 1952.0, 1953.0, 1954.0, 1955....\n",
       "2       [1360.0, 1361.0, 1362.0, 1363.0, 1364.0, 1365....\n",
       "3       [1686.0, 1687.0, 1688.0, 1689.0, 1690.0, 1691....\n",
       "4       [1755.0, 1756.0, 1757.0, 1758.0, 1759.0, 1760....\n",
       "                              ...                        \n",
       "4512    [1710.0, 1711.0, 1712.0, 1713.0, 1714.0, 1715....\n",
       "4513    [1740.0, 1741.0, 1742.0, 1743.0, 1744.0, 1745....\n",
       "4514    [1693.0, 1694.0, 1695.0, 1696.0, 1697.0, 1698....\n",
       "4515    [1780.0, 1781.0, 1782.0, 1783.0, 1784.0, 1785....\n",
       "4516    [850.0, 851.0, 852.0, 853.0, 854.0, 855.0, 856...\n",
       "Name: year, Length: 4517, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69c90b63-2d1e-4d8a-80d1-209e7902b46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ii in df.index:\n",
    "#     # if type(df.at[ii, 'paleoData_values'])==np.ma.core.MaskedArray: continue\n",
    "#     dd=f.convert_to_nparray(df.at[ii, 'paleoData_values'])\n",
    "#     # print\n",
    "#     df.at[ii, 'paleoData_values']=dd.data[~dd.mask]\n",
    "#     df.at[ii, 'year']=df.at[ii, 'year'][~dd.mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5b6f82-cc36-44b2-b28b-fa498e27bf40",
   "metadata": {},
   "source": [
    "# Duplicate Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbdca46-7c33-4b73-85cb-e9077f08ae5c",
   "metadata": {},
   "source": [
    "### Find duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e03440e-bd0d-46eb-a7e0-4fa7fc96c699",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dod2k_dupfree\n",
      "Start duplicate search:\n",
      "=================================\n",
      "checking parameters:\n",
      "proxy archive                  :  must match     \n",
      "proxy type                     :  must match     \n",
      "distance (km)                  < 8               \n",
      "elevation                      :  must match     \n",
      "time overlap                   > 10              \n",
      "correlation                    > 0.9             \n",
      "RMSE                           < 0.1             \n",
      "1st difference rmse            < 0.1             \n",
      "correlation of 1st difference  > 0.9             \n",
      "=================================\n",
      "Start duplicate search\n",
      "Progress: 0/4517\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[32m/tmp/ipykernel_1020936/464547348.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m## run the find duplicate algorithm\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m out = dupdet.find_duplicates(df, n_points_thresh=\u001b[32m10\u001b[39m)\n\u001b[32m      3\u001b[39m pot_dup_inds, pot_dup_IDs, distances_km, correlations = out\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m#OR if you want to load the duplicates from saved CSV then just comment this cell out\u001b[39;00m\n",
      "\u001b[32m~/dod2k/f_duplicate_search.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(df, dist_tolerance_km, n_points_thresh, corr_thresh, rmse_thresh, corr_diff_thresh, rmse_diff_thresh, elev_tolerance, ignore_same_database, save, print_output)\u001b[39m\n\u001b[32m    296\u001b[39m \n\u001b[32m    297\u001b[39m             \u001b[38;5;66;03m# DEFINE CRITERIA:\u001b[39;00m\n\u001b[32m    298\u001b[39m \n\u001b[32m    299\u001b[39m             meta_crit         = (arch_1 == arch_2) & (type_1 == type_2) \u001b[38;5;66;03m# archive types and proxy types must agree\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m             elevation_not_nan = ((~np.isnan(df[\u001b[33m'geo_meanElev'\u001b[39m].iloc[ii]))& ~np.isnan(df[\u001b[33m'geo_meanElev'\u001b[39m].iloc[jj]))\n\u001b[32m    301\u001b[39m             elevation_dist    = np.abs(df[\u001b[33m'geo_meanElev'\u001b[39m].iloc[ii]-df[\u001b[33m'geo_meanElev'\u001b[39m].iloc[jj])\n\u001b[32m    302\u001b[39m             \u001b[38;5;66;03m#print(elevation_not_nan, elevation_dist)\u001b[39;00m\n\u001b[32m    303\u001b[39m             location_crit = ((distances_km[ii,jj] <= dist_tolerance_km) & \n",
      "\u001b[32m~/.conda/envs/dod2k-env/lib/python3.13/site-packages/pandas/core/frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4067\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m __getitem__(self, key):\n\u001b[32m   4068\u001b[39m         check_dict_or_set_indexers(key)\n\u001b[32m-> \u001b[39m\u001b[32m4069\u001b[39m         key = lib.item_from_zerodim(key)\n\u001b[32m   4070\u001b[39m         key = com.apply_if_callable(key, self)\n\u001b[32m   4071\u001b[39m \n\u001b[32m   4072\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m is_hashable(key) \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m is_iterator(key):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "## run the find duplicate algorithm\n",
    "out = dupdet.find_duplicates(df, n_points_thresh=10)\n",
    "pot_dup_inds, pot_dup_IDs, distances_km, correlations = out\n",
    "\n",
    "#OR if you want to load the duplicates from saved CSV then just comment this cell out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ad514d-4f7d-43d3-a1ad-3cf58582b41d",
   "metadata": {},
   "source": [
    "### Plot duplicate candidate pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9a5c25-32c3-4226-8f7e-7029c2aaab91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dupdet.plot_duplicates(df, save_figures=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3df4c3-b538-423d-b637-e1bf166466fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# date =  '24-11-22'\n",
    "date = str(datetime.datetime.utcnow())[2:10]\n",
    "fn = f.find('pot_dup_meta_short_%s.csv'%df.name, \n",
    "     '%s/dup_detection'%df.name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb3fae3-3f2c-469d-b91c-d250787128a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fn != []:\n",
    "    print('----------------------------------------------------')\n",
    "    print('Sucessfully finished the duplicate detection process!'.upper())\n",
    "    print('----------------------------------------------------')\n",
    "    print('Saved the detection output file in:')\n",
    "    print()\n",
    "    print('%s.'%', '.join(fn))\n",
    "    print()\n",
    "    print('You are now able to proceed to the next notebook: dup_decision.ipynb')\n",
    "else:\n",
    "    print('Final output file is missing.')\n",
    "    print()\n",
    "    print('Please re-run the notebook to complete duplicate detection process.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db53498-1fd3-43a4-a91e-a31d733442d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dod2k-env)",
   "language": "python",
   "name": "dod2k-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
