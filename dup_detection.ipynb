{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b8b4008-054a-4980-baca-ec3aa2f8d184",
   "metadata": {},
   "source": [
    "## \n",
    "This notebook runs a duplicate detection algorithm on a dataframe with the following columns:\n",
    "- 'archiveType'       (used for duplicate detection algorithm)\n",
    "- 'climateInterpretation_variable'\n",
    "- 'dataSetName'\n",
    "- 'datasetId'\n",
    "- 'geo_meanElev'      (used for duplicate detection algorithm)\n",
    "- 'geo_meanLat'       (used for duplicate detection algorithm)\n",
    "- 'geo_meanLon'       (used for duplicate detection algorithm)\n",
    "- 'geo_siteName'      (used for duplicate detection algorithm)\n",
    "- 'originalDataURL'\n",
    "- 'originalDatabase'\n",
    "- 'paleoData_notes'\n",
    "- 'paleoData_proxy'   (used for duplicate detection algorithm)\n",
    "- 'paleoData_units'\n",
    "- 'paleoData_values'  (used for duplicate detection algorithm, test for correlation, RMSE, correlation of 1st difference, RMSE of 1st difference)\n",
    "- 'year'              (used for duplicate detection algorithm)\n",
    "- 'yearUnits'\n",
    "\n",
    "The key function for duplicate detection is find_duplicates in f_duplicate_search.py\n",
    "\n",
    "The output is saved as csvs in the directory dup_detection/DATABASENAME:\n",
    "- pot_dup_correlations_DATABASENAME.csv          \n",
    "- pot_dup_distances_km_DATABASENAME.csv          \n",
    "- pot_dup_IDs_DATABASENAME.csv                   (saves the IDs of each pair)\n",
    "- pot_dup_indices_DATABASENAME.csv               (saves the dataframe indices of each pair)\n",
    "\n",
    "Summary figures of the potential duplicate pairs are created and the plots are saved in the same directory, following:\n",
    "duplicatenumber_ID1_ID2_index1_index2.jpg\n",
    "\n",
    "27/11/2024: Fixed a bug in find_duplicates (in f_duplicate_search) and relaxed site criteria.\n",
    "27/9/2024 v0: Notebook written by Lucie J. Luecke \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bac31e0-9da1-44ab-9976-127a9bb30f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed1a8a9-03b3-48c2-bdee-9e8daf2ce17c",
   "metadata": {},
   "source": [
    "# Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4234ef93-7155-468e-97b9-140d8233f728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import gzip\n",
    "import os\n",
    "import pandas as pd\n",
    "# import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature \n",
    "from matplotlib.gridspec import GridSpec as GS\n",
    "from copy import deepcopy as dc\n",
    "import functions as f\n",
    "import geopy\n",
    "import datetime\n",
    "import f_duplicate_search as dupdet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a650a1b-4dc0-427b-b1db-2de3d4181919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-lluecke/compile_proxy_database_v2.1\n"
     ]
    }
   ],
   "source": [
    "# choose working directory\n",
    "wdir = '/home/jupyter-lluecke/compile_proxy_database_v2.1'\n",
    "os.chdir(wdir)\n",
    "print(wdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d18104-d138-47cc-94ee-97811e2cb1ff",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a26dfac-fed3-40a1-a97b-2d088fc3eaee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4517 entries, 0 to 4516\n",
      "Data columns (total 18 columns):\n",
      " #   Column                                Non-Null Count  Dtype  \n",
      "---  ------                                --------------  -----  \n",
      " 0   archiveType                           4517 non-null   object \n",
      " 1   climateInterpretation_variable        4517 non-null   object \n",
      " 2   climateInterpretation_variableDetail  4517 non-null   object \n",
      " 3   dataSetName                           4517 non-null   object \n",
      " 4   datasetId                             4517 non-null   object \n",
      " 5   geo_meanElev                          4434 non-null   float32\n",
      " 6   geo_meanLat                           4517 non-null   float32\n",
      " 7   geo_meanLon                           4517 non-null   float32\n",
      " 8   geo_siteName                          4517 non-null   object \n",
      " 9   originalDataURL                       4517 non-null   object \n",
      " 10  originalDatabase                      4517 non-null   object \n",
      " 11  paleoData_notes                       4517 non-null   object \n",
      " 12  paleoData_proxy                       4517 non-null   object \n",
      " 13  paleoData_sensorSpecies               4517 non-null   object \n",
      " 14  paleoData_units                       4517 non-null   object \n",
      " 15  paleoData_values                      0 non-null      float64\n",
      " 16  year                                  0 non-null      float64\n",
      " 17  yearUnits                             4517 non-null   object \n",
      "dtypes: float32(3), float64(2), object(13)\n",
      "memory usage: 582.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# read dataframe \n",
    "\n",
    "# db_name = 'dod2k'\n",
    "db_name = 'dod2k_dupfree'\n",
    "# db_name = 'ch2k'\n",
    "# db_name = 'fe23'\n",
    "# db_name = 'iso2k'\n",
    "# db_name = 'pages2k'\n",
    "# db_name = 'sisal'\n",
    "\n",
    "\n",
    "# load dataframe\n",
    "df = f.load_compact_dataframe_from_csv(db_name)\n",
    "# databasedir    = '%s/%s_compact.pkl'%(db_name, db_name)\n",
    "# df = pd.read_pickle(databasedir)\n",
    "\n",
    "print(df.info())\n",
    "df.name = db_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d80aff5-2706-4bb2-a856-adc2fd88d38f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      NaN\n",
       "1      NaN\n",
       "2      NaN\n",
       "3      NaN\n",
       "4      NaN\n",
       "        ..\n",
       "4512   NaN\n",
       "4513   NaN\n",
       "4514   NaN\n",
       "4515   NaN\n",
       "4516   NaN\n",
       "Name: year, Length: 4517, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69c90b63-2d1e-4d8a-80d1-209e7902b46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ii in df.index:\n",
    "#     # if type(df.at[ii, 'paleoData_values'])==np.ma.core.MaskedArray: continue\n",
    "#     dd=f.convert_to_nparray(df.at[ii, 'paleoData_values'])\n",
    "#     # print\n",
    "#     df.at[ii, 'paleoData_values']=dd.data[~dd.mask]\n",
    "#     df.at[ii, 'year']=df.at[ii, 'year'][~dd.mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5b6f82-cc36-44b2-b28b-fa498e27bf40",
   "metadata": {},
   "source": [
    "# Duplicate Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbdca46-7c33-4b73-85cb-e9077f08ae5c",
   "metadata": {},
   "source": [
    "### Find duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e03440e-bd0d-46eb-a7e0-4fa7fc96c699",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dod2k_dupfree\n",
      "Start duplicate search:\n",
      "=================================\n",
      "checking parameters:\n",
      "proxy archive                  :  must match     \n",
      "proxy type                     :  must match     \n",
      "distance (km)                  < 8               \n",
      "elevation                      :  must match     \n",
      "time overlap                   > 10              \n",
      "correlation                    > 0.9             \n",
      "RMSE                           < 0.1             \n",
      "1st difference rmse            < 0.1             \n",
      "correlation of 1st difference  > 0.9             \n",
      "=================================\n",
      "Start duplicate search\n",
      "Progress: 0/4517\n",
      "Progress: 10/4517\n",
      "Progress: 20/4517\n",
      "Progress: 30/4517\n",
      "Progress: 40/4517\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3589574/464547348.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## run the find duplicate algorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdupdet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_points_thresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpot_dup_inds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpot_dup_IDs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistances_km\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrelations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#OR if you want to load the duplicates from saved CSV then just comment this cell out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/compile_proxy_database_v2.1/f_duplicate_search.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(df, dist_tolerance_km, n_points_thresh, corr_thresh, rmse_thresh, corr_diff_thresh, rmse_diff_thresh, elev_tolerance, ignore_same_database, save, print_output)\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0;31m# data_2 = data_2_ma.data[~data_2_ma.mask]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0;31m# archive and proxy type of proxy 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0march_2\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'archiveType'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mtype_2\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'paleoData_proxy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0msite_2\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'geo_siteName'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m             \u001b[0mdb_2\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'originalDatabase'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0murl_2\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'originalDataURL'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jupyter-mnevans/.conda/envs/cfr-env/lib/python3.11/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4050\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4051\u001b[0m         \u001b[0mcheck_dict_or_set_indexers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4052\u001b[0;31m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4053\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4055\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## run the find duplicate algorithm\n",
    "out = dupdet.find_duplicates(df, n_points_thresh=10)\n",
    "pot_dup_inds, pot_dup_IDs, distances_km, correlations = out\n",
    "\n",
    "#OR if you want to load the duplicates from saved CSV then just comment this cell out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ad514d-4f7d-43d3-a1ad-3cf58582b41d",
   "metadata": {},
   "source": [
    "### Plot duplicate candidate pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9a5c25-32c3-4226-8f7e-7029c2aaab91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dupdet.plot_duplicates(df, save_figures=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3df4c3-b538-423d-b637-e1bf166466fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# date =  '24-11-22'\n",
    "date = str(datetime.datetime.utcnow())[2:10]\n",
    "fn = f.find('pot_dup_meta_short_%s.csv'%df.name, \n",
    "     '%s/dup_detection'%df.name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb3fae3-3f2c-469d-b91c-d250787128a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fn != []:\n",
    "    print('----------------------------------------------------')\n",
    "    print('Sucessfully finished the duplicate detection process!'.upper())\n",
    "    print('----------------------------------------------------')\n",
    "    print('Saved the detection output file in:')\n",
    "    print()\n",
    "    print('%s.'%', '.join(fn))\n",
    "    print()\n",
    "    print('You are now able to proceed to the next notebook: dup_decision.ipynb')\n",
    "else:\n",
    "    print('Final output file is missing.')\n",
    "    print()\n",
    "    print('Please re-run the notebook to complete duplicate detection process.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db53498-1fd3-43a4-a91e-a31d733442d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cfr-env)",
   "language": "python",
   "name": "cfr-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
