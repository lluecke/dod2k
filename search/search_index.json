{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"DoD2k: a Database of Databases for the Common Era <p>     A Python toolkit for integrating and standardizing global paleoclimate proxy databases   </p> Global distribution of paleoclimate proxy records in DoD2k v2.0"},{"location":"#welcome","title":"Welcome!","text":"<p>This documentation allows you to:</p> <ul> <li>Access and process DoD2k v2.0 or its input databases</li> <li>Produce DoD2k from source using the DT2k toolkit</li> <li>Merge several databases into a database using a set of standardised metadata</li> <li>Screen a databases for duplicates and remove duplicates</li> <li>Perform data filtering and plotting.</li> <li>Run analysis workflows through Jupyter notebooks (e.g., MT_analysis, T_analysis).</li> </ul>"},{"location":"#get-started","title":"Get Started","text":"<ul> <li> <p> Quickstart Guide</p> <p>Set up your environment and run your first analysis</p> <p> Get started</p> </li> <li> <p> Tutorials</p> <p>Step-by-step workflows for common tasks</p> <p> View tutorials</p> </li> <li> <p> API Reference</p> <p>Complete documentation of all functions</p> <p> Browse API</p> </li> <li> <p> Source Code</p> <p>Explore and contribute on GitHub</p> <p> View repository</p> </li> </ul>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Quickstart Guide - Environment setup and first steps</li> <li>Tutorials - Step-by-step workflows</li> <li>API Reference - Function documentation</li> </ul> <p>Source code: github.com/lluecke/dod2k Citation: DOI: 10.5281/zenodo.15676256</p>"},{"location":"dod2k/","title":"The DoD2k Database","text":""},{"location":"dod2k/#what-is-dod2k","title":"What is DoD2k?","text":"<p>DoD2k (Database of Databases 2k) integrates five major paleoclimate databases:</p> Database Version Data Reference Records Archives PAGES 2k v2.2.0 LiPDverse PAGES 2k Consortium 2017 1364 multi-proxy SISAL v3 ORA Kaushal et al. 2024 546 speleothems Iso2k v1.1.2 LiPDverse Konecky et al. 2020 435 multi-proxy CoralHydro2k v1.0.1 LiPDverse Walter et al. 2023 221 corals FE23 - NCEI Evans et al. 2022 2754 tree-rings <p>Since these databases may share a number of records, these databases were subject to a duplicate detection and removal process. The resulting output is DoD2k.</p>"},{"location":"dod2k/#visualise-dod2k","title":"Visualise DoD2k","text":"<p>Under <code>dod2k/notebooks</code> you can find the notebook <code>[df_plot_dod2k.ipynb](df_plot_dod2k.ipynb)</code>. This notebook visualises the dataframe and produces summary figures of the database. It also reproduces the manuscript figures. </p> <p>Import the python libraries python3/Jupyter<pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature \nfrom matplotlib.gridspec import GridSpec as GS\nfrom copy import deepcopy as dc\n\nfrom dod2k_utilities import ut_functions as utf # contains utility functions\nfrom dod2k_utilities import ut_plot as uplt # contains plotting functions\n</code></pre></p> <p>After loading the dataframe, start off by counting the number of records in each archive type:</p> python3/Jupyter<pre><code># count archive types\narchive_count = {}\nfor ii, at in enumerate(set(df['archiveType'])):\n    archive_count[at] = df.loc[df['archiveType']==at, 'archiveType'].count()\n</code></pre> <p>Now count the number of records for each proxy type, depending on the archive type: python3/Jupyter<pre><code>archive_proxy_count = {}\narchive_proxy_ticks = []\nfor ii, at in enumerate(set(df['archiveType'])):\n    proxy_types   = df['paleoData_proxy'][df['archiveType']==at].unique()\n    for pt in proxy_types:\n        cc = df['paleoData_proxy'][(df['paleoData_proxy']==pt)&amp;(df['archiveType']==at)].count()\n        archive_proxy_count['%s: %s'%(at, pt)] = cc\n        archive_proxy_ticks += [at+': '+pt]\n</code></pre></p> <p>For each archive type, specify colours for each archive, but also distinguish between major archives (which have the most records) and minor archives (rare ones, only including less than ten records): python3/Jupyter<pre><code>archive_colour = {'other': cols[-1]}\nother_archives = []\nmajor_archives = []\n\nsort = np.argsort([cc for cc in archive_count.values()])\narchives_sorted = np.array([cc for cc in archive_count.keys()])[sort][::-1]\n\nfor ii, at in enumerate(archives_sorted):\n    print(ii, at, archive_count[at])\n    if archive_count[at]&gt;10:\n        major_archives     +=[at]\n        archive_colour[at] = cols[ii]\n    else:\n        other_archives     +=[at]\n        archive_colour[at] = cols[-1]\n</code></pre> Now plot a bar chart of the major archives using <code>plot_count_proxy_by_archive_short()</code></p> python3/Jupyter<pre><code>uplt.plot_count_proxy_by_archive_short(df, archive_proxy_count, archive_proxy_ticks, archive_colour) \n</code></pre> Figure 1: Number of records for each proxy type, by archive. <p>Next plot a spatial plot of all the proxy records:</p> python3/Jupyter<pre><code>#%% plot the spatial distribution of all records\nproxy_lats = df['geo_meanLat'].values\nproxy_lons = df['geo_meanLon'].values\n\n# plots the map\nfig = plt.figure(figsize=(15, 12), dpi=350)\ngrid = GS(1, 3)\n\nax = plt.subplot(grid[:, :], projection=ccrs.Robinson()) # create axis with Robinson projection of globe\n\nax.add_feature(cfeature.LAND, alpha=0.5) # adds land features\nax.add_feature(cfeature.OCEAN, alpha=0.6, facecolor='#C5DEEA') # adds ocean features\nax.coastlines() # adds coastline features\n\nax.set_global()\n\n# loop through the data to generate a scatter plot of each data record:\n# 1st loop: go through archive types individually (determines marker type)\n# 2nd loop: through paleo proxy types attributed to the specific archive, which is colour coded\n\n\nmt = 'ov^s&lt;&gt;pP*XDdh'*10 # generates string of marker types\n\narchive_types = major_archives+other_archives\n# archive_types = [aa for aa in archive_types if aa!='other']\n\n\nijk=0\nfor jj, at in enumerate(archive_types):\n    arch_mask = df['archiveType']==at\n    arch_proxy_types = np.unique(df['paleoData_proxy'][arch_mask])\n    for ii, pt in enumerate(arch_proxy_types):\n        pt_mask = df['paleoData_proxy']==pt\n        at_mask = df['archiveType']==at\n        label = at+': '+pt+' ($n=%d$)'% df['paleoData_proxy'][(df['paleoData_proxy']==pt)&amp;(df['archiveType']==at)].count()\n        marker = mt[ii] if at in major_archives else mt[ijk]\n        plt.scatter(proxy_lons[pt_mask&amp;at_mask], proxy_lats[pt_mask&amp;at_mask], \n                    transform=ccrs.PlateCarree(), zorder=999,\n                    marker=marker, color=archive_colour[at], \n                    label=label,#.replace('marine sediment:', 'marine sediment:\\n'), \n                    lw=.3, ec='k', s=200)\n        if at not in major_archives: ijk+=1\n\nplt.legend(bbox_to_anchor=(-0.01,-0.01), loc='upper left', ncol=3, fontsize=13.5, framealpha=0)\ngrid.tight_layout(fig)\n\nutf.save_fig(fig, f'{df.name}_spatial_all', dir=df.name)\n</code></pre> <p>Which creates this plot</p> Figure 2: Spatial distribution of records by archive and proxy type. <p>For further guidance see the tutorial and/or the interactive notebook.</p>"},{"location":"api/","title":"API Reference","text":"<p>This section documents the core modules of DoD2k v2.0. Each page below contains detailed API documentation generated automatically from the code.</p> <ul> <li> <p> Core Utilities</p> <p>Essential functions for data loading, processing, and standardization</p> <p> View documentation</p> </li> <li> <p> Analysis Functions</p> <p>Statistical analysis and data processing workflows</p> <p> View documentation</p> </li> <li> <p> Plotting Utilities</p> <p>Visualization functions for proxy records and metadata</p> <p> View documentation</p> </li> <li> <p> Duplicate Search</p> <p>Algorithms for detecting and removing duplicate records</p> <p> View documentation</p> </li> </ul>"},{"location":"api/ut_analysis/","title":"Data Analysis Functions","text":"<p>Module containing functions for analysing, filtering, transforming, and processing climate data (written for use in e.g. <code>analysis_T.ipynb</code>, <code>analysis_M.ipynb</code>, <code>analysis_MT.ipynb</code>) .</p>"},{"location":"api/ut_analysis/#dod2k_utilities.ut_analysis","title":"<code>dod2k_utilities.ut_analysis</code>","text":"<p>Author: Lucie Luecke (includes functions by Feng Zhu)</p> <p>Provides functions for filtering, homogenising, manipulating and analysing data(frames).</p> <p>19/12/2025 last updated for publication for dod2k v2.0</p>"},{"location":"api/ut_analysis/#dod2k_utilities.ut_analysis.PCA","title":"<code>PCA(covariance)</code>","text":"<p>Perform Principal Component Analysis using singular value decomposition (SVD).</p> <p>Parameters:</p> Name Type Description Default <code>covariance</code> <code>ndarray</code> <p>Covariance matrix of shape (n_records, n_records).</p> required <p>Returns:</p> Name Type Description <code>eigenvalues</code> <code>ndarray</code> <p>Eigenvalues from SVD (singular values), sorted in descending order as returned by numpy's SVD implementation.</p> <code>eigenvectors</code> <code>ndarray</code> <p>Right singular vectors (Vh) corresponding to the eigenvectors of the covariance matrix.</p> Notes <p>This function performs PCA on paleoclimate records. The SVD decomposition yields: - U: left singular vectors (not returned) - s: singular values (returned as eigenvalues) - Vh: right singular vectors (returned as eigenvectors)</p> <p>The eigenvectors can be used to project the data onto principal components. The eigenvalues indicate the variance explained by each component.</p> Source code in <code>dod2k_utilities/ut_analysis.py</code> <pre><code>def PCA(covariance):\n    \"\"\"\n    Perform Principal Component Analysis using singular value decomposition (SVD).\n\n    Parameters\n    ----------\n    covariance : numpy.ndarray\n        Covariance matrix of shape (n_records, n_records).\n\n    Returns\n    -------\n    eigenvalues : numpy.ndarray\n        Eigenvalues from SVD (singular values), sorted in descending order\n        as returned by numpy's SVD implementation.\n    eigenvectors : numpy.ndarray\n        Right singular vectors (Vh) corresponding to the eigenvectors of\n        the covariance matrix.\n\n    Notes\n    -----\n    This function performs PCA on paleoclimate records. The SVD decomposition yields:\n    - U: left singular vectors (not returned)\n    - s: singular values (returned as eigenvalues)\n    - Vh: right singular vectors (returned as eigenvectors)\n\n    The eigenvectors can be used to project the data onto principal components.\n    The eigenvalues indicate the variance explained by each component.\n    \"\"\"\n    U, s, Vh = np.linalg.svd(covariance) # s eigenvalues, U, Vh rotation matrices\n\n    eigenvalues  = s\n    eigenvectors = Vh\n\n\n    return eigenvalues, eigenvectors\n</code></pre>"},{"location":"api/ut_analysis/#dod2k_utilities.ut_analysis.add_auxvars_plot_summary","title":"<code>add_auxvars_plot_summary(df_filtered, key, mincount=0, col='k', **kwargs)</code>","text":"<p>Add auxiliary variables to the DataFrame and generate summary plots.</p> <p>Adds 'length', 'miny', and 'maxy' columns, then plots coverage, resolution, and length distributions.</p> <p>Parameters:</p> Name Type Description Default <code>df_filtered</code> <code>DataFrame</code> <p>DataFrame containing 'year' and 'paleoData_values'.</p> required <code>key</code> <code>str</code> <p>Title for plots and archiveType label.</p> required <code>mincount</code> <code>int</code> <p>Minimum count threshold for plotting resolution and length. Default is 0.</p> <code>0</code> <code>col</code> <code>str</code> <p>Color for plotting. Default is 'k' (black).</p> <code>'k'</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments passed to <code>plot_coverage</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The DataFrame with added auxiliary columns ('length', 'miny', 'maxy').</p> Notes <p>This function modifies the DataFrame in place by adding new columns and also calls <code>add_resolution_to_df</code> to compute resolution.</p> Source code in <code>dod2k_utilities/ut_analysis.py</code> <pre><code>def add_auxvars_plot_summary(df_filtered, key, mincount=0, col='k', **kwargs):\n    \"\"\"\n    Add auxiliary variables to the DataFrame and generate summary plots.\n\n    Adds 'length', 'miny', and 'maxy' columns, then plots coverage, resolution,\n    and length distributions.\n\n    Parameters\n    ----------\n    df_filtered : pandas.DataFrame\n        DataFrame containing 'year' and 'paleoData_values'.\n    key : str\n        Title for plots and archiveType label.\n    mincount : int, optional\n        Minimum count threshold for plotting resolution and length. Default is 0.\n    col : str, optional\n        Color for plotting. Default is 'k' (black).\n    **kwargs : dict\n        Additional keyword arguments passed to `plot_coverage`.\n\n    Returns\n    -------\n    pandas.DataFrame\n        The DataFrame with added auxiliary columns ('length', 'miny', 'maxy').\n\n    Notes\n    -----\n    This function modifies the DataFrame in place by adding new columns and\n    also calls `add_resolution_to_df` to compute resolution.\n    \"\"\"\n\n    # add 'length, miny, maxy' to dataframe\n    df_filtered['length'] = df_filtered.paleoData_values.apply(len)\n    df_filtered['miny']   = df_filtered.year.apply(np.min)\n    df_filtered['maxy']   = df_filtered.year.apply(np.max)\n\n    add_resolution_to_df(df_filtered, print_output=True)\n\n    df_tmp = df_filtered.copy()\n    df_tmp['archiveType']=key\n\n    # years    = np.arange(min(df.miny), max(df.maxy)+1)\n    uplt.plot_coverage(df_tmp, [key], [key], [], {key: col}, **kwargs)\n\n    uplt.plot_resolution(df_tmp, key, mincount=mincount, col=col)\n    uplt.plot_length(df_tmp, key, mincount=mincount, col=col)\n\n    return df_filtered\n</code></pre>"},{"location":"api/ut_analysis/#dod2k_utilities.ut_analysis.add_resolution_to_df","title":"<code>add_resolution_to_df(df, print_output=False, plot_output=False)</code>","text":"<p>Compute the time resolution of each record and store in the DataFrame.</p> <p>Sorts the time and data values, then calculates the resolution as the  unique differences between consecutive years.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing 'year' and 'paleoData_values'.</p> required <code>print_output</code> <code>bool</code> <p>If True, prints debug information. Default is False.</p> <code>False</code> <code>plot_output</code> <code>bool</code> <p>Currently unused; reserved for future plotting. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>The function updates the 'resolution' column of the DataFrame in place.</p> Source code in <code>dod2k_utilities/ut_analysis.py</code> <pre><code>def add_resolution_to_df(df, print_output=False, plot_output=False):\n    \"\"\"\n    Compute the time resolution of each record and store in the DataFrame.\n\n    Sorts the time and data values, then calculates the resolution as the \n    unique differences between consecutive years.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame containing 'year' and 'paleoData_values'.\n    print_output : bool, optional\n        If True, prints debug information. Default is False.\n    plot_output : bool, optional\n        Currently unused; reserved for future plotting. Default is False.\n\n    Returns\n    -------\n    None\n        The function updates the 'resolution' column of the DataFrame in place.\n    \"\"\"\n\n\n    # sort year and data values and obtain resolution\n    df['paleoData_values']= df.apply(lambda x: x.paleoData_values[np.argsort(x.year)], axis=1)\n    df['year']= df.apply(lambda x: np.round(x.year[np.argsort(x.year)], 2), axis=1)\n    df['resolution']= df.year.apply(np.diff).apply(np.unique)\n\n\n    return \n</code></pre>"},{"location":"api/ut_analysis/#dod2k_utilities.ut_analysis.add_zscores_plot","title":"<code>add_zscores_plot(df, key, plot_output=True)</code>","text":"<p>Add z-scores of paleoData_values to the DataFrame.</p> <p>This function calculates the z-score for each record and adds a new column 'paleoData_zscores'. Optionally plots the original values and z-scores.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing 'year' and 'paleoData_values'.</p> required <code>key</code> <code>str</code> <p>Title for the plot.</p> required <code>plot_output</code> <code>bool</code> <p>If True, generates a diagnostic plot. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The DataFrame with an added 'paleoData_zscores' column.</p> Source code in <code>dod2k_utilities/ut_analysis.py</code> <pre><code>def add_zscores_plot(df, key, plot_output=True):\n    \"\"\"\n    Add z-scores of paleoData_values to the DataFrame.\n\n    This function calculates the z-score for each record and adds a new column\n    'paleoData_zscores'. Optionally plots the original values and z-scores.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame containing 'year' and 'paleoData_values'.\n    key : str\n        Title for the plot.\n    plot_output : bool, optional\n        If True, generates a diagnostic plot. Default is True.\n\n    Returns\n    -------\n    pandas.DataFrame\n        The DataFrame with an added 'paleoData_zscores' column.\n    \"\"\"\n\n    df['paleoData_zscores'] = df.apply(lambda x: calc_z_score(x), axis=1)\n\n\n    # plot paleoData_values and paleoData_zscores\n    fig = plt.figure()\n    plt.suptitle(key)\n    plt.subplot(211)\n    plt.ylabel('paleoData_values')#, y=0.85)\n    for ii in df.index:\n        plt.plot(df.at[ii, 'year'],\n                 df.at[ii, 'paleoData_values'], lw=1)\n    plt.xticks([])\n    plt.subplot(212)\n    plt.ylabel('paleoData_zscores')#, y=0.85)\n    plt.xlabel('year CE')\n    for ii in df.index:\n        plt.plot(df.at[ii, 'year'],\n                 df.at[ii, 'paleoData_zscores'], lw=1)\n    fig.tight_layout()\n    return df\n</code></pre>"},{"location":"api/ut_analysis/#dod2k_utilities.ut_analysis.calc_covariance_matrix","title":"<code>calc_covariance_matrix(df)</code>","text":"<p>Compute the covariance matrix and overlapping years for all records.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing homogenised z-score arrays (<code>paleoData_zscores_hom_avbl</code>)  and their corresponding available years (<code>year_hom_avbl</code>).</p> required <p>Returns:</p> Name Type Description <code>covariance</code> <code>ndarray</code> <p>Covariance matrix of shape (n_records, n_records) between all records.</p> <code>overlap</code> <code>ndarray</code> <p>Matrix of shape (n_records, n_records) containing the number of overlapping  years between each pair of records.</p> Source code in <code>dod2k_utilities/ut_analysis.py</code> <pre><code>def calc_covariance_matrix(df):\n    \"\"\"\n    Compute the covariance matrix and overlapping years for all records.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame containing homogenised z-score arrays (`paleoData_zscores_hom_avbl`) \n        and their corresponding available years (`year_hom_avbl`).\n\n    Returns\n    -------\n    covariance : numpy.ndarray\n        Covariance matrix of shape (n_records, n_records) between all records.\n    overlap : numpy.ndarray\n        Matrix of shape (n_records, n_records) containing the number of overlapping \n        years between each pair of records.\n    \"\"\"\n    n_recs = len(df)\n\n    covariance = np.zeros([n_recs, n_recs])\n    overlap    = np.zeros([n_recs, n_recs])\n    for ii in range(n_recs):\n        for jj in range(ii, n_recs):\n            # print(ii, jj)\n\n            time_1 = df.iloc[ii].year_hom_avbl\n            time_2 = df.iloc[jj].year_hom_avbl\n            time_12, int_1, int_2 = np.intersect1d(time_1, time_2, return_indices=True) # saves intersect between the records\n\n            overlap[ii, jj] = len(time_12)\n            overlap[jj, ii] = len(time_12)\n\n            data_1 = df.iloc[ii].paleoData_zscores_hom_avbl\n            data_1 -= np.mean(data_1)\n            # data_1 /= np.std(data_1)\n            data_2 = df.iloc[jj].paleoData_zscores_hom_avbl\n            data_2 -= np.mean(data_2)\n            # data_2 /= np.std(data_2)\n            covariance[ii, jj] = np.cov(data_1[int_1], data_2[int_2], bias=False)[0,1]      #Default normalization (False) is by (N - 1), where N is the number of observations given (unbiased estimate). If bias is True, then normalization is by N.\n\n            covariance[jj, ii] =covariance[ii, jj]\n\n    print('short records : ', overlap[overlap&lt;40])\n    return covariance, overlap\n</code></pre>"},{"location":"api/ut_analysis/#dod2k_utilities.ut_analysis.calc_z_score","title":"<code>calc_z_score(x)</code>","text":"<p>Calculate the z-score of paleoData_values for a single record.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Series</code> <p>Series representing a single record with a 'paleoData_values' array.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Z-scored values of the record.</p> Source code in <code>dod2k_utilities/ut_analysis.py</code> <pre><code>def calc_z_score(x):\n    \"\"\"\n    Calculate the z-score of paleoData_values for a single record.\n\n    Parameters\n    ----------\n    x : pandas.Series\n        Series representing a single record with a 'paleoData_values' array.\n\n    Returns\n    -------\n    numpy.ndarray\n        Z-scored values of the record.\n    \"\"\"\n    # calculate z-score \n    z = x.paleoData_values-np.mean(x.paleoData_values)\n    z /= np.std(x.paleoData_values)\n    return z\n</code></pre>"},{"location":"api/ut_analysis/#dod2k_utilities.ut_analysis.convert_subannual_to_annual_res","title":"<code>convert_subannual_to_annual_res(df)</code>","text":"<p>Convert sub-annual data to annual averages.</p> <p>For each record in the DataFrame, this function computes yearly averages of the 'paleoData_values' and replaces the original 'year' array with  integer years.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing 'year' and 'paleoData_values' columns, where 'year' may contain sub-annual (fractional) time values.</p> required <p>Returns:</p> Type Description <code>None</code> <p>The function modifies the DataFrame in place.</p> Notes <p>For each record: - Years are floored to get integer year values - Data values are averaged within each integer year - Both 'year' and 'paleoData_values' columns are replaced with annual values</p> Source code in <code>dod2k_utilities/ut_analysis.py</code> <pre><code>def convert_subannual_to_annual_res(df):\n    \"\"\"\n    Convert sub-annual data to annual averages.\n\n    For each record in the DataFrame, this function computes yearly averages\n    of the 'paleoData_values' and replaces the original 'year' array with \n    integer years.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame containing 'year' and 'paleoData_values' columns, where 'year'\n        may contain sub-annual (fractional) time values.\n\n    Returns\n    -------\n    None\n        The function modifies the DataFrame in place.\n\n    Notes\n    -----\n\n    For each record:\n    - Years are floored to get integer year values\n    - Data values are averaged within each integer year\n    - Both 'year' and 'paleoData_values' columns are replaced with annual values\n    \"\"\"\n    for ii in df.index:\n        year = df.at[ii, 'year']\n        sy = np.min(year)\n        year_ar = np.unique(np.floor(year))\n        # print(year_ar)\n        data_ar = []\n        for yy in year_ar:\n            mask = np.floor(year)==yy\n            # print(df.at[ii, 'year'][mask])\n            data_ar+=[np.mean(df.at[ii, 'paleoData_values'][mask])]\n        # print(data_ar)\n        df.at[ii, 'paleoData_values'] = np.array(data_ar)\n        df.at[ii, 'year'] = year_ar\n    return \n</code></pre>"},{"location":"api/ut_analysis/#dod2k_utilities.ut_analysis.filter_data_availability","title":"<code>filter_data_availability(df, mny, mxy)</code>","text":"<p>Filter records based on data availability within a given year range.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing a 'year' column, where each entry is an array-like of years available for the record.</p> required <code>mny</code> <code>int</code> <p>Start year of the range.</p> required <code>mxy</code> <code>int</code> <p>End year of the range.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A filtered DataFrame containing only records that have data available between <code>mny</code> and <code>mxy</code>.</p> Notes <p>Removes records that have no data within the specified year range. Prints the indices of removed records and summary statistics.</p> Source code in <code>dod2k_utilities/ut_analysis.py</code> <pre><code>def filter_data_availability(df, mny, mxy):\n    \"\"\"\n    Filter records based on data availability within a given year range.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame containing a 'year' column, where each entry is an array-like\n        of years available for the record.\n    mny : int\n        Start year of the range.\n    mxy : int\n        End year of the range.\n\n    Returns\n    -------\n    pandas.DataFrame\n        A filtered DataFrame containing only records that have data available\n        between `mny` and `mxy`.\n\n    Notes\n    -----\n    Removes records that have no data within the specified year range.\n    Prints the indices of removed records and summary statistics.\n    \"\"\"\n\n    remove = []\n    for ii in df.index:\n        if np.sum((df.at[ii, 'year']&gt;=mny)&amp;(df.at[ii, 'year']&lt;=mxy))==0:\n            # print('No available data', ii)\n            remove+=[ii]\n    df=df.drop(labels=remove)\n    print('No available data: ', remove)\n    print('Keep %d records with data available between %d-%d. Exclude %d records.'%(df.shape[0], mny, mxy, len(remove)))\n\n    return df\n</code></pre>"},{"location":"api/ut_analysis/#dod2k_utilities.ut_analysis.filter_record_length","title":"<code>filter_record_length(df, nyears, mny, mxy)</code>","text":"<p>Filter records based on the number of years with data in a given range.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing a 'year' column, where each entry is an array-like of years available for the record.</p> required <code>nyears</code> <code>int</code> <p>Minimum number of years required within the specified range.</p> required <code>mny</code> <code>int</code> <p>Start year of the range.</p> required <code>mxy</code> <code>int</code> <p>End year of the range.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A filtered DataFrame containing only records with at least <code>nyears</code> of data between <code>mny</code> and <code>mxy</code>.</p> Notes <p>Records are dropped in-place if they don't meet the minimum year requirement. Prints the number of records kept and excluded after filtering.</p> Source code in <code>dod2k_utilities/ut_analysis.py</code> <pre><code>def filter_record_length(df, nyears, mny, mxy):\n    \"\"\"\n    Filter records based on the number of years with data in a given range.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame containing a 'year' column, where each entry is an array-like\n        of years available for the record.\n    nyears : int\n        Minimum number of years required within the specified range.\n    mny : int\n        Start year of the range.\n    mxy : int\n        End year of the range.\n\n    Returns\n    -------\n    pandas.DataFrame\n        A filtered DataFrame containing only records with at least `nyears`\n        of data between `mny` and `mxy`.\n\n    Notes\n    -----\n    Records are dropped in-place if they don't meet the minimum year requirement.\n    Prints the number of records kept and excluded after filtering.\n    \"\"\"\n\n    remove = []\n    for ii in df.index:\n        if np.sum((df.at[ii, 'year']&gt;=mny)&amp;(df.at[ii, 'year']&lt;=mxy))&lt;nyears:\n            # print('No available data', ii)\n            remove+=[ii]\n\n    df=df.drop(labels=remove)\n\n    print('Keep %d records with nyears&gt;=%d during %d-%d. Exclude %d records.'%(df.shape[0], nyears, mny, mxy, len(remove)))    \n    return df\n</code></pre>"},{"location":"api/ut_analysis/#dod2k_utilities.ut_analysis.filter_resolution","title":"<code>filter_resolution(df, maxres)</code>","text":"<p>Filter records in a DataFrame based on maximum resolution.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing a 'resolution' column, where each entry is a list  of numeric resolution values.</p> required <code>maxres</code> <code>int or float</code> <p>Maximum allowed resolution. Records with all resolution values less than or equal to <code>maxres</code> are kept.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A filtered DataFrame containing only records with resolution &lt;= maxres.</p> Notes <p>Prints the number of records kept and excluded after filtering.</p> Source code in <code>dod2k_utilities/ut_analysis.py</code> <pre><code>def filter_resolution(df, maxres):\n    \"\"\"\n    Filter records in a DataFrame based on maximum resolution.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame containing a 'resolution' column, where each entry is a list \n        of numeric resolution values.\n    maxres : int or float\n        Maximum allowed resolution. Records with all resolution values less than\n        or equal to `maxres` are kept.\n\n    Returns\n    -------\n    pandas.DataFrame\n        A filtered DataFrame containing only records with resolution &lt;= maxres.\n\n    Notes\n    -----\n    Prints the number of records kept and excluded after filtering.\n    \"\"\"\n    rmask = df.resolution.apply(lambda x: np.all(np.array(x)&lt;=maxres))\n    print('Keep %d records with resolution &lt;=%d. Exclude %d records.'%(len(df[rmask]), maxres, len(df[~rmask])))\n\n    return df[rmask]\n</code></pre>"},{"location":"api/ut_analysis/#dod2k_utilities.ut_analysis.find_nearest2d","title":"<code>find_nearest2d(da, lat, lon, lat_name='lat', lon_name='lon', new_dim='sites', r=1)</code>","text":"<p>Find nearest valid grid points in 2D xarray DataArray to given coordinates.</p> <p>Selects the nearest grid point to specified lat/lon coordinates. If the nearest point is NaN, searches within a radius r for the closest valid point.</p> <p>Parameters:</p> Name Type Description Default <code>da</code> <code>DataArray</code> <p>Input DataArray with latitude and longitude dimensions.</p> required <code>lat</code> <code>float or array - like</code> <p>Target latitude(s) in decimal degrees.</p> required <code>lon</code> <code>float or array - like</code> <p>Target longitude(s) in decimal degrees.</p> required <code>lat_name</code> <code>str</code> <p>Name of latitude dimension in da. Default is 'lat'.</p> <code>'lat'</code> <code>lon_name</code> <code>str</code> <p>Name of longitude dimension in da. Default is 'lon'.</p> <code>'lon'</code> <code>new_dim</code> <code>str</code> <p>Name for new dimension when concatenating results for multiple sites. Default is 'sites'.</p> <code>'sites'</code> <code>r</code> <code>float</code> <p>Search radius in degrees when nearest point is invalid. Default is 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>DataArray</code> <p>DataArray values at nearest valid grid points. If multiple lat/lon pairs provided, returns concatenated results along new_dim.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no valid values found within search radius.</p> Notes <p>Author: Feng Zhu</p> <p>The function uses great circle distance to find the closest valid grid point when the simple nearest neighbor is NaN.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Extract data at single location\n&gt;&gt;&gt; temp_site = find_nearest2d(temp_data, 52.52, 13.40)\n</code></pre> <pre><code>&gt;&gt;&gt; # Extract data at multiple locations\n&gt;&gt;&gt; lats = [40.7, 51.5, 48.8]\n&gt;&gt;&gt; lons = [-74.0, -0.1, 2.3]\n&gt;&gt;&gt; temps_sites = find_nearest2d(temp_data, lats, lons, r=2)\n</code></pre> Source code in <code>dod2k_utilities/ut_analysis.py</code> <pre><code>def find_nearest2d(da:xr.DataArray, lat, lon, lat_name='lat', lon_name='lon', new_dim='sites', r=1):\n    \"\"\"\n    Find nearest valid grid points in 2D xarray DataArray to given coordinates.\n\n    Selects the nearest grid point to specified lat/lon coordinates. If the\n    nearest point is NaN, searches within a radius r for the closest valid point.\n\n    Parameters\n    ----------\n    da : xr.DataArray\n        Input DataArray with latitude and longitude dimensions.\n    lat : float or array-like\n        Target latitude(s) in decimal degrees.\n    lon : float or array-like\n        Target longitude(s) in decimal degrees.\n    lat_name : str, optional\n        Name of latitude dimension in da. Default is 'lat'.\n    lon_name : str, optional\n        Name of longitude dimension in da. Default is 'lon'.\n    new_dim : str, optional\n        Name for new dimension when concatenating results for multiple sites.\n        Default is 'sites'.\n    r : float, optional\n        Search radius in degrees when nearest point is invalid. Default is 1.\n\n    Returns\n    -------\n    xr.DataArray\n        DataArray values at nearest valid grid points. If multiple lat/lon pairs\n        provided, returns concatenated results along new_dim.\n\n    Raises\n    ------\n    ValueError\n        If no valid values found within search radius.\n\n    Notes\n    -----\n    Author: Feng Zhu\n\n    The function uses great circle distance to find the closest valid grid point\n    when the simple nearest neighbor is NaN.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Extract data at single location\n    &gt;&gt;&gt; temp_site = find_nearest2d(temp_data, 52.52, 13.40)\n\n    &gt;&gt;&gt; # Extract data at multiple locations\n    &gt;&gt;&gt; lats = [40.7, 51.5, 48.8]\n    &gt;&gt;&gt; lons = [-74.0, -0.1, 2.3]\n    &gt;&gt;&gt; temps_sites = find_nearest2d(temp_data, lats, lons, r=2)\n    \"\"\"\n    da_res = da.sel({lat_name: lat, lon_name:lon}, method='nearest')\n    if da_res.isnull().any():\n        if isinstance(lat, (int, float)): lat = [lat]\n        if isinstance(lon, (int, float)): lon = [lon]\n        da_res_list = []\n        for la, lo in zip(lat, lon):\n            mask_lat = (da.lat &gt; la-r)&amp;(da.lat &lt; la+r)\n            mask_lon = (da.lon &gt; lo-r)&amp;(da.lon &lt; lo+r)\n            # da_sub = da.sel({lat_name: slice(la-r, la+r), lon_name: slice(lo-r, lo+r)})\n            da_sub = da.sel({'lat': mask_lat, 'lon': mask_lon})\n            dist = gcd(da_sub[lat_name], da_sub[lon_name], la, lo)\n            da_sub_valid = da_sub.where(~np.isnan(da_sub), drop=True)\n            valid_mask = ~np.isnan(da_sub_valid)\n            if valid_mask.sum() == 0:\n                print('la:', la)\n                print('lo:', lo)\n                print('la+r:', la+r)\n                print('la-r:', la-r)\n                print('lo+r:', lo+r)\n                print('lo-r:', lo-r)\n                print(da_sub)\n                raise ValueError('No valid values found. Please try larger `r` values.')\n\n            dist_min = dist.where(dist == dist.where(~np.isnan(da_sub_valid)).min(), drop=True)\n            nearest_lat = dist_min[lat_name].values.item()\n            nearest_lon = dist_min[lon_name].values.item()\n            da_res = da_sub_valid.sel({lat_name: nearest_lat, lon_name: nearest_lon}, method='nearest')\n            da_res_list.append(da_res)\n        da_res = xr.concat(da_res_list, dim=new_dim).squeeze()\n\n    return da_res\n</code></pre>"},{"location":"api/ut_analysis/#dod2k_utilities.ut_analysis.find_shared_period","title":"<code>find_shared_period(df, minmax=False, time='year', data='paleoData_zscores')</code>","text":"<p>Determine the shared time period across all records.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing the records with time and data columns.</p> required <code>minmax</code> <code>tuple or list</code> <p>Year range to use for plotting if no shared period exists. Default is False.</p> <code>False</code> <code>time</code> <code>str</code> <p>Name of the column containing the time axis. Default is 'year'.</p> <code>'year'</code> <code>data</code> <code>str</code> <p>Name of the data column to plot if no shared period exists. Default is 'paleoData_zscores'.</p> <code>'paleoData_zscores'</code> <p>Returns:</p> Name Type Description <code>miny</code> <code>int or float</code> <p>Minimum year of the shared period, or np.nan if none.</p> <code>maxy</code> <code>int or float</code> <p>Maximum year of the shared period, or np.nan if none.</p> Source code in <code>dod2k_utilities/ut_analysis.py</code> <pre><code>def find_shared_period(df, minmax=False, time='year', data='paleoData_zscores'):\n    \"\"\"\n    Determine the shared time period across all records.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame containing the records with time and data columns.\n    minmax : tuple or list, optional\n        Year range to use for plotting if no shared period exists. Default is False.\n    time : str, optional\n        Name of the column containing the time axis. Default is 'year'.\n    data : str, optional\n        Name of the data column to plot if no shared period exists. Default is 'paleoData_zscores'.\n\n    Returns\n    -------\n    miny : int or float\n        Minimum year of the shared period, or np.nan if none.\n    maxy : int or float\n        Maximum year of the shared period, or np.nan if none.\n    \"\"\"\n    try:\n\n        miny = np.min(reduce(np.intersect1d, df[time]))\n        maxy = np.max(reduce(np.intersect1d, df[time]))\n        print('INTERSECT: %d-%d'%(miny, maxy))\n    except ValueError:\n        print('No shared period across all records.')\n        miny = np.nan\n        maxy = np.nan\n        # plt.figure()\n        # for jj, ii in enumerate(df.index):\n        #     dd = df.at[ii, data]\n        #     yy = df.at[ii, time]\n        #     plt.plot(yy, dd-np.mean(dd)+jj)\n        #     if minmax: plt.xlim(minmax[0], minmax[-1])\n    return miny, maxy\n</code></pre>"},{"location":"api/ut_analysis/#dod2k_utilities.ut_analysis.fraction_of_explained_var","title":"<code>fraction_of_explained_var(covariance, eigenvalues, n_recs, title='', db_name='', col='tab:blue')</code>","text":"<p>Compute and plot the fraction of variance explained by principal components.</p> <p>Parameters:</p> Name Type Description Default <code>covariance</code> <code>ndarray</code> <p>Covariance matrix of the records.</p> required <code>eigenvalues</code> <code>ndarray</code> <p>Eigenvalues from PCA/SVD.</p> required <code>n_recs</code> <code>int</code> <p>Number of records.</p> required <code>title</code> <code>str</code> <p>Title used in the plot. Default is empty string.</p> <code>''</code> <code>db_name</code> <code>str</code> <p>Name suffix for saving the figure. Default is empty string.</p> <code>''</code> <code>col</code> <code>str</code> <p>Color for plotting. Default is 'tab:blue'.</p> <code>'tab:blue'</code> <p>Returns:</p> Name Type Description <code>frac_explained_var</code> <code>ndarray</code> <p>Fraction of variance explained by each principal component.</p> Notes <p>Creates a dual-axis plot showing both individual and cumulative fraction of explained variance. The plot is saved using <code>utf.figsave</code>.</p> Source code in <code>dod2k_utilities/ut_analysis.py</code> <pre><code>def fraction_of_explained_var(covariance, eigenvalues, n_recs, title='', db_name='', col='tab:blue'):\n    \"\"\"\n    Compute and plot the fraction of variance explained by principal components.\n\n    Parameters\n    ----------\n    covariance : numpy.ndarray\n        Covariance matrix of the records.\n    eigenvalues : numpy.ndarray\n        Eigenvalues from PCA/SVD.\n    n_recs : int\n        Number of records.\n    title : str, optional\n        Title used in the plot. Default is empty string.\n    db_name : str, optional\n        Name suffix for saving the figure. Default is empty string.\n    col : str, optional\n        Color for plotting. Default is 'tab:blue'.\n\n    Returns\n    -------\n    frac_explained_var : numpy.ndarray\n        Fraction of variance explained by each principal component.\n\n    Notes\n    -----\n    Creates a dual-axis plot showing both individual and cumulative fraction\n    of explained variance. The plot is saved using `utf.figsave`.\n    \"\"\"\n    sorter = np.argsort(eigenvalues)[::-1] # sort eigenvalues in descending order\n\n    explained_var  = eigenvalues[sorter]**2/ (n_recs - 1) \n\n    total_var = np.sum(explained_var)\n    frac_explained_var = explained_var / total_var\n\n    cum_frac_explained_var = np.cumsum(frac_explained_var)\n\n    fig = plt.figure()\n    plt.title(title)\n    ax = plt.gca()\n    plt.plot(np.arange(len(frac_explained_var))+1, frac_explained_var, label='fraction of explained variance', color=col, lw=2)\n    plt.xlim(-1, 10)\n    plt.ylabel('fraction of explained variance')\n\n    plt.xlabel('PC')\n\n    ax1 = ax.twinx()\n    ax1.plot(np.arange(len(frac_explained_var))+1, cum_frac_explained_var, ls=':', \n             label='cumulative fraction of explained variance', color=col, lw=2)\n\n    plt.ylabel('cumulative fraction of explained variance') \n\n    utf.figsave(fig, 'foev_%s'%title, add=db_name)\n\n    return frac_explained_var\n</code></pre>"},{"location":"api/ut_analysis/#dod2k_utilities.ut_analysis.gcd","title":"<code>gcd(lat1, lon1, lat2, lon2, radius=6378.137)</code>","text":"<p>Calculate 2D great circle distance between points on Earth.</p> <p>Parameters:</p> Name Type Description Default <code>lat1</code> <code>float or array - like</code> <p>Latitude(s) of first point(s) in decimal degrees.</p> required <code>lon1</code> <code>float or array - like</code> <p>Longitude(s) of first point(s) in decimal degrees.</p> required <code>lat2</code> <code>float or array - like</code> <p>Latitude(s) of second point(s) in decimal degrees.</p> required <code>lon2</code> <code>float or array - like</code> <p>Longitude(s) of second point(s) in decimal degrees.</p> required <code>radius</code> <code>float</code> <p>Earth radius in kilometers. Default is 6378.137 km (equatorial radius).</p> <code>6378.137</code> <p>Returns:</p> Type Description <code>float or ndarray</code> <p>Great circle distance(s) in kilometers.</p> Notes <p>Uses the haversine formula for calculating distances on a sphere. Supports vectorized operations for arrays of coordinates.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Distance between London and Paris\n&gt;&gt;&gt; gcd(51.5074, -0.1278, 48.8566, 2.3522)\n343.5...\n</code></pre> <pre><code>&gt;&gt;&gt; # Multiple distances at once\n&gt;&gt;&gt; lats1 = [40.7, 51.5, 48.8]\n&gt;&gt;&gt; lons1 = [-74.0, -0.1, 2.3]\n&gt;&gt;&gt; gcd(lats1, lons1, 52.52, 13.40)  # Distances to Berlin\narray([6385.7..., 930.3..., 877.4...])\n</code></pre> Source code in <code>dod2k_utilities/ut_analysis.py</code> <pre><code>def gcd(lat1, lon1, lat2, lon2, radius=6378.137):\n    \"\"\"\n    Calculate 2D great circle distance between points on Earth.\n\n    Parameters\n    ----------\n    lat1 : float or array-like\n        Latitude(s) of first point(s) in decimal degrees.\n    lon1 : float or array-like\n        Longitude(s) of first point(s) in decimal degrees.\n    lat2 : float or array-like\n        Latitude(s) of second point(s) in decimal degrees.\n    lon2 : float or array-like\n        Longitude(s) of second point(s) in decimal degrees.\n    radius : float, optional\n        Earth radius in kilometers. Default is 6378.137 km (equatorial radius).\n\n    Returns\n    -------\n    float or numpy.ndarray\n        Great circle distance(s) in kilometers.\n\n    Notes\n    -----\n    Uses the haversine formula for calculating distances on a sphere.\n    Supports vectorized operations for arrays of coordinates.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Distance between London and Paris\n    &gt;&gt;&gt; gcd(51.5074, -0.1278, 48.8566, 2.3522)\n    343.5...\n\n    &gt;&gt;&gt; # Multiple distances at once\n    &gt;&gt;&gt; lats1 = [40.7, 51.5, 48.8]\n    &gt;&gt;&gt; lons1 = [-74.0, -0.1, 2.3]\n    &gt;&gt;&gt; gcd(lats1, lons1, 52.52, 13.40)  # Distances to Berlin\n    array([6385.7..., 930.3..., 877.4...])\n    \"\"\"\n    # Convert degrees to radians\n    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n    dlat, dlon = lat2 - lat1, lon2 - lon1\n    a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2\n    c = 2 * np.arcsin(np.sqrt(a))\n    dist = radius * c\n    return dist\n</code></pre>"},{"location":"api/ut_analysis/#dod2k_utilities.ut_analysis.homogenise_data_dimensions","title":"<code>homogenise_data_dimensions(df, years_hom, title='', print_output=False, plot_output=True)</code>","text":"<p>Homogenise the data arrays to a uniform time coordinate.</p> <p>This function assigns paleoData values and z-scores from each record in the DataFrame to a homogenised time axis (<code>years_hom</code>). Missing data are masked as zeros using numpy masked arrays. Optional plotting and printing of  intermediate checks is provided.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing the records with columns: - 'year': array-like of years - 'paleoData_values': array-like of values - 'paleoData_zscores': array-like of z-scores</p> required <code>years_hom</code> <code>array - like</code> <p>Homogenised time coordinate to which all records are aligned.</p> required <code>title</code> <code>str</code> <p>Title used for the plot if <code>plot_output</code> is True.</p> <code>''</code> <code>print_output</code> <code>bool</code> <p>If True, prints debug information about array sizes and resolutions. Default is False.</p> <code>False</code> <code>plot_output</code> <code>bool</code> <p>If True, generates diagnostic plots showing the homogenised and original paleoData values and z-scores. Default is True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>paleoData_values_hom</code> <code>MaskedArray</code> <p>Masked array of shape (n_records, n_years) containing homogenised paleoData values. Missing values are masked.</p> <code>paleoData_zscores_hom</code> <code>MaskedArray</code> <p>Masked array of shape (n_records, n_years) containing homogenised paleoData z-scores. Missing values are masked.</p> <code>year_hom_avbl</code> <code>list of numpy.ma.MaskedArray</code> <p>List of length n_records containing the homogenised data arrays for paleoData_values.</p> <code>zsco_hom_avbl</code> <code>list of numpy.ma.MaskedArray</code> <p>List of length n_records containing the homogenised data arrays for paleoData_zscores.</p> Source code in <code>dod2k_utilities/ut_analysis.py</code> <pre><code>def homogenise_data_dimensions(df, years_hom, title='', print_output=False, plot_output=True):\n    \"\"\"\n    Homogenise the data arrays to a uniform time coordinate.\n\n    This function assigns paleoData values and z-scores from each record in the\n    DataFrame to a homogenised time axis (`years_hom`). Missing data are masked\n    as zeros using numpy masked arrays. Optional plotting and printing of \n    intermediate checks is provided.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame containing the records with columns:\n        - 'year': array-like of years\n        - 'paleoData_values': array-like of values\n        - 'paleoData_zscores': array-like of z-scores\n    years_hom : array-like\n        Homogenised time coordinate to which all records are aligned.\n    title : str, optional\n        Title used for the plot if `plot_output` is True.\n    print_output : bool, optional\n        If True, prints debug information about array sizes and resolutions.\n        Default is False.\n    plot_output : bool, optional\n        If True, generates diagnostic plots showing the homogenised and original\n        paleoData values and z-scores. Default is True.\n\n    Returns\n    -------\n    paleoData_values_hom : numpy.ma.MaskedArray\n        Masked array of shape (n_records, n_years) containing homogenised\n        paleoData values. Missing values are masked.\n    paleoData_zscores_hom : numpy.ma.MaskedArray\n        Masked array of shape (n_records, n_years) containing homogenised\n        paleoData z-scores. Missing values are masked.\n    year_hom_avbl : list of numpy.ma.MaskedArray\n        List of length n_records containing the homogenised data arrays for\n        paleoData_values.\n    zsco_hom_avbl : list of numpy.ma.MaskedArray\n        List of length n_records containing the homogenised data arrays for\n        paleoData_zscores.\n    \"\"\"\n\n    mny = years_hom[0]\n    mxy = years_hom[-1]\n    minres = np.unique(np.diff(years_hom))[0]\n\n    n_recs = len(df) \n\n    # assign data values\n    paleoData_values_hom  = np.ma.masked_array(np.zeros([n_recs, len(years_hom)]), np.zeros([n_recs, len(years_hom)]))\n    paleoData_zscores_hom = np.ma.masked_array(np.zeros([n_recs, len(years_hom)]), np.zeros([n_recs, len(years_hom)]))\n\n    year_hom_avbl = []\n    zsco_hom_avbl = []\n\n    for ijk, ii in enumerate(df.index):\n        # create empty data arrays \n\n        time = df.at[ii, 'year']\n\n\n        data_LR = np.zeros(len(years_hom))\n        data_HR = df.at[ii, 'paleoData_values']\n\n        zsco_HR = df.at[ii, 'paleoData_zscores']\n        zsco_LR = np.zeros(len(years_hom))\n\n        tt = []\n        zz = []\n        for jj, xi in enumerate(years_hom):\n            window = (time&gt;xi-minres)&amp;(time&lt;=xi)\n            # print(xi, time[window])\n            if len(time[window])==0:\n                data_LR[jj] = 0#np.nan\n                zsco_LR[jj] = 0#np.nan\n            else:\n                data_LR[jj] = np.average(data_HR[window])\n                zsco_LR[jj] = np.average(zsco_HR[window])\n                tt+=[xi]\n                zz+=[np.average(zsco_HR[window])]\n\n\n        yh_base = np.ma.masked_array(data_LR, mask=data_LR==0, fill_value=0)\n        zh_base = np.ma.masked_array(zsco_LR, mask=zsco_LR==0, fill_value=0)\n\n        # # check array is correct\n        if print_output:\n            # print(ii, ijk, 'years_hom size: ', years_hom[hmask].shape, 'new array size: ', \n            print(ii, ijk, 'years_hom size: ', years_hom.shape, 'new array size: ', \n                  yh_base[~yh_base.mask].shape, 'resolution: ', np.unique(np.diff(time)), \n                  # 'time coord: from %s-%s'%(yy[(yy&gt;=mny)&amp;(yy&lt;=mxy)][0], yy[(yy&gt;=mny)&amp;(yy&lt;=mxy)][-1])\n                 )\n            print(paleoData_values_hom[ijk,:].shape, yh_base.shape)  \n\n        paleoData_values_hom[ijk,:]  = yh_base\n        paleoData_zscores_hom[ijk,:] = zh_base\n        year_hom_avbl.append(tt)\n        zsco_hom_avbl.append(zz)\n\n    print(paleoData_values_hom.shape)\n\n    if plot_output:\n        n_recs=min(len(df), 50)\n        # plot paleoData_values_hom and paleoData_zscores_hom as they appear in df\n        fig = plt.figure(figsize=(8,5))\n        plt.suptitle(title)\n        plt.subplot(221)\n        plt.title('paleoData_values HOM')\n        for ii in range(n_recs):\n            shift = ii\n            plt.plot(years_hom, paleoData_values_hom[ii,:]+shift, lw=1)\n        plt.xlim(mny, mxy)\n\n        plt.subplot(222)\n        plt.title('paleoData_values')\n        for ii in range(n_recs):\n            shift = ii\n            plt.plot(df.year.iloc[ii], df.paleoData_values.iloc[ii]+shift, lw=1)\n        plt.xlim(mny, mxy)\n\n        plt.subplot(223)\n        plt.title('paleoData_zscores HOM')\n        for ii in range(n_recs):\n            shift = ii\n            plt.plot(years_hom, paleoData_zscores_hom[ii,:]+shift , lw=1)\n        plt.xlim(mny, mxy)\n\n        plt.subplot(224)\n        plt.title('paleoData_zscores')\n        for ii in range(n_recs):\n            shift = ii\n            plt.plot(df.year.iloc[ii], df.paleoData_zscores.iloc[ii]+shift, lw=1)\n        plt.xlim(mny, mxy)\n        fig.tight_layout()\n    return paleoData_values_hom, paleoData_zscores_hom, year_hom_avbl, zsco_hom_avbl  \n</code></pre>"},{"location":"api/ut_analysis/#dod2k_utilities.ut_analysis.homogenise_time","title":"<code>homogenise_time(df, mny, mxy, minres)</code>","text":"<p>Homogenise the time coordinate of records in a DataFrame.</p> <p>This function creates a uniform time axis from <code>mny</code> to <code>mxy</code> with  steps of <code>minres</code> years and prints basic information about the  homogenised timeline. It also calls <code>find_shared_period</code> to report  the overlapping period across all records.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing climate or archive records. Must be compatible with <code>find_shared_period</code>.</p> required <code>mny</code> <code>int</code> <p>Start year of the homogenisation period.</p> required <code>mxy</code> <code>int</code> <p>End year of the homogenisation period.</p> required <code>minres</code> <code>int</code> <p>Minimum resolution (in years) for the homogenised timeline.</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>The input DataFrame (unchanged in this function).</p> <code>years_hom</code> <code>ndarray</code> <p>Array of homogenised years from <code>mny</code> to <code>mxy</code> with step <code>minres</code>.</p> Source code in <code>dod2k_utilities/ut_analysis.py</code> <pre><code>def homogenise_time(df, mny, mxy, minres):\n    \"\"\"\n    Homogenise the time coordinate of records in a DataFrame.\n\n    This function creates a uniform time axis from `mny` to `mxy` with \n    steps of `minres` years and prints basic information about the \n    homogenised timeline. It also calls `find_shared_period` to report \n    the overlapping period across all records.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame containing climate or archive records. Must be compatible\n        with `find_shared_period`.\n    mny : int\n        Start year of the homogenisation period.\n    mxy : int\n        End year of the homogenisation period.\n    minres : int\n        Minimum resolution (in years) for the homogenised timeline.\n\n    Returns\n    -------\n    df : pandas.DataFrame\n        The input DataFrame (unchanged in this function).\n    years_hom : numpy.ndarray\n        Array of homogenised years from `mny` to `mxy` with step `minres`.\n    \"\"\"\n\n    years_hom     = np.arange(mny, mxy+minres, minres)                                       #\n\n    print('Homogenised time coordinate: %d-%d CE'%(years_hom[0], years_hom[-1]))\n    print('Resolution: %s years'%str(np.unique(np.diff(years_hom))))\n\n    find_shared_period(df, minmax=(mny, mxy))\n\n    return df, years_hom\n</code></pre>"},{"location":"api/ut_analysis/#dod2k_utilities.ut_analysis.smooth","title":"<code>smooth(data, time, res)</code>","text":"<p>Apply simple moving average smoothing to time series data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Time series data array (1D or 2D). If 2D, smoothing is applied along the first dimension (rows).</p> required <code>time</code> <code>ndarray</code> <p>Corresponding time axis array. Should have same length as first dimension of data.</p> required <code>res</code> <code>int</code> <p>Window size for smoothing (number of points). The moving average uses non-overlapping windows.</p> required <p>Returns:</p> Name Type Description <code>smooth_time</code> <code>list</code> <p>Smoothed time axis values, containing the mean time value for each window.</p> <code>smooth_data</code> <code>list</code> <p>Smoothed data values, containing the mean data value for each window.</p> Notes <p>This function uses a simple non-overlapping moving average with window size <code>res</code>. The output length will be approximately len(data) / res (rounded down).</p> Source code in <code>dod2k_utilities/ut_analysis.py</code> <pre><code>def smooth(data, time, res):\n    \"\"\"\n    Apply simple moving average smoothing to time series data.\n\n    Parameters\n    ----------\n    data : numpy.ndarray\n        Time series data array (1D or 2D). If 2D, smoothing is applied\n        along the first dimension (rows).\n    time : numpy.ndarray\n        Corresponding time axis array. Should have same length as first\n        dimension of data.\n    res : int\n        Window size for smoothing (number of points). The moving average\n        uses non-overlapping windows.\n\n    Returns\n    -------\n    smooth_time : list\n        Smoothed time axis values, containing the mean time value for\n        each window.\n    smooth_data : list\n        Smoothed data values, containing the mean data value for\n        each window.\n\n    Notes\n    -----\n    This function uses a simple non-overlapping moving average with\n    window size `res`. The output length will be approximately\n    len(data) / res (rounded down).\n    \"\"\"\n    smooth_data = []\n    smooth_time = []\n    for ii in range(0, data.shape[0], 1):\n        smooth_data += [np.mean(data[ii:ii+res])]\n        smooth_time += [np.mean(time[ii:ii+res])]\n    return smooth_time, smooth_data\n</code></pre>"},{"location":"api/ut_duplicate_search/","title":"Duplicate detection functions","text":"<p>Module containing functions for finding and removing the duplicates among the input databases. Used for e.g. <code>dup_detection.ipynb</code>, <code>dup_decision.ipynb</code>, <code>dup_removal.ipynb</code>)</p>"},{"location":"api/ut_duplicate_search/#dod2k_utilities.ut_duplicate_search","title":"<code>dod2k_utilities.ut_duplicate_search</code>","text":"<p>This script includes functions which search for duplicates. </p> <p>Last updated 19/12/2025 by LL for publication of dod2k v2.0</p> <p>Update 22/10/24 updated duplicate_decisions:     - created backup decision file which is intermediately saved    - outputs URL which can be copied and pasted into browser    - implemented a composite option in the decision process, to create a composite of two records    - removed date (YY-MM-DD) of decision output filename Update 8/10/24 changed colours of figure in dup_plot (dropped highlighting differing metadata).                  replaced db_name with df.name</p> <p>Update 2/10/24 Implemented a commenting option in duplicate_decisions to comment on decision process and on individual decisions.</p> <p>Update 27/9/24 Updated directory names and changed the correlation and distances output in find_duplicates to only output data from potential duplicates (replaced saving all pairs)</p> <p>Update 9/9/24 Introduced timestamps and contact details into duplicate decision output csv and changed the file= and dirnames for streamlining purposes. Update 23/8/24 Replaced the function cleanup_database and split into two:  plot_duplicates: plots the candidate pairs, saves figures and a summary csv sheet. cleanup_database_2: goes through the candidate pairs and makes decisions based on the options: a) raw input b) keep all records c) automatically keep only updated records and eliminate the other candidate. Decisions and metadata are saved in csv file.</p> <p>Update 22/8/24: Fixed a bug in find_duplicates logical algorithm- wrong bracket closure. Also changed location_crit to account for nans in elevation. Calculation of z-scores only divided by std if std!=0 to avoid nans.</p> <p>Update 15/8/24: updated numerical checks for duplicate detection: implemented check for correlation and rmse of records plus correlation and rmse of first difference. </p> <p>Update 13/8/24: updated keys to account for updated dataframe terminology. Revised loading/saving of data in find_duplicates    find_duplicates:         updated the logic for duplicate detection:             overlap_crit now accounts for short records (allows short records to pass through without overlap check)             corr_crit only one numerical criterion needs to be satisfied for the data (either correlation or rmse or 1st difference)             location_crit now includes elevation too</p> <p>cleanup_database:        updated plot to include URL, Database and streamlined table Script written by Lucie Luecke, 6/7/23</p>"},{"location":"api/ut_duplicate_search/#dod2k_utilities.ut_duplicate_search.collect_dup_details","title":"<code>collect_dup_details(df_decisions, header)</code>","text":"<p>Generate duplicate details dictionary from decision records.</p> <p>Creates a nested dictionary structure containing information about all duplicate relationships and decisions made during the duplicate review process.</p> <p>Parameters:</p> Name Type Description Default <code>df_decisions</code> <code>DataFrame</code> <p>DataFrame containing duplicate decisions with columns: - 'Decision 1', 'Decision 2': Decision for each record (KEEP/REMOVE/COMPOSITE) - 'datasetId 1', 'datasetId 2': IDs of the duplicate pair - 'originalDatabase 1', 'originalDatabase 2': Source databases - 'Decision type': Type of decision (MANUAL/AUTO) - 'Decision comment': Comments on the decision (for manual decisions)</p> required <code>header</code> <code>list</code> <p>Header information from the decision file containing operator details: [0]: File description [1]: Operator name and initials</p> <p>[3]: Creation timestamp</p> required <p>Returns:</p> Name Type Description <code>dup_details</code> <code>dict</code> <p>Nested dictionary with structure: {record_id: {     duplicate_count: {         'duplicate ID': str,         'duplicate database': str,         'duplicate decision': str,         'decision type': str,         'operator': str (for manual decisions),         'note': str (for manual decisions)     } }}</p> Notes <ul> <li>Only processes pairs where at least one record is not kept (true duplicates)</li> <li>Each record gets an entry for every duplicate relationship it has</li> <li>Manual decisions include operator details extracted from the header</li> <li>Automatic decisions have 'N/A' for operator and note fields</li> </ul> <p>The returned dictionary can be used to populate the 'duplicateDetails' field in the final deduplicated database.</p> Source code in <code>dod2k_utilities/ut_duplicate_search.py</code> <pre><code>def collect_dup_details(df_decisions, header):\n    \"\"\"\n    Generate duplicate details dictionary from decision records.\n\n    Creates a nested dictionary structure containing information about all\n    duplicate relationships and decisions made during the duplicate review process.\n\n    Parameters\n    ----------\n    df_decisions : pandas.DataFrame\n        DataFrame containing duplicate decisions with columns:\n        - 'Decision 1', 'Decision 2': Decision for each record (KEEP/REMOVE/COMPOSITE)\n        - 'datasetId 1', 'datasetId 2': IDs of the duplicate pair\n        - 'originalDatabase 1', 'originalDatabase 2': Source databases\n        - 'Decision type': Type of decision (MANUAL/AUTO)\n        - 'Decision comment': Comments on the decision (for manual decisions)\n    header : list\n        Header information from the decision file containing operator details:\n        [0]: File description\n        [1]: Operator name and initials\n        [2]: Email\n        [3]: Creation timestamp\n\n    Returns\n    -------\n    dup_details : dict\n        Nested dictionary with structure:\n        {record_id: {\n            duplicate_count: {\n                'duplicate ID': str,\n                'duplicate database': str,\n                'duplicate decision': str,\n                'decision type': str,\n                'operator': str (for manual decisions),\n                'note': str (for manual decisions)\n            }\n        }}\n\n    Notes\n    -----\n    - Only processes pairs where at least one record is not kept (true duplicates)\n    - Each record gets an entry for every duplicate relationship it has\n    - Manual decisions include operator details extracted from the header\n    - Automatic decisions have 'N/A' for operator and note fields\n\n    The returned dictionary can be used to populate the 'duplicateDetails' field\n    in the final deduplicated database.\n    \"\"\"\n    dup_details = {}\n    dup_counts  = {}\n    for ind in df_decisions.index:\n        dec_1, dec_2 = df_decisions.loc[ind, ['Decision 1', 'Decision 2']]\n        if dec_1=='KEEP' and dec_2=='KEEP': continue # in this case no true duplicates!\n\n        id_1, id_2 = df_decisions.loc[ind, ['datasetId 1', 'datasetId 2']]\n        db_1, db_2 = df_decisions.loc[ind, ['originalDatabase 1', 'originalDatabase 2']]\n        for id in [id_1, id_2]:\n            if id not in dup_details:\n                dup_counts[id] = 0\n                dup_details[id] = {}\n            else:\n                dup_counts[id]+=1\n        dup_details[id_1][dup_counts[id_1]] = {'duplicate ID': id_2, 'duplicate database': db_2, 'duplicate decision': dec_2, 'decision type': df_decisions.loc[ind, 'Decision type']}\n        dup_details[id_2][dup_counts[id_2]] = {'duplicate ID': id_1, 'duplicate database': db_1, 'duplicate decision': dec_1, 'decision type': df_decisions.loc[ind, 'Decision type']}\n\n        if df_decisions.loc[ind, 'Decision type']=='MANUAL': \n            operator_details_str = ','.join(header[1:]).replace(' Modified ','')[:-2].replace(':','').replace('  E-Mail', '')\n            for id in [id_1, id_2]:\n                dup_details[id][dup_counts[id]]['operator'] = operator_details_str\n                dup_details[id][dup_counts[id]]['note'] = df_decisions.loc[ind, 'Decision comment']\n        else:\n            for id in [id_1, id_2]:\n                dup_details[id][dup_counts[id]]['operator'] = 'N/A'\n                dup_details[id][dup_counts[id]]['note'] = 'N/A'       \n    return dup_details\n</code></pre>"},{"location":"api/ut_duplicate_search/#dod2k_utilities.ut_duplicate_search.collect_record_decisions","title":"<code>collect_record_decisions(df_decisions)</code>","text":"<p>Collect per-record decisions from a pairwise decision table.</p> <p>Parameters:</p> Name Type Description Default <code>df_decisions</code> <code>DataFrame</code> <p>DataFrame containing pairwise record comparisons. Must include the columns <code>'datasetId 1'</code>, <code>'datasetId 2'</code>, <code>'Decision 1'</code>, and <code>'Decision 2'</code>, where each row represents a comparison between two dataset records and the associated decisions for each record.</p> required <p>Returns:</p> Name Type Description <code>decisions</code> <code>dict</code> <p>Dictionary mapping each dataset ID to a list of decisions associated with that record across all comparisons.</p> Notes <p>Each row contributes two entries: one for <code>'datasetId 1'</code> paired with <code>'Decision 1'</code> and one for <code>'datasetId 2'</code> paired with <code>'Decision 2'</code>. Records appearing in multiple rows accumulate multiple decision entries in their corresponding list.</p> Source code in <code>dod2k_utilities/ut_duplicate_search.py</code> <pre><code>def collect_record_decisions(df_decisions):\n    \"\"\"\n    Collect per-record decisions from a pairwise decision table.\n\n    Parameters\n    ----------\n    df_decisions : pandas.DataFrame\n        DataFrame containing pairwise record comparisons. Must include\n        the columns ``'datasetId 1'``, ``'datasetId 2'``,\n        ``'Decision 1'``, and ``'Decision 2'``, where each row represents\n        a comparison between two dataset records and the associated\n        decisions for each record.\n\n    Returns\n    -------\n    decisions : dict\n        Dictionary mapping each dataset ID to a list of decisions\n        associated with that record across all comparisons.\n\n    Notes\n    -----\n    Each row contributes two entries: one for ``'datasetId 1'`` paired\n    with ``'Decision 1'`` and one for ``'datasetId 2'`` paired with\n    ``'Decision 2'``. Records appearing in multiple rows accumulate\n    multiple decision entries in their corresponding list.\n    \"\"\"\n    decisions = {}\n    for ind in df_decisions.index:\n        id1, id2   = df_decisions.loc[ind, ['datasetId 1', 'datasetId 2']]\n        dec1, dec2 = df_decisions.loc[ind, ['Decision 1', 'Decision 2']]\n        for id, dec in zip([id1, id2], [dec1, dec2]):\n            if id not in decisions: decisions[id] = []\n            decisions[id]+=[dec]\n    return decisions\n</code></pre>"},{"location":"api/ut_duplicate_search/#dod2k_utilities.ut_duplicate_search.define_hierarchy","title":"<code>define_hierarchy(df, hierarchy='default')</code>","text":"<p>Define priority hierarchy for different paleoclimate databases.</p> <p>Assigns a numerical hierarchy value to each record based on its original  database, used for automatic duplicate resolution when records are identical. Lower values indicate higher priority.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing proxy records with an 'originalDatabase' column.</p> required <code>hierarchy</code> <code>str or dict</code> <p>Hierarchy definition method. If 'default', uses predefined hierarchy. If dict, should contain keys for database names with priority values: - 'pages2k': priority for PAGES 2k database - 'fe23': priority for FE23 (Breitenmoser et al. 2014) - 'ch2k': priority for CoralHydro2k - 'iso2k': priority for Iso2k - 'sisal': priority for SISAL Default is 'default'.</p> <code>'default'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The input DataFrame with added 'Hierarchy' column containing priority values for each record.</p> Notes <p>Default hierarchy (lower number = higher priority): 1. PAGES 2k v2.2.0 (highest priority) 2. SISAL v3 3. CoralHydro2k v1.0.1 4. Iso2k v1.1.2 5. FE23 (Breitenmoser et al. 2014) 99. All other databases (lowest priority)</p> <p>The hierarchy is used in duplicate_decisions functions to automatically choose which record to keep when duplicates are identical.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = define_hierarchy(df)  # Use default hierarchy\n&gt;&gt;&gt; custom = {'PAGES 2k v2.2.0', 'Hierarchy': 2, 'SISAL v3': 1, 'FE23 (Breitenmoser et al. (2014))': 3, 'CoralHydro2k v1.0.': 4, 'Iso2k v1.1.2': 5}\n&gt;&gt;&gt; df = define_hierarchy(df, hierarchy=custom)  # Custom hierarchy\n</code></pre> Source code in <code>dod2k_utilities/ut_duplicate_search.py</code> <pre><code>def define_hierarchy(df, hierarchy='default'):\n    \"\"\"\n    Define priority hierarchy for different paleoclimate databases.\n\n    Assigns a numerical hierarchy value to each record based on its original \n    database, used for automatic duplicate resolution when records are identical.\n    Lower values indicate higher priority.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame containing proxy records with an 'originalDatabase' column.\n    hierarchy : str or dict, optional\n        Hierarchy definition method. If 'default', uses predefined hierarchy.\n        If dict, should contain keys for database names with priority values:\n        - 'pages2k': priority for PAGES 2k database\n        - 'fe23': priority for FE23 (Breitenmoser et al. 2014)\n        - 'ch2k': priority for CoralHydro2k\n        - 'iso2k': priority for Iso2k\n        - 'sisal': priority for SISAL\n        Default is 'default'.\n\n    Returns\n    -------\n    pandas.DataFrame\n        The input DataFrame with added 'Hierarchy' column containing priority\n        values for each record.\n\n    Notes\n    -----\n    Default hierarchy (lower number = higher priority):\n    1. PAGES 2k v2.2.0 (highest priority)\n    2. SISAL v3\n    3. CoralHydro2k v1.0.1\n    4. Iso2k v1.1.2\n    5. FE23 (Breitenmoser et al. 2014)\n    99. All other databases (lowest priority)\n\n    The hierarchy is used in duplicate_decisions functions to automatically\n    choose which record to keep when duplicates are identical.\n\n    Examples\n    --------\n    &gt;&gt;&gt; df = define_hierarchy(df)  # Use default hierarchy\n    &gt;&gt;&gt; custom = {'PAGES 2k v2.2.0', 'Hierarchy': 2, 'SISAL v3': 1, 'FE23 (Breitenmoser et al. (2014))': 3, 'CoralHydro2k v1.0.': 4, 'Iso2k v1.1.2': 5}\n    &gt;&gt;&gt; df = define_hierarchy(df, hierarchy=custom)  # Custom hierarchy\n    \"\"\"\n    df['Hierarchy'] = 99\n    if hierarchy=='default':\n        df.loc[df['originalDatabase']=='PAGES 2k v2.2.0', 'Hierarchy'] = 1\n        df.loc[df['originalDatabase']=='FE23 (Breitenmoser et al. (2014))', 'Hierarchy'] = 5\n        df.loc[df['originalDatabase']=='CoralHydro2k v1.0.1', 'Hierarchy'] = 3\n        df.loc[df['originalDatabase']=='Iso2k v1.1.2', 'Hierarchy'] = 4\n        df.loc[df['originalDatabase']=='SISAL v3', 'Hierarchy'] = 2\n    else:\n        df.loc[df['originalDatabase']=='PAGES 2k v2.2.0', 'Hierarchy'] = hierarchy['pages2k']\n        df.loc[df['originalDatabase']=='FE23 (Breitenmoser et al. (2014))', 'Hierarchy'] = hierarchy['fe23']\n        df.loc[df['originalDatabase']=='CoralHydro2k v1.0.1', 'Hierarchy'] = hierarchy['ch2k']\n        df.loc[df['originalDatabase']=='Iso2k v1.1.2', 'Hierarchy'] = hierarchy['iso2k']\n        df.loc[df['originalDatabase']=='SISAL v3', 'Hierarchy'] = hierarchy['sisal']\n\n    print('Chosen hierarchy:')\n\n\n\n    for db, h in df.groupby('originalDatabase')['Hierarchy'].min().sort_values().items():\n        if h==1: \n            print(f'{h}. {db} (highest)')\n        elif h==len(df.originalDatabase.unique()): \n            print(f'{h}. {db} (lowest)')\n        else:\n            print(f'{h}. {db}')\n    return df\n</code></pre>"},{"location":"api/ut_duplicate_search/#dod2k_utilities.ut_duplicate_search.density_scatter","title":"<code>density_scatter(x, y, ax=None, fig=None, sort=True, bins=20, **kwargs)</code>","text":"<p>Scatter plot colored by 2d histogram</p> Source code in <code>dod2k_utilities/ut_duplicate_search.py</code> <pre><code>def density_scatter( x , y, ax = None, fig=None, sort = True, bins = 20, **kwargs )   :\n    \"\"\"\n    Scatter plot colored by 2d histogram\n    \"\"\"\n\n    from matplotlib import cm\n    from matplotlib.colors import Normalize \n    from scipy.interpolate import interpn\n    if ax is None :\n        fig , ax = plt.subplots()\n    data , x_e, y_e = np.histogram2d( x, y, bins = bins, density = True )\n    z = interpn( ( 0.5*(x_e[1:] + x_e[:-1]) , 0.5*(y_e[1:]+y_e[:-1]) ) , data , np.vstack([x,y]).T , method = \"splinef2d\", bounds_error = False)\n\n    #To be sure to plot all data\n    z[np.where(np.isnan(z))] = 0.0\n\n    # Sort the points by density, so that the densest points are plotted last\n    if sort :\n        idx = z.argsort()\n        x, y, z = x[idx], y[idx], z[idx]\n\n    ax.scatter( x, y, c=z, **kwargs )\n\n    norm = Normalize(vmin = np.min(z), vmax = np.max(z))\n    cbar = plt.colorbar(cm.ScalarMappable(norm = norm), ax=ax)\n    cbar.ax.set_ylabel('data density')\n\n    return ax\n</code></pre>"},{"location":"api/ut_duplicate_search/#dod2k_utilities.ut_duplicate_search.dup_plot","title":"<code>dup_plot(df, ii, jj, id_1, id_2, time_1, time_2, time_12, data_1, data_2, int_1, int_2, pot_dup_corr, keys_to_print=['originalDatabase', 'originalDataURL', 'datasetId', 'archiveType', 'proxy | variableName', 'geo_siteName', 'lat | lon | elev', 'mean | std | units', 'year'], dup_mdata_row=[], plot_text=True, fig_scale=1)</code>","text":"<p>Plots the duplicate candidates. Plots the record data as a timeseries of anomalies in a common panel (w.r.t. shared time period) and prints out the most relevant metadata. Highlights identical metadata in orange and different metadata in green.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing proxy records and metadata.</p> required <code>ii</code> <code>int</code> <p>Index of the first record.</p> required <code>jj</code> <code>int</code> <p>Index of the second record.</p> required <code>id_1</code> <code>str</code> <p>Dataset ID of the first record.</p> required <code>id_2</code> <code>str</code> <p>Dataset ID of the second record.</p> required <code>time_1</code> <code>array - like</code> <p>Time vector for the first record.</p> required <code>time_2</code> <code>array - like</code> <p>Time vector for the second record.</p> required <code>time_12</code> <code>array - like</code> <p>Shared time points between both records.</p> required <code>data_1</code> <code>array - like</code> <p>Data values of the first record.</p> required <code>data_2</code> <code>array - like</code> <p>Data values of the second record.</p> required <code>int_1</code> <code>array - like</code> <p>Indices of shared times in the first record.</p> required <code>int_2</code> <code>array - like</code> <p>Indices of shared times in the second record.</p> required <code>pot_dup_corr</code> <code>float</code> <p>Correlation between the two records.</p> required <code>keys_to_print</code> <code>list of str</code> <p>List of metadata keys to display. Default is a standard set of keys.</p> <code>['originalDatabase', 'originalDataURL', 'datasetId', 'archiveType', 'proxy | variableName', 'geo_siteName', 'lat | lon | elev', 'mean | std | units', 'year']</code> <code>dup_mdata_row</code> <code>list</code> <p>Stores metadata differences for output.</p> <code>[]</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>Figure object of the duplicate plot.</p> <code>dup_mdata_row</code> <code>list</code> <p>Metadata rows generated for display.</p> Source code in <code>dod2k_utilities/ut_duplicate_search.py</code> <pre><code>def dup_plot(df, ii, jj, id_1, id_2, time_1, time_2, time_12, data_1, data_2, int_1, int_2, \n             pot_dup_corr, keys_to_print=['originalDatabase', 'originalDataURL', 'datasetId', 'archiveType', \n                                          'proxy | variableName', #'archive|proxy', \n                                          'geo_siteName', 'lat | lon | elev', 'mean | std | units', 'year' ], \n             dup_mdata_row=[], plot_text=True, fig_scale=1):\n    \"\"\"\n    Plots the duplicate candidates. Plots the record data as a timeseries of anomalies in a common panel (w.r.t. shared time period) and prints out the most relevant metadata. Highlights identical metadata in orange and different metadata in green.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame containing proxy records and metadata.\n    ii : int\n        Index of the first record.\n    jj : int\n        Index of the second record.\n    id_1 : str\n        Dataset ID of the first record.\n    id_2 : str\n        Dataset ID of the second record.\n    time_1 : array-like\n        Time vector for the first record.\n    time_2 : array-like\n        Time vector for the second record.\n    time_12 : array-like\n        Shared time points between both records.\n    data_1 : array-like\n        Data values of the first record.\n    data_2 : array-like\n        Data values of the second record.\n    int_1 : array-like\n        Indices of shared times in the first record.\n    int_2 : array-like\n        Indices of shared times in the second record.\n    pot_dup_corr : float\n        Correlation between the two records.\n    keys_to_print : list of str, optional\n        List of metadata keys to display. Default is a standard set of keys.\n    dup_mdata_row : list, optional\n        Stores metadata differences for output.\n\n    Returns\n    -------\n    fig : matplotlib.figure.Figure\n        Figure object of the duplicate plot.\n    dup_mdata_row : list\n        Metadata rows generated for display.\n    \"\"\"\n    from matplotlib import font_manager\n    fs        = 10\n    scale     = 0.16\n    init_offs = -.6\n    str_limit = 50\n\n    fig  = plt.figure(figsize=(10, 8.5), dpi=300*fig_scale)\n    grid = GS(3,4, wspace=0.6)\n    ax   = plt.subplot(grid[0,:3])\n    #\n    label1 = f'#{ii}: {id_1}' \n    # print('int_1', int_1)\n    ax.scatter(time_1, data_1-np.mean(data_1[int_1]), facecolor='None', \n               edgecolor='tab:blue', marker='o',#marker='o',\n               s=15, label=label1, alpha=0.9)\n\n    label2 = f'#{jj}: {id_2}'# f'ID2: {id_2} (#{jj}))' \n    ax.scatter(time_2, data_2-np.mean(data_2[int_2]), color='tab:red', marker='x',#marker='D',\n               s=15, label=label2, alpha=0.9)\n\n\n    ax.legend(loc='upper right')\n    ax.set_ylabel(f'anomalies w.r.t. {int(time_12[0])}-{int(time_12[-1])} \\n '+\n                  f'#{ii}: '+str(df['paleoData_units'].iloc[ii])+f'#{jj}: '+str(df['paleoData_units'].iloc[jj]))\n\n    ax.set_xlabel(df['yearUnits'].iloc[ii])\n    ax.set_title('Possible duplicates. Correlation=%3.2f'%pot_dup_corr\n                 +'. Time overlap=%d (%d'%(len(time_12), len(time_12)/len(time_1)*100)+\n                 '%'+', %d'%(len(time_12)/len(time_2)*100)+'%)')\n\n    txt = ('Metadata (differences are highlighted in blue; only first '+\n           str(str_limit)+' characters are displayed)')\n\n    # column positions\n    cols = [.1, -.1+0, -.1+.3, -.1+.9+0.15]\n\n    rows = [-0.3]+[-0.45]*3\n\n    ax.text(cols[0], rows[0], txt, transform=ax.transAxes, ha='left', fontsize=fs-1)\n\n    ax.text(cols[1], rows[1],'Key', transform=ax.transAxes, fontsize=fs+2, ha='left')\n    ax.text(cols[2], rows[2],'Record 1 (blue circles)', transform=ax.transAxes, ha='left', fontsize=fs+1)\n    ax.text(cols[3], rows[3],'Record 2 (red crosses)',  transform=ax.transAxes, ha='left', fontsize=fs+1)\n    # plot the metadata \n    for ik, key in enumerate(keys_to_print):\n        if key=='lat | lon | elev':\n            metadata_field1 = '%3.1f | %3.1f | %3.1f'%(df['geo_meanLat'].iloc[ii],\n                                                   df['geo_meanLon'].iloc[ii], \n                                                   df['geo_meanElev'].iloc[ii])\n            metadata_field2 = '%3.1f | %3.1f | %3.1f'%(df['geo_meanLat'].iloc[jj],\n                                                   df['geo_meanLon'].iloc[jj], \n                                                   df['geo_meanElev'].iloc[jj])\n        elif key=='year':\n            metadata_field1 = '%3.1f - %3.1f %s'%(np.min(df['year'].iloc[ii]), \n                                                  np.max(df['year'].iloc[ii]), \n                                                  df['yearUnits'].iloc[ii])\n            metadata_field2 = '%3.1f - %3.1f %s'%(np.min(df['year'].iloc[jj]), \n                                                  np.max(df['year'].iloc[jj]), \n                                                  df['yearUnits'].iloc[jj])\n        elif key=='mean | std | units':\n            metadata_field1 = '%4.2f | %4.2f | %s'%(np.mean(df['paleoData_values'].iloc[ii]), \n                                                  np.std(df['paleoData_values'].iloc[ii]), \n                                                  df['paleoData_units'].iloc[ii])\n            metadata_field2 = '%4.2f | %4.2f | %s'%(np.mean(df['paleoData_values'].iloc[jj]), \n                                                  np.std(df['paleoData_values'].iloc[jj]), \n                                                  df['paleoData_units'].iloc[jj])\n        elif key=='archive | proxy': #'archiveType','paleoData_proxy',\n            metadata_field1 = '%s | %s'%(df['archiveType'].iloc[ii],\n                                       df['paleoData_proxy'].iloc[ii])\n            metadata_field2 = '%s | %s'%(df['archiveType'].iloc[jj],\n                                       df['paleoData_proxy'].iloc[jj])\n        elif key=='proxy | variableName': #'archiveType','paleoData_proxy',\n            metadata_field1 = '%s | %s'%(df['paleoData_proxy'].iloc[ii],\n                                       df['paleoData_variableName'].iloc[ii])\n            metadata_field2 = '%s | %s'%(df['paleoData_proxy'].iloc[jj],\n                                       df['paleoData_variableName'].iloc[jj])\n        else:\n            metadata_field1, metadata_field2 = df[key].iloc[[ii, jj]]\n\n        if 'URL' in key:\n            metadata_field1 = metadata_field1.replace('https://','')\n            metadata_field2 = metadata_field2.replace('https://','')\n\n        row = init_offs-scale*ik\n        col = [-.1, -.1+.2, -.1+.75]\n\n\n\n        plt.text(cols[1], row, ' '*180,transform=ax.transAxes, fontsize=fs+1, ha='left', \n                 bbox={'facecolor': 'tab:grey',   'alpha':.05, 'pad':2.5, 'edgecolor': 'None'\n                      })\n        # if metadata_field1 != metadata_field2:\n        #     c = 'white'\n        #     # plt.text(cols[1], row, ' '*250,transform=ax.transAxes, fontsize=fs+1, ha='left', \n        #     #          bbox={'facecolor': 'tab:blue', \n        #     #                'alpha':.2, 'pad':2.5, 'edgecolor': 'None'\n        #     #               })\n        # else:\n        #     c = 'tab:orange'\n        #     # plt.text(cols[1], row, ' '*250,transform=ax.transAxes, fontsize=fs+1, ha='left',\n        #     #          bbox={'facecolor':'tab:orange','alpha':.2, 'pad':2.5, 'edgecolor': 'None'})\n\n        # ax.text(cols[1], row, key, transform=ax.transAxes,fontsize=fs, ha='left', \n        #         bbox={'facecolor':c,'alpha':.2, 'pad':2.5, 'edgecolor': 'None'})\n        # ax.text(cols[2], row, str(metadata_field1)[0:str_limit],ha='left', \n        #         transform=ax.transAxes,fontsize=fs, \n        #         bbox={'facecolor':c,'alpha':.2, 'pad':2.5, 'edgecolor': 'None'})\n        # ax.text(cols[3], row, str(metadata_field2)[0:str_limit],ha='left', \n        #         transform=ax.transAxes,fontsize=fs, \n        #         bbox={'facecolor':c,'alpha':.2, 'pad':2.5, 'edgecolor': 'None'})\n\n\n        # In the metadata loop, replace the current text plotting with:\n\n\n\n        # if metadata_field1 != metadata_field2:\n            # Plot background for the whole row\n        plt.text(cols[1], row, ' '*180, transform=ax.transAxes, fontsize=fs+1, ha='left', \n                 bbox={'facecolor': 'tab:grey', 'alpha':.05, 'pad':2.5, 'edgecolor': 'None'})\n\n        # For compound fields, split and highlight individually\n\n        if '|' in str(metadata_field1):# Plot background for the whole row\n            plt.text(cols[1], row, ' '*180, transform=ax.transAxes, fontsize=fs+1, ha='left', \n                     bbox={'facecolor': 'tab:grey', 'alpha':.05, 'pad':2.5, 'edgecolor': 'None'})\n\n\n            parts1 = str(metadata_field1).split('|')\n            parts2 = str(metadata_field2).split('|')\n\n            # Plot the key normally\n            ax.text(cols[1], row, key, transform=ax.transAxes, fontsize=fs, #family='monospace',\n                    ha='left')\n\n            # Calculate spacing for each part\n            cumulative_offset = 0\n            char_width = 0.015  # Approximate character width in axes coordinates (adjust as needed)\n\n            for i, (p1, p2) in enumerate(zip(parts1, parts2)):\n                p1, p2 = p1.strip(), p2.strip()\n                color = 'tab:orange' if p1 == p2 else 'white'\n\n                # For record 1 (cols[2])\n                ax.text(cols[2] + cumulative_offset, row, p1, ha='left',#family='monospace',\n                       transform=ax.transAxes, fontsize=fs,\n                       bbox={'facecolor': color, 'alpha': .2, 'pad': 2.5, 'edgecolor': 'None'})\n\n                # For record 2 (cols[3])\n                ax.text(cols[3] + cumulative_offset, row, p2, ha='left',#family='monospace',\n                       transform=ax.transAxes, fontsize=fs,\n                       bbox={'facecolor': color, 'alpha': .2, 'pad': 2.5, 'edgecolor': 'None'})\n\n                # Add separator if not last element\n                if i &lt; len(parts1) - 1:\n                    sep_offset = cumulative_offset + len(p1) * char_width\n                    ax.text(cols[2] + sep_offset, row, ' | ', ha='left',#family='monospace',\n                           transform=ax.transAxes, fontsize=fs)\n                    ax.text(cols[3] + sep_offset, row, ' | ', ha='left',#family='monospace',\n                           transform=ax.transAxes, fontsize=fs)\n                    cumulative_offset = sep_offset + 3 * char_width\n                else:\n                    cumulative_offset += len(p1) * char_width\n\n        else:\n            if metadata_field1 != metadata_field2:\n                # Non-compound field - use original logic\n                c = 'white'\n                ax.text(cols[1], row, key, transform=ax.transAxes, fontsize=fs, ha='left', #family='monospace',\n                        bbox={'facecolor': c, 'alpha': .2, 'pad': 2.5, 'edgecolor': 'None'})\n                ax.text(cols[2], row, str(metadata_field1)[0:str_limit], ha='left', #family='monospace',\n                        transform=ax.transAxes, fontsize=fs,\n                        bbox={'facecolor': c, 'alpha': .2, 'pad': 2.5, 'edgecolor': 'None'})\n                ax.text(cols[3], row, str(metadata_field2)[0:str_limit], ha='left',#family='monospace',\n                        transform=ax.transAxes, fontsize=fs,\n                        bbox={'facecolor': c, 'alpha': .2, 'pad': 2.5, 'edgecolor': 'None'})\n            else:\n                # Fields match - keep original orange highlighting\n                c = 'tab:orange'\n                plt.text(cols[1], row, ' '*180, transform=ax.transAxes, fontsize=fs+1, ha='left', #family='monospace',\n                         bbox={'facecolor': 'tab:grey', 'alpha':.05, 'pad':2.5, 'edgecolor': 'None'})\n                ax.text(cols[1], row, key, transform=ax.transAxes, fontsize=fs, ha='left',#family='monospace',\n                        bbox={'facecolor': 'None', 'alpha': .2, 'pad': 2.5, 'edgecolor': 'None'})\n                ax.text(cols[2], row, str(metadata_field1)[0:str_limit], ha='left',#family='monospace',\n                        transform=ax.transAxes, fontsize=fs,\n                        bbox={'facecolor': c, 'alpha': .2, 'pad': 2.5, 'edgecolor': 'None'})\n                ax.text(cols[3], row, str(metadata_field2)[0:str_limit], ha='left',#family='monospace',\n                        transform=ax.transAxes, fontsize=fs,\n                        bbox={'facecolor': c, 'alpha': .2, 'pad': 2.5, 'edgecolor': 'None'})\n\n        dup_mdata_row += [[metadata_field1, metadata_field2][m_ind].split('|')[s_ind]   \n                          for s_ind in range(len(metadata_field1.split('|'))) \n                          for m_ind in [0,1]]\n    ax2   = plt.subplot(grid[0,3])\n\n    x  = data_1[int_1]-np.mean(data_1[int_1])\n    y  = data_2[int_2]-np.mean(data_2[int_2])\n\n    # density_scatter(x , y, ax = ax2, fig=fig)\n\n    # xy = np.vstack([x,y])\n    # z  = gaussian_kde(xy)(xy)\n\n    # idx = z.argsort()\n    # x, y, z = x[idx], y[idx], z[idx]\n\n\n    density_scatter(x , y, ax2, s=5)\n    # ax2.scatter(x, y, color='k', marker='+', alpha=0.5)#,\n    #             #s=15, linestyle='None')\n    plt.xlabel(label1)\n    plt.ylabel(label2)\n\n    # Add 1:1 line spanning the data range\n    min_val = min(x.min(), y.min())\n    max_val = max(x.max(), y.max())\n    ax2.plot([min_val, max_val], [min_val, max_val],color='tab:grey',  \n             alpha=0.75, zorder=0, label='1:1 line')\n\n\n    return fig, dup_mdata_row\n</code></pre>"},{"location":"api/ut_duplicate_search/#dod2k_utilities.ut_duplicate_search.duplicate_decisions_multiple","title":"<code>duplicate_decisions_multiple(df, operator_details=False, choose_recollection=True, plot=True, remove_identicals=True, dist_tolerance_km=8, backup=True, comment=True, automate_db_choice=False)</code>","text":"<p>Review potential duplicate pairs in a proxy database and decide which records to keep, remove, or combine.</p> <p>This function walks through each potential duplicate pair identified in a dataset, displays metadata and optionally plots the data, and allows the operator to make decisions. Decisions are saved to a CSV file, and a duplicate-free dataframe can be generated.</p> <p>Copied from duplicate_decisions but improved handling of multiple duplicates.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe containing proxy data and metadata. Expected columns include:     - 'geo_meanLat', 'geo_meanLon' : Latitude and longitude of the site.     - 'year', 'paleoData_values' : Time vector and proxy values.     - 'archiveType', 'paleoData_proxy' : Archive type and proxy type.     - 'datasetId' : Unique identifier or dataset name.     - 'geo_siteName' : Site name.     - 'originalDatabase' : Name of the original database.     - 'geo_meanElev' : Mean elevation of the site (optional for duplicate checks).     - 'Hierarchy' : Numeric value representing dataset priority (used for auto-decisions).     - 'originalDataURL' : URL of the original dataset.</p> required <code>operator_details</code> <code>tuple or bool</code> <p>Tuple containing (initials, fullname, email) of the operator. If False, user input is requested. Default is False.</p> <code>False</code> <code>choose_recollection</code> <code>bool</code> <p>If True, automatically selects the record that is a recollection or update when applicable. Default is True.</p> <code>True</code> <code>plot</code> <code>bool</code> <p>If True, generate plots for manual inspection of duplicate pairs. Default is True.</p> <code>True</code> <code>remove_identicals</code> <code>bool</code> <p>If True, automatically remove records that are identical in data and metadata. Default is True.</p> <code>True</code> <code>dist_tolerance_km</code> <code>float</code> <p>Maximum distance (in km) for considering records as recollection updates. Default is 8 km.</p> <code>8</code> <p>Returns:</p> Type Description <code>None</code> <p>Decisions are saved as CSV backup and final CSV in the <code>df.name/dup_detection/</code> directory.</p> Notes <ul> <li>Automatic decisions are made based on data identity, metadata identity, perfect correlation,   and recollection indicators in site names.</li> <li>Manual decisions are prompted via command-line input if automatic rules do not apply.</li> <li>Decision types include:<ul> <li>'AUTO: UPDATE' : Automatically select record that is a recollection/update.</li> <li>'AUTO: IDENTICAL' : Automatically select record based on hierarchy if records are identical.</li> <li>'MANUAL' : Decision requires operator input.</li> </ul> </li> <li>Figures are saved with a standardized naming convention and linked in the CSV output.</li> </ul> Source code in <code>dod2k_utilities/ut_duplicate_search.py</code> <pre><code>def duplicate_decisions_multiple(df, operator_details=False, choose_recollection=True, #keep_all=False, \n                                 plot=True, remove_identicals=True, dist_tolerance_km=8, backup=True,\n                                 comment=True, automate_db_choice=False):\n    \"\"\"\n    Review potential duplicate pairs in a proxy database and decide which records to keep, remove, or combine.\n\n    This function walks through each potential duplicate pair identified in a dataset,\n    displays metadata and optionally plots the data, and allows the operator to make decisions.\n    Decisions are saved to a CSV file, and a duplicate-free dataframe can be generated.\n\n    Copied from duplicate_decisions but improved handling of multiple duplicates.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        The dataframe containing proxy data and metadata. Expected columns include:\n            - 'geo_meanLat', 'geo_meanLon' : Latitude and longitude of the site.\n            - 'year', 'paleoData_values' : Time vector and proxy values.\n            - 'archiveType', 'paleoData_proxy' : Archive type and proxy type.\n            - 'datasetId' : Unique identifier or dataset name.\n            - 'geo_siteName' : Site name.\n            - 'originalDatabase' : Name of the original database.\n            - 'geo_meanElev' : Mean elevation of the site (optional for duplicate checks).\n            - 'Hierarchy' : Numeric value representing dataset priority (used for auto-decisions).\n            - 'originalDataURL' : URL of the original dataset.\n\n    operator_details : tuple or bool, optional\n        Tuple containing (initials, fullname, email) of the operator. If False, user input is requested.\n        Default is False.\n    choose_recollection : bool, optional\n        If True, automatically selects the record that is a recollection or update when applicable.\n        Default is True.\n    plot : bool, optional\n        If True, generate plots for manual inspection of duplicate pairs. Default is True.\n    remove_identicals : bool, optional\n        If True, automatically remove records that are identical in data and metadata. Default is True.\n    dist_tolerance_km : float, optional\n        Maximum distance (in km) for considering records as recollection updates. Default is 8 km.\n\n    Returns\n    -------\n    None\n        Decisions are saved as CSV backup and final CSV in the `df.name/dup_detection/` directory.\n\n    Notes\n    -----\n    - Automatic decisions are made based on data identity, metadata identity, perfect correlation,\n      and recollection indicators in site names.\n    - Manual decisions are prompted via command-line input if automatic rules do not apply.\n    - Decision types include:\n        - 'AUTO: UPDATE' : Automatically select record that is a recollection/update.\n        - 'AUTO: IDENTICAL' : Automatically select record based on hierarchy if records are identical.\n        - 'MANUAL' : Decision requires operator input.\n    - Figures are saved with a standardized naming convention and linked in the CSV output.\n    \"\"\"\n\n    def create_ID_dup_dict(pot_dup_IDs):\n        dup_dict         = {}\n        reverse_dup_dict = {}\n        for id1, id2 in pot_dup_IDs:\n            # check if id1 appears in reverse_dup_dict: \n            if id1 in reverse_dup_dict:\n                dup_id = reverse_dup_dict[id1] # if YES, it means it is already associated with a duplicate record, use this for mapping\n            else:\n                dup_id = id1 # if NO this is the first time id1 appears as a potential dup, then use this for mapping\n            # dup_id is the unique mapper to the duplicate category.\n            if dup_id not in dup_dict: \n                dup_dict[dup_id] = [] # create an empty dictionary key in case this is the first time that dup_id appears\n            # if dup_id!=id1: # if id1 is not identical to the unique mapper\n            #     if id1 not in dup_dict[dup_id]:\n            #         dup_dict[dup_id]+=[id1]\n            if id1 not in dup_dict[dup_id]:\n                if dup_id!=id1:\n                    dup_dict[dup_id]+=[id1] # add id1 to the dup_dict if not in there yet (keep entries unique) and id1 is not unique mapper\n            if id2 not in dup_dict[dup_id]:\n                dup_dict[dup_id]+=[id2]  # add id2 to the dup_dict if not in there yet (keep entries unique)\n\n            reverse_dup_dict[id1] = dup_id # make sure id1 gets mapped back to its unique mapper\n            reverse_dup_dict[id2] = dup_id # make sure id2 gets mapped back to its unique mapper\n\n\n        # # drop all PAIRS to get multiples only\n        dup_dict_multiples = {}\n        for kk, vv in dup_dict.items():\n            if len(vv)&gt;1:\n                dup_dict_multiples[kk]=vv\n        multiple_list = []\n        for kk, vv in dup_dict_multiples.items():\n            # print(kk, len(vv), vv)\n            multiple_list.append(kk)\n            for vvv in vv:\n                multiple_list.append(vvv)\n        print('------'*10)\n        if len(dup_dict_multiples)&gt;0:\n            print('Detected MULTIPLE duplicates, including:')\n            for kk, vv in dup_dict_multiples.items():\n                print(kk, len(vv), vv)\n            print('PLEASE PAY ATTENTION WHEN MAKING DECISIONS FOR THESE DUPLICATES!')\n            print('The decision process will go through the duplicates on a PAIR-BY-PAIR basis, which is not optimised for multiple duplicates.')\n            print('The multiples will be highlighted throughout the decision process.')\n            print('Should the operator want to go back and revise a previous decision based on the presentation of a new candidate pair, they can manually modify the backup file to alter any previous decisions.')\n        print('------'*10)\n        return dup_dict_multiples, reverse_dup_dict, multiple_list \n\n    import datetime\n    # Select the metadata keys to show on the output figures - keys depend on the \n    # dataframe and need to be determined according to the dataframe input. \n    # if using this code on fused_database.pkl, make sure that all entries are available \n    # in the dataframe1 Use fuse_datasets.py to fuse and homogenise metadata  of different databases.\n\n    if not operator_details:\n        initials = input('Please enter your initials here:')\n        fullname = input('Please enter your full name here:')\n        email    = input('Please enter your email address here:')\n    else:\n        initials, fullname, email = operator_details\n    date_time= str(datetime.datetime.utcnow())+' (UTC)'\n    header   = [['# Decisions for duplicate candidate pairs. ']]\n    header  += [['# Operated by %s (%s)'%(fullname, initials)]]\n    header  += [['# E-Mail: %s'%email]]\n    header  += [['# Created on: %s'%date_time]]\n\n\n    dirname = 'data/%s/dup_detection/'%(df.name)\n    filename = f'dup_decisions_{df.name}' # name of csv file which saves duplicate candidate pairs\n\n    filename+=f'_{initials}'\n\n    detection_file = dirname+'dup_detection_candidates_'+df.name\n\n\n    if automate_db_choice is not False:\n        auto_db_choice = True\n        auto_db_pref = automate_db_choice['preferred_db']\n        auto_db_rej  = automate_db_choice['rejected_db']\n    else:\n        auto_db_choice = False\n        auto_db_pref = ''\n        auto_db_rej = ''\n\n\n    if not os.path.exists(dirname):\n        os.makedirs(dirname)\n\n    # try to load data from backup!\n    try:\n        data, hh = read_csv(dirname+filename+'_BACKUP', header=True, last_header_row=4)\n        if backup:\n            backup_file=input(f'Found backup file ({dirname+filename}_BACKUP.csv). Do you want to start decision process from the backup file? [y/n]')\n        else:\n            backup_file='n'\n        if backup_file=='n':\n            raise FileNotFoundError\n        data, hh = read_csv(dirname+filename+'_BACKUP', header=True, last_header_row=4)\n        print('header', hh)\n        print('data', list(data))\n        data = list(data)\n\n        load_saved_data      = True\n        last_index = len(data)\n        print('start with index: ', last_index)\n    except FileNotFoundError:\n        print('No back up.')\n        load_saved_data      = False\n        data = []\n        pass\n\n    # load the potential duplicate data as found in find_duplicates function\n    pot_dup_meta, head = read_csv(detection_file, header=True)\n\n    pot_dup_inds   = np.array(np.array(pot_dup_meta)[:, :2], dtype=int)  # indices for each pairs\n    pot_dup_IDs    = np.array(np.array(pot_dup_meta)[:, 2:4], dtype=str) # IDs for each pairs\n    pot_dup_corrs  = np.array(np.array(pot_dup_meta)[:, 4], dtype=float)\n    pot_dup_dists  = np.array(np.array(pot_dup_meta)[:, 5], dtype=float)\n\n\n    dup_dict_multiples, reverse_dict, multiple_list   = create_ID_dup_dict(pot_dup_IDs)\n    # raise Exception\n\n    n_pot_dups   = pot_dup_inds.shape[0]\n\n    # loop through the potential duplicates.\n    decisions = ['N/A']*df.shape[0] #set False if index should be discarded from dataset.\n    cols    = [['index 1', 'index 2', 'figure path',\n                'datasetId 1', 'datasetId 2', \n                'originalDatabase 1', 'originalDatabase 2',\n                'geo_siteName 1', 'geo_siteName 2', \n                'geo_meanLat 1', 'geo_meanLat 2', \n                'geo_meanLon 1', 'geo_meanLon 2', \n                'geo_meanElevation 1', 'geo_meanElevation 2', \n                'archiveType 1', 'archiveType 2',\n                'paleoData_proxy 1', 'paleoData_proxy 2',\n                'originalDataURL 1', 'originalDataURL 2',\n                'year 1', 'year 2',\n                'Decision 1', 'Decision 2',\n                'Decision type', 'Decision comment' ]]\n    dup_dec = []\n\n\n    # Write header and existing data ONCE before the loop (only if starting fresh)\n    if not load_saved_data:\n        # start a new backup file. \n        with open(dirname+filename+'_BACKUP.csv', 'w', newline='') as f:\n            writer = csv.writer(f)\n            writer.writerows(header)\n            writer.writerows(cols)\n\n    else:\n        # If loading from backup, populate dup_dec with existing data\n        for data_row in list(data):\n            dup_dec += [list(data_row)]\n\n    # Open backup file in APPEND mode for writing new decisions\n    with open(dirname+filename+'_BACKUP.csv', 'a', newline='') as f:\n        writer = csv.writer(f)\n\n        for i_pot_dups, (ii, jj) in enumerate(pot_dup_inds):\n            if load_saved_data:\n                if i_pot_dups&lt;last_index: continue\n                else:\n                    print(ii, jj, last_index, i_pot_dups)\n            # data and metadata associated with the two potential duplicates\n            id_1, id_2       = df['datasetId'].loc[[ii, jj]]\n            time_1, time_2   = df['year'].loc[[ii, jj]]\n            time_12, int_1, int_2 = np.intersect1d(time_1, time_2, return_indices=True) \n            data_1           = np.array(df['paleoData_values'].loc[ii])\n            data_2           = np.array(df['paleoData_values'].loc[jj])\n\n            lat_1, lat_2     = df['geo_meanLat'].loc[[ii, jj]]\n            lon_1, lon_2     = df['geo_meanLon'].loc[[ii, jj]]\n            site_1, site_2   = df['geo_siteName'].loc[[ii, jj]]\n            or_db_1, or_db_2 = df['originalDatabase'].loc[[ii, jj]]\n\n            auto_choice = '1' if df['Hierarchy'].loc[ii]&gt;=df['Hierarchy'].loc[jj] else '2'\n\n            print('&gt; %d/%d'%(i_pot_dups+1, n_pot_dups), #set_txt, \n                   id_1, id_2, #var_1, var_2, season_1,season_2,\n                   pot_dup_dists[i_pot_dups], pot_dup_corrs[i_pot_dups],\n                   sep=',')\n\n            # Print the interpretation values \n            print('====================================================================')\n            print('=== POTENTIAL DUPLICATE %d/%d'%(i_pot_dups, n_pot_dups)+': %s+%s ==='%(df['datasetId'].loc[ii], df['datasetId'].loc[jj]))\n            print('=== URL 1: %s   ==='%(df['originalDataURL'].loc[ii]))\n            print('=== URL 2: %s   ==='%(df['originalDataURL'].loc[jj]))\n\n            keep = ''\n            dec_comment = ''\n\n\n            elevation_nan = (np.isnan(df['geo_meanElev'].loc[ii])|\n                             np.isnan(df['geo_meanElev'].loc[jj]))\n\n            metadata_identical = ((np.abs(lat_1-lat_2)&lt;=0.1) &amp; \n                                  (np.abs(lon_1-lon_2)&lt;=0.1)  &amp; \n                                  ((np.abs(df['geo_meanElev'].loc[ii]-df['geo_meanElev'].loc[jj])&lt;=1) \n                                    | elevation_nan) &amp; \n                                  (df['archiveType'].loc[ii]==df['archiveType'].loc[jj]) &amp; \n                                  (df['paleoData_proxy'].loc[ii]==df['paleoData_proxy'].loc[jj]) \n                                 )\n            sites_identical     = site_1==site_2\n            URL_identical       = (df['originalDataURL'].loc[ii]==df['originalDataURL'].loc[jj]) \n            # print(data_1, data_2)\n            data_identical      = (list(data_1)==list(data_2)) &amp; (list(time_1)==list(time_2))\n            correlation_perfect = (True if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False) &amp; (len(time_1)==len(time_2))\n\n            # if one db is the preferred and the other the rejected db, only if automate_db_choice is not False\n            auto_db_choice_cond = auto_db_choice &amp; (((or_db_1==auto_db_pref) &amp; (or_db_2==auto_db_rej))|((or_db_2==auto_db_pref) &amp; (or_db_1==auto_db_rej)))\n\n            print('True if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False', True if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False)\n            print('(len(time_1)==len(time_2))', (len(time_1)==len(time_2)))\n\n            print('metadata_identical: ', metadata_identical)\n            print('lat', (np.abs(lat_1-lat_2)&lt;=0.1), 'lon' , (np.abs(lon_1-lon_2)&lt;=0.1)   ,'elevation',\n                                  ( (np.abs(df['geo_meanElev'].loc[ii]-df['geo_meanElev'].loc[jj])&lt;=1) \n                                    | elevation_nan) , 'archivetype',\n                                  (df['archiveType'].loc[ii]==df['archiveType'].loc[jj]) ,'paleodata_proxy',\n                                  (df['paleoData_proxy'].loc[ii]==df['paleoData_proxy'].loc[jj]) \n                                 )\n            print('sites_identical: ', sites_identical)\n            print('URL_identical: ', URL_identical)\n            print('data_identical: ', data_identical)\n            print('correlation_perfect: ', correlation_perfect)\n            figpath = 'no figure'\n            while keep not in ['1', '2', 'b', 'n', 'c']:\n                # go through the data and metadata and check if decision is =automatic or manual\n                if (choose_recollection \n                    # if record 1 is update of record 2, choose 1\n                      &amp; np.any([(ss in site_1.lower()) for ss in ['recollect', 'update', 're-collect']]) \n                      &amp; (pot_dup_dists[i_pot_dups]&lt;dist_tolerance_km)\n                      &amp; ~np.any([ss in site_2.lower() for ss in ['recollect', 'update', 're-collect']])\n                     ):\n                    keep = '1'\n                    dec_comment = 'Record 1 (%s) is UPDATE of record 2(%s) . Automatically choose 1.'%(id_1, id_2)\n                    print(dec_comment)\n                    dec_type='AUTO: UPDATE'\n                elif (choose_recollection \n                      # if record 2 is update of record 1, choose 2\n                      &amp; np.any([ss in site_2.lower() for ss in ['recollect', 'update', 're-collect']]) \n                      &amp; (pot_dup_dists[i_pot_dups]&lt;dist_tolerance_km)\n                      &amp; ~np.any([ss in site_1.lower() for ss in ['recollect', 'update', 're-collect']])\n                     ):\n                    keep = '2'\n                    dec_comment = 'Record 2 (%s) is UPDATE of record 1 (%s). Automatically choose 2.'%(id_2, id_1)\n                    print(dec_comment)\n                    dec_type='AUTO: UPDATE'\n                elif (remove_identicals &amp; metadata_identical &amp; sites_identical &amp; URL_identical &amp; (data_identical|correlation_perfect)):\n                    # if all metadata and data matches, choose record according to hierarchy of databases\n                    if data_identical:\n                        dec_comment = 'RECORDS IDENTICAL (identical data). Automatically choose #%s.'%auto_choice\n                    else:\n                        dec_comment = 'RECORDS IDENTICAL (perfect correlation). Automatically choose #%s.'%auto_choice\n\n                    print(dec_comment)\n                    keep = auto_choice#'1'\n                    dec_type='AUTO: IDENTICAL'\n                elif (remove_identicals &amp; (data_identical|correlation_perfect)):\n                    # if most metadata and data matches except URL and/or site name, choose record according to hierarchy of databases\n                    if data_identical:\n                        dec_comment = 'RECORDS IDENTICAL (identical data) except for metadata. Automatically choose #%s.'%auto_choice\n                    else:\n                        dec_comment = 'RECORDS IDENTICAL (perfect correlation) except for metadata. Automatically choose #%s.'%auto_choice\n\n                    print(dec_comment)\n                    keep = auto_choice \n                    dec_type='AUTO: IDENTICAL except for URLs and/or geo_siteName.'\n                elif auto_db_choice_cond &amp; metadata_identical:\n                    reason = automate_db_choice['reason']\n                    dec_comment = f'Automated choice. Metadata identical, automatically choose {auto_db_pref} over {auto_db_rej}. {reason}'\n\n                    print(dec_comment)\n                    if or_db_1==auto_db_pref:\n                        keep     =  '1' \n                    elif or_db_2==auto_db_pref:\n                        keep     =  '2' \n                    else:\n                        raise Exception('Auto condition error.')\n                    dec_type = 'AUTO: preferred db and metadata identical.'\n\n                else:\n\n                    dec_type = 'MANUAL'\n                    fig, dup_mdata_row = dup_plot(df, ii, jj, id_1, id_2, time_1, time_2, time_12, data_1, data_2, int_1, int_2, pot_dup_corrs[i_pot_dups])\n                    plt.show(block=False)\n                    print('**Decision required for this duplicate pair (see figure above).**')\n\n                    if (id_1 in multiple_list) | (id_2 in multiple_list):\n                        print('-------'*15)\n                        print('***ATTENTION*** THIS RECORD IS ASSOCIATED WITH MULTIPLE DUPLICATES! PLEASE PAY SPECIAL ATTENTION WHEN MAKING DECISIONS FOR THIS RECORD!')\n                        print('The potential duplicates also associated with this record are:')\n                        if id_1!=reverse_dict[id_1]:\n                            print('- ', reverse_dict[id_1])\n                        for m_id in dup_dict_multiples[reverse_dict[id_1]]:\n                            if m_id==id_1: continue\n                            if m_id==id_2: continue\n                            print('......'*10)\n\n\n\n                            r  = df[df['datasetId']==m_id].iloc[0]  # Get first row as Series\n                            mm = df[df['datasetId']==m_id].index.values[0]\n\n\n\n                            print(f\"     - {'Dataset ID':20s}: {r['datasetId']}\")\n                            print(f\"     - {'URL':20s}: {r['originalDataURL']}\")\n\n\n                            # Prepare the text info\n                            info_text = (f\"{r['datasetId']} \\n {r['geo_siteName']} ({r['geo_meanLat']:.1f}, {r['geo_meanLon']:.1f}, {r['geo_meanElev']:.1f}) | \"\n                                         f\"{r['archiveType']} | {r['paleoData_proxy']} | {r['paleoData_variableName']}\")\n\n                            # Check for previous duplicates\n                            prev_dup_text = \"\"\n                            if len(dup_dec) &gt; 1:\n                                if str(mm) in np.array(dup_dec)[:, 0]:\n                                    mm_loc = np.array(dup_dec)[:, 0] == str(mm)\n                                    prev_dup_text = f\" \\n DECISION: {np.array(dup_dec)[mm_loc, -4][0]} (detected as pot dup of: {np.array(dup_dec)[mm_loc, 4][0]})\"\n                                elif str(mm) in np.array(dup_dec)[:, 1]:\n                                    mm_loc = np.array(dup_dec)[:, 1] == str(mm)\n                                    prev_dup_text = f\" \\n DECISION: {np.array(dup_dec)[mm_loc, -3][0]}  (detected as pot dup of: {np.array(dup_dec)[mm_loc, 3][0]})\"\n\n\n                            def line_11(x,y):\n                                if (len(x)&gt;0) &amp; (len(y)&gt;0):\n                                    min_val = min(x.min(), y.min())\n                                    max_val = max(x.max(), y.max())\n                                    plt.plot([min_val, max_val], [min_val, max_val],color='tab:grey', alpha=0.75, zorder=0, label='1:1 line')\n                                return\n\n                            # Create figure with info as title\n                            mt, mi1, mi2 = np.intersect1d(df['year'].loc[ii], df['year'].loc[mm], return_indices=True) \n\n\n                            multiple_fig = plt.figure(figsize=(4*2, 2), dpi=120)\n                            multiple_fig.suptitle(info_text + prev_dup_text, fontsize=8, y=0.98)\n\n\n\n\n                            # First subplot: \n                            plt.subplot(141)\n                            x = np.array(df['paleoData_values'].loc[mm])[mi2]\n                            y = np.array(df['paleoData_values'].loc[ii])[mi1]\n                            plt.scatter(x, y, s=5)\n                            plt.ylabel(f'{m_id[:20]}\\n{m_id[20:40]}\\n{m_id[40:]}'.replace('\\n\\n',''), fontsize=8)\n                            plt.xlabel(f'{id_1[:20]}\\n{id_1[20:40]}\\n{id_1[40:]}'.replace('\\n\\n',''), fontsize=8)\n                            plt.tick_params(labelsize=7)\n                            line_11(x,y)\n\n                            # Second subplot\n                            mt, mi1, mi2 = np.intersect1d(df['year'].loc[jj], df['year'].loc[mm], return_indices=True) \n                            plt.subplot(142)\n                            x = np.array(df['paleoData_values'].loc[mm])[mi2]\n                            y = np.array(df['paleoData_values'].loc[jj])[mi1]\n                            plt.scatter(x, y, s=5)\n                            plt.ylabel(f'{m_id[:20]}\\n{m_id[20:40]}\\n{m_id[40:]}'.replace('\\n\\n',''), fontsize=8)\n                            plt.xlabel(f'{id_2[:20]}\\n{id_2[20:40]}\\n{id_2[40:]}'.replace('\\n\\n',''), fontsize=8)\n                            plt.tick_params(labelsize=7)\n                            line_11(x,y)\n\n\n                            # Third subplot\n                            plt.subplot(143)\n                            plt.scatter(df['year'].loc[ii], df['paleoData_values'].loc[ii], label=df['datasetId'].loc[ii],\n                                       edgecolor='tab:blue', marker='o', s=6, facecolor='None', lw=0.7)\n                            plt.scatter(df['year'].loc[mm], df['paleoData_values'].loc[mm], label=df['datasetId'].loc[mm], \n                                        color='k', marker='o', zorder=0, alpha=0.9, s=3)\n                            plt.ylabel('year', fontsize=8)\n                            plt.xlabel('paleoData_values', fontsize=8)\n                            plt.tick_params(labelsize=7)\n                            plt.legend(fontsize=8)\n\n                            # Fourth subplot\n                            plt.subplot(144)\n                            plt.scatter(df['year'].loc[jj], df['paleoData_values'].loc[jj], label=df['datasetId'].loc[jj],\n                                       color='tab:red', marker='x', s=6,lw=.7)\n                            plt.scatter(df['year'].loc[mm], df['paleoData_values'].loc[mm], label=df['datasetId'].loc[mm], \n                                       color='k', marker='o', zorder=0, alpha=0.9, s=3)\n                            plt.ylabel('year', fontsize=8)\n                            plt.xlabel('paleoData_values', fontsize=8)\n                            plt.tick_params(labelsize=7)\n                            plt.legend(fontsize=8)\n\n\n                            plt.tight_layout()  # Leave room for suptitle\n                            plt.show(block=True)\n\n                            save_fig(multiple_fig, '%03d_%s_%s'%(i_pot_dups, id_1, id_2)+f'__{ii}_{jj}_MULTIPLE_{mm}', \n                                     dir=f'/dup_detection/{df.name}', figformat='pdf')\n\n\n\n                        print('-------'*15)\n\n                    if comment:\n                        print('Before inputting your decision. Would you like to leave a comment on your decision process?')\n                        dec_comment = input(f'{i_pot_dups+1}/{n_pot_dups}: **COMMENT** Please type your comment here and/or press enter.')\n                    else:\n                        dec_comment = ''\n                    keep = input(f'{i_pot_dups+1}/{n_pot_dups}: **DECISION** Keep record 1 (%s, blue circles) [1], record 2 (%s, red crosses) [2], keep both [b], keep none [n] or create a composite of both records [c]? Note: only overlapping timesteps are being composited. [Type 1/2/b/n/c]:'%(id_1, id_2))\n\n\n\n\n                    save_fig(fig, '%03d_%s_%s'%(i_pot_dups, id_1, id_2)+f'__{ii}_{jj}', dir=f'/dup_detection/{df.name}', figformat='pdf')\n\n\n                    figpath    = 'https://nzero.umd.edu:444/hub/user-redirect/lab/tree/dod2k_v2.0/figs/dup_detection/%s'%df.name\n                    figpath  += '%03d_%s_%s'%(i_pot_dups, id_1, id_2)+f'__{ii}_{jj}.jpg'\n\n                # now write down the decision\n                if keep=='1':\n                    print('KEEP BLUE CIRCLES: keep %s, remove %s.'%(id_1, id_2))\n                    decisions[ii]='KEEP'\n                    decisions[jj]='REMOVE'\n                elif keep=='2':\n                    print('KEEP RED CROSSES: remove %s, keep %s.'%(id_1, id_2))\n                    decisions[jj]='KEEP'\n                    decisions[ii]='REMOVE'\n                elif keep=='n':\n                    print('REMOVE BOTH: remove %s, remove %s.'%(id_2, id_1))\n                    decisions[ii]='REMOVE'\n                    decisions[jj]='REMOVE'\n                elif keep=='b':\n                    print('KEEP BOTH: keep %s, keep %s.'%(id_1, id_2))\n                    decisions[ii]='KEEP'\n                    decisions[jj]='KEEP'\n                elif keep=='c':\n                    print('CREATE A COMPOSITE OF BOTH RECORDS: %s, %s.'%(id_1, id_2))\n                    decisions[ii]='COMPOSITE'\n                    decisions[jj]='COMPOSITE'\n\n\n\n            cand_pair =[[ii, jj, figpath,\n                        id_1, id_2, or_db_1, or_db_2, site_1, site_2,\n                        lat_1, lat_2, lon_1, lon_2, \n                        df['geo_meanElev'].loc[ii], df['geo_meanElev'].loc[jj],\n                        df['archiveType'].loc[ii], df['archiveType'].loc[jj],\n                        df['paleoData_proxy'].loc[ii], df['paleoData_proxy'].loc[jj],\n                        df['originalDataURL'].loc[ii], df['originalDataURL'].loc[jj],\n                        '%3.1f-%3.1f'%(np.min(df['year'].loc[ii]), np.max(df['year'].loc[ii])), \n                        '%3.1f-%3.1f'%(np.min(df['year'].loc[jj]), np.max(df['year'].loc[jj])), \n                        decisions[ii], decisions[jj],\n                        dec_type, dec_comment]]\n\n            dup_dec  += cand_pair\n            writer.writerows(cand_pair)\n            print('write decision to backup file')\n            # raise Exception\n\n    print('=====================================================================')\n    print('END OF DUPLICATE DECISION PROCESS.')\n    print('=====================================================================')\n\n    comment = '# '+input('Type your comment on your decision process here and/or press enter:')\n    header  += [[comment]]\n    # header  += cols\n\n    print(np.array(dup_dec).shape)\n    filename +=f'_{date_time[2:10]}'\n    write_csv(np.array(dup_dec), dirname+filename, header=header, cols=cols)\n    print('Saved the decisions under %s.csv'%(dirname+filename))\n\n    print('Summary of all decisions made:')\n    for ii_d in range(len(dup_dec)):\n        keep_1=dup_dec[ii_d][-4]\n        keep_2=dup_dec[ii_d][-3]\n        print('#%d: %s record %s. %s record %s.'%(ii_d, keep_1, dup_dec[ii_d][3], \n                                                  keep_2, dup_dec[ii_d][4]))\n    return \n</code></pre>"},{"location":"api/ut_duplicate_search/#dod2k_utilities.ut_duplicate_search.find_duplicates","title":"<code>find_duplicates(df, dist_tolerance_km=8, n_points_thresh=10, corr_thresh=0.9, rmse_thresh=0.1, corr_diff_thresh=0.9, rmse_diff_thresh=0.1, elev_tolerance=0, ignore_same_database=False, save=True, print_output=False)</code>","text":"<p>Identify potential duplicate records in a dataset using metadata and time series similarity.</p> <p>This is a simpler version of <code>find_duplicates_optimized</code>.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing proxy records with metadata. Expected columns include: ['geo_meanLat', 'geo_meanLon', 'geo_meanElev', 'year', 'paleoData_values',   'archiveType', 'paleoData_proxy', 'geo_siteName', 'originalDatabase',   'originalDataURL', 'datasetId'].</p> required <code>dist_tolerance_km</code> <code>float</code> <p>Maximum allowed geographical distance between duplicates (km). Default is 8.</p> <code>8</code> <code>n_points_thresh</code> <code>int</code> <p>Minimum number of overlapping time points required. Default is 10.</p> <code>10</code> <code>corr_thresh</code> <code>float</code> <p>Minimum correlation threshold for duplicate detection. Default is 0.9.</p> <code>0.9</code> <code>rmse_thresh</code> <code>float</code> <p>Maximum RMSE allowed for duplicate detection. Default is 0.1.</p> <code>0.1</code> <code>corr_diff_thresh</code> <code>float</code> <p>Minimum correlation of first differences threshold. Default is 0.9.</p> <code>0.9</code> <code>rmse_diff_thresh</code> <code>float</code> <p>Maximum RMSE of first differences allowed. Default is 0.1.</p> <code>0.1</code> <code>elev_tolerance</code> <code>float</code> <p>Maximum allowed elevation difference. Default is 0.</p> <code>0</code> <code>ignore_same_database</code> <code>bool</code> <p>If True, ignores potential duplicates within the same database. Default is False.</p> <code>False</code> <code>save</code> <code>bool</code> <p>If True, saves results as a CSV file. Default is True.</p> <code>True</code> <code>print_output</code> <code>bool</code> <p>If True, prints progress and matching information. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>pot_dup_inds</code> <code>list of list of int</code> <p>Indices of detected potential duplicates.</p> <code>pot_dup_IDs</code> <code>list of list of str</code> <p>Dataset IDs of detected potential duplicates.</p> <code>distances_km</code> <code>ndarray</code> <p>Matrix of pairwise distances between records.</p> <code>correlations</code> <code>ndarray</code> <p>Matrix of pairwise correlations between records.</p> Source code in <code>dod2k_utilities/ut_duplicate_search.py</code> <pre><code>def find_duplicates(df, dist_tolerance_km=8, n_points_thresh=10, \n                    corr_thresh=0.9, rmse_thresh=0.1, corr_diff_thresh=0.9,\n                    rmse_diff_thresh=0.1, elev_tolerance=0, ignore_same_database=False, \n                    save=True, print_output=False\n                   ):\n    \"\"\"\n    Identify potential duplicate records in a dataset using metadata and time series similarity.\n\n    This is a simpler version of `find_duplicates_optimized`.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame containing proxy records with metadata. Expected columns include:\n        ['geo_meanLat', 'geo_meanLon', 'geo_meanElev', 'year', 'paleoData_values', \n         'archiveType', 'paleoData_proxy', 'geo_siteName', 'originalDatabase', \n         'originalDataURL', 'datasetId'].\n    dist_tolerance_km : float, optional\n        Maximum allowed geographical distance between duplicates (km). Default is 8.\n    n_points_thresh : int, optional\n        Minimum number of overlapping time points required. Default is 10.\n    corr_thresh : float, optional\n        Minimum correlation threshold for duplicate detection. Default is 0.9.\n    rmse_thresh : float, optional\n        Maximum RMSE allowed for duplicate detection. Default is 0.1.\n    corr_diff_thresh : float, optional\n        Minimum correlation of first differences threshold. Default is 0.9.\n    rmse_diff_thresh : float, optional\n        Maximum RMSE of first differences allowed. Default is 0.1.\n    elev_tolerance : float, optional\n        Maximum allowed elevation difference. Default is 0.\n    ignore_same_database : bool, optional\n        If True, ignores potential duplicates within the same database. Default is False.\n    save : bool, optional\n        If True, saves results as a CSV file. Default is True.\n    print_output : bool, optional\n        If True, prints progress and matching information. Default is False.\n\n    Returns\n    -------\n    pot_dup_inds : list of list of int\n        Indices of detected potential duplicates.\n    pot_dup_IDs : list of list of str\n        Dataset IDs of detected potential duplicates.\n    distances_km : numpy.ndarray\n        Matrix of pairwise distances between records.\n    correlations : numpy.ndarray\n        Matrix of pairwise correlations between records.\n    \"\"\"\n    n_proxies = len(df)\n    print(df.name)\n\n    #database_name = '_'.join(df.originalDatabase.unique())\n\n    # Loop through the proxies\n    distances_km    = np.zeros((n_proxies,n_proxies))\n    correlations    = np.zeros((n_proxies,n_proxies))\n    rmse            = np.zeros((n_proxies,n_proxies))\n    corr_diff       = np.zeros((n_proxies,n_proxies))\n    rmse_diff       = np.zeros((n_proxies,n_proxies))\n\n    n_pot_dups     = 0 # counts number of detected duplicates\n    pot_dup_inds   = [] # stores potential duplicate indices \n    pot_dup_IDs    = [] # stores potential duplicate IDs\n    pot_dup_corrs  = [] # stores potential duplicate correlations \n    pot_dup_dists  = [] # stores potential duplicate distances\n    pot_dup_meta   = [['index 1', 'index 2', 'ID 1', 'ID 2', 'correlation', 'distance (km)']]\n\n    print('Start duplicate search:')\n    print('='*33)\n    print('checking parameters:')\n    print('%-16s'%'proxy archive                ', ' : %-16s'%' must match')\n    print('%-16s'%'proxy type                   ', ' : %-16s'%' must match')\n    print('%-16s'%'distance (km)                ', ' &lt; %-16s'%str(dist_tolerance_km)) \n    print('%-16s'%'elevation                    ', ' : %-16s'%' must match')\n    print('%-16s'%'time overlap                 ', ' &gt; %-16s'%str(n_points_thresh))\n    print('%-16s'%'correlation                  ', ' &gt; %-16s'%str(corr_thresh))\n    print('%-16s'%'RMSE                         ', ' &lt; %-16s'%str(rmse_thresh))\n    print('%-16s'%'1st difference rmse          ', ' &lt; %-16s'%str(rmse_diff_thresh))\n    print('%-16s'%'correlation of 1st difference', ' &gt; %-16s'%str(corr_diff_thresh))\n    print('='*33)\n\n    ddir = 'data/%s/dup_detection/'%(df.name)\n    fn   = 'dup_detection_candidates_'+df.name\n    # fn   = 'pot_dup_%s_'+df.name\n\n    print('Start duplicate search')\n    # otherwise re-generate from scratch\n    for ii in range(n_proxies):\n        if ii in range(0, 10000, 10):\n            print('Progress: '+str(ii)+'/'+str(n_proxies))\n\n        # Location of proxy 1\n        lat_1   = df['geo_meanLat'].iloc[ii]\n        lon_1   = df['geo_meanLon'].iloc[ii]\n        # time and data of proxy 1\n        time_1_ma = df['year'].iloc[ii]\n        data_1_ma = np.array(df['paleoData_values'].iloc[ii])\n        if type(time_1_ma)==np.ma.core.MaskedArray:\n            time_1  = time_1_ma.data[~data_1_ma.mask]\n            data_1  = data_1_ma.data[~data_1_ma.mask]\n        else:\n            time_1  = time_1_ma\n            data_1  = data_1_ma\n        # archive and proxy type of proxy 1\n        arch_1  = df['archiveType'].iloc[ii].lower()\n        type_1  = df['paleoData_proxy'].iloc[ii].lower()\n        site_1  = df['geo_siteName'].iloc[ii].lower()\n        db_1    = df['originalDatabase'].iloc[ii].lower()\n        url_1   = df['originalDataURL'].iloc[ii].lower()\n        id_1    = df['datasetId'].iloc[ii].lower()\n        #\n        for jj in range(ii+1, n_proxies):\n            db_2 = df['originalDatabase'].iloc[jj].lower()\n            id_2    = df['datasetId'].iloc[jj].lower()\n\n            if (db_1==db_2) &amp; ignore_same_database: continue\n            # Location of proxy 2\n            lat_2 = df['geo_meanLat'].iloc[jj]\n            lon_2 = df['geo_meanLon'].iloc[jj]\n            # time and data of proxy 2\n            time_2_ma = df['year'].iloc[jj]\n            data_2_ma = np.array(df['paleoData_values'].iloc[jj])\n\n            # print(jj, db_2, type(time_2_ma), type(data_2_ma))\n\n            if type(time_2_ma)==np.ma.core.MaskedArray:\n                time_2  = time_2_ma.data[~data_2_ma.mask]\n                data_2  = data_2_ma.data[~data_2_ma.mask]\n            else:\n                time_2  = time_2_ma\n                data_2  = data_2_ma\n\n            # archive and proxy type of proxy 2\n            arch_2  = df['archiveType'].iloc[jj].lower()\n            type_2  = df['paleoData_proxy'].iloc[jj].lower()\n            site_2  = df['geo_siteName'].iloc[jj].lower()\n            db_2    = df['originalDatabase'].iloc[jj].lower()\n            url_2   = df['originalDataURL'].iloc[jj].lower()\n\n            # Calculate distance between proxy 1 and proxy 2 in km\n            distances_km[ii, jj] = geopy.distance.great_circle((lat_1,lon_1),\n                                                               (lat_2,lon_2)).km\n            # find shared time values \n            time_12, int_1, int_2 = np.intersect1d(time_1, time_2, return_indices=True) # possibly need to round these time series?\n\n            if time_12.shape[0]&lt;=n_points_thresh: \n                #print('%d|%d no shared time'%(ii,jj))\n                continue\n\n            # calculate correlation, rms, sum of differences between shared i and j data\n            # z_1 is the first difference between each datapoints for record 1\n            z_1  = (data_1[int_1][:-1]-data_1[int_1][1:])\n            z_1 -= np.mean(z_1)        \n            if np.std(z_1)!=0: \n                z_1 /= np.std(z_1)\n\n\n            # z_2 is the first difference between each datapoints for record 2\n            z_2  = (data_2[int_2][:-1]-data_2[int_2][1:])\n            z_2 -= np.mean(z_2)\n            if np.std(z_2)!=0: \n                z_2 /= np.std(z_2)\n\n\n            correlations[ii, jj] = np.corrcoef(np.vstack([data_1[int_1], data_2[int_2]]))[0,1]\n\n            rmse[ii, jj]         = np.sqrt(np.sum((data_1[int_1]-data_2[int_2])**2)/len(time_12))\n\n\n            rmse_diff[ii, jj]    = np.sqrt(np.sum((z_1-z_2)**2)/len(time_12))\n            corr_diff[ii, jj]    = np.corrcoef(np.vstack([z_1, z_2]))[0,1]\n\n\n\n            if ((np.isnan(correlations[ii,jj]) &amp; np.isnan(rmse[ii,jj]))|(np.isnan(corr_diff[ii,jj]) &amp; np.isnan(rmse_diff[ii,jj]))):\n                print('!!! %d|%d nan detected in both correlation and rmse of record data (or its 1st difference). Danger of missing duplicate!!'%(ii, jj))\n                for idd, dd in enumerate([correlations[ii,jj], rmse[ii,jj], rmse_diff[ii,jj],  corr_diff[ii,jj]]):\n                    if np.isnan(dd):\n                        print('nan detected in %s'%(['correlation','RMSE','RMSE of 1st difference', 'correlation of 1st difference'][idd]))\n\n            # DEFINE CRITERIA:\n\n            meta_crit         = (arch_1 == arch_2) &amp; (type_1 == type_2) # archive types and proxy types must agree\n            elevation_not_nan = ((~np.isnan(df['geo_meanElev'].iloc[ii]))&amp; ~np.isnan(df['geo_meanElev'].iloc[jj]))\n            elevation_dist    = np.abs(df['geo_meanElev'].iloc[ii]-df['geo_meanElev'].iloc[jj])\n\n\n            location_crit = ((distances_km[ii,jj] &lt;= dist_tolerance_km) &amp; \n                             (elevation_dist&lt;=elev_tolerance if elevation_not_nan else True))\n            #print('loc', location_crit)\n    # i,j are located within dist_tolerance_km of each others and have the same elevation (provided they're not nans)\n            overlap_crit  = ((len(time_12) &gt; n_points_thresh) |\n                             (len(time_1) &lt;= n_points_thresh) |\n                             (len(time_2)  &lt;= n_points_thresh) )  # we have a sufficient time overlap between the datasets \n            site_crit     = (np.any([s1 in s2 for s2 in site_2.split(' ') for s1 in site_1.split(' ')])| \n                             np.any([s1 in s2 for s2 in site_2.split('-') for s1 in site_1.split('-')])| \n                             np.any([s1 in s2 for s2 in site_2.split('_') for s1 in site_1.split('_')])|\n                             np.any([site_1 in site_2, site_2 in site_1])) # there is at least some overlap in the site name \n            corr_crit     = (((correlations[ii, jj] &gt; corr_thresh) | \n                             (corr_diff[ii, jj] &gt; corr_diff_thresh)) # correlation between shared datapoints exceeds correlation threshol dor correlation of first difference above threshold\n                             &amp; \n                             ((rmse[ii, jj] &lt; rmse_thresh) | \n                             (rmse_diff[ii, jj] &lt; rmse_diff_thresh))) #  RMSE of records or of first difference below threshold\n            url_crit       = (url_1==url_2 if db_1==db_2 else True)\n            if print_output:\n                print('--------')\n                print(id_1, id_2)\n                print('archive and proxy match: %s'%meta_crit)\n                print('site match: %s'%site_crit)\n                print('location match: %s'%location_crit)\n                print('correlation high: %s'%corr_crit)\n                print('overlap %s'%overlap_crit)\n            if (meta_crit &amp; site_crit #&amp; url_crit\n                &amp; location_crit\n                &amp; corr_crit\n                &amp; overlap_crit)|((correlations[ii, jj] &gt; 0.98)&amp; overlap_crit):\n                # if these criteria are satisfied, we found a possible duplicate!\n                n_pot_dups    += 1\n                pot_dup_inds  += [[ii, jj]]\n                pot_dup_IDs   += [df['datasetId'].iloc[[ii, jj]].values]\n                pot_dup_corrs += [[correlations[ii, jj]]]\n                pot_dup_dists += [[distances_km[ii,jj]]]\n                pot_dup_meta  += [[ii, jj, df['datasetId'].iloc[ii], df['datasetId'].iloc[jj], correlations[ii, jj], distances_km[ii,jj]]]\n                print('--&gt; Found potential duplicate: %d: %s&amp;%d: %s (n_potential_duplicates=%d)'%(ii, id_1, jj, id_2, n_pot_dups))\n\n    if save: \n        os.makedirs(ddir, exist_ok=True)\n        write_csv(pot_dup_meta, ddir+fn)\n\n    print('='*60)\n    print('Saved indices, IDs, distances, correlations in %s'%ddir)\n    print('='*60)\n    print('Detected %d possible duplicates in %s.'%(n_pot_dups, df.name))\n    print('='*60)\n    if len(pot_dup_inds)&lt;=10:\n        print('='*60)\n        print('Indices: %s'%(', ').join([str(pdi) for pdi in pot_dup_inds]))\n        print('IDs: %s'%(', ').join([pdi[0]+' + '+pdi[1] for pdi in pot_dup_IDs]))\n        print('='*60)\n    return #pot_dup_inds, pot_dup_IDs, distances_km, correlations\n</code></pre>"},{"location":"api/ut_duplicate_search/#dod2k_utilities.ut_duplicate_search.find_duplicates_optimized","title":"<code>find_duplicates_optimized(df, dist_tolerance_km=8, n_points_thresh=10, corr_thresh=0.9, rmse_thresh=0.1, corr_diff_thresh=0.9, rmse_diff_thresh=0.1, elev_tolerance=0, ignore_same_database=False, save=True, print_output=False, return_data=False)</code>","text":"<p>Identify potential duplicate records in a dataset based on metadata and time series similarity.</p> <p>Based on find_duplicates, optimized by Feng Zhu</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing proxy records with metadata. Expected columns include: ['geo_meanLat', 'geo_meanLon', 'geo_meanElev', 'year', 'paleoData_values',   'archiveType', 'paleoData_proxy', 'geo_siteName', 'originalDatabase',   'originalDataURL', 'datasetId'].</p> required <code>dist_tolerance_km</code> <code>float</code> <p>Maximum allowed geographical distance between duplicates (km). Default is 8.</p> <code>8</code> <code>n_points_thresh</code> <code>int</code> <p>Minimum number of overlapping time points required. Default is 10.</p> <code>10</code> <code>corr_thresh</code> <code>float</code> <p>Minimum correlation threshold for duplicate detection. Default is 0.9.</p> <code>0.9</code> <code>rmse_thresh</code> <code>float</code> <p>Maximum RMSE allowed for duplicate detection. Default is 0.1.</p> <code>0.1</code> <code>corr_diff_thresh</code> <code>float</code> <p>Minimum correlation of first differences threshold. Default is 0.9.</p> <code>0.9</code> <code>rmse_diff_thresh</code> <code>float</code> <p>Maximum RMSE of first differences allowed. Default is 0.1.</p> <code>0.1</code> <code>elev_tolerance</code> <code>float</code> <p>Maximum allowed elevation difference. Default is 0.</p> <code>0</code> <code>ignore_same_database</code> <code>bool</code> <p>If True, ignores potential duplicates within the same database. Default is False.</p> <code>False</code> <code>save</code> <code>bool</code> <p>If True, saves results as a CSV file. Default is True.</p> <code>True</code> <code>print_output</code> <code>bool</code> <p>If True, prints progress and matching information. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>pot_dup_inds</code> <code>list of list of int</code> <p>Indices of detected potential duplicates.</p> <code>pot_dup_IDs</code> <code>list of list of str</code> <p>Dataset IDs of detected potential duplicates.</p> <code>distances_km</code> <code>ndarray</code> <p>Matrix of pairwise distances between records.</p> <code>correlations</code> <code>ndarray</code> <p>Matrix of pairwise correlations between records.</p> Source code in <code>dod2k_utilities/ut_duplicate_search.py</code> <pre><code>def find_duplicates_optimized(df, dist_tolerance_km=8, n_points_thresh=10, \n                    corr_thresh=0.9, rmse_thresh=0.1, corr_diff_thresh=0.9,\n                    rmse_diff_thresh=0.1, elev_tolerance=0, ignore_same_database=False, \n                    save=True, print_output=False, return_data=False\n                   ):\n    \"\"\"\n    Identify potential duplicate records in a dataset based on metadata and time series similarity.\n\n    Based on find_duplicates, optimized by Feng Zhu\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame containing proxy records with metadata. Expected columns include:\n        ['geo_meanLat', 'geo_meanLon', 'geo_meanElev', 'year', 'paleoData_values', \n         'archiveType', 'paleoData_proxy', 'geo_siteName', 'originalDatabase', \n         'originalDataURL', 'datasetId'].\n    dist_tolerance_km : float, optional\n        Maximum allowed geographical distance between duplicates (km). Default is 8.\n    n_points_thresh : int, optional\n        Minimum number of overlapping time points required. Default is 10.\n    corr_thresh : float, optional\n        Minimum correlation threshold for duplicate detection. Default is 0.9.\n    rmse_thresh : float, optional\n        Maximum RMSE allowed for duplicate detection. Default is 0.1.\n    corr_diff_thresh : float, optional\n        Minimum correlation of first differences threshold. Default is 0.9.\n    rmse_diff_thresh : float, optional\n        Maximum RMSE of first differences allowed. Default is 0.1.\n    elev_tolerance : float, optional\n        Maximum allowed elevation difference. Default is 0.\n    ignore_same_database : bool, optional\n        If True, ignores potential duplicates within the same database. Default is False.\n    save : bool, optional\n        If True, saves results as a CSV file. Default is True.\n    print_output : bool, optional\n        If True, prints progress and matching information. Default is False.\n\n    Returns\n    -------\n    pot_dup_inds : list of list of int\n        Indices of detected potential duplicates.\n    pot_dup_IDs : list of list of str\n        Dataset IDs of detected potential duplicates.\n    distances_km : numpy.ndarray\n        Matrix of pairwise distances between records.\n    correlations : numpy.ndarray\n        Matrix of pairwise correlations between records.\n    \"\"\"\n    n_proxies = len(df)\n    print(df.name)\n\n    #database_name = '_'.join(df.originalDatabase.unique())\n\n    # Loop through the proxies\n    distances_km    = np.zeros((n_proxies,n_proxies))\n    correlations    = np.zeros((n_proxies,n_proxies))\n    rmse            = np.zeros((n_proxies,n_proxies))\n    corr_diff       = np.zeros((n_proxies,n_proxies))\n    rmse_diff       = np.zeros((n_proxies,n_proxies))\n\n    n_pot_dups     = 0 # counts number of detected duplicates\n    pot_dup_inds   = [] # stores potential duplicate indices \n    pot_dup_IDs    = [] # stores potential duplicate IDs\n    pot_dup_corrs  = [] # stores potential duplicate correlations \n    pot_dup_dists  = [] # stores potential duplicate distances\n    pot_dup_meta   = [['index 1', 'index 2', 'ID 1', 'ID 2', 'correlation', 'distance (km)']]\n\n    print('Start duplicate search:')\n    print('='*33)\n    print('checking parameters:')\n    print('%-16s'%'proxy archive                ', ' : %-16s'%' must match')\n    print('%-16s'%'proxy type                   ', ' : %-16s'%' must match')\n    print('%-16s'%'distance (km)                ', ' &lt; %-16s'%str(dist_tolerance_km)) \n    print('%-16s'%'elevation                    ', ' : %-16s'%' must match')\n    print('%-16s'%'time overlap                 ', ' &gt; %-16s'%str(n_points_thresh))\n    print('%-16s'%'correlation                  ', ' &gt; %-16s'%str(corr_thresh))\n    print('%-16s'%'RMSE                         ', ' &lt; %-16s'%str(rmse_thresh))\n    print('%-16s'%'1st difference rmse          ', ' &lt; %-16s'%str(rmse_diff_thresh))\n    print('%-16s'%'correlation of 1st difference', ' &gt; %-16s'%str(corr_diff_thresh))\n    print('='*33)\n\n    ddir = 'data/%s/dup_detection/'%(df.name)\n    fn   = 'dup_detection_candidates_'+df.name\n\n\n    print('Start duplicate search')\n    # otherwise re-generate from scratch\n    for ii in range(n_proxies):\n        if ii in range(0, 10000, 10):\n            print('Progress: '+str(ii)+'/'+str(n_proxies))\n\n        # Location of proxy 1\n        lat_1   = df['geo_meanLat'].iloc[ii]\n        lon_1   = df['geo_meanLon'].iloc[ii]\n        # time and data of proxy 1\n        time_1_ma = df['year'].iloc[ii]\n        data_1_ma = np.array(df['paleoData_values'].iloc[ii])\n        if type(time_1_ma)==np.ma.core.MaskedArray:\n            time_1  = time_1_ma.data[~data_1_ma.mask]\n            data_1  = data_1_ma.data[~data_1_ma.mask]\n        else:\n            time_1  = time_1_ma\n            data_1  = data_1_ma\n        # archive and proxy type of proxy 1\n        arch_1  = df['archiveType'].iloc[ii].lower()\n        type_1  = df['paleoData_proxy'].iloc[ii].lower()\n        site_1  = df['geo_siteName'].iloc[ii].lower()\n        db_1    = df['originalDatabase'].iloc[ii].lower()\n        url_1   = df['originalDataURL'].iloc[ii].lower()\n        id_1    = df['datasetId'].iloc[ii].lower()\n        #\n        for jj in range(ii+1, n_proxies):\n            db_2 = df['originalDatabase'].iloc[jj].lower()\n            id_2    = df['datasetId'].iloc[jj].lower()\n\n            if (db_1==db_2) &amp; ignore_same_database: continue\n            # Location of proxy 2\n            lat_2 = df['geo_meanLat'].iloc[jj]\n            lon_2 = df['geo_meanLon'].iloc[jj]\n            # time and data of proxy 2\n            time_2_ma = df['year'].iloc[jj]\n            data_2_ma = np.array(df['paleoData_values'].iloc[jj])\n\n\n            if type(time_2_ma)==np.ma.core.MaskedArray:\n                time_2  = time_2_ma.data[~data_2_ma.mask]\n                data_2  = data_2_ma.data[~data_2_ma.mask]\n            else:\n                time_2  = time_2_ma\n                data_2  = data_2_ma\n\n            # archive and proxy type of proxy 2\n            arch_2  = df['archiveType'].iloc[jj].lower()\n            type_2  = df['paleoData_proxy'].iloc[jj].lower()\n            site_2  = df['geo_siteName'].iloc[jj].lower()\n            db_2    = df['originalDatabase'].iloc[jj].lower()\n            url_2   = df['originalDataURL'].iloc[jj].lower()\n\n            ### Criterion on matching metadata\n            meta_crit         = (arch_1 == arch_2) &amp; (type_1 == type_2) # archive types and proxy types must agree\n            if not meta_crit: continue\n\n            ### Criterion on elevation\n            elevation_not_nan = ((~np.isnan(df['geo_meanElev'].iloc[ii]))&amp; ~np.isnan(df['geo_meanElev'].iloc[jj]))\n            elevation_dist    = np.abs(df['geo_meanElev'].iloc[ii]-df['geo_meanElev'].iloc[jj])\n            if not elevation_not_nan and elevation_dist&gt;elev_tolerance: continue\n\n            ### Criterion on time overlap\n            time_12, int_1, int_2 = np.intersect1d(time_1, time_2, return_indices=True) # possibly need to round these time series?\n            if time_12.shape[0]&lt;=n_points_thresh: continue\n\n            ### Criterion on distance\n            # Calculate distance between proxy 1 and proxy 2 in km\n            distances_km[ii, jj] = geopy.distance.great_circle((lat_1,lon_1), (lat_2,lon_2)).km\n            if distances_km[ii,jj] &gt; dist_tolerance_km: continue\n\n            # calculate correlation, rms, sum of differences between shared i and j data\n            z_1  = (data_1[int_1][:-1]-data_1[int_1][1:])\n            z_1 -= np.mean(z_1)        \n            if np.std(z_1)!=0: \n                z_1 /= np.std(z_1)\n            # else:\n            #     plt.figure()\n            #     plt.scatter(time_1, data_1, label='data_1 (%s)'%id_1)\n            #     plt.scatter(time_12, data_1[int_1], label='data_1 SHARED TIME (%s)'%id_1)\n            #     plt.scatter(time_2, data_2, label='data_2 (%s)'%id_2)\n            #     plt.scatter(time_12, data_2[int_2], label='data_2 SHARED TIME (%s)'%id_2)\n\n            #     plt.legend()\n            #     plt.show()\n\n\n            z_2  = (data_2[int_2][:-1]-data_2[int_2][1:])\n            z_2 -= np.mean(z_2)\n            if np.std(z_2)!=0: \n                z_2 /= np.std(z_2)\n            # else:\n\n            #     plt.figure()\n\n            #     plt.scatter(time_12, data_1[int_1]-np.mean(data_1[int_1]), label='data_1 SHARED TIME (%s)'%id_1)\n\n            #     plt.scatter(time_12, data_2[int_2]-np.mean(data_2[int_2]), label='data_2 SHARED TIME (%s)'%id_2)\n\n            #     plt.legend()\n            #     plt.show()\n\n\n            correlations[ii, jj] = np.corrcoef(np.vstack([data_1[int_1], data_2[int_2]]))[0,1]\n\n            rmse[ii, jj]         = np.sqrt(np.sum((data_1[int_1]-data_2[int_2])**2)/len(time_12))\n\n\n            rmse_diff[ii, jj]    = np.sqrt(np.sum((z_1-z_2)**2)/len(time_12))\n            corr_diff[ii, jj]    = np.corrcoef(np.vstack([z_1, z_2]))[0,1]\n\n\n\n            if ((np.isnan(correlations[ii,jj]) &amp; np.isnan(rmse[ii,jj]))|(np.isnan(corr_diff[ii,jj]) &amp; np.isnan(rmse_diff[ii,jj]))):\n                print('!!! %d|%d nan detected in both correlation and rmse of record data (or its 1st difference). Danger of missing duplicate!!'%(ii, jj))\n                for idd, dd in enumerate([correlations[ii,jj], rmse[ii,jj], rmse_diff[ii,jj],  corr_diff[ii,jj]]):\n                    if np.isnan(dd):\n                        print('nan detected in %s'%(['correlation','RMSE','RMSE of 1st difference', 'correlation of 1st difference'][idd]))\n\n            # DEFINE CRITERIA:\n\n\n    # i,j are located within dist_tolerance_km of each others and have the same elevation (provided they're not nans)\n            overlap_crit  = ((len(time_12) &gt; n_points_thresh) |\n                             (len(time_1) &lt;= n_points_thresh) |\n                             (len(time_2)  &lt;= n_points_thresh) )  # we have a sufficient time overlap between the datasets \n            site_crit     = np.any([s1 in s2 for s2 in site_2.split(' ') for s1 in site_1.split(' ')])| np.any([s1 in s2 for s2 in site_2.split('-') for s1 in site_1.split('-')])| np.any([s1 in s2 for s2 in site_2.split('_') for s1 in site_1.split('_')])|np.any([site_1 in site_2, site_2 in site_1]) # there is at least some overlap in the site name \n            corr_crit     = (((correlations[ii, jj] &gt; corr_thresh) | \n                             (corr_diff[ii, jj] &gt; corr_diff_thresh)) # correlation between shared datapoints exceeds correlation threshol dor correlation of first difference above threshold\n                             &amp; \n                             ((rmse[ii, jj] &lt; rmse_thresh) | \n                             (rmse_diff[ii, jj] &lt; rmse_diff_thresh))) #  RMSE of records or of first difference below threshold\n            url_crit       = (url_1==url_2 if db_1==db_2 else True)\n            if print_output:\n                print('--------')\n                print(id_1, id_2)\n                print('archive and proxy match: %s'%meta_crit)\n                print('site match: %s'%site_crit)\n                # print('location match: %s'%location_crit)\n                print('correlation high: %s'%corr_crit)\n                print('overlap %s'%overlap_crit)\n            if (meta_crit &amp; site_crit #&amp; url_crit\n                # &amp; location_crit\n                &amp; corr_crit\n                &amp; overlap_crit)|((correlations[ii, jj] &gt; 0.98)&amp; overlap_crit):\n                # if these criteria are satisfied, we found a possible duplicate!\n                n_pot_dups    += 1\n                pot_dup_inds  += [[ii, jj]]\n                pot_dup_IDs   += [df['datasetId'].iloc[[ii, jj]].values]\n                pot_dup_corrs += [[correlations[ii, jj]]]\n                pot_dup_dists += [[distances_km[ii,jj]]]\n                pot_dup_meta  += [[ii, jj, df['datasetId'].iloc[ii], df['datasetId'].iloc[jj], correlations[ii, jj], distances_km[ii,jj]]]\n                print('--&gt; Found potential duplicate: %d: %s&amp;%d: %s (n_potential_duplicates=%d)'%(ii, id_1, jj, id_2, n_pot_dups))\n\n    if save: \n        os.makedirs(ddir, exist_ok=True)\n        write_csv(pot_dup_meta, ddir+fn)\n\n        print('='*60)\n        print('Saved indices, IDs, distances, correlations in %s'%ddir)\n    print('='*60)\n    print('Detected %d possible duplicates in %s.'%(n_pot_dups, df.name))\n    print('='*60)\n    if len(pot_dup_inds)&lt;=10:\n        print('='*60)\n        print('Indices: %s'%(', ').join([str(pdi) for pdi in pot_dup_inds]))\n        print('IDs: %s'%(', ').join([pdi[0]+' + '+pdi[1] for pdi in pot_dup_IDs]))\n        print('='*60)\n    if return_data:\n        return pot_dup_IDs\n    return #pot_dup_inds, pot_dup_IDs, distances_km, correlations\n</code></pre>"},{"location":"api/ut_duplicate_search/#dod2k_utilities.ut_duplicate_search.join_composites_metadata","title":"<code>join_composites_metadata(df, comp_ID_pairs, df_decisions, header)</code>","text":"<p>Create composite records from overlapping duplicate proxy datasets and generate metadata.</p> <p>This function combines pairs of proxy records that were identified as duplicates and decided to be composited. It standardizes the data as z-scores, averages overlapping  periods, merges metadata, and generates a composite DataFrame. A scatter plot of the  original and composite data is created and saved for visual inspection.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Original DataFrame containing proxy data and metadata. Must include columns:     - 'paleoData_values' : proxy values     - 'year' : time vector     - 'geo_meanLat', 'geo_meanLon', 'geo_meanElev' : site coordinates     - 'archiveType', 'geo_siteName', 'paleoData_proxy' : metadata     - 'climateInterpretation_variable', 'dataSetName', 'originalDatabase', 'originalDataURL', 'paleoData_notes', 'duplicateDetails'     - 'datasetId' : unique record identifier</p> required <code>comp_ID_pairs</code> <code>DataFrame</code> <p>DataFrame listing pairs of record IDs to be composited. Must include columns:     - 'datasetId 1', 'datasetId 2'     - 'originalDatabase 1', 'originalDatabase 2'     - 'Decision type', 'Decision comment'</p> required <code>df_decisions</code> <code>DataFrame</code> <p>DataFrame containing the decisions made during duplicate evaluation. Used to annotate the composite metadata with decision type and comments.</p> required <code>header</code> <code>list</code> <p>Metadata header from the duplicate decision process. Used for documenting operator details in the composite notes.</p> required <p>Returns:</p> Name Type Description <code>df_comp</code> <code>DataFrame</code> <p>A new DataFrame containing the composited proxy records, including:     - Combined 'paleoData_values' as z-scores     - Merged 'year' vector     - Updated metadata fields, including a composite 'datasetId'     - Detailed 'duplicateDetails' recording the composition process</p> Notes <ul> <li>For numerical metadata that differs between records, the mean is taken.</li> <li>For categorical metadata, entries are concatenated into a composite string.</li> <li>Overlapping periods are averaged, and non-overlapping periods are appended.</li> <li>A scatter plot is generated for each composite showing the original records   and the resulting composite, and it is saved using <code>save_fig</code>.</li> <li>The function maintains provenance of original datasets, including notes and URLs,   in the 'duplicateDetails' field.</li> </ul> Source code in <code>dod2k_utilities/ut_duplicate_search.py</code> <pre><code>def join_composites_metadata(df, comp_ID_pairs, df_decisions, header):\n    \"\"\"\n    Create composite records from overlapping duplicate proxy datasets and generate metadata.\n\n    This function combines pairs of proxy records that were identified as duplicates\n    and decided to be composited. It standardizes the data as z-scores, averages overlapping \n    periods, merges metadata, and generates a composite DataFrame. A scatter plot of the \n    original and composite data is created and saved for visual inspection.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Original DataFrame containing proxy data and metadata. Must include columns:\n            - 'paleoData_values' : proxy values\n            - 'year' : time vector\n            - 'geo_meanLat', 'geo_meanLon', 'geo_meanElev' : site coordinates\n            - 'archiveType', 'geo_siteName', 'paleoData_proxy' : metadata\n            - 'climateInterpretation_variable', 'dataSetName', 'originalDatabase', 'originalDataURL', 'paleoData_notes', 'duplicateDetails'\n            - 'datasetId' : unique record identifier\n\n    comp_ID_pairs : pd.DataFrame\n        DataFrame listing pairs of record IDs to be composited. Must include columns:\n            - 'datasetId 1', 'datasetId 2'\n            - 'originalDatabase 1', 'originalDatabase 2'\n            - 'Decision type', 'Decision comment'\n\n    df_decisions : pd.DataFrame\n        DataFrame containing the decisions made during duplicate evaluation.\n        Used to annotate the composite metadata with decision type and comments.\n\n    header : list\n        Metadata header from the duplicate decision process. Used for documenting\n        operator details in the composite notes.\n\n    Returns\n    -------\n    df_comp : pd.DataFrame\n        A new DataFrame containing the composited proxy records, including:\n            - Combined 'paleoData_values' as z-scores\n            - Merged 'year' vector\n            - Updated metadata fields, including a composite 'datasetId'\n            - Detailed 'duplicateDetails' recording the composition process\n\n    Notes\n    -----\n    - For numerical metadata that differs between records, the mean is taken.\n    - For categorical metadata, entries are concatenated into a composite string.\n    - Overlapping periods are averaged, and non-overlapping periods are appended.\n    - A scatter plot is generated for each composite showing the original records\n      and the resulting composite, and it is saved using `save_fig`.\n    - The function maintains provenance of original datasets, including notes and URLs,\n      in the 'duplicateDetails' field.\n    \"\"\"\n\n    # create composite dataframe\n    df_comp = pd.DataFrame()\n\n    for ii in comp_ID_pairs.index:\n\n        # if ii not in df.datasetId.values: \n        #     print(ii)\n        #     print(df.datasetId.values)\n        #     print(f'Skip {ii} - allowed for testing only!!')\n        #     continue\n\n        row = {}\n        ID_1, ID_2   = comp_ID_pairs.loc[ii, ['datasetId 1', 'datasetId 2']]\n        db_1, db_2   = comp_ID_pairs.loc[ii, ['originalDatabase 1', 'originalDatabase 2']]\n        dec_1, dec_2 = comp_ID_pairs.loc[ii, ['Decision 1', 'Decision 2']]\n        print(ID_1, ID_2)\n\n        # metadata should match exactly for the following columns (check), if not choose metadata values:\n        add_dup_note = ''\n        for key in ['archiveType', 'geo_meanElev', 'geo_meanLat', 'geo_meanLon',\n                    'geo_siteName', 'paleoData_proxy', 'yearUnits', 'interpretation_variable', 'interpretation_direction', 'interpretation_seasonality']:\n\n            md1, md2 = df.at[ID_1, key], df.at[ID_2, key]\n\n            if isinstance(md1, (float, np.floating, np.float32, np.float64)):\n                md1 = np.round(md1, 3)\n\n            if isinstance(md2, (float, np.floating, np.float32, np.float64)):\n                md2 = np.round(md2, 3)\n\n            if md1==md2:\n                row[key]=md1\n                add_dup_note += f'{key}: Metadata identical'\n            else:\n                print('--------------------------------------------------------------------------------')\n                add_dup_note = 'Metadata differs for %s in original records: %s (%s) and %s (%s). '%(key, ID_1, str(df.at[ID_1, key]), \n                                                                                 ID_2, str(df.at[ID_2, key]))\n                print('Metadata different for &gt;&gt;&gt;%s&lt;&lt;&lt; in: %s (%s) and %s (%s). '%(key, ID_1, str(df.at[ID_1, key]), \n                                                                                 ID_2, str(df.at[ID_2, key])))\n                try:\n                    entry='COMPOSITE: '+df.at[ID_1, key]+' + '+df.at[ID_2, key]\n                    add_dup_note += f'{key}: Metadata composited to: '+entry\n                    if key in ['interpretation_variable', 'interpretation_direction', 'interpretation_seasonality']:\n                        if df.at[ID_1, key]=='N/A': entry=df.at[ID_2, key]\n                        if df.at[ID_2, key]=='N/A': entry=df.at[ID_1, key]\n                        if (df.at[ID_1, key] in df.at[ID_2, key]):  entry=df.at[ID_2, key]\n                        if (df.at[ID_2, key] in df.at[ID_1, key]):  entry=df.at[ID_1, key]\n                    loop=False\n                except TypeError:\n                    # print('Can not create composites for numerical metadata! Create average instead.')\n                    # av = input('Type [y] if you want to average the metadata. Otherwise type [n].')\n                    # if av.lower() in ['y', 'yes']:\n                    entry = np.mean([df.at[ID_1, key], df.at[ID_2, key]])                        \n                    add_dup_note += f'{key}: Metadata averaged to: '+str(entry)\n                    loop=False\n                row[key]= entry\n            # print('Add the following note to duplicateDetails:', add_dup_note)\n        # create new entries for the following columns:\n        # create z-scores of data\n        data_1, data_2 = np.array(df.at[ID_1, 'paleoData_values']), np.array(df.at[ID_2, 'paleoData_values'])\n\n        # year\n        time_1, time_2 = np.array(df.at[ID_1, 'year']), np.array(df.at[ID_2, 'year'])\n        time_12, ii_1, ii_2 = np.intersect1d(time_1, time_2, return_indices=True)\n        ii_1x   = [ii for ii in range(len(time_1)) if ii not in ii_1]\n        ii_2x   = [ii for ii in range(len(time_2)) if ii not in ii_2]\n\n        data_1 /= np.std(data_1[ii_1])\n        data_1 -= np.mean(data_1[ii_1])\n        data_2 /= np.std(data_2[ii_2])\n        data_2 -= np.mean(data_2[ii_2])\n\n        data_12 = (data_1[ii_1]+data_2[ii_2])/2.\n\n        data = list(data_1[ii_1x])+list(data_12)+list(data_2[ii_2x])\n        time = list(time_1[ii_1x])+list(time_12)+list(time_2[ii_2x])\n\n        row['paleoData_values'] = data\n        row['year'] = time\n\n        fig = plt.figure(figsize=(6, 2), dpi=100)\n\n        plt.scatter(time_1, data_1, s=20, color='tab:blue', label=ID_1)\n        plt.scatter(time_2, data_2, s=20, color='tab:orange', label=ID_2)\n        plt.scatter(time, data, s=10, color='k', label='composite')\n        plt.legend()\n        plt.show()\n        save_fig(fig, 'composite_%s_%s'%(ID_1, ID_2), dir='/%s/dup_detection/'%df.name)\n\n\n        # new metadata for identification etc.\n        row['dataSetName']      = df.at[ID_1, 'dataSetName']+', '+df.at[ID_2, 'dataSetName']\n        row['originalDatabase'] = 'dod2k_composite_z'\n        row['originalDataURL']  = '%s: %s, %s: %s'%(ID_1, df.at[ID_1, 'originalDataURL'], ID_2, df.at[ID_2, 'originalDataURL'])\n        row['paleoData_notes']  = '%s: %s, %s: %s'%(ID_1, df.at[ID_1, 'paleoData_notes'], ID_2, df.at[ID_2, 'paleoData_notes'])\n        row['interpretation_variableDetail']  = '%s: %s, %s: %s'%(ID_1, df.at[ID_1, 'interpretation_variableDetail'], ID_2, df.at[ID_2, 'interpretation_variableDetail'])\n        row['datasetId']        = 'dod2k_composite_z_'+ID_1.replace('dod2k_composite_z_','')+'_'+ID_2.replace('dod2k_composite_z_','')   \n        row['paleoData_units']  = 'z-scores'\n\n\n        # save details on duplicates in duplicateDetails:\n        row['duplicateDetails']={'duplicate ID': f'{ID_1} and {ID_2}',\n                                 'duplicate database': f'{db_1} and {db_2}',\n                                 'duplicate decision': 'COMPOSITE',\n                                 'decision type': df_decisions.loc[ii, 'Decision type']}\n\n\n        # save details on composition process in duplicateDetails:\n        row['duplicateDetails']['composite details'] = add_dup_note\n\n\n        # save operator details\n        if comp_ID_pairs.at[ii, 'Decision type']=='MANUAL': \n            operator_details = ' '.join(header[1:]).replace(' Modified ','')[:-2].replace(':','').replace('  E-Mail', '')\n            row['duplicateDetails']['operator'] = operator_details\n            row['duplicateDetails']['note'] = comp_ID_pairs.at[ii, 'Decision comment']\n\n\n\n        # migrate original duplicate details\n\n        row['duplicateDetails']['original duplicate details'] = {ID_1: df.at[ID_1, 'duplicateDetails'], ID_2: df.at[ID_2, 'duplicateDetails']}\n\n\n        # create dataframe for composites\n        df_comp = pd.concat([df_comp, pd.DataFrame({kk: [vv] for kk, vv in row.items()})], ignore_index = True, axis=0)\n\n        # print(df_comp)\n    return df_comp\n</code></pre>"},{"location":"api/ut_duplicate_search/#dod2k_utilities.ut_duplicate_search.plot_duplicates","title":"<code>plot_duplicates(df, save_figures=True, write_output=True, display=False)</code>","text":"<p>Generate plots of potential duplicate records in a proxy database and optionally save their metadata.</p> <p>This function identifies potential duplicate entries in a paleoclimate proxy dataset, visualizes them using the <code>dup_plot</code> function, and saves the metadata of duplicates as a CSV.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe containing proxy data and metadata. Expected columns include:     - 'geo_meanLat', 'geo_meanLon' : Latitude and longitude of the site.     - 'year', 'paleoData_values' : Time vector and proxy values.     - 'archiveType', 'paleoData_proxy' : Archive type and proxy type.     - 'datasetId' : Unique identifier or dataset name.     - 'geo_siteName' : Site name.     - 'originalDatabase' : Name of the original database.</p> required <code>save_figures</code> <code>bool</code> <p>If True, save the generated figures to disk. Default is True.</p> <code>True</code> <code>write_output</code> <code>bool</code> <p>If True, save the duplicate metadata to a CSV file. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>The function primarily generates plots and writes metadata; it does not return any objects.</p> Notes <ul> <li>The function assumes that potential duplicates have already been identified by a separate   process (e.g., <code>find_duplicates</code>) and that a corresponding CSV exists in <code>df.name/dup_detection/</code>.</li> <li>Metadata printed and saved includes site names, coordinates, dataset IDs, original database,   and summary statistics (mean, std, units, etc.).</li> <li>Figures are saved in PDF format by default using <code>save_fig</code>.</li> </ul> Source code in <code>dod2k_utilities/ut_duplicate_search.py</code> <pre><code>def plot_duplicates(df, save_figures=True, write_output=True, display=False):\n    \"\"\"\n    Generate plots of potential duplicate records in a proxy database and optionally save their metadata.\n\n    This function identifies potential duplicate entries in a paleoclimate proxy dataset,\n    visualizes them using the `dup_plot` function, and saves the metadata of duplicates as a CSV.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        The dataframe containing proxy data and metadata. Expected columns include:\n            - 'geo_meanLat', 'geo_meanLon' : Latitude and longitude of the site.\n            - 'year', 'paleoData_values' : Time vector and proxy values.\n            - 'archiveType', 'paleoData_proxy' : Archive type and proxy type.\n            - 'datasetId' : Unique identifier or dataset name.\n            - 'geo_siteName' : Site name.\n            - 'originalDatabase' : Name of the original database.\n    save_figures : bool, optional\n        If True, save the generated figures to disk. Default is True.\n    write_output : bool, optional\n        If True, save the duplicate metadata to a CSV file. Default is True.\n\n    Returns\n    -------\n    None\n        The function primarily generates plots and writes metadata; it does not return any objects.\n\n    Notes\n    -----\n    - The function assumes that potential duplicates have already been identified by a separate\n      process (e.g., `find_duplicates`) and that a corresponding CSV exists in `df.name/dup_detection/`.\n    - Metadata printed and saved includes site names, coordinates, dataset IDs, original database,\n      and summary statistics (mean, std, units, etc.).\n    - Figures are saved in PDF format by default using `save_fig`.\n    \"\"\"\n\n\n    keys_to_print = ['originalDatabase', 'originalDataURL', 'datasetId', 'archiveType', 'proxy | variableName', #'archive|proxy',\n                     'geo_siteName', 'lat | lon | elev', 'mean | std | units', 'year' ]\n\n    ddir = 'data/%s/dup_detection/'%(df.name)+'dup_detection_candidates_'+df.name\n    # load the potential duplicate data as found in find_duplicates function\n    pot_dup_meta, head = read_csv(ddir, header=True)\n\n    pot_dup_inds   = np.array(np.array(pot_dup_meta)[:, :2], dtype=int)\n    pot_dup_corrs  = np.array(np.array(pot_dup_meta)[:, 4], dtype=float)\n    pot_dup_dists  = np.array(np.array(pot_dup_meta)[:, 5], dtype=float)\n\n    n_pot_dups   = pot_dup_inds.shape[0]\n\n    dup_mdata  = []\n    dup_mdata += [['ind 1', 'ind 2']+[ki+' %d'%ii_kk for kk in keys_to_print for ki in kk.split('|')  for ii_kk in [1, 2]]]\n\n    # loop through the potential duplicates.\n    for i_pot_dups, (ii, jj) in enumerate(pot_dup_inds):\n        dup_mdata_row    = [ii,jj]\n        # data and metadata associated with the two potential duplicates\n        id_1, id_2       = df['datasetId'].iloc[[ii, jj]]\n        time_1, time_2   = df['year'].iloc[[ii, jj]]\n        time_12, int_1, int_2 = np.intersect1d(time_1, time_2, return_indices=True) # possibly need to round these time series?\n        data_1           = np.array(df['paleoData_values'].iloc[ii])\n        data_2           = np.array(df['paleoData_values'].iloc[jj])\n\n        lat_1, lat_2     = df['geo_meanLat'].iloc[[ii, jj]]\n        lon_1, lon_2     = df['geo_meanLon'].iloc[[ii, jj]]\n        site_1, site_2   = df['geo_siteName'].iloc[[ii, jj]]\n        or_db_1, or_db_2 = df['originalDatabase'].iloc[[ii, jj]]\n\n        print('&gt; %d/%d'%(i_pot_dups, n_pot_dups), #set_txt, \n               id_1, id_2, #var_1,var_2, season_1,season_2,\n               pot_dup_dists[i_pot_dups], pot_dup_corrs[i_pot_dups],\n               sep=',')\n\n        # Plot observations\n        #-----------------------------------------------------\n\n        # Metadata display parameters\n        fig, dup_mdata_row = dup_plot(df, ii, jj, id_1, id_2, time_1, time_2, time_12, data_1, \n                                      data_2, int_1, int_2, pot_dup_corrs[i_pot_dups],\n                                      keys_to_print, dup_mdata_row)\n\n\n        dup_mdata += [dup_mdata_row]\n        plt.show(block=False)\n        if save_figures:\n            save_fig(fig, f'%03d_{id_1}_{id_2}'%i_pot_dups+'_'+f'_{ii}_{jj}', dir='%s/dup_detection'%df.name, figformat='pdf')\n        if not display:\n            plt.close(fig)\n\n\n        print('=== POTENTIAL DUPLICATE %d/%d'%(i_pot_dups, n_pot_dups)+': %s+%s ==='%(df['datasetId'].iloc[ii], df['datasetId'].iloc[jj]))\n    return \n</code></pre>"},{"location":"api/ut_functions/","title":"Utility functions","text":"<p>Module containing utility functions used across all notebooks.</p>"},{"location":"api/ut_functions/#dod2k_utilities.ut_functions","title":"<code>dod2k_utilities.ut_functions</code>","text":"<p>Created on Fri Jun 23 09:42:11 2023</p> <p>Author: Lucie Luecke</p> <p>Utility functions for loading, saving and cleaning up datasets.</p>"},{"location":"api/ut_functions/#dod2k_utilities.ut_functions.cleanup","title":"<code>cleanup(string)</code>","text":"<p>Remove special characters and leading/trailing whitespace from a string.</p> <p>Parameters:</p> Name Type Description Default <code>string</code> <code>str</code> <p>Input string to clean.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Cleaned string with '#', tabs, newlines removed and whitespace stripped.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; cleanup('#  Temperature  \\n')\n'Temperature'\n&gt;&gt;&gt; cleanup('\\t Data \\t')\n'Data'\n</code></pre> Source code in <code>dod2k_utilities/ut_functions.py</code> <pre><code>def cleanup(string):\n    \"\"\"\n    Remove special characters and leading/trailing whitespace from a string.\n\n    Parameters\n    ----------\n    string : str\n        Input string to clean.\n\n    Returns\n    -------\n    str\n        Cleaned string with '#', tabs, newlines removed and whitespace stripped.\n\n    Examples\n    --------\n    &gt;&gt;&gt; cleanup('#  Temperature  \\\\n')\n    'Temperature'\n    &gt;&gt;&gt; cleanup('\\\\t Data \\\\t')\n    'Data'\n    \"\"\"\n    string = string.replace('#', '')\n    string = string.replace('\\t', '')\n    string = string.replace('\\n', '')\n    while string.startswith(' '): string = string[1:]\n    while string.endswith(' '): string = string[:-1]\n    return string\n</code></pre>"},{"location":"api/ut_functions/#dod2k_utilities.ut_functions.conv_nan","title":"<code>conv_nan(value)</code>","text":"<p>Convert NaN values to missing data indicator (-9999.99).</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>float or str</code> <p>Value to check and potentially convert.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Original value if valid, or -9999.99 if NaN/missing.</p> Notes <p>Values already equal to -9999.99 are preserved to avoid double-conversion.</p> Source code in <code>dod2k_utilities/ut_functions.py</code> <pre><code>def conv_nan(value):\n    \"\"\"\n    Convert NaN values to missing data indicator (-9999.99).\n\n    Parameters\n    ----------\n    value : float or str\n        Value to check and potentially convert.\n\n    Returns\n    -------\n    float\n        Original value if valid, or -9999.99 if NaN/missing.\n\n    Notes\n    -----\n    Values already equal to -9999.99 are preserved to avoid double-conversion.\n    \"\"\"\n    if value=='nan' or np.isnan(value):\n        return -9999.99\n    elif np.round(value, 2)==-9999.99:\n        return value\n    else:\n        return value\n</code></pre>"},{"location":"api/ut_functions/#dod2k_utilities.ut_functions.convert_to_float","title":"<code>convert_to_float(txt)</code>","text":"<p>Convert input to a float, returning a sentinel value on failure.</p> <p>Parameters:</p> Name Type Description Default <code>txt</code> <code>any</code> <p>Input to be converted to a float. Typically a string, but any object accepted by <code>float()</code> is allowed.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Parsed floating-point value if conversion succeeds. Returns <code>-9999.99</code> if conversion fails for any reason (e.g., invalid string, None, incompatible type).</p> Source code in <code>dod2k_utilities/ut_functions.py</code> <pre><code>def convert_to_float(txt):\n    \"\"\"\n    Convert input to a float, returning a sentinel value on failure.\n\n    Parameters\n    ----------\n    txt : any\n        Input to be converted to a float. Typically a string, but any\n        object accepted by ``float()`` is allowed.\n\n    Returns\n    -------\n    float\n        Parsed floating-point value if conversion succeeds.\n        Returns ``-9999.99`` if conversion fails for any reason\n        (e.g., invalid string, None, incompatible type).\n    \"\"\"\n    try:\n        return float(txt)\n    except:\n        return -9999.99\n</code></pre>"},{"location":"api/ut_functions/#dod2k_utilities.ut_functions.convert_to_nparray","title":"<code>convert_to_nparray(data)</code>","text":"<p>Convert data array to masked array with missing values marked.</p> <p>Converts NaN values and 'nan' strings to -9999.99, then creates a masked array with these values masked out.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array - like</code> <p>Input data that may contain NaN or 'nan' string values.</p> required <p>Returns:</p> Type Description <code>MaskedArray</code> <p>Masked array with -9999.99 values masked.</p> Source code in <code>dod2k_utilities/ut_functions.py</code> <pre><code>def convert_to_nparray(data):\n    \"\"\"\n    Convert data array to masked array with missing values marked.\n\n    Converts NaN values and 'nan' strings to -9999.99, then creates a\n    masked array with these values masked out.\n\n    Parameters\n    ----------\n    data : array-like\n        Input data that may contain NaN or 'nan' string values.\n\n    Returns\n    -------\n    numpy.ma.MaskedArray\n        Masked array with -9999.99 values masked.\n    \"\"\"\n    data = np.array([conv_nan(vv) for vv in data])#, -9999.99)\n    mask = np.round(data, 2)==-9999.99#np.array([False if np.round(vv, 2)!=-9999.99 else True for vv in data ])\n    return np.ma.masked_array(data, mask)\n</code></pre>"},{"location":"api/ut_functions/#dod2k_utilities.ut_functions.figsave","title":"<code>figsave(fig, name, trans=False, add='/', fc='white', form='pdf', close=False, addfigs='/figs/')</code>","text":"<p>Save a matplotlib figure to file with optional formats.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Figure object to save.</p> required <code>name</code> <code>str</code> <p>Filename without extension.</p> required <code>trans</code> <code>bool</code> <p>If True, save with transparent background. Default is False.</p> <code>False</code> <code>add</code> <code>str</code> <p>Additional subdirectory path within addfigs. Default is '/'.</p> <code>'/'</code> <code>fc</code> <code>str</code> <p>Face color for non-transparent backgrounds. Default is 'white'.</p> <code>'white'</code> <code>form</code> <code>str</code> <p>File format ('pdf', 'png', 'jpg', etc.). Default is 'pdf'.</p> <code>'pdf'</code> <code>close</code> <code>bool</code> <p>If True, close the figure after saving. Default is False.</p> <code>False</code> <code>addfigs</code> <code>str</code> <p>Base directory for saving figures. Default is '/figs/'.</p> <code>'/figs/'</code> <p>Returns:</p> Type Description <code>None</code> Notes <ul> <li>Creates directory structure if it doesn't exist</li> <li>If format is 'pdf', also saves a JPG version at 100 dpi</li> <li>Saves with tight bounding box and no padding</li> </ul> Source code in <code>dod2k_utilities/ut_functions.py</code> <pre><code>def figsave(fig, name, trans=False, add='/', fc='white',\n            form='pdf', close=False, addfigs='/figs/'):\n    \"\"\"\n    Save a matplotlib figure to file with optional formats.\n\n    Parameters\n    ----------\n    fig : matplotlib.figure.Figure\n        Figure object to save.\n    name : str\n        Filename without extension.\n    trans : bool, optional\n        If True, save with transparent background. Default is False.\n    add : str, optional\n        Additional subdirectory path within addfigs. Default is '/'.\n    fc : str, optional\n        Face color for non-transparent backgrounds. Default is 'white'.\n    form : str, optional\n        File format ('pdf', 'png', 'jpg', etc.). Default is 'pdf'.\n    close : bool, optional\n        If True, close the figure after saving. Default is False.\n    addfigs : str, optional\n        Base directory for saving figures. Default is '/figs/'.\n\n    Returns\n    -------\n    None\n\n    Notes\n    -----\n    - Creates directory structure if it doesn't exist\n    - If format is 'pdf', also saves a JPG version at 100 dpi\n    - Saves with tight bounding box and no padding\n    \"\"\"\n    # from datetime import date\n    # day = date.today().isoformat()\n    # ddir = day+'/'+add if save_date else add\n    ddir = add\n    if not os.path.exists(os.getcwd()+addfigs+ddir):\n        os.makedirs(os.getcwd()+addfigs+ddir)\n\n    fig.savefig(os.getcwd()+addfigs+ddir+'/'+name+'.'+form, \n                transparent=trans, facecolor=fig.get_facecolor(),\n                format=form, bbox_inches='tight', pad_inches=0.0)\n    if form=='pdf': \n        fig.savefig(os.getcwd()+addfigs+ddir+'/'+name+'.jpg', \n                    transparent=trans, facecolor=fig.get_facecolor(),\n                    format='png', dpi=300, bbox_inches='tight', pad_inches=0.0)\n    print('saved figure in '+ addfigs+ddir+'/'+name+'.'+form)\n    if close: plt.close()\n    return\n</code></pre>"},{"location":"api/ut_functions/#dod2k_utilities.ut_functions.find","title":"<code>find(pattern, path)</code>","text":"<p>Find all files containing a pattern in their name within a directory tree.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>str</code> <p>String pattern to search for in filenames.</p> required <code>path</code> <code>str</code> <p>Root directory path to search recursively.</p> required <p>Returns:</p> Type Description <code>list of str</code> <p>List of full file paths for all matching files.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; csv_files = find('.csv', '/data/output')\n&gt;&gt;&gt; metadata_files = find('metadata', '.')\n</code></pre> Source code in <code>dod2k_utilities/ut_functions.py</code> <pre><code>def find(pattern, path):\n    \"\"\"\n    Find all files containing a pattern in their name within a directory tree.\n\n    Parameters\n    ----------\n    pattern : str\n        String pattern to search for in filenames.\n    path : str\n        Root directory path to search recursively.\n\n    Returns\n    -------\n    list of str\n        List of full file paths for all matching files.\n\n    Examples\n    --------\n    &gt;&gt;&gt; csv_files = find('.csv', '/data/output')\n    &gt;&gt;&gt; metadata_files = find('metadata', '.')\n    \"\"\"\n    import datetime\n\n    result = []\n    for root, dirs, files in os.walk(path):\n        for name in files:\n            if pattern in name:\n                result.append(os.path.join(root, name))\n    return result\n</code></pre>"},{"location":"api/ut_functions/#dod2k_utilities.ut_functions.fns","title":"<code>fns(path, end='.nc', start='', other_cond='', print_dir=True)</code>","text":"<p>Find filenames in a directory matching specific criteria.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Directory path to search.</p> required <code>end</code> <code>str</code> <p>File extension to match. Default is '.nc'.</p> <code>'.nc'</code> <code>start</code> <code>str</code> <p>String that filename must start with. Default is '' (any).</p> <code>''</code> <code>other_cond</code> <code>str</code> <p>Additional substring that must be in filename. Default is '' (any).</p> <code>''</code> <code>print_dir</code> <code>bool</code> <p>If True, print the list of found files. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Sorted array of matching filenames.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Find all NetCDF files\n&gt;&gt;&gt; files = fns('/data/climate/')\n&gt;&gt;&gt; # Find CSV files starting with 'temp'\n&gt;&gt;&gt; files = fns('/data/', end='.csv', start='temp')\n&gt;&gt;&gt; # Find files containing 'annual' in name\n&gt;&gt;&gt; files = fns('/data/', other_cond='annual')\n</code></pre> Source code in <code>dod2k_utilities/ut_functions.py</code> <pre><code>def fns(path, end='.nc', start='', other_cond='', print_dir=True):\n    \"\"\"\n    Find filenames in a directory matching specific criteria.\n\n    Parameters\n    ----------\n    path : str\n        Directory path to search.\n    end : str, optional\n        File extension to match. Default is '.nc'.\n    start : str, optional\n        String that filename must start with. Default is '' (any).\n    other_cond : str, optional\n        Additional substring that must be in filename. Default is '' (any).\n    print_dir : bool, optional\n        If True, print the list of found files. Default is True.\n\n    Returns\n    -------\n    numpy.ndarray\n        Sorted array of matching filenames.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Find all NetCDF files\n    &gt;&gt;&gt; files = fns('/data/climate/')\n    &gt;&gt;&gt; # Find CSV files starting with 'temp'\n    &gt;&gt;&gt; files = fns('/data/', end='.csv', start='temp')\n    &gt;&gt;&gt; # Find files containing 'annual' in name\n    &gt;&gt;&gt; files = fns('/data/', other_cond='annual')\n    \"\"\"\n    fn = []\n    for root, dirs, files in os.walk(path):\n        for fil in files:\n            if fil.endswith(end) and fil.startswith(start) and (other_cond in fil):\n                fn.append(fil)\n    if print_dir: print(fn)\n    fn = np.sort(fn)\n    return fn \n</code></pre>"},{"location":"api/ut_functions/#dod2k_utilities.ut_functions.load_compact_dataframe_from_csv","title":"<code>load_compact_dataframe_from_csv(df_name, readfrom='df.name', index_col=0)</code>","text":"<p>Load a compact dataframe from three CSV files.</p> <p>Reconstructs a complete dataframe from separate metadata, paleoData_values, and year CSV files created by write_compact_dataframe_to_csv.</p> <p>Parameters:</p> Name Type Description Default <code>df_name</code> <code>str</code> <p>Name of the dataframe (used for path and filename construction).</p> required <code>readfrom</code> <code>str or tuple</code> <p>If 'df.name' (default), uses df_name for path and filename. If tuple, should be (path, filename_template) where filename_template contains '%s' placeholder for component name.</p> <code>'df.name'</code> <code>index_col</code> <code>int</code> <p>Column number to use as index when reading metadata CSV. Default is 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Complete dataframe with all metadata columns plus paleoData_values and year columns. Data types are automatically converted to appropriate types (float32 for numeric arrays, str for text fields).</p> Notes <p>The function: - Joins metadata, paleoData_values, and year dataframes on datasetId - Converts array strings to numpy float32 arrays - Enforces specific data types for standard columns - Resets index to sequential integers</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = load_compact_dataframe_from_csv('dod2k')\n&gt;&gt;&gt; df = load_compact_dataframe_from_csv('pages2k', readfrom=('/data', 'p2k_%s'))\n</code></pre> Source code in <code>dod2k_utilities/ut_functions.py</code> <pre><code>def load_compact_dataframe_from_csv(df_name, readfrom='df.name', index_col=0):\n    \"\"\"\n    Load a compact dataframe from three CSV files.\n\n    Reconstructs a complete dataframe from separate metadata, paleoData_values,\n    and year CSV files created by write_compact_dataframe_to_csv.\n\n    Parameters\n    ----------\n    df_name : str\n        Name of the dataframe (used for path and filename construction).\n    readfrom : str or tuple, optional\n        If 'df.name' (default), uses df_name for path and filename.\n        If tuple, should be (path, filename_template) where filename_template\n        contains '%s' placeholder for component name.\n    index_col : int, optional\n        Column number to use as index when reading metadata CSV. Default is 0.\n\n    Returns\n    -------\n    pandas.DataFrame\n        Complete dataframe with all metadata columns plus paleoData_values\n        and year columns. Data types are automatically converted to appropriate\n        types (float32 for numeric arrays, str for text fields).\n\n    Notes\n    -----\n    The function:\n    - Joins metadata, paleoData_values, and year dataframes on datasetId\n    - Converts array strings to numpy float32 arrays\n    - Enforces specific data types for standard columns\n    - Resets index to sequential integers\n\n    Examples\n    --------\n    &gt;&gt;&gt; df = load_compact_dataframe_from_csv('dod2k')\n    &gt;&gt;&gt; df = load_compact_dataframe_from_csv('pages2k', readfrom=('/data', 'p2k_%s'))\n    \"\"\"\n    # load the compact dataframe from three csv files \n    if readfrom=='df.name':\n        path = f'/data/{df_name}'\n        filename = df_name+'_compact_%s'\n    else:\n        path=readfrom[0]\n        filename=readfrom[1]\n\n    df_meta = pd.read_csv(os.getcwd()+path+'/'+filename%'metadata'+'.csv', index_col=index_col, keep_default_na=False)\n    df_data = read_compact_dataframe_columns_from_csv('paleoData_values', filename%'paleoData_values', path)\n    df_year = read_compact_dataframe_columns_from_csv('year', filename%'year', path)\n\n    df_csv = df_meta.join(df_data).join(df_year)\n\n    df_csv['datasetId'] = df_csv.index\n    df_csv.index=range(len(df_csv))\n\n\n    df_csv = df_csv[sorted(df_csv.columns)]\n\n\n    df_csv['year'] = df_csv['year'].map(parse_array_string)\n    df_csv['paleoData_values'] = df_csv['paleoData_values'].map(parse_array_string)\n\n    # df_csv['year'] = df_csv['year'].map(lambda x: np.array(x, dtype = np.float32))\n    # df_csv['paleoData_values'] = df_csv['paleoData_values'].map(lambda x: np.array(x, dtype = np.float32))\n\n    df_csv = df_csv.astype({'archiveType': str, \n                            'dataSetName': str, \n                            'datasetId': str, \n                            'geo_meanElev': np.float32, \n                            'geo_meanLat': np.float32, \n                            'geo_meanLon': np.float32, \n                            'geo_siteName': str, \n                            'interpretation_direction': str,\n                            'interpretation_seasonality': str,\n                            'interpretation_variable': str,\n                            'interpretation_variableDetail': str,\n                            'originalDatabase': str, \n                            'originalDataURL': str, \n                            'paleoData_notes': str, \n                            'paleoData_proxy': str,\n                            'paleoData_sensorSpecies': str,\n                            'paleoData_units': str, \n                            'paleoData_variableName': str, \n                            'yearUnits': str})\n\n    df_csv.name = df_name\n    return df_csv\n</code></pre>"},{"location":"api/ut_functions/#dod2k_utilities.ut_functions.parse_array_string","title":"<code>parse_array_string(x)</code>","text":"<p>Parse a string representation of a numeric array into a NumPy array.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>str, list, or array-like</code> <p>Input representing an array. Supported forms include: - A string of comma-separated numbers, optionally enclosed   in square brackets (e.g., <code>\"[1, 2, 3]\"</code> or <code>\"1,2,3\"</code>). - A list containing a single such string. - A list or array-like object of numeric values.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>One-dimensional NumPy array of type <code>np.float32</code> constructed from the parsed values.</p> Source code in <code>dod2k_utilities/ut_functions.py</code> <pre><code>def parse_array_string(x):\n    \"\"\"\n    Parse a string representation of a numeric array into a NumPy array.\n\n    Parameters\n    ----------\n    x : str, list, or array-like\n        Input representing an array. Supported forms include:\n        - A string of comma-separated numbers, optionally enclosed\n          in square brackets (e.g., ``\"[1, 2, 3]\"`` or ``\"1,2,3\"``).\n        - A list containing a single such string.\n        - A list or array-like object of numeric values.\n\n    Returns\n    -------\n    numpy.ndarray\n        One-dimensional NumPy array of type ``np.float32`` constructed\n        from the parsed values.\n    \"\"\"\n    if isinstance(x, str):\n        # Remove brackets and split by comma\n        x = x.strip('[]').replace('\\n', '').replace(' ', '')\n        if x:\n            try:\n                return np.fromstring(x, sep=',', dtype=np.float32)\n            except:\n                # If parsing fails, try eval (careful with this!)\n                return np.array(eval('[' + x + ']'), dtype=np.float32)\n    elif isinstance(x, list) and len(x) == 1 and isinstance(x[0], str):\n        # Handle case where x is a list containing a string\n        return parse_array_string(x[0])\n    return np.array(x, dtype=np.float32)\n</code></pre>"},{"location":"api/ut_functions/#dod2k_utilities.ut_functions.read_compact_dataframe_columns_from_csv","title":"<code>read_compact_dataframe_columns_from_csv(key, filename, path)</code>","text":"<p>Read a single column from a compact dataframe CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Column name for the data being read (e.g., 'paleoData_values' or 'year').</p> required <code>filename</code> <code>str</code> <p>Filename without the '.csv' extension.</p> required <code>path</code> <code>str</code> <p>Relative path from current working directory.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with index set to IDs from first column and a single column named <code>key</code> containing the data.</p> Notes <p>Expects CSV format with first column as ID and remaining columns as data. First row is treated as header and skipped.</p> Source code in <code>dod2k_utilities/ut_functions.py</code> <pre><code>def read_compact_dataframe_columns_from_csv(key, filename, path):\n    \"\"\"\n    Read a single column from a compact dataframe CSV file.\n\n    Parameters\n    ----------\n    key : str\n        Column name for the data being read (e.g., 'paleoData_values' or 'year').\n    filename : str\n        Filename without the '.csv' extension.\n    path : str\n        Relative path from current working directory.\n\n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame with index set to IDs from first column and a single column\n        named `key` containing the data.\n\n    Notes\n    -----\n    Expects CSV format with first column as ID and remaining columns as data.\n    First row is treated as header and skipped.\n    \"\"\"\n    ID = []\n    data = []\n    with open(os.getcwd()+path+'/'+filename+'.csv', 'r', newline='') as f:\n        reader = csv.reader(f)\n        for ii, row in enumerate(reader):\n            if ii==0:\n                header=row\n                continue\n            ID   += [row[0]]\n            data += [[row[1:]]]\n    df = pd.DataFrame(np.array(data, dtype=object), index=ID, columns=[key])\n    return df\n</code></pre>"},{"location":"api/ut_functions/#dod2k_utilities.ut_functions.read_csv","title":"<code>read_csv(filename, dtype=str, header=False, last_header_row=0)</code>","text":"<p>Read data from a CSV file with optional header extraction.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Filename without the '.csv' extension.</p> required <code>dtype</code> <code>type or None</code> <p>Data type for the output array. If None, returns list. Default is str.</p> <code>str</code> <code>header</code> <code>bool</code> <p>If True, extracts header rows (lines starting with '#' or before last_header_row). Default is False.</p> <code>False</code> <code>last_header_row</code> <code>int</code> <p>Index of the last header row (0-indexed). Default is 0.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>data</code> <code>ndarray or list</code> <p>The CSV data as array or list depending on dtype.</p> <code>header</code> <code>(list, optional)</code> <p>List of header lines (only returned if header=True).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = read_csv('myfile')  # Returns array of strings\n&gt;&gt;&gt; data, hdr = read_csv('myfile', header=True, last_header_row=2)\n&gt;&gt;&gt; data = read_csv('myfile', dtype=None)  # Returns list\n</code></pre> Source code in <code>dod2k_utilities/ut_functions.py</code> <pre><code>def read_csv(filename, dtype=str, header=False, last_header_row=0):\n    \"\"\"\n    Read data from a CSV file with optional header extraction.\n\n    Parameters\n    ----------\n    filename : str\n        Filename without the '.csv' extension.\n    dtype : type or None, optional\n        Data type for the output array. If None, returns list. Default is str.\n    header : bool, optional\n        If True, extracts header rows (lines starting with '#' or before\n        last_header_row). Default is False.\n    last_header_row : int, optional\n        Index of the last header row (0-indexed). Default is 0.\n\n    Returns\n    -------\n    data : numpy.ndarray or list\n        The CSV data as array or list depending on dtype.\n    header : list, optional\n        List of header lines (only returned if header=True).\n\n    Examples\n    --------\n    &gt;&gt;&gt; data = read_csv('myfile')  # Returns array of strings\n    &gt;&gt;&gt; data, hdr = read_csv('myfile', header=True, last_header_row=2)\n    &gt;&gt;&gt; data = read_csv('myfile', dtype=None)  # Returns list\n    \"\"\"\n    # reads csv file\n    if header:\n        header = []\n        data   = []\n        with open(filename+'.csv', 'r', newline='') as f:\n            reader = csv.reader(f)\n            for irow, row in enumerate(reader):\n                if row[0].startswith('#') or irow&lt;=last_header_row:\n                    # header.append(row[0].replace('#',''))\n                    if row[0].startswith('#'):\n                        row[0] = row[0].replace('#','')\n                    header.append(','.join(row))  # Changed from row[0] to row\n                else:\n                    data.append(row)\n        if not dtype:\n            return np.array(data, dtype=dtype), header\n        else:\n            return data, header\n    else:\n        data = []\n        with open(filename+'.csv', 'r', newline='') as f:\n            reader = csv.reader(f)\n            for row in reader:\n                data.append(row)\n        if not dtype:\n            return np.array(data, dtype=dtype)\n        else:\n            return data\n</code></pre>"},{"location":"api/ut_functions/#dod2k_utilities.ut_functions.save_fig","title":"<code>save_fig(fig, filename, trans=False, dir='/', fc='white', figformat='pdf', close=False, addfigs=True)</code>","text":"<p>Save a matplotlib figure to file with optional formats.</p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>Figure object to save.</p> required <code>filename</code> <code>str</code> <p>Filename without extension.</p> required <code>trans</code> <code>bool</code> <p>If True, save with transparent background. Default is False.</p> <code>False</code> <code>dir</code> <code>str</code> <p>Additional subdirectory path. Default is '/'.</p> <code>'/'</code> <code>fc</code> <code>str</code> <p>Face color for non-transparent backgrounds. Default is 'white'.</p> <code>'white'</code> <code>figformat</code> <code>str</code> <p>File format ('pdf', 'png', 'jpg', etc.). Default is 'pdf'.</p> <code>'pdf'</code> <code>close</code> <code>bool</code> <p>If True, close the figure after saving. Default is False.</p> <code>False</code> <code>addfigs</code> <code>bool</code> <p>If True, add '/figs/' to the rooth path. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> Notes <ul> <li>Creates directory structure if it doesn't exist</li> <li>If format is 'pdf', also saves a JPG version at 100 dpi</li> <li>Saves with tight bounding box and no padding</li> </ul> Source code in <code>dod2k_utilities/ut_functions.py</code> <pre><code>def save_fig(fig, filename, trans=False, dir='/', fc='white',\n            figformat='pdf', close=False, addfigs=True):\n    \"\"\"\n    Save a matplotlib figure to file with optional formats.\n\n    Parameters\n    ----------\n    fig : matplotlib.figure.Figure\n        Figure object to save.\n    filename : str\n        Filename without extension.\n    trans : bool, optional\n        If True, save with transparent background. Default is False.\n    dir : str, optional\n        Additional subdirectory path. Default is '/'.\n    fc : str, optional\n        Face color for non-transparent backgrounds. Default is 'white'.\n    figformat : str, optional\n        File format ('pdf', 'png', 'jpg', etc.). Default is 'pdf'.\n    close : bool, optional\n        If True, close the figure after saving. Default is False.\n    addfigs : bool, optional\n        If True, add '/figs/' to the rooth path. Default is True.\n\n    Returns\n    -------\n    None\n\n    Notes\n    -----\n    - Creates directory structure if it doesn't exist\n    - If format is 'pdf', also saves a JPG version at 100 dpi\n    - Saves with tight bounding box and no padding\n    \"\"\"\n    fig_dir = os.getcwd()\n    if addfigs: fig_dir+='/figs/'\n    fig_dir+=dir\n\n    if not os.path.exists(fig_dir):\n        os.makedirs(fig_dir)\n\n    fig.savefig(fig_dir+'/'+filename+'.'+figformat, \n                transparent=trans, facecolor=fig.get_facecolor(),\n                format=figformat, bbox_inches='tight', pad_inches=0.0)\n    if figformat=='pdf': \n        fig.savefig(fig_dir+'/'+filename+'.png', \n                    transparent=trans, facecolor=fig.get_facecolor(),\n                    format='png', dpi=300, bbox_inches='tight', pad_inches=0.0)\n\n    print('saved figure in '+ fig_dir+'/'+filename+'.'+figformat)\n    if close: plt.close()\n    return\n</code></pre>"},{"location":"api/ut_functions/#dod2k_utilities.ut_functions.write_compact_dataframe_to_csv","title":"<code>write_compact_dataframe_to_csv(df, saveto='df.name')</code>","text":"<p>Save a compact dataframe to three separate CSV files.</p> <p>Splits a dataframe into metadata, paleoData_values, and year components, saving each as a separate CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe containing 'datasetId', 'paleoData_values', 'year' columns and additional metadata columns.</p> required <code>saveto</code> <code>str or tuple</code> <p>If 'df.name' (default), uses df.name for path and filename. If tuple, should be (path, filename_template) where filename_template contains '%s' placeholder for component name.</p> <code>'df.name'</code> <p>Returns:</p> Type Description <code>None</code> Notes <p>Creates three CSV files: - _compact_metadata.csv : All columns except paleoData_values and year - _compact_paleoData_values.csv : datasetId and paleoData_values columns - *_compact_year.csv : datasetId and year columns</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; write_compact_dataframe_to_csv(df)  # Uses df.name\n&gt;&gt;&gt; write_compact_dataframe_to_csv(df, saveto=('/output', 'data_%s'))\n</code></pre> Source code in <code>dod2k_utilities/ut_functions.py</code> <pre><code>def  write_compact_dataframe_to_csv(df, saveto='df.name'):\n    \"\"\"\n    Save a compact dataframe to three separate CSV files.\n\n    Splits a dataframe into metadata, paleoData_values, and year components,\n    saving each as a separate CSV file.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        Dataframe containing 'datasetId', 'paleoData_values', 'year' columns\n        and additional metadata columns.\n    saveto : str or tuple, optional\n        If 'df.name' (default), uses df.name for path and filename.\n        If tuple, should be (path, filename_template) where filename_template\n        contains '%s' placeholder for component name.\n\n    Returns\n    -------\n    None\n\n    Notes\n    -----\n    Creates three CSV files:\n    - *_compact_metadata.csv : All columns except paleoData_values and year\n    - *_compact_paleoData_values.csv : datasetId and paleoData_values columns\n    - *_compact_year.csv : datasetId and year columns\n\n    Examples\n    --------\n    &gt;&gt;&gt; write_compact_dataframe_to_csv(df)  # Uses df.name\n    &gt;&gt;&gt; write_compact_dataframe_to_csv(df, saveto=('/output', 'data_%s'))\n    \"\"\"\n\n    # sort dataframe alphabetically, with 'datasetId' first:\n    other_keys = [kk for kk in df.keys() if kk!='datasetId']\n    other_keys.sort()\n    keys_sorted = ['datasetId']+other_keys\n    # print(keys_sorted)\n    df_name = df.name\n    df = df[keys_sorted]\n    df.name=df_name\n    # print(df.info())\n    # save to a list of csv files (metadata, data, year)\n    for fn in ['metadata', 'paleoData_values', 'year']:\n        if saveto=='df.name':\n            path = f'/data/{df.name}'\n            filename = f'{df.name}_compact_%s'\n        else:\n            path=saveto[0]\n            filename=saveto[1]\n        if fn in ['paleoData_values', 'year']:\n            header = ['datasetId', fn]\n            write_dataframe_columns_to_csv(df[fn].values, \n                                             header, \n                                             filename%fn, path, \n                                             ID=df['datasetId'].values)\n        else:\n            keys = [key for key in df.columns if key not in ['paleoData_values', 'year']]\n            print('METADATA: %s'%', '.join(keys))\n            write_dataframe_columns_to_csv(df[keys].values, \n                                             keys, filename%fn, path,\n                                             ID=False)\n    print(f'Saved to {os.getcwd()+path}/{filename}.csv')\n    return\n</code></pre>"},{"location":"api/ut_functions/#dod2k_utilities.ut_functions.write_csv","title":"<code>write_csv(data, filename, header=False, cols=False)</code>","text":"<p>Write data to a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array - like</code> <p>2D array or list of rows to write to CSV. Each row should be an iterable of values.</p> required <code>filename</code> <code>str</code> <p>Output filename without the '.csv' extension.</p> required <p>Returns:</p> Type Description <code>None</code> Notes <p>The file will be created in the current working directory with '.csv' extension automatically appended.</p> Source code in <code>dod2k_utilities/ut_functions.py</code> <pre><code>def write_csv(data, filename, header=False, cols=False):\n\n    \"\"\"\n    Write data to a CSV file.\n\n    Parameters\n    ----------\n    data : array-like\n        2D array or list of rows to write to CSV. Each row should be an\n        iterable of values.\n    filename : str\n        Output filename without the '.csv' extension.\n\n    Returns\n    -------\n    None\n\n    Notes\n    -----\n    The file will be created in the current working directory with '.csv'\n    extension automatically appended.\n    \"\"\"\n\n\n    with open(filename+'.csv', 'w', newline='') as f:\n\n        writer = csv.writer(f)\n        if header is not False:\n            writer.writerows(header)\n        if cols is not False:\n            writer.writerows(cols)\n\n        writer.writerows(data)\n    return\n</code></pre>"},{"location":"api/ut_functions/#dod2k_utilities.ut_functions.write_dataframe_columns_to_csv","title":"<code>write_dataframe_columns_to_csv(data, header, filename, path, ID=False)</code>","text":"<p>Write dataframe column(s) to a CSV file with optional ID column.</p> <p>Creates the output directory if it doesn't exist and writes data with a header row. Optionally prepends an ID column to each row.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array - like</code> <p>Iterable of column values to write. Each element can be a single value (str, float, np.float64) or an iterable of values representing a row.</p> required <code>header</code> <code>list of str</code> <p>Column names for the CSV header row.</p> required <code>filename</code> <code>str</code> <p>Output filename without the '.csv' extension.</p> required <code>path</code> <code>str</code> <p>Relative path from current working directory where file will be saved. Should start with '/'.</p> required <code>ID</code> <code>array - like</code> <p>Array of ID values to prepend to each row. Must have same length as data. If False (default), no ID column is added.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> Notes <ul> <li>Directory structure is created automatically if it doesn't exist</li> <li>Single values (str, float, np.float64) are automatically converted to lists</li> <li>The file is saved as: <code>cwd + path + filename + '.csv'</code></li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = [25.5, 30.2, 28.1]\n&gt;&gt;&gt; header = ['ID', 'Temperature']\n&gt;&gt;&gt; IDs = ['Site_A', 'Site_B', 'Site_C']\n&gt;&gt;&gt; write_dataframe_columns_to_csv(data, header, 'temps', '/output', ID=IDs)\n</code></pre> <pre><code>&gt;&gt;&gt; # Without ID column\n&gt;&gt;&gt; data = [[1, 2], [3, 4], [5, 6]]\n&gt;&gt;&gt; header = ['X', 'Y']\n&gt;&gt;&gt; write_dataframe_columns_to_csv(data, header, 'coords', '/output')\n</code></pre> Source code in <code>dod2k_utilities/ut_functions.py</code> <pre><code>def write_dataframe_columns_to_csv(data, header, filename, path, ID=False):\n    \"\"\"\n    Write dataframe column(s) to a CSV file with optional ID column.\n\n    Creates the output directory if it doesn't exist and writes data with\n    a header row. Optionally prepends an ID column to each row.\n\n    Parameters\n    ----------\n    data : array-like\n        Iterable of column values to write. Each element can be a single value\n        (str, float, np.float64) or an iterable of values representing a row.\n    header : list of str\n        Column names for the CSV header row.\n    filename : str\n        Output filename without the '.csv' extension.\n    path : str\n        Relative path from current working directory where file will be saved.\n        Should start with '/'.\n    ID : array-like, optional\n        Array of ID values to prepend to each row. Must have same length as\n        data. If False (default), no ID column is added.\n\n    Returns\n    -------\n    None\n\n    Notes\n    -----\n    - Directory structure is created automatically if it doesn't exist\n    - Single values (str, float, np.float64) are automatically converted to lists\n    - The file is saved as: `cwd + path + filename + '.csv'`\n\n    Examples\n    --------\n    &gt;&gt;&gt; data = [25.5, 30.2, 28.1]\n    &gt;&gt;&gt; header = ['ID', 'Temperature']\n    &gt;&gt;&gt; IDs = ['Site_A', 'Site_B', 'Site_C']\n    &gt;&gt;&gt; write_dataframe_columns_to_csv(data, header, 'temps', '/output', ID=IDs)\n\n    &gt;&gt;&gt; # Without ID column\n    &gt;&gt;&gt; data = [[1, 2], [3, 4], [5, 6]]\n    &gt;&gt;&gt; header = ['X', 'Y']\n    &gt;&gt;&gt; write_dataframe_columns_to_csv(data, header, 'coords', '/output')\n    \"\"\"\n    # write dataframe column(s) as csv\n    if not os.path.exists(os.getcwd()+path):\n        os.makedirs(os.getcwd()+path)\n\n    with open(os.getcwd()+path+'/'+filename+'.csv', 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(header)\n        for ii, row in enumerate(data):\n            if (type(row) is str) or (type(row) is np.float64) or (type(row) is float):\n                row=[row]\n            if ID is not False:\n                writer.writerow([ID[ii]]+ list(row))\n            else:\n                writer.writerow(list(row))\n    return\n</code></pre>"},{"location":"api/ut_plot/","title":"Plotting functions","text":"<p>Module containing plotting functions for summarising and visualising data and dataframe objects.</p>"},{"location":"api/ut_plot/#dod2k_utilities.ut_plot","title":"<code>dod2k_utilities.ut_plot</code>","text":"<p>@author: Lucie Luecke</p> <p>Plotting functions for displaying data(frames).</p> <p>Last updated 19/12/2025 for publication of dod2k v2.0</p>"},{"location":"api/ut_plot/#dod2k_utilities.ut_plot.df_colours_markers","title":"<code>df_colours_markers(db_name='dod2k_v2.0')</code>","text":"<p>Generate archive colours and proxy markers for plotting functions.</p> <p>Parameters:</p> Name Type Description Default <code>db_name</code> <code>str</code> <p>Name of the database CSV file to load. Default is 'dod2k_dupfree_dupfree'.</p> <code>'dod2k_v2.0'</code> <p>Returns:</p> Name Type Description <code>archive_colour</code> <code>dict</code> <p>Dictionary mapping archive types to color codes.</p> <code>archives_sorted</code> <code>ndarray</code> <p>Sorted list of archive types based on record count.</p> <code>proxy_marker</code> <code>dict</code> <p>Dictionary mapping each archive type and proxy to a specific marker.</p> Source code in <code>dod2k_utilities/ut_plot.py</code> <pre><code>def df_colours_markers(db_name='dod2k_v2.0'):\n    \"\"\"\n    Generate archive colours and proxy markers for plotting functions.\n\n    Parameters\n    ----------\n    db_name : str, optional\n        Name of the database CSV file to load. Default is 'dod2k_dupfree_dupfree'.\n\n    Returns\n    -------\n    archive_colour : dict\n        Dictionary mapping archive types to color codes.\n    archives_sorted : numpy.ndarray\n        Sorted list of archive types based on record count.\n    proxy_marker : dict\n        Dictionary mapping each archive type and proxy to a specific marker.\n    \"\"\"\n    cols = [ '#4477AA', '#EE6677', '#228833', '#CCBB44', '#66CCEE', '#AA3377', '#BBBBBB', '#44AA99']\n\n    df = utf.load_compact_dataframe_from_csv(db_name)\n\n\n    # count archive types\n    archive_count = {}\n    for ii, at in enumerate(set(df['archiveType'])):\n        archive_count[at] = df.loc[df['archiveType']==at, 'paleoData_proxy'].count()\n\n    archive_colour = {'other': cols[-1]}\n    proxy_marker   = {}\n    other_archives = []\n    major_archives = []\n\n\n    mt = 'ov^s&lt;&gt;pP*XDdh'*10 # generates string of marker types\n\n    ijk=0\n    sort = np.argsort([cc for cc in archive_count.values()])\n    archives_sorted = np.array([cc for cc in archive_count.keys()])[sort][::-1]\n    for ii, at in enumerate(archives_sorted):\n        print(ii, at, archive_count[at])\n        if archive_count[at]&gt;10:\n            archive_colour[at] = cols[ii]\n            major_archives+=[at]\n        else:\n            archive_colour[at] = cols[-1]\n            other_archives+=[at]\n        arch_mask = df['archiveType']==at\n        arch_proxy_types = np.unique(df['paleoData_proxy'][arch_mask])\n        proxy_marker[at]={}\n        for jj, pt in enumerate(arch_proxy_types):\n            marker = mt[jj] if at in major_archives else mt[ijk]\n            proxy_marker[at][pt]=marker\n        if at not in major_archives: ijk+=1\n\n    return archive_colour, archives_sorted, proxy_marker \n</code></pre>"},{"location":"api/ut_plot/#dod2k_utilities.ut_plot.geo_EOF_plot","title":"<code>geo_EOF_plot(df, pca_rec, EOFs, keys, fs=(13, 8), dpi=350, barlabel='EOF', which_EOF=0)</code>","text":"<p>Plot geographic distribution of records colored by EOF loadings.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with paleo-proxy records. Must include columns 'geo_meanLat', 'geo_meanLon', 'datasetId'.</p> required <code>pca_rec</code> <code>dict</code> <p>Dictionary mapping keys to lists of dataset IDs included in PCA.</p> required <code>EOFs</code> <code>dict</code> <p>Dictionary mapping keys to EOF arrays.</p> required <code>keys</code> <code>list</code> <p>List of keys (record types) to plot.</p> required <code>fs</code> <code>tuple</code> <p>Figure size in inches. Default is (13, 8).</p> <code>(13, 8)</code> <code>dpi</code> <code>int</code> <p>Figure resolution in dots per inch. Default is 350.</p> <code>350</code> <code>barlabel</code> <code>str</code> <p>Label for the colorbar. Default is 'EOF'.</p> <code>'EOF'</code> <code>which_EOF</code> <code>int</code> <p>Index of the EOF to plot. Default is 0 (first EOF).</p> <code>0</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>Matplotlib figure object containing the EOF-colored map.</p> Source code in <code>dod2k_utilities/ut_plot.py</code> <pre><code>def geo_EOF_plot(df, pca_rec, EOFs, keys, fs=(13,8), dpi=350, barlabel='EOF', which_EOF=0):\n    \"\"\"\n    Plot geographic distribution of records colored by EOF loadings.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame with paleo-proxy records. Must include columns\n        'geo_meanLat', 'geo_meanLon', 'datasetId'.\n    pca_rec : dict\n        Dictionary mapping keys to lists of dataset IDs included in PCA.\n    EOFs : dict\n        Dictionary mapping keys to EOF arrays. \n    keys : list\n        List of keys (record types) to plot.\n    fs : tuple, optional\n        Figure size in inches. Default is (13, 8).\n    dpi : int, optional\n        Figure resolution in dots per inch. Default is 350.\n    barlabel : str, optional\n        Label for the colorbar. Default is 'EOF'.\n    which_EOF : int, optional\n        Index of the EOF to plot. Default is 0 (first EOF).\n\n    Returns\n    -------\n    fig : matplotlib.figure.Figure\n        Matplotlib figure object containing the EOF-colored map.\n    \"\"\"\n    #%% plot the spatial distribution of all records\n    proxy_lats = df['geo_meanLat'].values\n    proxy_lons = df['geo_meanLon'].values\n\n    # plots the map\n    fig = plt.figure(figsize=fs, dpi=dpi) #fs=(13,8), dpi=350\n    grid = GS(1, 3)\n\n    ax = plt.subplot(grid[:, :], projection=ccrs.Robinson()) # create axis with Robinson projection of globe\n\n    # ax.stock_img(clip_on=False)\n\n    ax.add_feature(cfeature.LAND, alpha=0.6) # adds land features\n    ax.add_feature(cfeature.OCEAN, alpha=0.6, facecolor='#C5DEEA') # adds ocean features\n    ax.coastlines() # adds coastline features\n\n    ax.set_global()\n\n    mt = 'v^soD&lt;''osD&gt;pP*Xdh' # generates string of marker types\n\n    # some of the following lines are hard-coded to plot EOF1, \n    # but asking for EOFs[key][0] here and also in f.get_colours will give the plot of EOF2\n    # also need to modify the colorscale label cax.set_ylabel('EOF 2')\n\n    # if we are multipling the PCs x -1, multiply the EOF loadings by -1 as well\n    a= {}\n    label={}\n    for key in keys:\n        if key in ['tree_d18O', 'coral_d18O']:# multiply EOF sign by -1\n            a[key] = -1\n            label[key] = key+' ($\\\\ast(-1)$)'\n        else:\n            a[key] = 1\n            label[key]=key\n    print(a)\n\n    all_EOFs = [a[key]*EOFs[key][which_EOF][ii]  for key in keys for ii in range(len(EOFs[key][which_EOF]))]\n\n    colors, sm, norm = get_colours2(all_EOFs, \n                                colormap='RdBu_r',minval=-0.6,maxval=0.6)\n\n    ijk=0\n\n\n\n    for key in keys:\n\n        marker  = mt[ijk]\n\n        colors = get_colours(a[key]*EOFs[key][which_EOF], colormap='RdBu_r',minval=-0.6,maxval=0.6)\n        id_mask = np.isin(df['datasetId'], pca_rec[key]) \n        for jj in range(len(pca_rec[key])):\n\n            scat_label   = label[key]+' (n=%d)'%len(pca_rec[key]) if jj==0 else None\n\n            plt.scatter(proxy_lons[id_mask][jj], proxy_lats[id_mask][jj], \n                        transform=ccrs.PlateCarree(), zorder=999,\n                        marker=marker, \n                        color=colors[jj], \n                        label=None,\n                        lw=.3, ec='k', s=200)\n            plt.scatter(proxy_lons[id_mask][jj], proxy_lats[id_mask][jj], \n                        transform=ccrs.PlateCarree(), zorder=999,\n                        marker=marker, \n                        color='none', \n                        label=scat_label, \n                        lw=1, ec='k', s=200)\n        ijk+=1\n\n    cax=ax.inset_axes([1.02, 0.1, 0.035, 0.8])\n    sm.set_array([])\n\n    matplotlib.colorbar.ColorbarBase(cax, cmap='RdBu_r', norm=norm)\n    cax.set_ylabel(barlabel, fontsize=13.5)\n\n    plt.legend(bbox_to_anchor=(0.03,-0.01), loc='upper left', ncol=3, fontsize=13.5, framealpha=0)\n    grid.tight_layout(fig)\n\n    return fig\n</code></pre>"},{"location":"api/ut_plot/#dod2k_utilities.ut_plot.geo_plot","title":"<code>geo_plot(df, fs=(9, 4.5), dpi=350, return_col=False, **kwargs)</code>","text":"<p>Plot the spatial distribution of paleo-proxy records on a global map.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing the records. Must include columns: 'geo_meanLat', 'geo_meanLon', 'archiveType', 'paleoData_proxy', 'datasetId'.</p> required <code>fs</code> <code>tuple</code> <p>Figure size (width, height) in inches. Default is (9, 4.5).</p> <code>(9, 4.5)</code> <code>dpi</code> <code>int</code> <p>Figure resolution in dots per inch. Default is 350.</p> <code>350</code> <code>**kwargs</code> <code>dict</code> <p>Optional keyword arguments. Supported keys: - 'mark_records': dict, to highlight specific datasets on the map. - 'mark_archives': list of archive keys to mark.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>Matplotlib figure object containing the map.</p> Source code in <code>dod2k_utilities/ut_plot.py</code> <pre><code>def geo_plot(df, fs=(9,4.5), dpi=350, return_col=False,  **kwargs):\n    \"\"\"\n    Plot the spatial distribution of paleo-proxy records on a global map.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame containing the records. Must include columns:\n        'geo_meanLat', 'geo_meanLon', 'archiveType', 'paleoData_proxy', 'datasetId'.\n    fs : tuple, optional\n        Figure size (width, height) in inches. Default is (9, 4.5).\n    dpi : int, optional\n        Figure resolution in dots per inch. Default is 350.\n    **kwargs : dict\n        Optional keyword arguments. Supported keys:\n        - 'mark_records': dict, to highlight specific datasets on the map.\n        - 'mark_archives': list of archive keys to mark.\n\n    Returns\n    -------\n    fig : matplotlib.figure.Figure\n        Matplotlib figure object containing the map.\n    \"\"\"\n    archive_colour, archives_sorted, proxy_marker = df_colours_markers()\n\n    #%% plot the spatial distribution of all records\n    proxy_lats = df['geo_meanLat'].values\n    proxy_lons = df['geo_meanLon'].values\n\n    # plots the map\n    fig = plt.figure(figsize=fs, dpi=dpi) #fs=(13,8), dpi=350\n    grid = GS(1, 3)\n\n    ax = plt.subplot(grid[:, :], projection=ccrs.Robinson()) # create axis with Robinson projection of globe\n\n\n    ax.add_feature(cfeature.LAND, alpha=0.5) # adds land features\n    ax.add_feature(cfeature.OCEAN, alpha=0.3, facecolor='#C5DEEA') # adds ocean features\n    ax.coastlines() # adds coastline features\n\n    ax.set_global()\n\n\n    mt = 'ov^s&lt;&gt;pP*XDdh'*10 # generates string of marker types\n\n    ijk=0\n    h1, l1 = [], []\n    h2, l2 = [], []\n    for at in archives_sorted:\n        at_mask = df['archiveType']==at\n        for ii, pt in enumerate(set(df[at_mask]['paleoData_proxy'])):\n            marker  = mt[ii]\n            pt_mask = df['paleoData_proxy']==pt\n            label   = at+': '+pt+' (n=%d)'%len(df[at_mask&amp;pt_mask])\n            plt.scatter(proxy_lons[pt_mask&amp;at_mask], proxy_lats[pt_mask&amp;at_mask], \n                        transform=ccrs.PlateCarree(), zorder=999,\n                        marker=proxy_marker[at][pt], \n                        color=archive_colour[at], \n                        label=label,\n                        lw=.3, ec='k', s=200)\n\n            if kwargs and 'mark_records' in kwargs:\n                hh, ll = ax.get_legend_handles_labels()\n                key = '%s_%s'%(at, pt) if at!='lake sediment' else 'lake sediment_d18O+d2H'\n                if key in kwargs['mark_archives']:\n                    id_mask = np.isin(df['datasetId'], kwargs['mark_records'][key])\n                    label='included in PCA'\n                    # if label in ll:\n                    #     label = None\n                    plt.scatter(proxy_lons[pt_mask&amp;at_mask&amp;id_mask], proxy_lats[pt_mask&amp;at_mask&amp;id_mask], \n                                transform=ccrs.PlateCarree(), zorder=999,\n                                marker=proxy_marker[at][pt], #label=label,\n                                lw=2, ec='k', color=archive_colour[at], s=200)\n\n\n    hh, ll = ax.get_legend_handles_labels()\n    plt.legend(hh, ll, bbox_to_anchor=(0.03,-0.01), loc='upper left', ncol=3, fontsize=12, framealpha=0)\n    grid.tight_layout(fig)\n\n    if return_col:\n        return fig, archive_colour\n\n    return fig\n</code></pre>"},{"location":"api/ut_plot/#dod2k_utilities.ut_plot.get_archive_colours","title":"<code>get_archive_colours(archives_sorted, archive_count, cols=['#4477AA', '#EE6677', '#228833', '#CCBB44', '#66CCEE', '#AA3377', '#BBBBBB', '#44AA99', '#332288'])</code>","text":"<p>Assign colors to archive types based on record abundance.</p> <p>Parameters:</p> Name Type Description Default <code>archives_sorted</code> <code>list of str</code> <p>Archive types sorted in descending or preferred order, typically by record count.</p> required <code>archive_count</code> <code>dict</code> <p>Dictionary mapping archive type to total number of records.</p> required <code>cols</code> <code>list of str</code> <p>List of color hex codes used to assign colors to major archives. The last color in the list is reserved for minor archives and the aggregated <code>'other'</code> category.</p> <code>['#4477AA', '#EE6677', '#228833', '#CCBB44', '#66CCEE', '#AA3377', '#BBBBBB', '#44AA99', '#332288']</code> <p>Returns:</p> Name Type Description <code>archive_colour</code> <code>dict</code> <p>Mapping from archive type to assigned color. Includes an <code>'other'</code> entry for minor archives.</p> <code>major_archives</code> <code>list of str</code> <p>Archive types with more than 10 records.</p> <code>other_archives</code> <code>list of str</code> <p>Archive types with 10 or fewer records.</p> Notes <p>Archive types with more than 10 records are treated as major archives and assigned unique colors. All remaining archive types are grouped under <code>'other'</code> and share a common color.</p> Source code in <code>dod2k_utilities/ut_plot.py</code> <pre><code>def get_archive_colours(archives_sorted, archive_count, cols= [ '#4477AA', '#EE6677', '#228833', '#CCBB44', '#66CCEE', '#AA3377', '#BBBBBB', '#44AA99', '#332288']):\n    \"\"\"\n    Assign colors to archive types based on record abundance.\n\n    Parameters\n    ----------\n    archives_sorted : list of str\n        Archive types sorted in descending or preferred order, typically\n        by record count.\n    archive_count : dict\n        Dictionary mapping archive type to total number of records.\n    cols : list of str, optional\n        List of color hex codes used to assign colors to major archives.\n        The last color in the list is reserved for minor archives and\n        the aggregated ``'other'`` category.\n\n    Returns\n    -------\n    archive_colour : dict\n        Mapping from archive type to assigned color. Includes an\n        ``'other'`` entry for minor archives.\n    major_archives : list of str\n        Archive types with more than 10 records.\n    other_archives : list of str\n        Archive types with 10 or fewer records.\n\n    Notes\n    -----\n    Archive types with more than 10 records are treated as major archives\n    and assigned unique colors. All remaining archive types are grouped\n    under ``'other'`` and share a common color.\n    \"\"\"\n\n    archive_colour = {'other': cols[-1]}\n    other_archives = []\n    major_archives = []\n\n    for ii, at in enumerate(archives_sorted):\n        print(ii, at, archive_count[at])\n        if archive_count[at]&gt;10:\n            major_archives     +=[at]\n            archive_colour[at] = cols[ii]\n        else:\n            other_archives     +=[at]\n            archive_colour[at] = cols[-1]\n    return archive_colour, major_archives, other_archives\n</code></pre>"},{"location":"api/ut_plot/#dod2k_utilities.ut_plot.get_colours","title":"<code>get_colours(data, colormap='brewer_RdBu_11', minval=False, maxval=False, return_mappable=False)</code>","text":"<p>Generate colors from a colormap based on data values.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>array - like</code> <p>Array or list of numerical values to map to colors.</p> required <code>colormap</code> <code>str</code> <p>Matplotlib colormap name. Default is 'brewer_RdBu_11'.</p> <code>'brewer_RdBu_11'</code> <code>minval</code> <code>float or False</code> <p>Minimum value for color normalization. If False, uses min(data). Default is False.</p> <code>False</code> <code>maxval</code> <code>float or False</code> <p>Maximum value for color normalization. If False, uses max(data). Default is False.</p> <code>False</code> <code>return_mappable</code> <code>bool</code> <p>If True, also return ScalarMappable and Normalize objects for colorbar. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>list of tuple</code> <p>List of RGBA color tuples, one for each data value, in same order as data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; temps = [15, 20, 25, 30, 35]\n&gt;&gt;&gt; colors = get_colours(temps, colormap='coolwarm')\n&gt;&gt;&gt; # Use colors for scatter plot\n&gt;&gt;&gt; plt.scatter(x, y, c=colors)\n</code></pre> Source code in <code>dod2k_utilities/ut_plot.py</code> <pre><code>def get_colours(data, colormap='brewer_RdBu_11', minval=False,\n                maxval=False, return_mappable=False):\n    \"\"\"\n    Generate colors from a colormap based on data values.\n\n    Parameters\n    ----------\n    data : array-like\n        Array or list of numerical values to map to colors.\n    colormap : str, optional\n        Matplotlib colormap name. Default is 'brewer_RdBu_11'.\n    minval : float or False, optional\n        Minimum value for color normalization. If False, uses min(data).\n        Default is False.\n    maxval : float or False, optional\n        Maximum value for color normalization. If False, uses max(data).\n        Default is False.\n    return_mappable : bool, optional\n        If True, also return ScalarMappable and Normalize objects for colorbar.\n        Default is False.\n\n    Returns\n    -------\n    list of tuple\n        List of RGBA color tuples, one for each data value, in same order as data.\n\n    Examples\n    --------\n    &gt;&gt;&gt; temps = [15, 20, 25, 30, 35]\n    &gt;&gt;&gt; colors = get_colours(temps, colormap='coolwarm')\n    &gt;&gt;&gt; # Use colors for scatter plot\n    &gt;&gt;&gt; plt.scatter(x, y, c=colors)\n    \"\"\"\n    from matplotlib.colors import Normalize\n    import matplotlib.cm as cm\n    if not minval:\n        minval = np.min(data)\n    if not maxval:\n        maxval = np.max(data)\n    N = len(data)\n    cmap         = cm.get_cmap(colormap)\n    sm           = cm.ScalarMappable(cmap = colormap)\n    sm.set_array(range(N))\n    norm         = Normalize(vmin=minval, vmax=maxval)\n    rgba         = cmap(norm(data))\n    cols         = list(rgba)\n    if return_mappable:\n        return cols, sm, norm\n    return cols\n</code></pre>"},{"location":"api/ut_plot/#dod2k_utilities.ut_plot.get_colours2","title":"<code>get_colours2(data, colormap='brewer_RdBu_11', minval=False, maxval=False)</code>","text":"<p>generates colours from a colormap based on the data values (array or list) returns cols: list of colours, in same order as data</p> Source code in <code>dod2k_utilities/ut_plot.py</code> <pre><code>def get_colours2(data, colormap='brewer_RdBu_11', minval=False,\n                maxval=False):\n    \"\"\"\n    generates colours from a colormap based on the *data* values (array or list)\n    returns *cols*: list of colours, in same order as data\n    \"\"\"\n    from matplotlib.colors import Normalize\n    import matplotlib.cm as cm\n    if not minval:\n        minval = np.min(data)\n    if not maxval:\n        maxval = np.max(data)\n    print(minval)\n    print(maxval)\n    N = len(data)\n    cmap         = cm.get_cmap(colormap)\n    sm           = cm.ScalarMappable(cmap = colormap)\n    sm.set_array(range(N))\n    norm         = Normalize(vmin=minval, vmax=maxval)\n    rgba         = cmap(norm(data))\n    cols         = list(rgba)\n    return cols, sm, norm\n</code></pre>"},{"location":"api/ut_plot/#dod2k_utilities.ut_plot.plot_PCs","title":"<code>plot_PCs(years_hom, eigenvectors, paleoData_zscores_hom, title='', name='', col='tab:blue')</code>","text":"<p>Plot principal components and reconstructed time series.</p> <p>Parameters:</p> Name Type Description Default <code>years_hom</code> <code>ndarray</code> <p>Homogenised time axis.</p> required <code>eigenvectors</code> <code>ndarray</code> <p>Eigenvectors from PCA.</p> required <code>paleoData_zscores_hom</code> <code>MaskedArray</code> <p>Homogenised z-score data array of shape (n_records, n_years).</p> required <code>title</code> <code>str</code> <p>Title for plots.</p> <code>''</code> <code>name</code> <code>str</code> <p>Name suffix for saving figures.</p> <code>''</code> <p>Returns:</p> Name Type Description <code>PCs</code> <code>ndarray</code> <p>Principal component time series.</p> <code>eigenvectors</code> <code>ndarray</code> <p>Eigenvectors (EOF loadings) corresponding to PCs.</p> Source code in <code>dod2k_utilities/ut_plot.py</code> <pre><code>def plot_PCs(years_hom, eigenvectors, paleoData_zscores_hom, title='', name='', col='tab:blue'):\n    \"\"\"\n    Plot principal components and reconstructed time series.\n\n    Parameters\n    ----------\n    years_hom : numpy.ndarray\n        Homogenised time axis.\n    eigenvectors : numpy.ndarray\n        Eigenvectors from PCA.\n    paleoData_zscores_hom : numpy.ma.MaskedArray\n        Homogenised z-score data array of shape (n_records, n_years).\n    title : str, optional\n        Title for plots.\n    name : str, optional\n        Name suffix for saving figures.\n\n    Returns\n    -------\n    PCs : numpy.ndarray\n        Principal component time series.\n    eigenvectors : numpy.ndarray\n        Eigenvectors (EOF loadings) corresponding to PCs.\n    \"\"\"\n    PCs = np.dot(eigenvectors.T, paleoData_zscores_hom.data)\n\n    Dz   = paleoData_zscores_hom.data\n    Dzr  = np.ma.masked_array(np.dot(eigenvectors, PCs), mask=paleoData_zscores_hom.mask)\n\n\n    fig = plt.figure()\n    plt.suptitle(title)\n    ax = plt.subplot(311)\n\n    plt.plot(years_hom, np.ma.mean(paleoData_zscores_hom, axis=0), #color='k', \n             zorder=999, color=col)\n\n    ax.axes.xaxis.set_ticklabels([])\n    plt.axvline(years_hom[0], color='k', lw=.5, alpha=.5)\n    plt.axvline(years_hom[-1], color='k', lw=.5, alpha=.5)\n    plt.xlim(years_hom[0]-20, years_hom[-1]+20)\n    plt.ylabel('paleoData_zscores')\n    for ii in range(2):\n        ax = plt.subplot(311+ii+1)\n        plt.plot(years_hom, PCs[ii], color=col)\n        if ii==1: plt.xlabel('time (year CE)')\n        plt.ylabel('PC %d'%(ii+1))\n        plt.axhline(0, color='k', alpha=0.5, lw=0.5)\n        plt.axvline(years_hom[0], color='k', lw=.5, alpha=.5)\n        plt.axvline(years_hom[-1], color='k', lw=.5, alpha=.5)\n        plt.xlim(years_hom[0]-20, years_hom[-1]+20)\n        if ii==0: ax.axes.xaxis.set_ticklabels([])\n\n    utf.save_fig(fig, 'PCs_%s'%title, dir=name)\n\n    plt.figure()\n    for ii in range(paleoData_zscores_hom.shape[0]):\n        plt.plot(paleoData_zscores_hom[ii,:], Dzr[ii,:],  alpha=0.4, lw=1, color=col)\n    plt.xlabel('paleoData_zscores')\n    plt.ylabel('paleoData_zscores_reconstructed')\n\n\n    fig = plt.figure()\n    plt.suptitle(title)\n    ax = plt.subplot(211)\n    for ii in range(paleoData_zscores_hom.shape[0]):\n        plt.plot(years_hom, paleoData_zscores_hom[ii,:], color=col, alpha=0.4, lw=1)\n    plt.plot(years_hom, np.ma.mean(paleoData_zscores_hom, axis=0), color='k', zorder=999)\n\n    ax.axes.xaxis.set_ticklabels([])\n    plt.axvline(years_hom[0], color='k', lw=.5, alpha=.5)\n    plt.axvline(years_hom[-1], color='k', lw=.5, alpha=.5)\n    plt.xlim(years_hom[0]-20, years_hom[-1]+20)\n    plt.ylabel('paleoData_zscores')\n\n    ax = plt.subplot(212)\n    for ii in range(Dzr.shape[0]):\n        plt.plot(years_hom, Dzr[ii,:], color=col, alpha=0.4, lw=1)\n    plt.plot(years_hom, np.ma.mean(Dzr, axis=0), color='k', zorder=999)\n\n    ax.axes.xaxis.set_ticklabels([])\n    plt.axvline(years_hom[0], color='k', lw=.5, alpha=.5)\n    plt.axvline(years_hom[-1], color='k', lw=.5, alpha=.5)\n    plt.xlim(years_hom[0]-20, years_hom[-1]+20)\n    plt.ylabel('paleoData_zscores \\n (reconstructed)')\n\n\n    n_recs = paleoData_zscores_hom.data.shape[0]\n    fig = plt.figure()\n    plt.suptitle(title)\n    for ii in range(2):\n        plt.subplot(211+ii)\n        plt.plot(range(n_recs), eigenvectors[ii], color=col)\n        if ii==1: plt.xlabel('rec')\n        plt.ylabel('EOF %d load'%(ii+1))\n        plt.axhline(0, color='k', alpha=0.5, lw=0.5)\n\n    utf.save_fig(fig, 'EOFloading_%s'%title, dir=name)\n\n    return PCs, eigenvectors\n</code></pre>"},{"location":"api/ut_plot/#dod2k_utilities.ut_plot.plot_count_proxy_by_archive_all","title":"<code>plot_count_proxy_by_archive_all(df, archive_proxy_count, archive_proxy_ticks, archive_colour)</code>","text":"<p>Plot proxy counts by archive for all proxy types.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing proxy and archive metadata (not directly used for plotting but retained for consistency).</p> required <code>archive_proxy_count</code> <code>dict</code> <p>Dictionary mapping proxy identifiers (e.g., <code>\"archive: proxy\"</code>) to record counts.</p> required <code>archive_proxy_ticks</code> <code>list of str</code> <p>Ordered list of proxy identifiers used for tick labels.</p> required <code>archive_colour</code> <code>dict</code> <p>Mapping from archive type to color.</p> required <p>Returns:</p> Type Description <code>Figure</code> <p>Figure containing the bar chart.</p> Notes <p>All proxy types are included regardless of count. Bars are sorted in descending order of record count. Archive colors are derived from the archive prefix of each proxy identifier.</p> Source code in <code>dod2k_utilities/ut_plot.py</code> <pre><code>def plot_count_proxy_by_archive_all(df, archive_proxy_count, archive_proxy_ticks, archive_colour) :\n    \"\"\"\n    Plot proxy counts by archive for all proxy types.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame containing proxy and archive metadata (not directly\n        used for plotting but retained for consistency).\n    archive_proxy_count : dict\n        Dictionary mapping proxy identifiers (e.g., ``\"archive: proxy\"``)\n        to record counts.\n    archive_proxy_ticks : list of str\n        Ordered list of proxy identifiers used for tick labels.\n    archive_colour : dict\n        Mapping from archive type to color.\n\n    Returns\n    -------\n    matplotlib.figure.Figure\n        Figure containing the bar chart.\n\n    Notes\n    -----\n    All proxy types are included regardless of count. Bars are sorted in\n    descending order of record count. Archive colors are derived from\n    the archive prefix of each proxy identifier.\n    \"\"\"\n    fig = plt.figure(figsize=(10, 7), dpi=500)\n    ax  = plt.gca()\n    count_by_proxy_long   = [archive_proxy_count[tt] for tt in archive_proxy_ticks]\n    ticks_by_proxy_long   = [tt for tt in archive_proxy_ticks]\n    cols_by_proxy_long    = [archive_colour[tt.split(':')[0]] for tt in archive_proxy_ticks ]\n    archive_by_proxy_long = [tt.split(':')[0] for tt in archive_proxy_ticks]\n\n    sort = np.argsort(count_by_proxy_long)[::-1]\n\n    # create placeholder artists for legend and clean axis again\n    plt.bar(range(len(set(archive_by_proxy_long))), range(len(set(archive_by_proxy_long))), \n            color=[archive_colour[aa] for aa in set(archive_by_proxy_long)],\n            label=set(archive_by_proxy_long))\n    h, l = ax.get_legend_handles_labels()\n    plt.legend()\n    ax.cla()\n\n    plt.bar(np.arange(len(ticks_by_proxy_long)), \n            np.array(count_by_proxy_long)[sort], \n            color=np.array(cols_by_proxy_long)[sort])\n\n    plt.xlabel('proxy type')\n    plt.ylabel('count')\n    ax.set_xticks(np.arange(len(ticks_by_proxy_long)), \n                  [ticks_by_proxy_long[ii] for ii in sort], \n                  rotation=45, ha='right', fontsize=9)\n    plt.legend(h[::-1], l[::-1], ncol=2)\n\n\n    fig.tight_layout()\n    return fig\n</code></pre>"},{"location":"api/ut_plot/#dod2k_utilities.ut_plot.plot_count_proxy_by_archive_short","title":"<code>plot_count_proxy_by_archive_short(df, archive_proxy_count, archive_proxy_ticks, archive_colour)</code>","text":"<p>Plot proxy counts by archive for proxy types exceeding a count threshold.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing proxy and archive metadata (not directly used for plotting but retained for consistency).</p> required <code>archive_proxy_count</code> <code>dict</code> <p>Dictionary mapping proxy identifiers (e.g., <code>\"archive: proxy\"</code>) to record counts.</p> required <code>archive_proxy_ticks</code> <code>list of str</code> <p>Ordered list of proxy identifiers used for tick labels.</p> required <code>archive_colour</code> <code>dict</code> <p>Mapping from archive type to color.</p> required <p>Returns:</p> Type Description <code>Figure</code> <p>Figure containing the bar chart.</p> Notes <p>Only proxy types with more than 10 records are included. Bars are sorted in descending order of count. Archive type is inferred from the prefix of each proxy identifier and used for color coding and legend construction.</p> Source code in <code>dod2k_utilities/ut_plot.py</code> <pre><code>def plot_count_proxy_by_archive_short(df, archive_proxy_count, archive_proxy_ticks, archive_colour) :\n    \"\"\"\n    Plot proxy counts by archive for proxy types exceeding a count threshold.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame containing proxy and archive metadata (not directly\n        used for plotting but retained for consistency).\n    archive_proxy_count : dict\n        Dictionary mapping proxy identifiers (e.g., ``\"archive: proxy\"``)\n        to record counts.\n    archive_proxy_ticks : list of str\n        Ordered list of proxy identifiers used for tick labels.\n    archive_colour : dict\n        Mapping from archive type to color.\n\n    Returns\n    -------\n    matplotlib.figure.Figure\n        Figure containing the bar chart.\n\n    Notes\n    -----\n    Only proxy types with more than 10 records are included. Bars are\n    sorted in descending order of count. Archive type is inferred from\n    the prefix of each proxy identifier and used for color coding and\n    legend construction.\n    \"\"\"\n\n    fig = plt.figure(figsize=(8, 5), dpi=500)\n    ax  = plt.gca()\n    count_by_proxy_short   = [archive_proxy_count[tt] for tt in archive_proxy_ticks if archive_proxy_count[tt]&gt;10 ]\n    ticks_by_proxy_short   = [tt for tt in archive_proxy_ticks if archive_proxy_count[tt]&gt;10 ]\n    cols_by_proxy_short    = [archive_colour[tt.split(':')[0]] for tt in archive_proxy_ticks if archive_proxy_count[tt]&gt;10 ]\n    archive_by_proxy_short = [tt.split(':')[0] for tt in archive_proxy_ticks if archive_proxy_count[tt]&gt;10 ]\n\n    sort = np.argsort(count_by_proxy_short)[::-1]\n\n    # create placeholder artists for legend and clean axis again\n    plt.bar(range(len(set(archive_by_proxy_short))), range(len(set(archive_by_proxy_short))), \n            color=[archive_colour[aa] for aa in set(archive_by_proxy_short)],\n            label=set(archive_by_proxy_short))\n    h, l = ax.get_legend_handles_labels()\n    plt.legend()\n    ax.cla()\n\n    plt.bar(np.arange(len(ticks_by_proxy_short)), np.array(count_by_proxy_short)[sort], \n            color=np.array(cols_by_proxy_short)[sort])\n\n    plt.xlabel('proxy type')\n    plt.ylabel('count')\n    ax.set_xticks(np.arange(len(ticks_by_proxy_short)), \n                  [ticks_by_proxy_short[ii] for ii in sort], \n                  rotation=45, ha='right', fontsize=10)\n    plt.legend(h[::-1], l[::-1])\n\n\n    fig.tight_layout()\n    return fig\n</code></pre>"},{"location":"api/ut_plot/#dod2k_utilities.ut_plot.plot_coverage","title":"<code>plot_coverage(df, archives_sorted, major_archives, other_archives, archive_colour, all=False, ysc='linear', return_data=False)</code>","text":"<p>Plot temporal coverage of proxy records, optionally separated by archive type.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing a <code>'year'</code> column with iterable year values for each record and an <code>'archiveType'</code> column.</p> required <code>archives_sorted</code> <code>list of str</code> <p>Ordered list of archive types present in the dataset.</p> required <code>major_archives</code> <code>list of str</code> <p>Archive types treated as major and plotted individually.</p> required <code>other_archives</code> <code>list of str</code> <p>Archive types grouped under the <code>'other'</code> category.</p> required <code>archive_colour</code> <code>dict</code> <p>Mapping from archive type (and <code>'other'</code>) to color.</p> required <code>all</code> <code>bool</code> <p>If True, plot total coverage across all archives.</p> <code>False</code> <code>ysc</code> <code>(linear, log)</code> <p>Y-axis scale.</p> <code>'linear'</code> <code>return_data</code> <code>bool</code> <p>If True, return coverage arrays in addition to the figure.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Figure</code> <p>Coverage plot figure.</p> <code>years</code> <code>(ndarray, optional)</code> <p>Array of years spanning the full temporal range.</p> <code>coverage</code> <code>(ndarray, optional)</code> <p>Total number of records available for each year.</p> <code>coverage_by_archive</code> <code>(dict, optional)</code> <p>Dictionary mapping archive type to yearly coverage arrays.</p> Notes <p>Coverage is defined as the number of records overlapping each year. Archive types not classified as major are aggregated into an <code>'other'</code> category.</p> Source code in <code>dod2k_utilities/ut_plot.py</code> <pre><code>def plot_coverage(df, archives_sorted, major_archives, other_archives, archive_colour, all=False, ysc='linear', return_data=False):\n    \"\"\"\n    Plot temporal coverage of proxy records, optionally separated by archive type.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame containing a ``'year'`` column with iterable year values\n        for each record and an ``'archiveType'`` column.\n    archives_sorted : list of str\n        Ordered list of archive types present in the dataset.\n    major_archives : list of str\n        Archive types treated as major and plotted individually.\n    other_archives : list of str\n        Archive types grouped under the ``'other'`` category.\n    archive_colour : dict\n        Mapping from archive type (and ``'other'``) to color.\n    all : bool, optional\n        If True, plot total coverage across all archives.\n    ysc : {'linear', 'log'}, optional\n        Y-axis scale.\n    return_data : bool, optional\n        If True, return coverage arrays in addition to the figure.\n\n    Returns\n    -------\n    matplotlib.figure.Figure\n        Coverage plot figure.\n    years : numpy.ndarray, optional\n        Array of years spanning the full temporal range.\n    coverage : numpy.ndarray, optional\n        Total number of records available for each year.\n    coverage_by_archive : dict, optional\n        Dictionary mapping archive type to yearly coverage arrays.\n\n    Notes\n    -----\n    Coverage is defined as the number of records overlapping each year.\n    Archive types not classified as major are aggregated into an\n    ``'other'`` category.\n    \"\"\"\n    #%% compute the coverage of all records and coverage per archive \n\n    MinY     = np.array([min([float(sy) for sy in yy])  for yy in df['year']]) # find minimum year for each record\n    MaxY     = np.array([max([float(sy) for sy in yy])  for yy in df['year']]) # find maximum year for each record\n    years    = np.arange(min(MinY), max(MaxY)+1)\n\n    # generate array of coverage (how many records are available each year, in total)\n    coverage = np.zeros(years.shape[0])\n    for ii in range(len(df['year'])):\n        coverage[(years&gt;=MinY[ii])&amp;(years&lt;=MaxY[ii])] += 1\n    # generate array of coverage for each archive type\n    coverage_by_archive = {arch: np.zeros(years.shape[0]) for arch in major_archives+['other'] }\n    for arch in archives_sorted:\n        arch_mask = df['archiveType']==arch \n        for ii in range(len(df[arch_mask]['year'])):\n            if arch not in major_archives: arch='other'\n            cc = coverage_by_archive[arch]\n            coverage_by_archive[arch][(years&gt;=MinY[arch_mask][ii])&amp;(years&lt;=MaxY[arch_mask][ii])] += 1\n\n    fig = plt.figure(figsize=(8, 4), dpi=200)\n    ax = plt.gca()\n    if all:\n        plt.step(years, coverage, color='k', label='all records', lw=3)\n    plt.xlabel('year')\n    plt.ylabel('total # of records')\n\n    plt.xlim(-100, 2020)\n    ax.grid(False)\n    if np.sum(coverage_by_archive['other'])==0:\n        archives = major_archives\n    else: archives = major_archives+['other']\n    for ii, arch in enumerate(archives):\n        plt.step(years, coverage_by_archive[arch], color=archive_colour[arch],\n                 label=arch, lw=1.8)\n\n    h1, l1 = ax.get_legend_handles_labels()\n    if ysc=='log':plt.legend(h1, [ll.replace(' ',' ') for ll in l1], \n                             ncol=4, framealpha=0, bbox_to_anchor=(0,1), loc='lower left' )\n    else:plt.legend(h1, l1, ncol=3, framealpha=0)\n    plt.ylabel('# of records per archive')\n    fig.tight_layout()\n    plt.yscale(ysc)\n    if return_data:\n        return fig, years, coverage, coverage_by_archive\n    return fig\n</code></pre>"},{"location":"api/ut_plot/#dod2k_utilities.ut_plot.plot_coverage2","title":"<code>plot_coverage2(df, years, title='')</code>","text":"<p>Plot the coverage of records over a range of years.</p> <p>This function counts how many records in the DataFrame overlap with each  year in the given range and produces a step plot showing total coverage.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing 'year' data for each record. Each row should have 'miny' and 'maxy' indicating the start and end year of the record.</p> required <code>years</code> <code>array - like</code> <p>Array of years over which to compute coverage.</p> required <code>title</code> <code>str</code> <p>Title of the plot. Default is an empty string.</p> <code>''</code> <p>Returns:</p> Type Description <code>Figure</code> <p>The matplotlib Figure object containing the plot.</p> Source code in <code>dod2k_utilities/ut_plot.py</code> <pre><code>def plot_coverage2(df, years, title=''):\n    \"\"\"\n    Plot the coverage of records over a range of years.\n\n    This function counts how many records in the DataFrame overlap with each \n    year in the given range and produces a step plot showing total coverage.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame containing 'year' data for each record. Each row should have\n        'miny' and 'maxy' indicating the start and end year of the record.\n    years : array-like\n        Array of years over which to compute coverage.\n    title : str, optional\n        Title of the plot. Default is an empty string.\n\n    Returns\n    -------\n    matplotlib.figure.Figure\n        The matplotlib Figure object containing the plot.\n    \"\"\"\n    coverage_filt = np.zeros(years.shape[0])\n\n    miny, maxy = years[0], years[-1]\n    for ii in range(len(df['year'])):\n        # time_12, int_1, int_2 = np.intersect1d(years, df.iloc[ii].year, return_indices=True)\n        coverage_filt[(years&gt;=df.iloc[ii].miny)&amp;(years&lt;=df.iloc[ii].maxy)] += 1\n        # coverage_filt[int_1]+=1\n\n    fig = plt.figure(figsize=(6, 3), dpi=100)\n    plt.title(title)\n    ax = plt.gca()\n    plt.step(years, coverage_filt, color='k', label='all records', lw=3)\n    plt.xlabel('year')\n    plt.ylabel('total # of records')\n\n    h1, l1 = ax.get_legend_handles_labels()\n    plt.legend(h1, l1, ncol=3, framealpha=0)\n    plt.ylabel('# of records per archive')\n    plt.show()\n    return fig\n</code></pre>"},{"location":"api/ut_plot/#dod2k_utilities.ut_plot.plot_coverage_analysis","title":"<code>plot_coverage_analysis(df, years, key, col, title='')</code>","text":"<p>Plot the coverage of records over a range of years.</p> <p>This function counts how many records in the DataFrame overlap with each  year in the given range and produces a step plot showing total coverage.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing 'year' data for each record. Each row should have 'miny' and 'maxy' indicating the start and end year of the record.</p> required <code>years</code> <code>array - like</code> <p>Array of years over which to compute coverage.</p> required <code>title</code> <code>str</code> <p>Title of the plot. Default is an empty string.</p> <code>''</code> <p>Returns:</p> Type Description <code>Figure</code> <p>The matplotlib Figure object containing the plot.</p> Source code in <code>dod2k_utilities/ut_plot.py</code> <pre><code>def plot_coverage_analysis(df, years, key, col, title=''):\n    \"\"\"\n    Plot the coverage of records over a range of years.\n\n    This function counts how many records in the DataFrame overlap with each \n    year in the given range and produces a step plot showing total coverage.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame containing 'year' data for each record. Each row should have\n        'miny' and 'maxy' indicating the start and end year of the record.\n    years : array-like\n        Array of years over which to compute coverage.\n    title : str, optional\n        Title of the plot. Default is an empty string.\n\n    Returns\n    -------\n    matplotlib.figure.Figure\n        The matplotlib Figure object containing the plot.\n    \"\"\"\n    coverage_filt = np.zeros(years.shape[0])\n\n    miny, maxy = years[0], years[-1]\n    for ii in range(len(df['year'])):\n        # time_12, int_1, int_2 = np.intersect1d(years, df.iloc[ii].year, return_indices=True)\n        coverage_filt[(years&gt;=df.iloc[ii].miny)&amp;(years&lt;=df.iloc[ii].maxy)] += 1\n        # coverage_filt[int_1]+=1\n\n    fig = plt.figure(figsize=(6, 3), dpi=100)\n    plt.title(title)\n    ax = plt.gca()\n    plt.step(years, coverage_filt, color=col, label=key, lw=3)\n    plt.xlabel('year')\n    plt.ylabel('total # of records')\n\n    h1, l1 = ax.get_legend_handles_labels()\n    plt.legend(h1, l1, ncol=3, framealpha=0)\n    plt.ylabel('# of records per archive')\n    plt.show()\n    return fig\n</code></pre>"},{"location":"api/ut_plot/#dod2k_utilities.ut_plot.plot_geo_archive_proxy","title":"<code>plot_geo_archive_proxy(df, archive_colour, highlight_archives=[], marker='default', size='default', figsize='default')</code>","text":"<p>Plot global distribution of proxy records grouped by archive and proxy type.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing geographic coordinates and proxy metadata. Must include <code>'geo_meanLat'</code>, <code>'geo_meanLon'</code>, <code>'archiveType'</code>, and <code>'paleoData_proxy'</code>.</p> required <code>archive_colour</code> <code>dict</code> <p>Mapping from archive type to color.</p> required <code>highlight_archives</code> <code>list of str</code> <p>Archive types to emphasize using archive-specific marker cycling.</p> <code>[]</code> <code>marker</code> <code>str or sequence</code> <p>Marker specification. If <code>'default'</code>, a predefined sequence of marker styles is used.</p> <code>'default'</code> <code>size</code> <code>int or float</code> <p>Marker size. If <code>'default'</code>, a preset size is used.</p> <code>'default'</code> <code>figsize</code> <code>tuple or str</code> <p>Figure size. If <code>'default'</code>, a predefined size is used.</p> <code>'default'</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Figure containing the global map.</p> Notes <p>Marker shape distinguishes proxy types, while color denotes archive type. Highlighted archives reuse marker cycling per archive, whereas non-highlighted archives use a global marker index.</p> Source code in <code>dod2k_utilities/ut_plot.py</code> <pre><code>def plot_geo_archive_proxy(df, archive_colour, highlight_archives=[], marker='default', size='default', figsize='default'):\n    \"\"\"\n    Plot global distribution of proxy records grouped by archive and proxy type.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame containing geographic coordinates and proxy metadata.\n        Must include ``'geo_meanLat'``, ``'geo_meanLon'``,\n        ``'archiveType'``, and ``'paleoData_proxy'``.\n    archive_colour : dict\n        Mapping from archive type to color.\n    highlight_archives : list of str, optional\n        Archive types to emphasize using archive-specific marker cycling.\n    marker : str or sequence, optional\n        Marker specification. If ``'default'``, a predefined sequence\n        of marker styles is used.\n    size : int or float, optional\n        Marker size. If ``'default'``, a preset size is used.\n    figsize : tuple or str, optional\n        Figure size. If ``'default'``, a predefined size is used.\n\n    Returns\n    -------\n    matplotlib.figure.Figure\n        Figure containing the global map.\n\n    Notes\n    -----\n    Marker shape distinguishes proxy types, while color denotes archive\n    type. Highlighted archives reuse marker cycling per archive, whereas\n    non-highlighted archives use a global marker index.\n    \"\"\"\n\n    proxy_lats = df['geo_meanLat'].values\n    proxy_lons = df['geo_meanLon'].values\n\n    # plots the map\n    figsize=(15, 12) if figsize=='default' else figsize\n    fig = plt.figure(figsize=figsize, dpi=350)\n    grid = GS(1, 3)\n\n    ax = plt.subplot(grid[:, :], projection=ccrs.Robinson()) # create axis with Robinson projection of globe\n\n    ax.add_feature(cfeature.LAND, alpha=0.5) # adds land features\n    ax.add_feature(cfeature.OCEAN, alpha=0.6, facecolor='#C5DEEA') # adds ocean features\n    ax.coastlines() # adds coastline features\n\n    ax.set_global()\n\n    # loop through the data to generate a scatter plot of each data record:\n    # 1st loop: go through archive types individually (determines marker type)\n    # 2nd loop: through paleo proxy types attributed to the specific archive, which is colour coded\n\n    if marker=='default':\n        mt = 'ov^s&lt;&gt;pP*XDdh'*10 # generates string of marker types\n    else:\n        mt = marker\n\n    if size=='default':\n        s = 200\n    else:\n        s = size\n    archive_types = np.unique(df['archiveType'])\n\n    ijk=0\n    for jj, at in enumerate(archive_types):\n        arch_mask = df['archiveType']==at\n        arch_proxy_types = np.unique(df['paleoData_proxy'][arch_mask])\n        for ii, pt in enumerate(arch_proxy_types):\n            pt_mask = df['paleoData_proxy']==pt\n            at_mask = df['archiveType']==at\n            label = at+': '+pt+' ($n=%d$)'% df['paleoData_proxy'][(df['paleoData_proxy']==pt)&amp;(df['archiveType']==at)].count()\n            marker = mt[ii] if at in highlight_archives else mt[ijk]\n            plt.scatter(proxy_lons[pt_mask&amp;at_mask], proxy_lats[pt_mask&amp;at_mask], \n                        transform=ccrs.PlateCarree(), zorder=999,\n                        marker=marker, color=archive_colour[at], \n                        label=label,#.replace('marine sediment:', 'marine sediment:\\n'), \n                        lw=.3, ec='k', s=s)\n            if at not in highlight_archives: ijk+=1\n\n    plt.legend(bbox_to_anchor=(-0.01,-0.01), loc='upper left', ncol=3, fontsize=13.5, framealpha=0)\n    grid.tight_layout(fig)\n    return fig\n</code></pre>"},{"location":"api/ut_plot/#dod2k_utilities.ut_plot.plot_geo_archive_proxy_short","title":"<code>plot_geo_archive_proxy_short(df, archives_sorted, archive_proxy_count_short, archive_colour)</code>","text":"<p>Plot geographical distribution of proxy records for major archive\u2013proxy combinations.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing geographic coordinates and proxy metadata. Must include <code>'geo_meanLat'</code>, <code>'geo_meanLon'</code>, <code>'archiveType'</code>, and <code>'paleoData_proxy'</code>.</p> required <code>archives_sorted</code> <code>list of str</code> <p>Ordered list of archive types to control plotting and legend order.</p> required <code>archive_proxy_count_short</code> <code>dict</code> <p>Nested dictionary mapping archive types to proxy counts, including grouped <code>'other'</code> proxy categories.</p> required <code>archive_colour</code> <code>dict</code> <p>Mapping from archive type to color.</p> required <p>Returns:</p> Type Description <code>Figure</code> <p>Figure containing the global map.</p> Notes <p>Each archive\u2013proxy combination is plotted with a distinct marker, while colors indicate archive type. Proxies classified as <code>'other'</code> are plotted using masks that exclude explicitly listed proxy types.</p> Source code in <code>dod2k_utilities/ut_plot.py</code> <pre><code>def plot_geo_archive_proxy_short(df, archives_sorted, archive_proxy_count_short, archive_colour):\n    \"\"\"\n    Plot geographical distribution of proxy records for major archive\u2013proxy combinations.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame containing geographic coordinates and proxy metadata.\n        Must include ``'geo_meanLat'``, ``'geo_meanLon'``,\n        ``'archiveType'``, and ``'paleoData_proxy'``.\n    archives_sorted : list of str\n        Ordered list of archive types to control plotting and legend order.\n    archive_proxy_count_short : dict\n        Nested dictionary mapping archive types to proxy counts, including\n        grouped ``'other'`` proxy categories.\n    archive_colour : dict\n        Mapping from archive type to color.\n\n    Returns\n    -------\n    matplotlib.figure.Figure\n        Figure containing the global map.\n\n    Notes\n    -----\n    Each archive\u2013proxy combination is plotted with a distinct marker,\n    while colors indicate archive type. Proxies classified as ``'other'``\n    are plotted using masks that exclude explicitly listed proxy types.\n    \"\"\"\n\n    proxy_lats = df['geo_meanLat'].values\n    proxy_lons = df['geo_meanLon'].values\n\n    # plots the map\n    fig = plt.figure(figsize=(13, 8), dpi=350)\n    grid = GS(1, 3)\n\n    ax = plt.subplot(grid[:, :], projection=ccrs.Robinson()) # create axis with Robinson projection of globe\n    # ax.stock_img()\n\n\n    ax.add_feature(cfeature.LAND, alpha=0.6) # adds land features\n    ax.add_feature(cfeature.OCEAN, alpha=0.6, facecolor='#C5DEEA') # adds ocean features\n    ax.coastlines() # adds coastline features\n\n    ax.set_global()\n\n\n    mt = 'ov^s&lt;&gt;pP*XDdh'*10 # generates string of marker types\n\n    ijk=0\n    for at in archives_sorted:\n        print(sorted(archive_proxy_count_short[at]))\n        for ii, key in enumerate(sorted(archive_proxy_count_short[at])):\n            marker = mt[ii]\n            if 'other' not in key: \n                at, pt = key.split(': ')\n                at_mask = df['archiveType']==at\n                pt_mask = df['paleoData_proxy']==pt\n                label = key+' (n=%d)'%archive_proxy_count_short[at][key]\n            else:\n                at= key.split('other ')[-1]\n                exclude_types = [kk.split(': ')[-1] for kk in archive_proxy_count_short[at].keys() if at in kk if 'other' not in kk]\n                at_mask = df['archiveType']==at\n                pt_mask = ~np.isin(df['paleoData_proxy'], exclude_types)\n                label = key+' (n=%d)'%df['paleoData_proxy'][pt_mask&amp;at_mask].count()\n                if exclude_types==[]:\n                    marker=mt[ijk]\n                    ijk+=1\n                    label = label.replace('other ','')\n            plt.scatter(proxy_lons[pt_mask&amp;at_mask], proxy_lats[pt_mask&amp;at_mask], \n                        transform=ccrs.PlateCarree(), zorder=999,\n                        marker=marker, \n                        color=archive_colour[at], \n                        label=label,\n                        lw=.3, ec='k', s=150)\n\n    plt.legend(bbox_to_anchor=(0.03,-0.01), loc='upper left', ncol=3, fontsize=12, framealpha=0)\n    grid.tight_layout(fig)\n    return fig\n</code></pre>"},{"location":"api/ut_plot/#dod2k_utilities.ut_plot.plot_length","title":"<code>plot_length(df, title='', mincount=0, col='tab:blue')</code>","text":"<p>Plot a histogram of lengths from a DataFrame.</p> <p>This function bins the 'length' values in the DataFrame into predefined ranges, optionally filters bins with counts below <code>mincount</code>, and displays a bar plot.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing a column named 'length' with numeric values.</p> required <code>title</code> <code>str</code> <p>Title of the plot.</p> <code>''</code> <code>mincount</code> <code>int</code> <p>Minimum count threshold for bins. Bins with fewer counts than <code>mincount</code> are excluded from the plot. Default is 0 (all bins shown).</p> <code>0</code> <p>Returns:</p> Type Description <code>None</code> <p>The function displays a matplotlib bar plot and does not return any value.</p> Source code in <code>dod2k_utilities/ut_plot.py</code> <pre><code>def plot_length(df, title='', mincount=0, col='tab:blue'):\n    \"\"\"\n    Plot a histogram of lengths from a DataFrame.\n\n    This function bins the 'length' values in the DataFrame into predefined ranges,\n    optionally filters bins with counts below `mincount`, and displays a bar plot.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame containing a column named 'length' with numeric values.\n    title : str, optional\n        Title of the plot.\n    mincount : int, optional\n        Minimum count threshold for bins. Bins with fewer counts than `mincount`\n        are excluded from the plot. Default is 0 (all bins shown).\n\n    Returns\n    -------\n    None\n        The function displays a matplotlib bar plot and does not return any value.\n    \"\"\"\n    count_res = {'%s-%s'%(ii, ii+50): 0 for ii in range(0, 200, 50) }\n    count_res.update({'%s-%s'%(ii, ii+100): 0 for ii in range(200, 800, 100) })\n    count_res['&gt;800'] = 0\n    for dd in df['length'].values:\n        if dd&gt;800:\n            count_res['&gt;800']+=1\n        for ii in range(0, 200, 50):\n            if dd in range(ii, ii+50):\n                count_res['%s-%s'%(ii, ii+50)]+=1\n        for ii in range(200, 800, 100):\n            if dd in range(ii, ii+100):\n                count_res['%s-%s'%(ii, ii+100)]+=1\n\n\n\n    plt.figure(dpi=100, figsize=(5,3))\n    plt.title(title)\n    ax=plt.gca()\n    ii=0\n    rr=[]\n    for res, count in count_res.items():\n        if count&lt;mincount: continue\n        plt.bar(ii, count, color=col)\n        ii+=1\n        rr+=[res]\n    ax.set_xticks(range(ii))\n    ax.set_xticklabels(rr, rotation=45, ha='right', fontsize=7)\n    plt.xlabel('length')\n    plt.ylabel('count')\n    plt.show()\n    return \n</code></pre>"},{"location":"api/ut_plot/#dod2k_utilities.ut_plot.plot_resolution","title":"<code>plot_resolution(df, title='', mincount=0, col='tab:blue')</code>","text":"<p>Plot a histogram of resolutions from a DataFrame.</p> <p>This function counts the occurrences of each \"resolution\" in the DataFrame, optionally merges bins with counts below <code>mincount</code>, and displays a bar plot.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing a column named 'resolution', where each entry is  a list of integers representing resolution values.</p> required <code>title</code> <code>str</code> <p>Title of the plot.</p> <code>''</code> <code>mincount</code> <code>int</code> <p>Minimum count threshold for individual resolution bins. Bins with fewer counts than <code>mincount</code> are merged into a coarser bin. Default is 0  (no merging).</p> <code>0</code> <p>Returns:</p> Type Description <code>None</code> <p>The function displays a matplotlib bar plot and does not return any value.</p> Source code in <code>dod2k_utilities/ut_plot.py</code> <pre><code>def plot_resolution(df, title='', mincount=0, col='tab:blue'):\n    \"\"\"\n    Plot a histogram of resolutions from a DataFrame.\n\n    This function counts the occurrences of each \"resolution\" in the DataFrame,\n    optionally merges bins with counts below `mincount`, and displays a bar plot.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame containing a column named 'resolution', where each entry is \n        a list of integers representing resolution values.\n    title : str, optional\n        Title of the plot.\n    mincount : int, optional\n        Minimum count threshold for individual resolution bins. Bins with fewer\n        counts than `mincount` are merged into a coarser bin. Default is 0 \n        (no merging).\n\n    Returns\n    -------\n    None\n        The function displays a matplotlib bar plot and does not return any value.\n\n    \"\"\"\n    count_res = {}\n    for dd in df['resolution'].values:\n        if len(dd)&gt;1:\n            res = '%d - %d'%(min(dd), max(dd))\n        else:\n            res='%d'%min(dd)\n        if res not in count_res: \n            count_res[res]=0\n\n        count_res[res]+=1\n\n    if mincount!=0:\n        rmv = []\n        for kk in list(count_res):\n            if count_res[kk]&lt;mincount:\n                maxres = float(kk.split(' - ')[-1])\n                if maxres&lt;6:\n                    maxres=5+np.round(maxres/10.)*10\n                    newkey='   &lt;%d'%maxres\n                elif maxres&lt;95:\n                    maxres=5+np.round(maxres/10.)*10\n                    newkey='  &lt;%d'%maxres\n                else:\n                    maxres=50+np.round(maxres/100.)*100\n                    newkey=' &lt;%d'%maxres\n                if newkey not in count_res:\n                    count_res[newkey]=0\n                    print(kk, newkey)\n                count_res[newkey]+=1\n                rmv+=[kk]\n        for kk in rmv: del count_res[kk]\n\n\n    plt.figure(dpi=100, figsize=(5,3))\n    plt.title(title)\n    ax=plt.gca()\n    ii=0\n    rr=[]\n    for kk in np.sort(list(count_res)):\n        plt.bar(ii, count_res[kk], color=col)\n        ii+=1\n        rr+=[kk]\n    ax.set_xticks(range(ii))\n    ax.set_xticklabels(rr, rotation=45, ha='right', fontsize=7)\n    plt.xlabel('resolution')\n    plt.ylabel('count')\n    plt.show()\n    return \n</code></pre>"},{"location":"api/ut_plot/#dod2k_utilities.ut_plot.shade_percentiles","title":"<code>shade_percentiles(x, y, color, ax, alpha=1, lu=False, zorder=None, lw=1, ups=[60, 70, 80, 90, 95], label=None)</code>","text":"<p>Shade percentile ranges of an ensemble on a matplotlib axis.</p> <p>Creates overlapping shaded regions showing different percentile ranges of an ensemble, useful for visualizing uncertainty in climate data.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array - like</code> <p>Time or x-axis values (1D array of length n).</p> required <code>y</code> <code>array - like</code> <p>Ensemble data as m\u00d7n array where m is ensemble dimension and n is time dimension.</p> required <code>color</code> <code>str or tuple</code> <p>Color for shading (matplotlib color specification).</p> required <code>ax</code> <code>Axes</code> <p>Axes object to plot on.</p> required <code>alpha</code> <code>float</code> <p>Overall transparency multiplier (0-1). Default is 1.</p> <code>1</code> <code>lu</code> <code>bool</code> <p>If True, plot dotted lines at 5th and 95th percentiles. Default is False.</p> <code>False</code> <code>zorder</code> <code>float</code> <p>Drawing order for the shaded regions. Default is None.</p> <code>None</code> <code>lw</code> <code>float</code> <p>Line width for percentile boundary lines if lu=True. Default is 1.</p> <code>1</code> <code>ups</code> <code>list of float</code> <p>Upper percentiles to shade. Default is [60, 70, 80, 90, 95].</p> <code>[60, 70, 80, 90, 95]</code> <code>label</code> <code>str</code> <p>Label for legend (applied to outermost shading). Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> Notes <p>Shades symmetric percentile ranges with decreasing opacity: - Innermost: 40th-60th percentile (darkest) - Outermost: 5th-95th percentile (lightest)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fig, ax = plt.subplots()\n&gt;&gt;&gt; x = np.arange(100)\n&gt;&gt;&gt; y = np.random.randn(50, 100)  # 50 ensemble members, 100 time steps\n&gt;&gt;&gt; shade_percentiles(x, y, 'blue', ax, lu=True, label='Ensemble')\n</code></pre> Source code in <code>dod2k_utilities/ut_plot.py</code> <pre><code>def shade_percentiles(x, y, color, ax, alpha=1, lu=False, zorder=None, lw=1,\n                      ups=[60, 70, 80, 90, 95], label=None):\n    \"\"\"\n    Shade percentile ranges of an ensemble on a matplotlib axis.\n\n    Creates overlapping shaded regions showing different percentile ranges\n    of an ensemble, useful for visualizing uncertainty in climate data.\n\n    Parameters\n    ----------\n    x : array-like\n        Time or x-axis values (1D array of length n).\n    y : array-like\n        Ensemble data as m\u00d7n array where m is ensemble dimension and n is\n        time dimension.\n    color : str or tuple\n        Color for shading (matplotlib color specification).\n    ax : matplotlib.axes.Axes\n        Axes object to plot on.\n    alpha : float, optional\n        Overall transparency multiplier (0-1). Default is 1.\n    lu : bool, optional\n        If True, plot dotted lines at 5th and 95th percentiles. Default is False.\n    zorder : float, optional\n        Drawing order for the shaded regions. Default is None.\n    lw : float, optional\n        Line width for percentile boundary lines if lu=True. Default is 1.\n    ups : list of float, optional\n        Upper percentiles to shade. Default is [60, 70, 80, 90, 95].\n    label : str, optional\n        Label for legend (applied to outermost shading). Default is None.\n\n    Returns\n    -------\n    None\n\n    Notes\n    -----\n    Shades symmetric percentile ranges with decreasing opacity:\n    - Innermost: 40th-60th percentile (darkest)\n    - Outermost: 5th-95th percentile (lightest)\n\n    Examples\n    --------\n    &gt;&gt;&gt; fig, ax = plt.subplots()\n    &gt;&gt;&gt; x = np.arange(100)\n    &gt;&gt;&gt; y = np.random.randn(50, 100)  # 50 ensemble members, 100 time steps\n    &gt;&gt;&gt; shade_percentiles(x, y, 'blue', ax, lu=True, label='Ensemble')\n    \"\"\"\n    # shades the percentiles of an mxn array (ensemble dimension: m, time dimension: n)\n    lows = [100-ii for ii in ups]\n    alps = np.array([0.5,0.4, 0.35, 0.3, 0.25])*alpha\n    ii=0\n    for l, u, a in zip(lows, ups, alps):\n        mina = np.nanpercentile(dc(y), l, axis=0)\n        maxa = np.nanpercentile(dc(y), u, axis=0)\n        X    = dc(np.array(x))\n        ll=label if ii==0 else None\n        ax.fill_between(X, y1=mina, y2=maxa, color=color, alpha=a, lw=0, \n                        zorder=zorder, label=ll)\n        if (l, u) == (5, 95) and lu:\n            ax.plot(X, mina, color=color, lw=lw, ls=':', zorder=zorder*2)\n            ax.plot(X, maxa, color=color, lw=lw, ls=':', zorder=zorder*2)\n        ii+=1\n    return\n</code></pre>"},{"location":"getting_started/","title":"Overview","text":"<ul> <li> <p> Installation Guide</p> <p>Step-by-step installation </p> <p> Installation</p> </li> <li> <p> Quickstart Guide</p> <p>Set up your environment and run your first analysis</p> <p> Quickstart</p> </li> </ul>"},{"location":"getting_started/installation/","title":"Installation","text":"<p>This page explains how to set up the DoD2k Python package and its dependencies.</p>"},{"location":"getting_started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.9+</li> <li>Conda</li> <li>JupyterLab or Jupyter Notebook</li> <li>Required Python packages (see <code>dod2k-env.yml</code>)</li> </ul>"},{"location":"getting_started/installation/#setup","title":"Setup","text":"<ol> <li> <p>Clone the repository</p> <pre><code>git clone https://github.com/lluecke/dod2k.git\ncd dod2k\n</code></pre> </li> <li> <p>Create the Conda environment from the YAML file:</p> <p><code>bash    conda env create -f dod2k-environment.yml</code></p> </li> <li> <p>Activate the environment:</p> <pre><code>conda activate dod2k\n</code></pre> </li> <li> <p>Launch JupyterLab or Jupyter Notebook:</p> <p><pre><code>jupyter lab\n</code></pre> or  <pre><code>jupyter notebook\n</code></pre></p> </li> </ol>"},{"location":"getting_started/installation/#next-steps","title":"Next Steps","text":"<p>Once the environment is set up:</p> <ul> <li>Explore the Quickstart Tutorial</li> <li>Check out the Tutorials to e.g.<ul> <li>Load and visualise DoD2k and use for data analysis</li> <li>Generate DoD2k from scratch</li> <li>Run a duplicate detection workflow on the merged database</li> </ul> </li> <li>Run example notebooks:<ul> <li>df_info.ipynb</li> <li>df_plot_dod2k.ipynb</li> </ul> </li> <li>Check the API Reference for detailed module documentation</li> </ul>"},{"location":"getting_started/quickstart/","title":"Quickstart","text":"<p>How to get started with the dod2k environment, functions, notebooks and products.</p>"},{"location":"getting_started/quickstart/#for-database-use-dod2k","title":"For database use (DoD2k)","text":"<ol> <li> <p>Get the project: in a working directory,</p> <pre><code>git clone https://github.com/lluecke/dod2k.git\n</code></pre> </li> <li> <p>Create and activate the python environment: in dod2k/, </p> <pre><code>conda env create -n dod2k-env -f dod2k-env.yml\nconda activate dod2k-env\n</code></pre> </li> <li> <p>Explore DoD2k: use the notebooks</p> <pre><code>notebooks/df_info.ipynb\nnotebooks/df_plot_dod2k.ipynb\nnotebooks/df_filter.ipynb\n</code></pre> </li> <li> <p>Applications of DoD2k</p> </li> <li> <p>For analysis of moisture/temperature/moisture and temperature sensitive records use</p> <pre><code>```\nnotebooks/analysis_M.ipynb\nnotebooks/analysis_MT.ipynb\nnotebooks/analysis_T.ipynb\n```\n</code></pre> <ol> <li>For speleothem analysis:</li> </ol> <p>To run <code>notebooks/S_analysis_v1.6.ipynb</code> you will first need to create the directory <code>data/speleothem_modeling_inputs</code>, and download into it data from their source urls:</p> <pre><code>mkdir speleothem_modeling_inputs\ncd speleothem_modeling_inputs\nwget https://wateriso.utah.edu/waterisotopes/media/ArcGrids/GlobalPrecip.zip\nunzip GlobalPrecip.zip\nwget https://crudata.uea.ac.uk/cru/data/hrg/cru_ts_4.07/cruts.2304141047.v4.07/tmp/cru_ts4.07.1901.2022.tmp.dat.nc.gz\ngunzip cru_ts4.07.1901.2022.tmp.dat.nc.gz\n</code></pre> </li> </ol>"},{"location":"getting_started/quickstart/#for-toolkit-use-dt2k","title":"For toolkit use (DT2k)","text":"<ol> <li> <p>Get the project: in a working directory,</p> <pre><code>git clone https://github.com/lluecke/dod2k.git\n</code></pre> </li> <li> <p>Create and activate the python environment: in dod2k/, </p> <pre><code>conda env create -n dod2k-env -f dod2k-env.yml\nconda activate dod2k-env\n</code></pre> </li> <li> <p>Create a common dataframe from source databases (OPTIONAL)</p> <ol> <li> <p>Load scripts for input databases:</p> <pre><code>data/pages2k/load_pages2k.ipynb\ndata/fe23/load_fe23.ipynb\ndata/iso2k/load_iso2k.ipynb\ndata/sisal/load_sisal.ipynb\ndata/ch2k/load_ch2k.ipynb\n</code></pre> </li> <li> <p>Merge databases</p> <pre><code>data/dod2k/merge_databases.ipynb\n</code></pre> </li> </ol> <p>Note: these notebooks serve for creating compact dataframes from source data and for creating a common dataframe by merging all the databases into one dataframe. If you are not interested in this step, it can be skipped and you can use the compact dataframes as provided in the directories (<code>csv</code> or <code>pkl</code> files). For altering the source data (e.g. updating a database or adding one), you can add/edit these notebooks accordingly.</p> </li> <li> <p>Run duplicate workflow</p> <p>The following steps recreate the complete duplicate workflow.</p> <ol> <li> <p>Duplicate detection: If you have altered any source data, run:</p> <pre><code>notebooks/dup_detection.ipynb \n</code></pre> <p>This notebook goes through each pair of records to identify potential duplicate candidates. Careful, this will be computationally heavy and may take some time to run!        The notebooks outputs the file        <pre><code>root/data/dod2k/dup_detection/dup_detection_candidates_dod2k.csv\n</code></pre>        This file will be used for the decision process (next step). If you have not changed any source data, you may skip this step and proceed with the next step.</p> </li> <li> <p>Duplicate decision process: run</p> <p><pre><code>notebooks/dup_decision.ipynb\n</code></pre>  This file walks you through all the potential duplicate candidates and asks for decisions on certain duplicate candidate pairs. The decisions are saved in</p> <pre><code>root/data/dod2k/dup_detection/dup_decisions_dod2k_{INITIALS}_{DATECREATED}.csv\n</code></pre> <p>Note: The decision process may be lengthy and get interrupted by server issues. However a backup file is created during the workflow and it should be possible to restart where you left off when running the file. However in order for this to work it is required that your initials and the date match the backup file!! If you restart on another day, it is necessary that you alter the date of the backup file accordingly. The backup file can be found here: <pre><code>root/data/dod2k/dup_detection/dup_decisions_dod2k_{INITIALS}_{DATECREATED}_BACKUP.csv\n</code></pre></p> </li> <li> <p>Duplicate removal process: run</p> <p><pre><code>notebooks/dup_removal.ipynb\n</code></pre> to implement all the decisions and to create a duplicate free compact dataframe.</p> </li> </ol> </li> <li> <p>Rerun the duplicate process (check for remaining duplicates) for <code>dod2k_dupfree</code>. Creates <code>dod2k_dupfree_dupfree</code> (which is published as DoD2k)</p> </li> <li> <p>Explore output (see step #2 for database use)</p> <p>If you want to see your own output you will need to alter the <code>key</code> for loading according to your initials and the date of the file created:</p> <pre><code>db_name = 'dod2k_dupfree_dupfree'\npath = 'data/dod2k/'\nfile = 'dod2k_dupfree_{INITIALS}_{DATECREATED}_dupfree'\n# load dataframe\ndf = utf.load_compact_dataframe_from_csv(db_name, readfrom=(path, filename))\nprint(df.info())\ndf.name = db_name\n</code></pre> </li> </ol>"},{"location":"notebooks/","title":"Notebooks","text":"<p>Interactive Jupyter notebooks for working with DoD2k. Each notebook provides a complete workflow for specific tasks.</p>"},{"location":"notebooks/#loading-individual-databases","title":"Loading Individual Databases","text":"<ul> <li> <p> Load CoralHydro2k</p> <p>Load and process the CoralHydro2k database</p> <p> Open notebook</p> </li> <li> <p> Load FE23</p> <p>Load and process the FE23 database</p> <p> Open notebook</p> </li> <li> <p> Load Iso2k</p> <p>Load and process the Iso2k database</p> <p> Open notebook</p> </li> <li> <p> Load PAGES2k</p> <p>Load and process the PAGES2k database</p> <p> Open notebook</p> </li> <li> <p> Load SISAL</p> <p>Load and process the SISAL speleothem database</p> <p> Open notebook</p> </li> <li> <p> Merge Databases</p> <p>Combine multiple databases into DoD2k</p> <p> Open notebook</p> </li> </ul>"},{"location":"notebooks/#duplicate-detection-workflow","title":"Duplicate Detection Workflow","text":"<ul> <li> <p> Step 1: Detect Duplicates</p> <p>Identify potential duplicate records across databases</p> <p> Open notebook</p> </li> <li> <p> Step 2: Review Duplicates</p> <p>Review and classify detected duplicate candidates</p> <p> Open notebook</p> </li> <li> <p> Step 3: Remove Duplicates</p> <p>Remove confirmed duplicates from the database</p> <p> Open notebook</p> </li> </ul>"},{"location":"notebooks/#visualization-exploration","title":"Visualization &amp; Exploration","text":"<ul> <li> <p> Explore Database Info</p> <p>View summary statistics and metadata of the compact dataframe</p> <p> Open notebook</p> </li> <li> <p> Plot DoD2k</p> <p>Create visualizations of proxy records and spatial distributions</p> <p> Open notebook</p> </li> <li> <p> Filter Database</p> <p>Filter records by metadata criteria (archive type, location, etc.)</p> <p> Open notebook</p> </li> </ul>"},{"location":"notebooks/#analysis-of-filtered-database","title":"Analysis of filtered database","text":"<ul> <li> <p> Moisture and temperature-moisture records</p> <p>Analyse moisture and moisture-temperature sensitive records </p> <p> Open notebook</p> </li> <li> <p> Speleothem records only</p> <p>Analyse speleothem records</p> <p> Open notebook</p> </li> <li> <p> Moisture records only</p> <p>Analyse moisture sensitive records </p> <p> Open notebook</p> </li> <li> <p> Temperature records</p> <p>Analyse temperature sensitive records </p> <p> Open notebook</p> </li> </ul>"},{"location":"notebooks/#quick-access","title":"Quick Access","text":"<p>Typical workflows:</p> <ol> <li>Load data \u2192 Use individual load notebooks or merge_databases.ipynb</li> <li>Clean data \u2192 Run duplicate detection workflow (3 steps)</li> <li>Explore data \u2192 Use df_info.ipynb and df_plot_dod2k.ipynb</li> <li>Filter data \u2192 Use df_filter.ipynb for targeted analysis</li> </ol>"},{"location":"notebooks/analysis_moisttemp/","title":"Analyse moisture and moisture-temperature records","text":"<p>This notebook performs a PCA for a subset of records, filtered by archiveType/paleoData_proxy. For each subset, the following algorithm is being used:</p> <ol> <li>Filter archive_type and paleoData_proxy (defines subset) and produces summary plots of the data, in particular regarding: coverage, resolution and length of the records. This gives us information for the next step, in which we need to choose the parameters for the PCA</li> <li>Define proxy specific parameters for the PCA:<ul> <li>period (start and end year): choose a period of sufficient data density (all records chosen for the analysis need to at least overlap during this period)</li> <li>minimum resolution: records exceeding this resolution are being excluded from the analysis. Records with higher resolution will be subsampled to create homogeneous resolution across all the records.</li> <li>record length: records shorter than the record length are being excluded from the analysis.</li> <li>The choice of parameters will determine the success of the PCA. There is a trade-off between the number of records included and the quality (i.e. period/record length/resolution).</li> <li>Summary figures are being produced for the filtered data</li> <li>z-scores added to dataframe (mean=0 and std=1 over the entire record) as 'paleoData_zscores'</li> <li>note: z-scores may be biased if records are only partly overlapping in time, or increase in availability over time, or both.</li> </ul> </li> <li>Homogenise data dimensions across the records<ul> <li>defines a homogenised time variable over the target period and with the target resolution (as defined in the last step), which is saved as a new column in the dataframe named 'years_hom'</li> <li>creates a data matrix with dimensions n_records x n_time which is saved as a new column in df, named 'paleoData_values_hom' and 'paleoData_zscores_hom'.</li> <li>Note that this data is formatted as a np.ma.masked_array, where missing data is set to zero and masked out.</li> </ul> </li> <li>PCA<ul> <li>obtains covariance matrix of paleoData_zscores_hom (note that for every two records the covariance is calculated over their intersect of data availability)</li> <li>obtains eigenvectors and eigenvalues via SVD composition</li> <li>obtains and plots fraction of explained variance, first two PCs and load for first two EOFs vs ordering in the data frame.</li> </ul> </li> </ol> <p>2025/01/08 v0: Outlined by Michael N. Evans for MT analysis; MT analysis notebook written by Lucie J. Luecke.</p> <p>2025/02/07-03/19 v0.9: Notebook now includes summary figures at the end which plot results for different proxies on same axis and the global map of EOF loadings. Added documentation.  Minor changes to make table3 of the manuscript for ESSD, and to revise MT_PCA.pdf output.</p> <p>2025/12/17: Tidied up and updated for DoD2k v2.0</p> <p>Make sure the repo_root is added correctly, it should be: <code>your_root_dir/dod2k</code> This should be the working directory throughout this notebook (and all other notebooks).</p> In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n\nimport sys\nimport os\nfrom pathlib import Path\n\n# Add parent directory to path (works from any notebook in notebooks/)\n# the repo_root should be the parent directory of the notebooks folder\ninit_dir = Path().resolve()\n# Determine repo root\nif init_dir.name == 'dod2k': repo_root = init_dir\nelif init_dir.parent.name == 'dod2k': repo_root = init_dir.parent\nelse: raise Exception('Please review the repo root structure (see first cell).')\n\n# Update cwd and path only if needed\nif os.getcwd() != str(repo_root):\n    os.chdir(repo_root)\nif str(repo_root) not in sys.path:\n    sys.path.insert(0, str(repo_root))\n\nprint(f\"Repo root: {repo_root}\")\nif str(os.getcwd())==str(repo_root):\n    print(f\"Working directory matches repo root. \")\n</pre> %load_ext autoreload %autoreload 2  import sys import os from pathlib import Path  # Add parent directory to path (works from any notebook in notebooks/) # the repo_root should be the parent directory of the notebooks folder init_dir = Path().resolve() # Determine repo root if init_dir.name == 'dod2k': repo_root = init_dir elif init_dir.parent.name == 'dod2k': repo_root = init_dir.parent else: raise Exception('Please review the repo root structure (see first cell).')  # Update cwd and path only if needed if os.getcwd() != str(repo_root):     os.chdir(repo_root) if str(repo_root) not in sys.path:     sys.path.insert(0, str(repo_root))  print(f\"Repo root: {repo_root}\") if str(os.getcwd())==str(repo_root):     print(f\"Working directory matches repo root. \") <pre>Repo root: /home/jupyter-lluecke/dod2k\nWorking directory matches repo root. \n</pre> In\u00a0[73]: Copied! <pre># Import packages\nimport numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec as GS\n\n\nfrom dod2k_utilities import ut_functions as utf # contains utility functions\nfrom dod2k_utilities import ut_plot as uplt # contains plotting functions\nfrom dod2k_utilities import ut_analysis as uta # contains plotting functions\n</pre> # Import packages import numpy as np import pandas as pd import os import matplotlib.pyplot as plt from matplotlib.gridspec import GridSpec as GS   from dod2k_utilities import ut_functions as utf # contains utility functions from dod2k_utilities import ut_plot as uplt # contains plotting functions from dod2k_utilities import ut_analysis as uta # contains plotting functions <p>Read compact dataframe.</p> <p>{db_name} refers to the database, including</p> <pre><code>- dod2k_v2.0_filtered_M_TM (filtered for moisture and temperature+moisture sensitive records only</code></pre> <p>All compact dataframes are saved in {repo_root}/data/{db_name} as {db_name}_compact.csv.</p> In\u00a0[3]: Copied! <pre># read dataframe, choose from the list below, or specify your own\n\ndb_name = 'dod2k_v2.0_filtered_M_TM'\n\n# load dataframe\ndf = utf.load_compact_dataframe_from_csv(db_name)\nprint(df.info())\ndf.name = db_name\n</pre> # read dataframe, choose from the list below, or specify your own  db_name = 'dod2k_v2.0_filtered_M_TM'  # load dataframe df = utf.load_compact_dataframe_from_csv(db_name) print(df.info()) df.name = db_name  <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1416 entries, 0 to 1415\nData columns (total 22 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   archiveType                    1416 non-null   object \n 1   dataSetName                    1416 non-null   object \n 2   datasetId                      1416 non-null   object \n 3   duplicateDetails               1416 non-null   object \n 4   geo_meanElev                   1398 non-null   float32\n 5   geo_meanLat                    1416 non-null   float32\n 6   geo_meanLon                    1416 non-null   float32\n 7   geo_siteName                   1416 non-null   object \n 8   interpretation_direction       1416 non-null   object \n 9   interpretation_seasonality     1416 non-null   object \n 10  interpretation_variable        1416 non-null   object \n 11  interpretation_variableDetail  1416 non-null   object \n 12  originalDataURL                1416 non-null   object \n 13  originalDatabase               1416 non-null   object \n 14  paleoData_notes                1416 non-null   object \n 15  paleoData_proxy                1416 non-null   object \n 16  paleoData_sensorSpecies        1416 non-null   object \n 17  paleoData_units                1416 non-null   object \n 18  paleoData_values               1416 non-null   object \n 19  paleoData_variableName         1416 non-null   object \n 20  year                           1416 non-null   object \n 21  yearUnits                      1416 non-null   object \ndtypes: float32(3), object(19)\nmemory usage: 226.9+ KB\nNone\n</pre> In\u00a0[4]: Copied! <pre># cols = [ '#4477AA', '#EE6677', '#228833', '#CCBB44', '#66CCEE', '#AA3377', '#BBBBBB', '#44AA99', '#332288']\n</pre> # cols = [ '#4477AA', '#EE6677', '#228833', '#CCBB44', '#66CCEE', '#AA3377', '#BBBBBB', '#44AA99', '#332288'] In\u00a0[5]: Copied! <pre>time    = {}\ndata    = {}\nPCs     = {}\nEOFs    = {}\nfoev    = {}\npca_rec = {}\n\nkeys    = []\n</pre> time    = {} data    = {} PCs     = {} EOFs    = {} foev    = {} pca_rec = {}  keys    = [] In\u00a0[6]: Copied! <pre># (1) filter for archiveType and/or paleoData_proxy: \n\nat = 'Wood'\npt = 'ring width'\nkey = '%s_%s'%(at, pt)\n\nkeys += [key]\n\ndf_proxy = df.copy().loc[(df['archiveType']==at)].loc[(df['paleoData_proxy']==pt)] # filter records for specific archive and proxy type\n\nn_recs = len(df_proxy) # number of records\nprint('n_records   : ', n_recs)\n\nprint('archive type: ', set(df_proxy['archiveType']))\nprint('proxy type:   ', set(df_proxy['paleoData_proxy']))\n\n# (2) plot the spatial distribution of records\ngeo_fig, col = uplt.geo_plot(df_proxy, return_col=True)\n\n# (3) plot the coverage for proxy types and plot resolution\n\nuta.convert_subannual_to_annual_res(df_proxy)\n\ndf_proxy = uta.add_auxvars_plot_summary(df_proxy, key, col=col[at])\n</pre> # (1) filter for archiveType and/or paleoData_proxy:   at = 'Wood' pt = 'ring width' key = '%s_%s'%(at, pt)  keys += [key]  df_proxy = df.copy().loc[(df['archiveType']==at)].loc[(df['paleoData_proxy']==pt)] # filter records for specific archive and proxy type  n_recs = len(df_proxy) # number of records print('n_records   : ', n_recs)  print('archive type: ', set(df_proxy['archiveType'])) print('proxy type:   ', set(df_proxy['paleoData_proxy']))  # (2) plot the spatial distribution of records geo_fig, col = uplt.geo_plot(df_proxy, return_col=True)  # (3) plot the coverage for proxy types and plot resolution  uta.convert_subannual_to_annual_res(df_proxy)  df_proxy = uta.add_auxvars_plot_summary(df_proxy, key, col=col[at])   <pre>n_records   :  1065\narchive type:  {'Wood'}\nproxy type:    {'ring width'}\n0 Wood 3650\n1 Speleothem 567\n2 Coral 312\n3 GlacierIce 162\n4 MarineSediment 126\n5 LakeSediment 111\n6 Documents 13\n7 Sclerosponge 6\n8 GroundIce 4\n9 Borehole 3\n10 Other 2\n11 MolluskShell 1\n</pre> In\u00a0[7]: Copied! <pre>#========================= PROXY SPECIFIC: Wood ring width =========================\nminres    = 1                         # homogenised resolution\nmny       = 1000                      # start year of homogenised time coord\nmxy       = 2000                      # end year of homogenised time coord\nnyears    = np.min([600, mxy-mny])    # minimum length of each record\n#====================================================================\n\n# filter for record length during target period\ndf_proxy = uta.filter_record_length(df_proxy, nyears, mny, mxy)\n\n# filter for resolution\ndf_proxy = uta.filter_resolution(df_proxy, minres)\n\n# plot coverage and resolution\nuplt.plot_coverage_analysis(df_proxy, np.arange(mny, mxy+minres, minres), key, col[at])\nuplt.plot_resolution(df_proxy, key, col=col[at])\nuplt.plot_length(df_proxy, key, col=col[at])\n\nn_recs = len(df_proxy) # final number of records\n\n\nprint(df_proxy[['miny', 'maxy', 'originalDatabase']])\n\npca_rec[key] = df_proxy['datasetId']\n</pre> #========================= PROXY SPECIFIC: Wood ring width ========================= minres    = 1                         # homogenised resolution mny       = 1000                      # start year of homogenised time coord mxy       = 2000                      # end year of homogenised time coord nyears    = np.min([600, mxy-mny])    # minimum length of each record #====================================================================  # filter for record length during target period df_proxy = uta.filter_record_length(df_proxy, nyears, mny, mxy)  # filter for resolution df_proxy = uta.filter_resolution(df_proxy, minres)  # plot coverage and resolution uplt.plot_coverage_analysis(df_proxy, np.arange(mny, mxy+minres, minres), key, col[at]) uplt.plot_resolution(df_proxy, key, col=col[at]) uplt.plot_length(df_proxy, key, col=col[at])  n_recs = len(df_proxy) # final number of records   print(df_proxy[['miny', 'maxy', 'originalDatabase']])  pca_rec[key] = df_proxy['datasetId'] <pre>Keep 71 records with nyears&gt;=600 during 1000-2000. Exclude 994 records.\nKeep 71 records with resolution &lt;=1. Exclude 0 records.\n</pre> <pre>        miny    maxy                   originalDatabase\n3     1360.0  1983.0  FE23 (Breitenmoser et al. (2014))\n84    1357.0  1975.0  FE23 (Breitenmoser et al. (2014))\n174   1153.0  1986.0  FE23 (Breitenmoser et al. (2014))\n226   1123.0  2001.0  FE23 (Breitenmoser et al. (2014))\n231   1381.0  2000.0  FE23 (Breitenmoser et al. (2014))\n...      ...     ...                                ...\n1003   850.0  1989.0  FE23 (Breitenmoser et al. (2014))\n1007  1390.0  1998.0  FE23 (Breitenmoser et al. (2014))\n1023   980.0  1985.0  FE23 (Breitenmoser et al. (2014))\n1026  1236.0  1984.0  FE23 (Breitenmoser et al. (2014))\n1063  1377.0  1999.0  FE23 (Breitenmoser et al. (2014))\n\n[71 rows x 3 columns]\n</pre> In\u00a0[8]: Copied! <pre># add 'z-scores' to dataframe and plot z-scores and values\ndf_proxy = uta.add_zscores_plot(df_proxy, key, plot_output=True)\n</pre> # add 'z-scores' to dataframe and plot z-scores and values df_proxy = uta.add_zscores_plot(df_proxy, key, plot_output=True)  In\u00a0[9]: Copied! <pre># define new homogenised time coordinate\ndf_proxy, years_hom = uta.homogenise_time(df_proxy, mny, mxy, minres)\ntime[key] = years_hom\n</pre> # define new homogenised time coordinate df_proxy, years_hom = uta.homogenise_time(df_proxy, mny, mxy, minres) time[key] = years_hom <pre>Homogenised time coordinate: 1000-2000 CE\nResolution: [1] years\nINTERSECT: 1400-1963\n</pre> In\u00a0[10]: Copied! <pre># assign the paleoData_values to the non-missing values in the homogenised data array\n\nout = uta.homogenise_data_dimensions(df_proxy, years_hom, key, plot_output=False) # returns list of homogenised paleoData_values and paleoData_zscores\npaleoData_values_hom, paleoData_zscores_hom, year_hom_avbl, zsco_hom_avbl = out\n\n# define new columns in df_filt\nnew_columns = {'year_hom': [years_hom]*n_recs, \n               'year_hom_avbl': year_hom_avbl, \n               'paleoData_values_hom': [paleoData_values_hom[ii, :] for ii in range(n_recs)], \n               'paleoData_zscores_hom': [paleoData_zscores_hom[ii, :] for ii in range(n_recs)], \n               'paleoData_zscores_hom_avbl': zsco_hom_avbl}\ndf_proxy = df_proxy.assign(**new_columns)\n\nprint('Real intersect after homogenising resolution: ')\nintrsct = uta.find_shared_period(df_proxy, minmax=(mny, mxy), time='year_hom_avbl', \n                             data='paleoData_zscores_hom_avbl')\ndata[key] = paleoData_zscores_hom\n</pre> # assign the paleoData_values to the non-missing values in the homogenised data array  out = uta.homogenise_data_dimensions(df_proxy, years_hom, key, plot_output=False) # returns list of homogenised paleoData_values and paleoData_zscores paleoData_values_hom, paleoData_zscores_hom, year_hom_avbl, zsco_hom_avbl = out  # define new columns in df_filt new_columns = {'year_hom': [years_hom]*n_recs,                 'year_hom_avbl': year_hom_avbl,                 'paleoData_values_hom': [paleoData_values_hom[ii, :] for ii in range(n_recs)],                 'paleoData_zscores_hom': [paleoData_zscores_hom[ii, :] for ii in range(n_recs)],                 'paleoData_zscores_hom_avbl': zsco_hom_avbl} df_proxy = df_proxy.assign(**new_columns)  print('Real intersect after homogenising resolution: ') intrsct = uta.find_shared_period(df_proxy, minmax=(mny, mxy), time='year_hom_avbl',                               data='paleoData_zscores_hom_avbl') data[key] = paleoData_zscores_hom  <pre>(71, 1001)\nReal intersect after homogenising resolution: \nINTERSECT: 1400-1963\n</pre> In\u00a0[11]: Copied! <pre>covariance, overlap = uta.calc_covariance_matrix(df_proxy)\n</pre> covariance, overlap = uta.calc_covariance_matrix(df_proxy) <pre>short records :  []\n</pre> In\u00a0[12]: Copied! <pre>eigenvalues, eigenvectors = uta.PCA(covariance)\n\nfoev[key] = uta.fraction_of_explained_var(covariance, eigenvalues, n_recs, key, df.name)\n</pre> eigenvalues, eigenvectors = uta.PCA(covariance)  foev[key] = uta.fraction_of_explained_var(covariance, eigenvalues, n_recs, key, df.name) <pre>saved figure in /figs/dod2k_v2.0_filtered_M_TM/foev_Wood_ring width.pdf\n</pre> In\u00a0[13]: Copied! <pre>PCs[key], EOFs[key] = uplt.plot_PCs(years_hom, eigenvectors, paleoData_zscores_hom, key, df.name)\n</pre> PCs[key], EOFs[key] = uplt.plot_PCs(years_hom, eigenvectors, paleoData_zscores_hom, key, df.name) <pre>saved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0_filtered_M_TM/PCs_Wood_ring width.pdf\nsaved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0_filtered_M_TM/EOFloading_Wood_ring width.pdf\n</pre> In\u00a0[14]: Copied! <pre>print('Finished for %s'%key)\n</pre> print('Finished for %s'%key) <pre>Finished for Wood_ring width\n</pre> In\u00a0[15]: Copied! <pre># (1) filter for archiveType and/or paleoData_proxy: \n\nat = 'Wood'\npt = 'd18O'\nkey = '%s_%s'%(at, pt)\n\nkeys += [key]\n\ndf_proxy = df.copy().loc[(df['archiveType']==at)].loc[(df['paleoData_proxy']==pt)] # filter records for specific archive and proxy type\n\nn_recs = len(df_proxy) # number of records\nprint('n_records   : ', n_recs)\n\nprint('archive type: ', set(df_proxy['archiveType']))\nprint('proxy type:   ', set(df_proxy['paleoData_proxy']))\n\n# (2) plot the spatial distribution of records\ngeo_fig, col = uplt.geo_plot(df_proxy, return_col=True)\n\n# (3) plot the coverage for proxy types and plot resolution\n\nuta.convert_subannual_to_annual_res(df_proxy)\n\ndf_proxy = uta.add_auxvars_plot_summary(df_proxy, key, col=col[at])\n</pre> # (1) filter for archiveType and/or paleoData_proxy:   at = 'Wood' pt = 'd18O' key = '%s_%s'%(at, pt)  keys += [key]  df_proxy = df.copy().loc[(df['archiveType']==at)].loc[(df['paleoData_proxy']==pt)] # filter records for specific archive and proxy type  n_recs = len(df_proxy) # number of records print('n_records   : ', n_recs)  print('archive type: ', set(df_proxy['archiveType'])) print('proxy type:   ', set(df_proxy['paleoData_proxy']))  # (2) plot the spatial distribution of records geo_fig, col = uplt.geo_plot(df_proxy, return_col=True)  # (3) plot the coverage for proxy types and plot resolution  uta.convert_subannual_to_annual_res(df_proxy)  df_proxy = uta.add_auxvars_plot_summary(df_proxy, key, col=col[at])   <pre>n_records   :  27\narchive type:  {'Wood'}\nproxy type:    {'d18O'}\n0 Wood 3650\n1 Speleothem 567\n2 Coral 312\n3 GlacierIce 162\n4 MarineSediment 126\n5 LakeSediment 111\n6 Documents 13\n7 Sclerosponge 6\n8 GroundIce 4\n9 Borehole 3\n10 Other 2\n11 MolluskShell 1\n</pre> In\u00a0[16]: Copied! <pre>df_proxy.keys()\n</pre> df_proxy.keys() Out[16]: <pre>Index(['archiveType', 'dataSetName', 'datasetId', 'duplicateDetails',\n       'geo_meanElev', 'geo_meanLat', 'geo_meanLon', 'geo_siteName',\n       'interpretation_direction', 'interpretation_seasonality',\n       'interpretation_variable', 'interpretation_variableDetail',\n       'originalDataURL', 'originalDatabase', 'paleoData_notes',\n       'paleoData_proxy', 'paleoData_sensorSpecies', 'paleoData_units',\n       'paleoData_values', 'paleoData_variableName', 'year', 'yearUnits',\n       'length', 'miny', 'maxy', 'resolution'],\n      dtype='object')</pre> In\u00a0[17]: Copied! <pre>#========================= PROXY SPECIFIC: Wood d18O =========================\nminres    = 2                         # homogenised resolution\nmny       = 1700                      # start year of homogenised time coord\nmxy       = 2000                      # end year of homogenised time coord\nnyears    = np.min([100, mxy-mny])    # minimum length of each record\n#====================================================================\n\n# filter for record length during target period\ndf_proxy = uta.filter_record_length(df_proxy, nyears, mny, mxy)\n\n# filter for resolution\ndf_proxy = uta.filter_resolution(df_proxy, minres)\n\n# plot coverage and resolution\nuplt.plot_coverage_analysis(df_proxy, np.arange(mny, mxy+minres, minres), key, col[at])\nuplt.plot_resolution(df_proxy, key, col=col[at])\nuplt.plot_length(df_proxy, key, col=col[at])\n\nn_recs = len(df_proxy) # final number of records\n\n\nprint(df_proxy[['miny', 'maxy', 'originalDatabase']])\n\npca_rec[key] = df_proxy['datasetId']\n</pre> #========================= PROXY SPECIFIC: Wood d18O ========================= minres    = 2                         # homogenised resolution mny       = 1700                      # start year of homogenised time coord mxy       = 2000                      # end year of homogenised time coord nyears    = np.min([100, mxy-mny])    # minimum length of each record #====================================================================  # filter for record length during target period df_proxy = uta.filter_record_length(df_proxy, nyears, mny, mxy)  # filter for resolution df_proxy = uta.filter_resolution(df_proxy, minres)  # plot coverage and resolution uplt.plot_coverage_analysis(df_proxy, np.arange(mny, mxy+minres, minres), key, col[at]) uplt.plot_resolution(df_proxy, key, col=col[at]) uplt.plot_length(df_proxy, key, col=col[at])  n_recs = len(df_proxy) # final number of records   print(df_proxy[['miny', 'maxy', 'originalDatabase']])  pca_rec[key] = df_proxy['datasetId'] <pre>Keep 25 records with nyears&gt;=100 during 1700-2000. Exclude 2 records.\nKeep 23 records with resolution &lt;=2. Exclude 2 records.\n</pre> <pre>        miny    maxy originalDatabase\n1141  1778.0  2000.0     Iso2k v1.1.2\n1142  1780.0  2003.0     Iso2k v1.1.2\n1153  1901.0  2009.0     Iso2k v1.1.2\n1155  1901.0  2001.0     Iso2k v1.1.2\n1156  1900.0  2002.0     Iso2k v1.1.2\n1160  1000.0  1998.0     Iso2k v1.1.2\n1161  1163.0  2005.0     Iso2k v1.1.2\n1163  1820.0  2004.0     Iso2k v1.1.2\n1171  1352.0  2012.0     Iso2k v1.1.2\n1182  1901.0  2010.0     Iso2k v1.1.2\n1184  1865.0  1969.0     Iso2k v1.1.2\n1185  1830.0  2010.0     Iso2k v1.1.2\n1186  1877.0  1998.0     Iso2k v1.1.2\n1187  1877.0  1998.0     Iso2k v1.1.2\n1188   489.0  2010.0     Iso2k v1.1.2\n1189  1801.0  2000.0     Iso2k v1.1.2\n1200  1767.0  2008.0     Iso2k v1.1.2\n1204  1743.0  2011.0     Iso2k v1.1.2\n1212  1705.0  2004.0     Iso2k v1.1.2\n1213  1705.0  2004.0     Iso2k v1.1.2\n1214  1705.0  2004.0     Iso2k v1.1.2\n1226  1592.0  2011.0     Iso2k v1.1.2\n1229  1850.0  2012.0     Iso2k v1.1.2\n</pre> In\u00a0[18]: Copied! <pre># add 'z-scores' to dataframe and plot z-scores and values\ndf_proxy = uta.add_zscores_plot(df_proxy, key, plot_output=True)\n</pre> # add 'z-scores' to dataframe and plot z-scores and values df_proxy = uta.add_zscores_plot(df_proxy, key, plot_output=True)  In\u00a0[19]: Copied! <pre># define new homogenised time coordinate\ndf_proxy, years_hom = uta.homogenise_time(df_proxy, mny, mxy, minres)\ntime[key] = years_hom\n</pre> # define new homogenised time coordinate df_proxy, years_hom = uta.homogenise_time(df_proxy, mny, mxy, minres) time[key] = years_hom <pre>Homogenised time coordinate: 1700-2000 CE\nResolution: [2] years\nINTERSECT: 1901-1969\n</pre> In\u00a0[20]: Copied! <pre># assign the paleoData_values to the non-missing values in the homogenised data array\n\nout = uta.homogenise_data_dimensions(df_proxy, years_hom, key, plot_output=False) # returns list of homogenised paleoData_values and paleoData_zscores\npaleoData_values_hom, paleoData_zscores_hom, year_hom_avbl, zsco_hom_avbl = out\n\n# define new columns in df_filt\nnew_columns = {'year_hom': [years_hom]*n_recs, \n               'year_hom_avbl': year_hom_avbl, \n               'paleoData_values_hom': [paleoData_values_hom[ii, :] for ii in range(n_recs)], \n               'paleoData_zscores_hom': [paleoData_zscores_hom[ii, :] for ii in range(n_recs)], \n               'paleoData_zscores_hom_avbl': zsco_hom_avbl}\ndf_proxy = df_proxy.assign(**new_columns)\n\nprint('Real intersect after homogenising resolution: ')\nintrsct = uta.find_shared_period(df_proxy, minmax=(mny, mxy), time='year_hom_avbl', \n                             data='paleoData_zscores_hom_avbl')\ndata[key] = paleoData_zscores_hom\n</pre> # assign the paleoData_values to the non-missing values in the homogenised data array  out = uta.homogenise_data_dimensions(df_proxy, years_hom, key, plot_output=False) # returns list of homogenised paleoData_values and paleoData_zscores paleoData_values_hom, paleoData_zscores_hom, year_hom_avbl, zsco_hom_avbl = out  # define new columns in df_filt new_columns = {'year_hom': [years_hom]*n_recs,                 'year_hom_avbl': year_hom_avbl,                 'paleoData_values_hom': [paleoData_values_hom[ii, :] for ii in range(n_recs)],                 'paleoData_zscores_hom': [paleoData_zscores_hom[ii, :] for ii in range(n_recs)],                 'paleoData_zscores_hom_avbl': zsco_hom_avbl} df_proxy = df_proxy.assign(**new_columns)  print('Real intersect after homogenising resolution: ') intrsct = uta.find_shared_period(df_proxy, minmax=(mny, mxy), time='year_hom_avbl',                               data='paleoData_zscores_hom_avbl') data[key] = paleoData_zscores_hom  <pre>(23, 151)\nReal intersect after homogenising resolution: \nINTERSECT: 1902-1970\n</pre> In\u00a0[21]: Copied! <pre>covariance, overlap = uta.calc_covariance_matrix(df_proxy)\n</pre> covariance, overlap = uta.calc_covariance_matrix(df_proxy) <pre>short records :  [35. 35. 36. 35. 35. 35. 36. 35.]\n</pre> In\u00a0[22]: Copied! <pre>eigenvalues, eigenvectors = uta.PCA(covariance)\n\nfoev[key] = uta.fraction_of_explained_var(covariance, eigenvalues, n_recs, key, df.name)\n</pre> eigenvalues, eigenvectors = uta.PCA(covariance)  foev[key] = uta.fraction_of_explained_var(covariance, eigenvalues, n_recs, key, df.name) <pre>saved figure in /figs/dod2k_v2.0_filtered_M_TM/foev_Wood_d18O.pdf\n</pre> In\u00a0[23]: Copied! <pre>PCs[key], EOFs[key] = uplt.plot_PCs(years_hom, eigenvectors, paleoData_zscores_hom, key, df.name)\n</pre> PCs[key], EOFs[key] = uplt.plot_PCs(years_hom, eigenvectors, paleoData_zscores_hom, key, df.name) <pre>saved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0_filtered_M_TM/PCs_Wood_d18O.pdf\nsaved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0_filtered_M_TM/EOFloading_Wood_d18O.pdf\n</pre> In\u00a0[24]: Copied! <pre>print('Finished for %s'%key)\n</pre> print('Finished for %s'%key) <pre>Finished for Wood_d18O\n</pre> In\u00a0[25]: Copied! <pre># (1) filter for archiveType and/or paleoData_proxy: \n\nat = 'Coral'\npt = 'd18O'\nkey = '%s_%s'%(at, pt)\n\nkeys += [key]\n\ndf_proxy = df.copy().loc[(df['archiveType']==at)].loc[(df['paleoData_proxy']==pt)] # filter records for specific archive and proxy type\n\nn_recs = len(df_proxy) # number of records\nprint('n_records   : ', n_recs)\n\nprint('archive type: ', set(df_proxy['archiveType']))\nprint('proxy type:   ', set(df_proxy['paleoData_proxy']))\n\n# (2) plot the spatial distribution of records\ngeo_fig, col = uplt.geo_plot(df_proxy, return_col=True)\n\n# (3) plot the coverage for proxy types and plot resolution\n\nuta.convert_subannual_to_annual_res(df_proxy)\n\ndf_proxy = uta.add_auxvars_plot_summary(df_proxy, key, col=col[at])\n</pre> # (1) filter for archiveType and/or paleoData_proxy:   at = 'Coral' pt = 'd18O' key = '%s_%s'%(at, pt)  keys += [key]  df_proxy = df.copy().loc[(df['archiveType']==at)].loc[(df['paleoData_proxy']==pt)] # filter records for specific archive and proxy type  n_recs = len(df_proxy) # number of records print('n_records   : ', n_recs)  print('archive type: ', set(df_proxy['archiveType'])) print('proxy type:   ', set(df_proxy['paleoData_proxy']))  # (2) plot the spatial distribution of records geo_fig, col = uplt.geo_plot(df_proxy, return_col=True)  # (3) plot the coverage for proxy types and plot resolution  uta.convert_subannual_to_annual_res(df_proxy)  df_proxy = uta.add_auxvars_plot_summary(df_proxy, key, col=col[at])   <pre>n_records   :  67\narchive type:  {'Coral'}\nproxy type:    {'d18O'}\n0 Wood 3650\n1 Speleothem 567\n2 Coral 312\n3 GlacierIce 162\n4 MarineSediment 126\n5 LakeSediment 111\n6 Documents 13\n7 Sclerosponge 6\n8 GroundIce 4\n9 Borehole 3\n10 Other 2\n11 MolluskShell 1\n</pre> In\u00a0[26]: Copied! <pre>#========================= PROXY SPECIFIC: Coral d18O =========================\nminres    = 1                         # homogenised resolution\nmny       = 1750                      # start year of homogenised time coord\nmxy       = 2000                      # end year of homogenised time coord\nnyears    = np.min([90, mxy-mny])    # minimum length of each record\n#====================================================================\n# filter for record length during target period\ndf_proxy = uta.filter_record_length(df_proxy, nyears, mny, mxy)\n\n# filter for resolution\ndf_proxy = uta.filter_resolution(df_proxy, minres)\n\n# plot coverage and resolution\nuplt.plot_coverage_analysis(df_proxy, np.arange(mny, mxy+minres, minres), key, col[at])\nuplt.plot_resolution(df_proxy, key, col=col[at])\nuplt.plot_length(df_proxy, key, col=col[at])\n\nn_recs = len(df_proxy) # final number of records\n\n\nprint(df_proxy[['miny', 'maxy', 'originalDatabase']])\n\npca_rec[key] = df_proxy['datasetId']\n</pre> #========================= PROXY SPECIFIC: Coral d18O ========================= minres    = 1                         # homogenised resolution mny       = 1750                      # start year of homogenised time coord mxy       = 2000                      # end year of homogenised time coord nyears    = np.min([90, mxy-mny])    # minimum length of each record #==================================================================== # filter for record length during target period df_proxy = uta.filter_record_length(df_proxy, nyears, mny, mxy)  # filter for resolution df_proxy = uta.filter_resolution(df_proxy, minres)  # plot coverage and resolution uplt.plot_coverage_analysis(df_proxy, np.arange(mny, mxy+minres, minres), key, col[at]) uplt.plot_resolution(df_proxy, key, col=col[at]) uplt.plot_length(df_proxy, key, col=col[at])  n_recs = len(df_proxy) # final number of records   print(df_proxy[['miny', 'maxy', 'originalDatabase']])  pca_rec[key] = df_proxy['datasetId'] <pre>Keep 25 records with nyears&gt;=90 during 1750-2000. Exclude 42 records.\nKeep 24 records with resolution &lt;=1. Exclude 1 records.\n</pre> <pre>        miny    maxy     originalDatabase\n1068  1860.0  1990.0  CoralHydro2k v1.0.1\n1069  1887.0  2011.0  CoralHydro2k v1.0.1\n1072  1726.0  1996.0  CoralHydro2k v1.0.1\n1075  1905.0  2016.0  CoralHydro2k v1.0.1\n1084  1899.0  2008.0  CoralHydro2k v1.0.1\n1085  1780.0  1997.0  CoralHydro2k v1.0.1\n1086  1884.0  1993.0  CoralHydro2k v1.0.1\n1090  1824.0  2016.0  CoralHydro2k v1.0.1\n1094  1896.0  1998.0  CoralHydro2k v1.0.1\n1097  1819.0  1998.0  CoralHydro2k v1.0.1\n1102  1520.0  2011.0  CoralHydro2k v1.0.1\n1103  1873.0  1994.0  CoralHydro2k v1.0.1\n1110  1899.0  1996.0  CoralHydro2k v1.0.1\n1116  1751.0  1994.0  CoralHydro2k v1.0.1\n1118  1882.0  1994.0  CoralHydro2k v1.0.1\n1122  1781.0  1998.0  CoralHydro2k v1.0.1\n1123  1808.0  2009.0  CoralHydro2k v1.0.1\n1124  1852.0  1990.0  CoralHydro2k v1.0.1\n1125  1782.0  1990.0  CoralHydro2k v1.0.1\n1127  1846.0  1995.0  CoralHydro2k v1.0.1\n1148  1899.0  1996.0         Iso2k v1.1.2\n1162  1886.0  1998.0         Iso2k v1.1.2\n1166  1824.0  1985.0         Iso2k v1.1.2\n1170  1751.0  1986.0         Iso2k v1.1.2\n</pre> In\u00a0[27]: Copied! <pre># add 'z-scores' to dataframe and plot z-scores and values\ndf_proxy = uta.add_zscores_plot(df_proxy, key, plot_output=True)\n</pre> # add 'z-scores' to dataframe and plot z-scores and values df_proxy = uta.add_zscores_plot(df_proxy, key, plot_output=True)  In\u00a0[28]: Copied! <pre># define new homogenised time coordinate\ndf_proxy, years_hom = uta.homogenise_time(df_proxy, mny, mxy, minres)\ntime[key] = years_hom\n</pre> # define new homogenised time coordinate df_proxy, years_hom = uta.homogenise_time(df_proxy, mny, mxy, minres) time[key] = years_hom <pre>Homogenised time coordinate: 1750-2000 CE\nResolution: [1] years\nINTERSECT: 1905-1985\n</pre> In\u00a0[29]: Copied! <pre># assign the paleoData_values to the non-missing values in the homogenised data array\n\nout = uta.homogenise_data_dimensions(df_proxy, years_hom, key, plot_output=False) # returns list of homogenised paleoData_values and paleoData_zscores\npaleoData_values_hom, paleoData_zscores_hom, year_hom_avbl, zsco_hom_avbl = out\n\n# define new columns in df_filt\nnew_columns = {'year_hom': [years_hom]*n_recs, \n               'year_hom_avbl': year_hom_avbl, \n               'paleoData_values_hom': [paleoData_values_hom[ii, :] for ii in range(n_recs)], \n               'paleoData_zscores_hom': [paleoData_zscores_hom[ii, :] for ii in range(n_recs)], \n               'paleoData_zscores_hom_avbl': zsco_hom_avbl}\ndf_proxy = df_proxy.assign(**new_columns)\n\nprint('Real intersect after homogenising resolution: ')\nintrsct = uta.find_shared_period(df_proxy, minmax=(mny, mxy), time='year_hom_avbl', \n                             data='paleoData_zscores_hom_avbl')\ndata[key] = paleoData_zscores_hom\n</pre> # assign the paleoData_values to the non-missing values in the homogenised data array  out = uta.homogenise_data_dimensions(df_proxy, years_hom, key, plot_output=False) # returns list of homogenised paleoData_values and paleoData_zscores paleoData_values_hom, paleoData_zscores_hom, year_hom_avbl, zsco_hom_avbl = out  # define new columns in df_filt new_columns = {'year_hom': [years_hom]*n_recs,                 'year_hom_avbl': year_hom_avbl,                 'paleoData_values_hom': [paleoData_values_hom[ii, :] for ii in range(n_recs)],                 'paleoData_zscores_hom': [paleoData_zscores_hom[ii, :] for ii in range(n_recs)],                 'paleoData_zscores_hom_avbl': zsco_hom_avbl} df_proxy = df_proxy.assign(**new_columns)  print('Real intersect after homogenising resolution: ') intrsct = uta.find_shared_period(df_proxy, minmax=(mny, mxy), time='year_hom_avbl',                               data='paleoData_zscores_hom_avbl') data[key] = paleoData_zscores_hom  <pre>(24, 251)\nReal intersect after homogenising resolution: \nINTERSECT: 1905-1985\n</pre> In\u00a0[30]: Copied! <pre>covariance, overlap = uta.calc_covariance_matrix(df_proxy)\n</pre> covariance, overlap = uta.calc_covariance_matrix(df_proxy) <pre>short records :  []\n</pre> In\u00a0[31]: Copied! <pre>eigenvalues, eigenvectors = uta.PCA(covariance)\n\nfoev[key] = uta.fraction_of_explained_var(covariance, eigenvalues, n_recs, key, df.name, col=col[at])\n</pre> eigenvalues, eigenvectors = uta.PCA(covariance)  foev[key] = uta.fraction_of_explained_var(covariance, eigenvalues, n_recs, key, df.name, col=col[at]) <pre>saved figure in /figs/dod2k_v2.0_filtered_M_TM/foev_Coral_d18O.pdf\n</pre> In\u00a0[32]: Copied! <pre>PCs[key], EOFs[key] = uplt.plot_PCs(years_hom, eigenvectors, paleoData_zscores_hom, key, df.name, col=col[at])\n</pre> PCs[key], EOFs[key] = uplt.plot_PCs(years_hom, eigenvectors, paleoData_zscores_hom, key, df.name, col=col[at]) <pre>saved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0_filtered_M_TM/PCs_Coral_d18O.pdf\nsaved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0_filtered_M_TM/EOFloading_Coral_d18O.pdf\n</pre> In\u00a0[33]: Copied! <pre>print('Finished for %s'%key)\n</pre> print('Finished for %s'%key) <pre>Finished for Coral_d18O\n</pre> In\u00a0[34]: Copied! <pre># (1) filter for archiveType and/or paleoData_proxy: \n\nat = 'Speleothem'\npt = 'd18O'\nkey = '%s_%s'%(at, pt)\n\nkeys += [key]\n\ndf_proxy = df.copy().loc[(df['archiveType']==at)].loc[(df['paleoData_proxy']==pt)] # filter records for specific archive and proxy type\n\nn_recs = len(df_proxy) # number of records\nprint('n_records   : ', n_recs)\n\nprint('archive type: ', set(df_proxy['archiveType']))\nprint('proxy type:   ', set(df_proxy['paleoData_proxy']))\n\n# (2) plot the spatial distribution of records\ngeo_fig, col = uplt.geo_plot(df_proxy, return_col=True)\n\n# (3) plot the coverage for proxy types and plot resolution\n\nuta.convert_subannual_to_annual_res(df_proxy)\n\ndf_proxy = uta.add_auxvars_plot_summary(df_proxy, key, col=col[at])\n</pre> # (1) filter for archiveType and/or paleoData_proxy:   at = 'Speleothem' pt = 'd18O' key = '%s_%s'%(at, pt)  keys += [key]  df_proxy = df.copy().loc[(df['archiveType']==at)].loc[(df['paleoData_proxy']==pt)] # filter records for specific archive and proxy type  n_recs = len(df_proxy) # number of records print('n_records   : ', n_recs)  print('archive type: ', set(df_proxy['archiveType'])) print('proxy type:   ', set(df_proxy['paleoData_proxy']))  # (2) plot the spatial distribution of records geo_fig, col = uplt.geo_plot(df_proxy, return_col=True)  # (3) plot the coverage for proxy types and plot resolution  uta.convert_subannual_to_annual_res(df_proxy)  df_proxy = uta.add_auxvars_plot_summary(df_proxy, key, col=col[at])   <pre>n_records   :  210\narchive type:  {'Speleothem'}\nproxy type:    {'d18O'}\n0 Wood 3650\n1 Speleothem 567\n2 Coral 312\n3 GlacierIce 162\n4 MarineSediment 126\n5 LakeSediment 111\n6 Documents 13\n7 Sclerosponge 6\n8 GroundIce 4\n9 Borehole 3\n10 Other 2\n11 MolluskShell 1\n</pre> In\u00a0[35]: Copied! <pre>#========================= PROXY SPECIFIC: Speleothem d18O =========================\nminres    = 11                         # homogenised resolution\nmny       = 650                      # start year of homogenised time coord\nmxy       = 1950                      # end year of homogenised time coord\nnyears    = np.min([500, mxy-mny])    # minimum length of each record\n#====================================================================\n\n# filter for record length during target period\ndf_proxy = uta.filter_record_length(df_proxy, nyears, mny, mxy)\n\n# filter for resolution\ndf_proxy = uta.filter_resolution(df_proxy, minres)\n\n# plot coverage and resolution\nuplt.plot_coverage_analysis(df_proxy, np.arange(mny, mxy+minres, minres), key, col[at])\nuplt.plot_resolution(df_proxy, key, col=col[at])\nuplt.plot_length(df_proxy, key, col=col[at])\n\nn_recs = len(df_proxy) # final number of records\n\n\nprint(df_proxy[['miny', 'maxy', 'originalDatabase']])\n\npca_rec[key] = df_proxy['datasetId']\n</pre> #========================= PROXY SPECIFIC: Speleothem d18O ========================= minres    = 11                         # homogenised resolution mny       = 650                      # start year of homogenised time coord mxy       = 1950                      # end year of homogenised time coord nyears    = np.min([500, mxy-mny])    # minimum length of each record #====================================================================  # filter for record length during target period df_proxy = uta.filter_record_length(df_proxy, nyears, mny, mxy)  # filter for resolution df_proxy = uta.filter_resolution(df_proxy, minres)  # plot coverage and resolution uplt.plot_coverage_analysis(df_proxy, np.arange(mny, mxy+minres, minres), key, col[at]) uplt.plot_resolution(df_proxy, key, col=col[at]) uplt.plot_length(df_proxy, key, col=col[at])  n_recs = len(df_proxy) # final number of records   print(df_proxy[['miny', 'maxy', 'originalDatabase']])  pca_rec[key] = df_proxy['datasetId'] <pre>Keep 23 records with nyears&gt;=500 during 650-1950. Exclude 187 records.\nKeep 17 records with resolution &lt;=11. Exclude 6 records.\n</pre> <pre>        miny    maxy originalDatabase\n1136   624.0  1562.0     Iso2k v1.1.2\n1256   790.0  1953.0         SISAL v3\n1261   491.0  1860.0         SISAL v3\n1266   487.0  2004.0         SISAL v3\n1279     1.0  2005.0         SISAL v3\n1289     1.0  2010.0         SISAL v3\n1290   949.0  1957.0         SISAL v3\n1296  1076.0  2007.0         SISAL v3\n1310     2.0  1986.0         SISAL v3\n1311  1423.0  2010.0         SISAL v3\n1325   746.0  2000.0         SISAL v3\n1352   851.0  2007.0         SISAL v3\n1364     2.0  1879.0         SISAL v3\n1382     1.0  1868.0         SISAL v3\n1388     2.0  2006.0         SISAL v3\n1390   920.0  1699.0         SISAL v3\n1414  1035.0  2006.0         SISAL v3\n</pre> In\u00a0[36]: Copied! <pre># add 'z-scores' to dataframe and plot z-scores and values\ndf_proxy = uta.add_zscores_plot(df_proxy, key, plot_output=True)\n</pre> # add 'z-scores' to dataframe and plot z-scores and values df_proxy = uta.add_zscores_plot(df_proxy, key, plot_output=True)  In\u00a0[37]: Copied! <pre># define new homogenised time coordinate\ndf_proxy, years_hom = uta.homogenise_time(df_proxy, mny, mxy, minres)\ntime[key] = years_hom\n</pre> # define new homogenised time coordinate df_proxy, years_hom = uta.homogenise_time(df_proxy, mny, mxy, minres) time[key] = years_hom <pre>Homogenised time coordinate: 650-1959 CE\nResolution: [11] years\nNo shared period across all records.\n</pre> In\u00a0[38]: Copied! <pre># assign the paleoData_values to the non-missing values in the homogenised data array\n\nout = uta.homogenise_data_dimensions(df_proxy, years_hom, key, plot_output=False) # returns list of homogenised paleoData_values and paleoData_zscores\npaleoData_values_hom, paleoData_zscores_hom, year_hom_avbl, zsco_hom_avbl = out\n\n# define new columns in df_filt\nnew_columns = {'year_hom': [years_hom]*n_recs, \n               'year_hom_avbl': year_hom_avbl, \n               'paleoData_values_hom': [paleoData_values_hom[ii, :] for ii in range(n_recs)], \n               'paleoData_zscores_hom': [paleoData_zscores_hom[ii, :] for ii in range(n_recs)], \n               'paleoData_zscores_hom_avbl': zsco_hom_avbl}\ndf_proxy = df_proxy.assign(**new_columns)\n\nprint('Real intersect after homogenising resolution: ')\nintrsct = uta.find_shared_period(df_proxy, minmax=(mny, mxy), time='year_hom_avbl', \n                             data='paleoData_zscores_hom_avbl')\ndata[key] = paleoData_zscores_hom\n</pre> # assign the paleoData_values to the non-missing values in the homogenised data array  out = uta.homogenise_data_dimensions(df_proxy, years_hom, key, plot_output=False) # returns list of homogenised paleoData_values and paleoData_zscores paleoData_values_hom, paleoData_zscores_hom, year_hom_avbl, zsco_hom_avbl = out  # define new columns in df_filt new_columns = {'year_hom': [years_hom]*n_recs,                 'year_hom_avbl': year_hom_avbl,                 'paleoData_values_hom': [paleoData_values_hom[ii, :] for ii in range(n_recs)],                 'paleoData_zscores_hom': [paleoData_zscores_hom[ii, :] for ii in range(n_recs)],                 'paleoData_zscores_hom_avbl': zsco_hom_avbl} df_proxy = df_proxy.assign(**new_columns)  print('Real intersect after homogenising resolution: ') intrsct = uta.find_shared_period(df_proxy, minmax=(mny, mxy), time='year_hom_avbl',                               data='paleoData_zscores_hom_avbl') data[key] = paleoData_zscores_hom  <pre>(17, 120)\nReal intersect after homogenising resolution: \nINTERSECT: 1431-1563\n</pre> In\u00a0[39]: Copied! <pre>covariance, overlap = uta.calc_covariance_matrix(df_proxy)\n</pre> covariance, overlap = uta.calc_covariance_matrix(df_proxy) <pre>short records :  [13. 13. 26. 26.]\n</pre> In\u00a0[40]: Copied! <pre>eigenvalues, eigenvectors = uta.PCA(covariance)\n\nfoev[key] = uta.fraction_of_explained_var(covariance, eigenvalues, n_recs, key, df.name, col=col[at])\n</pre> eigenvalues, eigenvectors = uta.PCA(covariance)  foev[key] = uta.fraction_of_explained_var(covariance, eigenvalues, n_recs, key, df.name, col=col[at]) <pre>saved figure in /figs/dod2k_v2.0_filtered_M_TM/foev_Speleothem_d18O.pdf\n</pre> In\u00a0[41]: Copied! <pre>PCs[key], EOFs[key] = uplt.plot_PCs(years_hom, eigenvectors, paleoData_zscores_hom, key, df.name, col=col[at])\n</pre> PCs[key], EOFs[key] = uplt.plot_PCs(years_hom, eigenvectors, paleoData_zscores_hom, key, df.name, col=col[at]) <pre>saved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0_filtered_M_TM/PCs_Speleothem_d18O.pdf\nsaved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0_filtered_M_TM/EOFloading_Speleothem_d18O.pdf\n</pre> In\u00a0[42]: Copied! <pre>print('Finished for %s'%key)\n</pre> print('Finished for %s'%key) <pre>Finished for Speleothem_d18O\n</pre> In\u00a0[43]: Copied! <pre># (1) filter for archiveType and/or paleoData_proxy: \n\nat = 'LakeSediment'\npt  = 'd18O+dD'\nkey = '%s_%s'%(at, pt)\n\nkeys += [key]\n\ndf_proxy = df.loc[(df['archiveType']==at)].loc[(df['paleoData_proxy']==pt.split('+')[0])|(df['paleoData_proxy']==pt.split('+')[1])]\n\nn_recs = len(df_proxy) # number of records\nprint('n_records   : ', n_recs)\n\nprint('archive type: ', set(df_proxy['archiveType']))\nprint('proxy type:   ', set(df_proxy['paleoData_proxy']))\n\n# (2) plot the spatial distribution of records\ngeo_fig, col = uplt.geo_plot(df_proxy, return_col=True)\n\n# (3) plot the coverage for proxy types and plot resolution\n\nuta.convert_subannual_to_annual_res(df_proxy)\n\ndf_proxy = uta.add_auxvars_plot_summary(df_proxy, key, col=col[at])\n</pre> # (1) filter for archiveType and/or paleoData_proxy:   at = 'LakeSediment' pt  = 'd18O+dD' key = '%s_%s'%(at, pt)  keys += [key]  df_proxy = df.loc[(df['archiveType']==at)].loc[(df['paleoData_proxy']==pt.split('+')[0])|(df['paleoData_proxy']==pt.split('+')[1])]  n_recs = len(df_proxy) # number of records print('n_records   : ', n_recs)  print('archive type: ', set(df_proxy['archiveType'])) print('proxy type:   ', set(df_proxy['paleoData_proxy']))  # (2) plot the spatial distribution of records geo_fig, col = uplt.geo_plot(df_proxy, return_col=True)  # (3) plot the coverage for proxy types and plot resolution  uta.convert_subannual_to_annual_res(df_proxy)  df_proxy = uta.add_auxvars_plot_summary(df_proxy, key, col=col[at])   <pre>n_records   :  35\narchive type:  {'LakeSediment'}\nproxy type:    {'dD', 'd18O'}\n0 Wood 3650\n1 Speleothem 567\n2 Coral 312\n3 GlacierIce 162\n4 MarineSediment 126\n5 LakeSediment 111\n6 Documents 13\n7 Sclerosponge 6\n8 GroundIce 4\n9 Borehole 3\n10 Other 2\n11 MolluskShell 1\n</pre> In\u00a0[44]: Copied! <pre>#========================= PROXY SPECIFIC: LakeSediment d18O+dD =========================\nminres    = 55                         # homogenised resolution\nmny       = 300                      # start year of homogenised time coord\nmxy       = 1800                      # end year of homogenised time coord\nnyears    = np.min([100, mxy-mny])    # minimum length of each record\n#====================================================================\n\n# filter for record length during target period\ndf_proxy = uta.filter_record_length(df_proxy, nyears, mny, mxy)\n\n# filter for resolution\ndf_proxy = uta.filter_resolution(df_proxy, minres)\n\n# plot coverage and resolution\nuplt.plot_coverage_analysis(df_proxy, np.arange(mny, mxy+minres, minres), key, col[at])\nuplt.plot_resolution(df_proxy, key, col=col[at])\nuplt.plot_length(df_proxy, key, col=col[at])\n\nn_recs = len(df_proxy) # final number of records\n\n\nprint(df_proxy[['miny', 'maxy', 'originalDatabase']])\n\npca_rec[key] = df_proxy['datasetId']\n</pre> #========================= PROXY SPECIFIC: LakeSediment d18O+dD ========================= minres    = 55                         # homogenised resolution mny       = 300                      # start year of homogenised time coord mxy       = 1800                      # end year of homogenised time coord nyears    = np.min([100, mxy-mny])    # minimum length of each record #====================================================================  # filter for record length during target period df_proxy = uta.filter_record_length(df_proxy, nyears, mny, mxy)  # filter for resolution df_proxy = uta.filter_resolution(df_proxy, minres)  # plot coverage and resolution uplt.plot_coverage_analysis(df_proxy, np.arange(mny, mxy+minres, minres), key, col[at]) uplt.plot_resolution(df_proxy, key, col=col[at]) uplt.plot_length(df_proxy, key, col=col[at])  n_recs = len(df_proxy) # final number of records   print(df_proxy[['miny', 'maxy', 'originalDatabase']])  pca_rec[key] = df_proxy['datasetId'] <pre>Keep 10 records with nyears&gt;=100 during 300-1800. Exclude 25 records.\nKeep 9 records with resolution &lt;=55. Exclude 1 records.\n</pre> <pre>       miny    maxy originalDatabase\n1132    7.0  2007.0     Iso2k v1.1.2\n1151    2.0  2009.0     Iso2k v1.1.2\n1158  276.0  2001.0     Iso2k v1.1.2\n1192  958.0  1940.0     Iso2k v1.1.2\n1194  341.0  2004.0     Iso2k v1.1.2\n1202    2.0  1842.0     Iso2k v1.1.2\n1203   13.0  1963.0     Iso2k v1.1.2\n1210  500.0  2000.0     Iso2k v1.1.2\n1222  500.0  2000.0     Iso2k v1.1.2\n</pre> In\u00a0[45]: Copied! <pre># add 'z-scores' to dataframe and plot z-scores and values\ndf_proxy = uta.add_zscores_plot(df_proxy, key, plot_output=True)\n</pre> # add 'z-scores' to dataframe and plot z-scores and values df_proxy = uta.add_zscores_plot(df_proxy, key, plot_output=True)  In\u00a0[46]: Copied! <pre># define new homogenised time coordinate\ndf_proxy, years_hom = uta.homogenise_time(df_proxy, mny, mxy, minres)\ntime[key] = years_hom\n</pre> # define new homogenised time coordinate df_proxy, years_hom = uta.homogenise_time(df_proxy, mny, mxy, minres) time[key] = years_hom <pre>Homogenised time coordinate: 300-1840 CE\nResolution: [55] years\nNo shared period across all records.\n</pre> In\u00a0[47]: Copied! <pre># assign the paleoData_values to the non-missing values in the homogenised data array\n\nout = uta.homogenise_data_dimensions(df_proxy, years_hom, key, plot_output=False) # returns list of homogenised paleoData_values and paleoData_zscores\npaleoData_values_hom, paleoData_zscores_hom, year_hom_avbl, zsco_hom_avbl = out\n\n# define new columns in df_filt\nnew_columns = {'year_hom': [years_hom]*n_recs, \n               'year_hom_avbl': year_hom_avbl, \n               'paleoData_values_hom': [paleoData_values_hom[ii, :] for ii in range(n_recs)], \n               'paleoData_zscores_hom': [paleoData_zscores_hom[ii, :] for ii in range(n_recs)], \n               'paleoData_zscores_hom_avbl': zsco_hom_avbl}\ndf_proxy = df_proxy.assign(**new_columns)\n\nprint('Real intersect after homogenising resolution: ')\nintrsct = uta.find_shared_period(df_proxy, minmax=(mny, mxy), time='year_hom_avbl', \n                             data='paleoData_zscores_hom_avbl')\ndata[key] = paleoData_zscores_hom\n</pre> # assign the paleoData_values to the non-missing values in the homogenised data array  out = uta.homogenise_data_dimensions(df_proxy, years_hom, key, plot_output=False) # returns list of homogenised paleoData_values and paleoData_zscores paleoData_values_hom, paleoData_zscores_hom, year_hom_avbl, zsco_hom_avbl = out  # define new columns in df_filt new_columns = {'year_hom': [years_hom]*n_recs,                 'year_hom_avbl': year_hom_avbl,                 'paleoData_values_hom': [paleoData_values_hom[ii, :] for ii in range(n_recs)],                 'paleoData_zscores_hom': [paleoData_zscores_hom[ii, :] for ii in range(n_recs)],                 'paleoData_zscores_hom_avbl': zsco_hom_avbl} df_proxy = df_proxy.assign(**new_columns)  print('Real intersect after homogenising resolution: ') intrsct = uta.find_shared_period(df_proxy, minmax=(mny, mxy), time='year_hom_avbl',                               data='paleoData_zscores_hom_avbl') data[key] = paleoData_zscores_hom  <pre>(9, 29)\nReal intersect after homogenising resolution: \nINTERSECT: 960-1840\n</pre> In\u00a0[48]: Copied! <pre>covariance, overlap = uta.calc_covariance_matrix(df_proxy)\n</pre> covariance, overlap = uta.calc_covariance_matrix(df_proxy) <pre>short records :  [29. 29. 29. 17. 28. 29. 29. 25. 25. 29. 29. 29. 17. 28. 29. 29. 25. 25.\n 29. 29. 29. 17. 28. 29. 29. 25. 25. 17. 17. 17. 17. 17. 17. 17. 17. 17.\n 28. 28. 28. 17. 28. 28. 28. 25. 25. 29. 29. 29. 17. 28. 29. 29. 25. 25.\n 29. 29. 29. 17. 28. 29. 29. 25. 25. 25. 25. 25. 17. 25. 25. 25. 25. 25.\n 25. 25. 25. 17. 25. 25. 25. 25. 25.]\n</pre> In\u00a0[49]: Copied! <pre>eigenvalues, eigenvectors = uta.PCA(covariance)\n\nfoev[key] = uta.fraction_of_explained_var(covariance, eigenvalues, n_recs, key, df.name, col=col[at])\n</pre> eigenvalues, eigenvectors = uta.PCA(covariance)  foev[key] = uta.fraction_of_explained_var(covariance, eigenvalues, n_recs, key, df.name, col=col[at]) <pre>saved figure in /figs/dod2k_v2.0_filtered_M_TM/foev_LakeSediment_d18O+dD.pdf\n</pre> In\u00a0[50]: Copied! <pre>PCs[key], EOFs[key] = uplt.plot_PCs(years_hom, eigenvectors, paleoData_zscores_hom, key, df.name, col=col[at])\n</pre> PCs[key], EOFs[key] = uplt.plot_PCs(years_hom, eigenvectors, paleoData_zscores_hom, key, df.name, col=col[at]) <pre>saved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0_filtered_M_TM/PCs_LakeSediment_d18O+dD.pdf\nsaved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0_filtered_M_TM/EOFloading_LakeSediment_d18O+dD.pdf\n</pre> In\u00a0[51]: Copied! <pre>print('Finished for %s'%key)\n</pre> print('Finished for %s'%key) <pre>Finished for LakeSediment_d18O+dD\n</pre> In\u00a0[52]: Copied! <pre># # (1) filter for archiveType and/or paleoData_proxy: \n\n# at = 'MarineSediment'\n# pt = 'd18O'\n# key = '%s_%s'%(at, pt)\n\n# keys += [key]\n\n# df_proxy = df.copy().loc[(df['archiveType']==at)].loc[(df['paleoData_proxy']==pt)] # filter records for specific archive and proxy type\n\n# n_recs = len(df_proxy) # number of records\n# print('n_records   : ', n_recs)\n\n# print('archive type: ', set(df_proxy['archiveType']))\n# print('proxy type:   ', set(df_proxy['paleoData_proxy']))\n\n# # (2) plot the spatial distribution of records\n# geo_fig, col = uplt.geo_plot(df_proxy, return_col=True)\n\n# # (3) plot the coverage for proxy types and plot resolution\n\n# uta.convert_subannual_to_annual_res(df_proxy)\n\n# df_proxy = uta.add_auxvars_plot_summary(df_proxy, key, col=col[at])\n</pre> # # (1) filter for archiveType and/or paleoData_proxy:   # at = 'MarineSediment' # pt = 'd18O' # key = '%s_%s'%(at, pt)  # keys += [key]  # df_proxy = df.copy().loc[(df['archiveType']==at)].loc[(df['paleoData_proxy']==pt)] # filter records for specific archive and proxy type  # n_recs = len(df_proxy) # number of records # print('n_records   : ', n_recs)  # print('archive type: ', set(df_proxy['archiveType'])) # print('proxy type:   ', set(df_proxy['paleoData_proxy']))  # # (2) plot the spatial distribution of records # geo_fig, col = uplt.geo_plot(df_proxy, return_col=True)  # # (3) plot the coverage for proxy types and plot resolution  # uta.convert_subannual_to_annual_res(df_proxy)  # df_proxy = uta.add_auxvars_plot_summary(df_proxy, key, col=col[at])   In\u00a0[53]: Copied! <pre># #========================= PROXY SPECIFIC: MarineSediment d18O =========================\n# minres    = 100                         # homogenised resolution\n# mny       = 100                      # start year of homogenised time coord\n# mxy       = 1800                      # end year of homogenised time coord\n# nyears    = np.min([100, mxy-mny])    # minimum length of each record\n# #====================================================================\n\n# # filter for record length during target period\n# df_proxy = uta.filter_record_length(df_proxy, nyears, mny, mxy)\n\n# # filter for resolution\n# df_proxy = uta.filter_resolution(df_proxy, minres)\n\n# # plot coverage and resolution\n# uplt.plot_coverage_analysis(df_proxy, np.arange(mny, mxy+minres, minres), key, col[at])\n# uplt.plot_resolution(df_proxy, key)\n# uplt.plot_length(df_proxy, key)\n\n# n_recs = len(df_proxy) # final number of records\n\n\n# print(df_proxy[['miny', 'maxy', 'originalDatabase']])\n\n# pca_rec[key] = df_proxy['datasetId']\n</pre> # #========================= PROXY SPECIFIC: MarineSediment d18O ========================= # minres    = 100                         # homogenised resolution # mny       = 100                      # start year of homogenised time coord # mxy       = 1800                      # end year of homogenised time coord # nyears    = np.min([100, mxy-mny])    # minimum length of each record # #====================================================================  # # filter for record length during target period # df_proxy = uta.filter_record_length(df_proxy, nyears, mny, mxy)  # # filter for resolution # df_proxy = uta.filter_resolution(df_proxy, minres)  # # plot coverage and resolution # uplt.plot_coverage_analysis(df_proxy, np.arange(mny, mxy+minres, minres), key, col[at]) # uplt.plot_resolution(df_proxy, key) # uplt.plot_length(df_proxy, key)  # n_recs = len(df_proxy) # final number of records   # print(df_proxy[['miny', 'maxy', 'originalDatabase']])  # pca_rec[key] = df_proxy['datasetId'] In\u00a0[54]: Copied! <pre># # add 'z-scores' to dataframe and plot z-scores and values\n# df_proxy = uta.add_zscores_plot(df_proxy, key, plot_output=True)\n</pre> # # add 'z-scores' to dataframe and plot z-scores and values # df_proxy = uta.add_zscores_plot(df_proxy, key, plot_output=True)  In\u00a0[55]: Copied! <pre># # define new homogenised time coordinate\n# df_proxy, years_hom = uta.homogenise_time(df_proxy, mny, mxy, minres)\n# time[key] = years_hom\n</pre> # # define new homogenised time coordinate # df_proxy, years_hom = uta.homogenise_time(df_proxy, mny, mxy, minres) # time[key] = years_hom In\u00a0[56]: Copied! <pre># # assign the paleoData_values to the non-missing values in the homogenised data array\n\n# out = uta.homogenise_data_dimensions(df_proxy, years_hom, key, plot_output=False) # returns list of homogenised paleoData_values and paleoData_zscores\n# paleoData_values_hom, paleoData_zscores_hom, year_hom_avbl, zsco_hom_avbl = out\n\n# # define new columns in df_filt\n# new_columns = {'year_hom': [years_hom]*n_recs, \n#                'year_hom_avbl': year_hom_avbl, \n#                'paleoData_values_hom': [paleoData_values_hom[ii, :] for ii in range(n_recs)], \n#                'paleoData_zscores_hom': [paleoData_zscores_hom[ii, :] for ii in range(n_recs)], \n#                'paleoData_zscores_hom_avbl': zsco_hom_avbl}\n# df_proxy = df_proxy.assign(**new_columns)\n\n# print('Real intersect after homogenising resolution: ')\n# intrsct = uta.find_shared_period(df_proxy, minmax=(mny, mxy), time='year_hom_avbl', \n#                              data='paleoData_zscores_hom_avbl')\n# data[key] = paleoData_zscores_hom\n</pre> # # assign the paleoData_values to the non-missing values in the homogenised data array  # out = uta.homogenise_data_dimensions(df_proxy, years_hom, key, plot_output=False) # returns list of homogenised paleoData_values and paleoData_zscores # paleoData_values_hom, paleoData_zscores_hom, year_hom_avbl, zsco_hom_avbl = out  # # define new columns in df_filt # new_columns = {'year_hom': [years_hom]*n_recs,  #                'year_hom_avbl': year_hom_avbl,  #                'paleoData_values_hom': [paleoData_values_hom[ii, :] for ii in range(n_recs)],  #                'paleoData_zscores_hom': [paleoData_zscores_hom[ii, :] for ii in range(n_recs)],  #                'paleoData_zscores_hom_avbl': zsco_hom_avbl} # df_proxy = df_proxy.assign(**new_columns)  # print('Real intersect after homogenising resolution: ') # intrsct = uta.find_shared_period(df_proxy, minmax=(mny, mxy), time='year_hom_avbl',  #                              data='paleoData_zscores_hom_avbl') # data[key] = paleoData_zscores_hom  In\u00a0[57]: Copied! <pre># covariance, overlap = uta.calc_covariance_matrix(df_proxy)\n</pre> # covariance, overlap = uta.calc_covariance_matrix(df_proxy) In\u00a0[58]: Copied! <pre># eigenvalues, eigenvectors = uta.PCA(covariance)\n\n# foev[key] = uta.fraction_of_explained_var(covariance, eigenvalues, n_recs, key, df.name)\n</pre> # eigenvalues, eigenvectors = uta.PCA(covariance)  # foev[key] = uta.fraction_of_explained_var(covariance, eigenvalues, n_recs, key, df.name) In\u00a0[59]: Copied! <pre># PCs[key], EOFs[key] = uplt.plot_PCs(years_hom, eigenvectors, paleoData_zscores_hom, key, df.name)\n</pre> # PCs[key], EOFs[key] = uplt.plot_PCs(years_hom, eigenvectors, paleoData_zscores_hom, key, df.name) In\u00a0[60]: Copied! <pre># print('Finished for %s'%key)\n</pre> # print('Finished for %s'%key) In\u00a0[61]: Copied! <pre>archive_colour, archives_sorted, proxy_marker = uplt.df_colours_markers()\n</pre> archive_colour, archives_sorted, proxy_marker = uplt.df_colours_markers() <pre>0 Wood 3650\n1 Speleothem 567\n2 Coral 312\n3 GlacierIce 162\n4 MarineSediment 126\n5 LakeSediment 111\n6 Documents 13\n7 Sclerosponge 6\n8 GroundIce 4\n9 Borehole 3\n10 Other 2\n11 MolluskShell 1\n</pre> In\u00a0[62]: Copied! <pre>key_colour = {}\nfor key in keys:\n    key_colour[key]=archive_colour[key.split('_')[0]]\n\nkey_colour['Wood_d18O']='k'\n</pre> key_colour = {} for key in keys:     key_colour[key]=archive_colour[key.split('_')[0]]  key_colour['Wood_d18O']='k' In\u00a0[63]: Copied! <pre># as a comparison plot the mean over all records\nfig = plt.figure(figsize=(9, 4), dpi=300)\nfor key in keys:\n    lw = 1 if key in ['Wood_ring width', 'Wood_d18O', 'Coral_d18O'] else 2\n    plt.plot(time[key], np.ma.mean(data[key], axis=0)-np.mean(np.ma.mean(data[key], axis=0)), \n             label=key, lw=lw, color=key_colour[key])\nplt.ylabel('paleoData_zscores')\nplt.xlabel('year CE')\nplt.legend(ncol=3, bbox_to_anchor=(0,-0.2), loc='upper left', framealpha=0)\nplt.show()\n</pre> # as a comparison plot the mean over all records fig = plt.figure(figsize=(9, 4), dpi=300) for key in keys:     lw = 1 if key in ['Wood_ring width', 'Wood_d18O', 'Coral_d18O'] else 2     plt.plot(time[key], np.ma.mean(data[key], axis=0)-np.mean(np.ma.mean(data[key], axis=0)),               label=key, lw=lw, color=key_colour[key]) plt.ylabel('paleoData_zscores') plt.xlabel('year CE') plt.legend(ncol=3, bbox_to_anchor=(0,-0.2), loc='upper left', framealpha=0) plt.show() In\u00a0[65]: Copied! <pre># as a comparison plot the mean over all records SMOOTHED\nfig = plt.figure(figsize=(9, 4), dpi=300)\nfor key in keys:\n    if key in ['Wood_ring width', 'Wood_d18O', 'Coral_d18O']:\n        tt, dd = uta.smooth(np.ma.mean(data[key], axis=0), time[key], 11)\n        label=key+' 11yr-mean'\n    else:\n        tt, dd = time[key], np.ma.mean(data[key], axis=0)\n        label=key\n    plt.plot(tt, dd, label=label, lw=2, color=key_colour[key])\nplt.ylabel('paleoData_zscores')\nplt.xlabel('year CE')    \nplt.legend(ncol=3, bbox_to_anchor=(0,-0.2), loc='upper left', framealpha=0)\nplt.show()\n</pre> # as a comparison plot the mean over all records SMOOTHED fig = plt.figure(figsize=(9, 4), dpi=300) for key in keys:     if key in ['Wood_ring width', 'Wood_d18O', 'Coral_d18O']:         tt, dd = uta.smooth(np.ma.mean(data[key], axis=0), time[key], 11)         label=key+' 11yr-mean'     else:         tt, dd = time[key], np.ma.mean(data[key], axis=0)         label=key     plt.plot(tt, dd, label=label, lw=2, color=key_colour[key]) plt.ylabel('paleoData_zscores') plt.xlabel('year CE')     plt.legend(ncol=3, bbox_to_anchor=(0,-0.2), loc='upper left', framealpha=0) plt.show() In\u00a0[71]: Copied! <pre>fig = plt.figure(figsize=(7,3), dpi=300)\nax  = plt.gca()\nax2 = ax.twinx()\nfor key in keys:\n    print(key)\n    ax.plot(range(1,len(foev[key][:10])+1), foev[key][:10], label=key, color=key_colour[key])\n    ax2.plot(range(1,len(foev[key][:10])+1), np.cumsum(foev[key][:10]), label=key, ls=':', color=key_colour[key])\nax.set_ylabel('Fraction of explained variance')\n# ax2.set_ylabel('Cumulative fraction of explained variance')\nax.set_xlabel('PC')    \nax.legend(ncol=3, bbox_to_anchor=(0,-0.15), loc='upper left', framealpha=0)\n</pre> fig = plt.figure(figsize=(7,3), dpi=300) ax  = plt.gca() ax2 = ax.twinx() for key in keys:     print(key)     ax.plot(range(1,len(foev[key][:10])+1), foev[key][:10], label=key, color=key_colour[key])     ax2.plot(range(1,len(foev[key][:10])+1), np.cumsum(foev[key][:10]), label=key, ls=':', color=key_colour[key]) ax.set_ylabel('Fraction of explained variance') # ax2.set_ylabel('Cumulative fraction of explained variance') ax.set_xlabel('PC')     ax.legend(ncol=3, bbox_to_anchor=(0,-0.15), loc='upper left', framealpha=0) <pre>Wood_ring width\nWood_d18O\nCoral_d18O\nSpeleothem_d18O\nLakeSediment_d18O+dD\n</pre> Out[71]: <pre>&lt;matplotlib.legend.Legend at 0x7fe1bcc5a410&gt;</pre> In\u00a0[74]: Copied! <pre># plot PCs of different proxy types on the same axis. \n# Note that these are plotted as calculated in the respective analysis- NOT standardised differently!\n\nfig = plt.figure(figsize=(8,5), dpi=150)\n\ngrid = GS(2,1)\nfor ii in range(2):\n    ax = plt.subplot(grid[ii,:])\n    for key in keys:\n        a=1 if key in ['Wood_ring width', 'Speleothem_d18O', 'LakeSediment_d18O+dD'] else -1\n        label = key+ '\\n ($\\\\ast(-1)$)' if key in ['Wood_ring width', 'Speleothem_d18O', 'LakeSediment_d18O+dD'] else key\n        plt.plot(time[key], a*PCs[key][ii], label=label, color=key_colour[key])\n    if ii==0: \n        ax.axes.xaxis.set_ticklabels([])\n    plt.ylabel('PC %d'%(ii+1)) \nplt.xlabel('year CE')\n\nplt.legend(ncol=3, bbox_to_anchor=(0,-0.2), \n           loc='upper left', framealpha=0)\ngrid.tight_layout(fig)\n</pre> # plot PCs of different proxy types on the same axis.  # Note that these are plotted as calculated in the respective analysis- NOT standardised differently!  fig = plt.figure(figsize=(8,5), dpi=150)  grid = GS(2,1) for ii in range(2):     ax = plt.subplot(grid[ii,:])     for key in keys:         a=1 if key in ['Wood_ring width', 'Speleothem_d18O', 'LakeSediment_d18O+dD'] else -1         label = key+ '\\n ($\\\\ast(-1)$)' if key in ['Wood_ring width', 'Speleothem_d18O', 'LakeSediment_d18O+dD'] else key         plt.plot(time[key], a*PCs[key][ii], label=label, color=key_colour[key])     if ii==0:          ax.axes.xaxis.set_ticklabels([])     plt.ylabel('PC %d'%(ii+1))  plt.xlabel('year CE')  plt.legend(ncol=3, bbox_to_anchor=(0,-0.2),             loc='upper left', framealpha=0) grid.tight_layout(fig)   In\u00a0[76]: Copied! <pre># plot PCs of different proxy types on the same axis. \n# SMOOTHED VIA 11YEAR RUNNING MEAN\nfig = plt.figure(figsize=(8,5), dpi=150)\n\ngrid = GS(2,1)\nfor ii in range(2):\n    ax = plt.subplot(grid[ii,:])\n    for key in keys:\n        label = key\n        if ((key in ['Wood_d18O', 'Coral_d18O'])):\n            a = -1 \n            label+= '\\n ($\\\\ast(-1)$)'\n        else:\n            a = 1\n        resolution = np.unique(np.diff(time[key]))[0]\n        if key in ['Wood_ring width', 'Wood_d18O', 'Coral_d18O']:\n            smooth = 11\n            tt, dd = uta.smooth(PCs[key][ii], time[key], int(smooth/resolution))\n            # label+=' %d-yr mean'%smooth\n        else:\n            tt, dd = time[key], PCs[key][ii]\n        plt.plot(tt, a*np.array(dd), label=label, color=key_colour[key])\n        \n    plt.ylabel('PC %d \\n (%d yr running mean)'%(ii+1, smooth))\n    plt.xlim(1750,2000)\n    if ii==0: \n        ax.axes.xaxis.set_ticklabels([])\n        hh, ll = ax.get_legend_handles_labels()\nplt.xlabel('year CE')\n\ngrid.tight_layout(fig)\n\nplt.legend(hh, ll, ncol=3, bbox_to_anchor=(0,-0.2), \n           loc='upper left', framealpha=0)\nplt.show()\n</pre> # plot PCs of different proxy types on the same axis.  # SMOOTHED VIA 11YEAR RUNNING MEAN fig = plt.figure(figsize=(8,5), dpi=150)  grid = GS(2,1) for ii in range(2):     ax = plt.subplot(grid[ii,:])     for key in keys:         label = key         if ((key in ['Wood_d18O', 'Coral_d18O'])):             a = -1              label+= '\\n ($\\\\ast(-1)$)'         else:             a = 1         resolution = np.unique(np.diff(time[key]))[0]         if key in ['Wood_ring width', 'Wood_d18O', 'Coral_d18O']:             smooth = 11             tt, dd = uta.smooth(PCs[key][ii], time[key], int(smooth/resolution))             # label+=' %d-yr mean'%smooth         else:             tt, dd = time[key], PCs[key][ii]         plt.plot(tt, a*np.array(dd), label=label, color=key_colour[key])              plt.ylabel('PC %d \\n (%d yr running mean)'%(ii+1, smooth))     plt.xlim(1750,2000)     if ii==0:          ax.axes.xaxis.set_ticklabels([])         hh, ll = ax.get_legend_handles_labels() plt.xlabel('year CE')  grid.tight_layout(fig)  plt.legend(hh, ll, ncol=3, bbox_to_anchor=(0,-0.2),             loc='upper left', framealpha=0) plt.show()  In\u00a0[77]: Copied! <pre>fig = plt.figure(figsize=(8,5), dpi=150)\n\ngrid = GS(2,1)\nfor key in keys:\n    n_recs = data[key].shape[0]\n    for ii in range(2):\n        ax = plt.subplot(grid[ii,:])\n        a = -1 if ((key in ['Wood_d18O', 'Coral_d18O'])) else 1\n        label = key+'\\n ($\\\\ast(-1)$)' if ((key in ['Wood_d18O', 'Coral_d18O'])) else key\n        plt.plot(range(n_recs), a*EOFs[key][ii], label=label, color=key_colour[key])\n        if ii==1: plt.xlabel('record')\n        plt.ylabel('EOF %d load'%(ii+1))\n        plt.axhline(0, color='k', alpha=0.5, lw=0.5)\n        if ii==0: \n            ax.axes.xaxis.set_ticklabels([])\n        if ii==0: plt.legend(bbox_to_anchor=(0,1.1), loc='lower left', ncol=3, framealpha=0)\ngrid.tight_layout(fig)\n</pre>  fig = plt.figure(figsize=(8,5), dpi=150)  grid = GS(2,1) for key in keys:     n_recs = data[key].shape[0]     for ii in range(2):         ax = plt.subplot(grid[ii,:])         a = -1 if ((key in ['Wood_d18O', 'Coral_d18O'])) else 1         label = key+'\\n ($\\\\ast(-1)$)' if ((key in ['Wood_d18O', 'Coral_d18O'])) else key         plt.plot(range(n_recs), a*EOFs[key][ii], label=label, color=key_colour[key])         if ii==1: plt.xlabel('record')         plt.ylabel('EOF %d load'%(ii+1))         plt.axhline(0, color='k', alpha=0.5, lw=0.5)         if ii==0:              ax.axes.xaxis.set_ticklabels([])         if ii==0: plt.legend(bbox_to_anchor=(0,1.1), loc='lower left', ncol=3, framealpha=0) grid.tight_layout(fig) In\u00a0[85]: Copied! <pre>fig = uplt.geo_EOF_plot(df, pca_rec, EOFs, keys, barlabel='EOF 1 load', which_EOF=0)\n#print(keys)\nutf.save_fig(fig, 'MT_spatial_EOF1', dir=df.name)\n</pre> fig = uplt.geo_EOF_plot(df, pca_rec, EOFs, keys, barlabel='EOF 1 load', which_EOF=0) #print(keys) utf.save_fig(fig, 'MT_spatial_EOF1', dir=df.name)  <pre>{'Wood_ring width': 1, 'Wood_d18O': 1, 'Coral_d18O': 1, 'Speleothem_d18O': 1, 'LakeSediment_d18O+dD': 1}\n-0.6\n0.6\nsaved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0_filtered_M_TM/MT_spatial_EOF1.pdf\n</pre> In\u00a0[86]: Copied! <pre>fig = uplt.geo_EOF_plot(df, pca_rec, EOFs, keys, barlabel='EOF 2 load', which_EOF=1)\n#print(keys)\nutf.save_fig(fig, 'MT_spatial_EOF2', dir=df.name)\n</pre> fig = uplt.geo_EOF_plot(df, pca_rec, EOFs, keys, barlabel='EOF 2 load', which_EOF=1) #print(keys) utf.save_fig(fig, 'MT_spatial_EOF2', dir=df.name)  <pre>{'Wood_ring width': 1, 'Wood_d18O': 1, 'Coral_d18O': 1, 'Speleothem_d18O': 1, 'LakeSediment_d18O+dD': 1}\n-0.6\n0.6\nsaved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0_filtered_M_TM/MT_spatial_EOF2.pdf\n</pre> In\u00a0[88]: Copied! <pre>fig = uplt.geo_plot(df, fs=(13,8))\n\nutf.save_fig(fig, 'MT_spatial', dir=df.name)\n</pre> fig = uplt.geo_plot(df, fs=(13,8))  utf.save_fig(fig, 'MT_spatial', dir=df.name) <pre>0 Wood 3650\n1 Speleothem 567\n2 Coral 312\n3 GlacierIce 162\n4 MarineSediment 126\n5 LakeSediment 111\n6 Documents 13\n7 Sclerosponge 6\n8 GroundIce 4\n9 Borehole 3\n10 Other 2\n11 MolluskShell 1\nsaved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0_filtered_M_TM/MT_spatial.pdf\n</pre> In\u00a0[92]: Copied! <pre># as a comparison plot the mean over all records SMOOTHED\nfig = plt.figure(figsize=(9, 5), dpi=300)\n\ngrid = GS(2,3)\n\nax=plt.subplot(grid[:,0])\nax  = plt.gca()\n# ax2 = ax.twinx()\nfor key in keys:\n    # ax.plot(range(1,9), foev[key][:8], label=key, color=key_colour[key])\n    ax.plot(range(1,len(foev[key][:10])+1), np.cumsum(foev[key][:10]), label=key, color=key_colour[key], lw=2.5)\nax.set_ylabel('Cumulative fraction of explained variance')\nax.set_xlabel('PC')    \nax.set_xlim(.9,10.1)\nplt.title('A', x=0.1, y=0.91)\n\ngrid.tight_layout(fig)\ngrid.update(hspace=0.1,wspace=0.3)\nplt.legend(ncol=1, bbox_to_anchor=(0.1,0.3), \n           loc='upper left', framealpha=0, fontsize=8, facecolor='white')\n# plt.grid()\n\nsmooth = 11\n            \nfor key in keys:\n    label = key\n    resolution = np.unique(np.diff(time[key]))[0]\n    #print(resolution, smooth, int(smooth/resolution))\n\n    if resolution&lt;smooth:\n        tt, dd = uta.smooth(np.ma.mean(data[key], axis=0), time[key], smooth)\n    else:\n        tt, dd = time[key], np.ma.mean(data[key], axis=0)\n    \n    for ii in range(2):\n        ax = plt.subplot(grid[ii,1:4])\n        # warmer and wetter is more negative in coral d18O; \n        # warmer is more positive and wetter is more negative (maybe?) in tree d18O\n        if ((key in ['Wood_d18O', 'Coral_d18O'])):\n            a = -1 # multiply PC sign by -1\n            label+= ' ($\\\\ast(-1)$)' # and label it as such\n        else:\n            a = 1\n        if resolution&lt;smooth:\n            tt, dd = uta.smooth(PCs[key][ii], time[key], int(smooth/resolution))\n        else:\n            tt, dd = time[key], PCs[key][ii]\n        plt.plot(tt, a*np.array(dd), label=label, color=key_colour[key], lw=1.5)\n        plt.ylabel('PC %d \\n (%d yr MA)'%(ii+1, smooth))\n        plt.grid()\n        if ii==0: \n            ax.axes.xaxis.set_ticklabels([])\n        plt.title('BC'[ii], x=0.05, y=0.8)\n        # plt.xlim(1750, 2010)\n        \nplt.xlabel('year CE')\n\n\nutf.save_fig(fig, 'MT_PCA', dir=df.name)\n</pre> # as a comparison plot the mean over all records SMOOTHED fig = plt.figure(figsize=(9, 5), dpi=300)  grid = GS(2,3)  ax=plt.subplot(grid[:,0]) ax  = plt.gca() # ax2 = ax.twinx() for key in keys:     # ax.plot(range(1,9), foev[key][:8], label=key, color=key_colour[key])     ax.plot(range(1,len(foev[key][:10])+1), np.cumsum(foev[key][:10]), label=key, color=key_colour[key], lw=2.5) ax.set_ylabel('Cumulative fraction of explained variance') ax.set_xlabel('PC')     ax.set_xlim(.9,10.1) plt.title('A', x=0.1, y=0.91)  grid.tight_layout(fig) grid.update(hspace=0.1,wspace=0.3) plt.legend(ncol=1, bbox_to_anchor=(0.1,0.3),             loc='upper left', framealpha=0, fontsize=8, facecolor='white') # plt.grid()  smooth = 11              for key in keys:     label = key     resolution = np.unique(np.diff(time[key]))[0]     #print(resolution, smooth, int(smooth/resolution))      if resolution <pre>saved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0_filtered_M_TM/MT_PCA.pdf\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/analysis_moisttemp/#analyse-moisture-and-moisture-temperature-records","title":"Analyse moisture and moisture-temperature records\u00b6","text":""},{"location":"notebooks/analysis_moisttemp/#set-up-working-environment","title":"Set up working environment\u00b6","text":""},{"location":"notebooks/analysis_moisttemp/#read-already-filtered-dataframe","title":"Read already filtered dataframe\u00b6","text":""},{"location":"notebooks/analysis_moisttemp/#data-analysis","title":"data analysis\u00b6","text":""},{"location":"notebooks/analysis_moisttemp/#tree-trw","title":"tree TRW\u00b6","text":""},{"location":"notebooks/analysis_moisttemp/#filter-archive_type-and-paleodata_proxy","title":"filter archive_type and paleoData_proxy\u00b6","text":""},{"location":"notebooks/analysis_moisttemp/#filter-for-target-period-resolution-and-minimum-record-length","title":"filter for target period, resolution and minimum record length\u00b6","text":""},{"location":"notebooks/analysis_moisttemp/#homogenise-data-dimensions-across-the-records","title":"homogenise data dimensions across the records\u00b6","text":""},{"location":"notebooks/analysis_moisttemp/#pca","title":"PCA\u00b6","text":""},{"location":"notebooks/analysis_moisttemp/#tree-d18o","title":"tree d18O\u00b6","text":""},{"location":"notebooks/analysis_moisttemp/#filter-archive_type-and-paleodata_proxy","title":"filter archive_type and paleoData_proxy\u00b6","text":""},{"location":"notebooks/analysis_moisttemp/#filter-for-target-period-resolution-and-minimum-record-length","title":"filter for target period, resolution and minimum record length\u00b6","text":""},{"location":"notebooks/analysis_moisttemp/#homogenise-data-dimensions-across-the-records","title":"homogenise data dimensions across the records\u00b6","text":""},{"location":"notebooks/analysis_moisttemp/#pca","title":"PCA\u00b6","text":""},{"location":"notebooks/analysis_moisttemp/#coral-d18o","title":"coral d18O\u00b6","text":""},{"location":"notebooks/analysis_moisttemp/#filter-archive_type-and-paleodata_proxy","title":"filter archive_type and paleoData_proxy\u00b6","text":""},{"location":"notebooks/analysis_moisttemp/#filter-for-target-period-resolution-and-minimum-record-length","title":"filter for target period, resolution and minimum record length\u00b6","text":""},{"location":"notebooks/analysis_moisttemp/#homogenise-data-dimensions-across-the-records","title":"homogenise data dimensions across the records\u00b6","text":""},{"location":"notebooks/analysis_moisttemp/#pca","title":"PCA\u00b6","text":""},{"location":"notebooks/analysis_moisttemp/#speleothem-d18o","title":"speleothem d18O\u00b6","text":""},{"location":"notebooks/analysis_moisttemp/#filter-archive_type-and-paleodata_proxy","title":"filter archive_type and paleoData_proxy\u00b6","text":""},{"location":"notebooks/analysis_moisttemp/#filter-for-target-period-resolution-and-minimum-record-length","title":"filter for target period, resolution and minimum record length\u00b6","text":""},{"location":"notebooks/analysis_moisttemp/#homogenise-data-dimensions-across-the-records","title":"homogenise data dimensions across the records\u00b6","text":""},{"location":"notebooks/analysis_moisttemp/#pca","title":"PCA\u00b6","text":""},{"location":"notebooks/analysis_moisttemp/#lake-sediment-d18o-dd","title":"lake sediment d18O + dD\u00b6","text":""},{"location":"notebooks/analysis_moisttemp/#filter-archive_type-and-paleodata_proxy","title":"filter archive_type and paleoData_proxy\u00b6","text":""},{"location":"notebooks/analysis_moisttemp/#filter-for-target-period-resolution-and-minimum-record-length","title":"filter for target period, resolution and minimum record length\u00b6","text":""},{"location":"notebooks/analysis_moisttemp/#homogenise-data-dimensions-across-the-records","title":"homogenise data dimensions across the records\u00b6","text":""},{"location":"notebooks/analysis_moisttemp/#pca","title":"PCA\u00b6","text":""},{"location":"notebooks/analysis_moisttemp/#marine-sediment-d18o","title":"marine sediment d18O\u00b6","text":""},{"location":"notebooks/analysis_moisttemp/#filter-archive_type-and-paleodata_proxy","title":"filter archive_type and paleoData_proxy\u00b6","text":""},{"location":"notebooks/analysis_moisttemp/#filter-for-target-period-resolution-and-minimum-record-length","title":"filter for target period, resolution and minimum record length\u00b6","text":""},{"location":"notebooks/analysis_moisttemp/#homogenise-data-dimensions-across-the-records","title":"homogenise data dimensions across the records\u00b6","text":""},{"location":"notebooks/analysis_moisttemp/#pca","title":"PCA\u00b6","text":""},{"location":"notebooks/analysis_moisttemp/#summary-figures","title":"summary figures\u00b6","text":""},{"location":"notebooks/analysis_moisttemp/#mean-z-score","title":"mean z-score\u00b6","text":""},{"location":"notebooks/analysis_moisttemp/#fraction-of-explained-variance","title":"Fraction of Explained Variance\u00b6","text":""},{"location":"notebooks/analysis_moisttemp/#1st-and-2nd-pc","title":"1st and 2nd PC\u00b6","text":""},{"location":"notebooks/analysis_moisttemp/#1st-and-2nd-eof","title":"1st and 2nd EOF\u00b6","text":""},{"location":"notebooks/analysis_moisttemp/#spatial-eof-load","title":"spatial EOF load\u00b6","text":""},{"location":"notebooks/analysis_moisttemp/#spatial-distribution","title":"spatial distribution\u00b6","text":""},{"location":"notebooks/analysis_moisture/","title":"Analyse moisture records","text":"<p>This notebook performs a PCA for a subset of records, filtered by archiveType/paleoData_proxy. For each subset, the following algorithm is being used:</p> <ol> <li>Filter archive_type and paleoData_proxy (defines subset) and produces summary plots of the data, in particular regarding: coverage, resolution and length of the records. This gives us information for the next step, in which we need to choose the parameters for the PCA</li> <li>Define proxy specific parameters for the PCA:<ul> <li>period (start and end year): choose a period of sufficient data density (all records chosen for the analysis need to at least overlap during this period)</li> <li>minimum resolution: records exceeding this resolution are being excluded from the analysis. Records with higher resolution will be subsampled to create homogeneous resolution across all the records.</li> <li>record length: records shorter than the record length are being excluded from the analysis.</li> <li>The choice of parameters will determine the success of the PCA. There is a trade-off between the number of records included and the quality (i.e. period/record length/resolution).</li> <li>Summary figures are being produced for the filtered data</li> <li>z-scores added to dataframe (mean=0 and std=1 over the entire record) as 'paleoData_zscores'</li> <li>note: z-scores may be biased if records are only partly overlapping in time, or increase in availability over time, or both.</li> </ul> </li> <li>Homogenise data dimensions across the records<ul> <li>defines a homogenised time variable over the target period and with the target resolution (as defined in the last step), which is saved as a new column in the dataframe named 'years_hom'</li> <li>creates a data matrix with dimensions n_records x n_time which is saved as a new column in df, named 'paleoData_values_hom' and 'paleoData_zscores_hom'.</li> <li>Note that this data is formatted as a np.ma.masked_array, where missing data is set to zero and masked out.</li> </ul> </li> <li>PCA<ul> <li>obtains covariance matrix of paleoData_zscores_hom (note that for every two records the covariance is calculated over their intersect of data availability)</li> <li>obtains eigenvectors and eigenvalues via SVD composition</li> <li>obtains and plots fraction of explained variance, first two PCs and load for first two EOFs vs ordering in the data frame.</li> </ul> </li> </ol> <p>2025/03/15 v0.3: MNE cleaned up for M analysis only (some archiveTypes do not have enough M-only data to continue; these are commented out).  -1 multiplier on the summary PC plotting eliminated because these are presumably proxies of precipitation or moisture d18O, not ice volume indicators.</p> <p>2025/12/17: Tidied up and updated for DoD2k v2.0. Copied from analysis_MT for moisture records only.</p> <p>Make sure the repo_root is added correctly, it should be: <code>your_root_dir/dod2k</code> This should be the working directory throughout this notebook (and all other notebooks).</p> In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n\nimport sys\nimport os\nfrom pathlib import Path\n\n# Add parent directory to path (works from any notebook in notebooks/)\n# the repo_root should be the parent directory of the notebooks folder\ninit_dir = Path().resolve()\n# Determine repo root\nif init_dir.name == 'dod2k': repo_root = init_dir\nelif init_dir.parent.name == 'dod2k': repo_root = init_dir.parent\nelse: raise Exception('Please review the repo root structure (see first cell).')\n\n# Update cwd and path only if needed\nif os.getcwd() != str(repo_root):\n    os.chdir(repo_root)\nif str(repo_root) not in sys.path:\n    sys.path.insert(0, str(repo_root))\n\nprint(f\"Repo root: {repo_root}\")\nif str(os.getcwd())==str(repo_root):\n    print(f\"Working directory matches repo root. \")\n</pre> %load_ext autoreload %autoreload 2  import sys import os from pathlib import Path  # Add parent directory to path (works from any notebook in notebooks/) # the repo_root should be the parent directory of the notebooks folder init_dir = Path().resolve() # Determine repo root if init_dir.name == 'dod2k': repo_root = init_dir elif init_dir.parent.name == 'dod2k': repo_root = init_dir.parent else: raise Exception('Please review the repo root structure (see first cell).')  # Update cwd and path only if needed if os.getcwd() != str(repo_root):     os.chdir(repo_root) if str(repo_root) not in sys.path:     sys.path.insert(0, str(repo_root))  print(f\"Repo root: {repo_root}\") if str(os.getcwd())==str(repo_root):     print(f\"Working directory matches repo root. \") <pre>Repo root: /home/jupyter-lluecke/dod2k\nWorking directory matches repo root. \n</pre> In\u00a0[79]: Copied! <pre># Import packages\nimport numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec as GS\n\n\nfrom dod2k_utilities import ut_functions as utf # contains utility functions\nfrom dod2k_utilities import ut_plot as uplt # contains plotting functions\nfrom dod2k_utilities import ut_analysis as uta # contains plotting functions\n</pre> # Import packages import numpy as np import pandas as pd import os import matplotlib.pyplot as plt from matplotlib.gridspec import GridSpec as GS   from dod2k_utilities import ut_functions as utf # contains utility functions from dod2k_utilities import ut_plot as uplt # contains plotting functions from dod2k_utilities import ut_analysis as uta # contains plotting functions <p>Read compact dataframe.</p> <p>{db_name} refers to the database, including</p> <pre><code>- dod2k_v2.0_filtered_M (filtered for moisture sensitive records only</code></pre> <p>All compact dataframes are saved in {repo_root}/data/{db_name} as {db_name}_compact.csv.</p> In\u00a0[3]: Copied! <pre># read dataframe, choose from the list below, or specify your own\n\ndb_name = 'dod2k_v2.0_filtered_M'\n\n# load dataframe\ndf = utf.load_compact_dataframe_from_csv(db_name)\nprint(df.info())\ndf.name = db_name\n</pre> # read dataframe, choose from the list below, or specify your own  db_name = 'dod2k_v2.0_filtered_M'  # load dataframe df = utf.load_compact_dataframe_from_csv(db_name) print(df.info()) df.name = db_name  <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 996 entries, 0 to 995\nData columns (total 22 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   archiveType                    996 non-null    object \n 1   dataSetName                    996 non-null    object \n 2   datasetId                      996 non-null    object \n 3   duplicateDetails               996 non-null    object \n 4   geo_meanElev                   986 non-null    float32\n 5   geo_meanLat                    996 non-null    float32\n 6   geo_meanLon                    996 non-null    float32\n 7   geo_siteName                   996 non-null    object \n 8   interpretation_direction       996 non-null    object \n 9   interpretation_seasonality     996 non-null    object \n 10  interpretation_variable        996 non-null    object \n 11  interpretation_variableDetail  996 non-null    object \n 12  originalDataURL                996 non-null    object \n 13  originalDatabase               996 non-null    object \n 14  paleoData_notes                996 non-null    object \n 15  paleoData_proxy                996 non-null    object \n 16  paleoData_sensorSpecies        996 non-null    object \n 17  paleoData_units                996 non-null    object \n 18  paleoData_values               996 non-null    object \n 19  paleoData_variableName         996 non-null    object \n 20  year                           996 non-null    object \n 21  yearUnits                      996 non-null    object \ndtypes: float32(3), object(19)\nmemory usage: 159.6+ KB\nNone\n</pre> In\u00a0[4]: Copied! <pre># cols = [ '#4477AA', '#EE6677', '#228833', '#CCBB44', '#66CCEE', '#AA3377', '#BBBBBB', '#44AA99', '#332288']\n</pre> # cols = [ '#4477AA', '#EE6677', '#228833', '#CCBB44', '#66CCEE', '#AA3377', '#BBBBBB', '#44AA99', '#332288'] In\u00a0[5]: Copied! <pre>time    = {}\ndata    = {}\nPCs     = {}\nEOFs    = {}\nfoev    = {}\npca_rec = {}\n\nkeys    = []\n</pre> time    = {} data    = {} PCs     = {} EOFs    = {} foev    = {} pca_rec = {}  keys    = [] In\u00a0[6]: Copied! <pre># (1) filter for archiveType and/or paleoData_proxy: \n\nat = 'Wood'\npt = 'ring width'\nkey = '%s_%s'%(at, pt)\n\nkeys += [key]\n\ndf_proxy = df.copy().loc[(df['archiveType']==at)].loc[(df['paleoData_proxy']==pt)] # filter records for specific archive and proxy type\n\nn_recs = len(df_proxy) # number of records\nprint('n_records   : ', n_recs)\n\nprint('archive type: ', set(df_proxy['archiveType']))\nprint('proxy type:   ', set(df_proxy['paleoData_proxy']))\n\n# (2) plot the spatial distribution of records\ngeo_fig, col = uplt.geo_plot(df_proxy, return_col=True)\n\n# (3) plot the coverage for proxy types and plot resolution\n\nuta.convert_subannual_to_annual_res(df_proxy)\n\ndf_proxy = uta.add_auxvars_plot_summary(df_proxy, key, col=col[at])\n</pre> # (1) filter for archiveType and/or paleoData_proxy:   at = 'Wood' pt = 'ring width' key = '%s_%s'%(at, pt)  keys += [key]  df_proxy = df.copy().loc[(df['archiveType']==at)].loc[(df['paleoData_proxy']==pt)] # filter records for specific archive and proxy type  n_recs = len(df_proxy) # number of records print('n_records   : ', n_recs)  print('archive type: ', set(df_proxy['archiveType'])) print('proxy type:   ', set(df_proxy['paleoData_proxy']))  # (2) plot the spatial distribution of records geo_fig, col = uplt.geo_plot(df_proxy, return_col=True)  # (3) plot the coverage for proxy types and plot resolution  uta.convert_subannual_to_annual_res(df_proxy)  df_proxy = uta.add_auxvars_plot_summary(df_proxy, key, col=col[at])   <pre>n_records   :  890\narchive type:  {'Wood'}\nproxy type:    {'ring width'}\n0 Wood 3650\n1 Speleothem 567\n2 Coral 312\n3 GlacierIce 162\n4 MarineSediment 126\n5 LakeSediment 111\n6 Documents 13\n7 Sclerosponge 6\n8 GroundIce 4\n9 Borehole 3\n10 Other 2\n11 MolluskShell 1\n</pre> In\u00a0[7]: Copied! <pre>#========================= PROXY SPECIFIC: Wood ring width =========================\nminres    = 1                         # homogenised resolution\nmny       = 1000                      # start year of homogenised time coord\nmxy       = 2000                      # end year of homogenised time coord\nnyears    = np.min([600, mxy-mny])    # minimum length of each record\n#====================================================================\n\n# filter for record length during target period\ndf_proxy = uta.filter_record_length(df_proxy, nyears, mny, mxy)\n\n# filter for resolution\ndf_proxy = uta.filter_resolution(df_proxy, minres)\n\n# plot coverage and resolution\nuplt.plot_coverage_analysis(df_proxy, np.arange(mny, mxy+minres, minres), key, col[at])\nuplt.plot_resolution(df_proxy, key, col=col[at])\nuplt.plot_length(df_proxy, key, col=col[at])\n\nn_recs = len(df_proxy) # final number of records\n\n\nprint(df_proxy[['miny', 'maxy', 'originalDatabase']])\n\npca_rec[key] = df_proxy['datasetId']\n</pre> #========================= PROXY SPECIFIC: Wood ring width ========================= minres    = 1                         # homogenised resolution mny       = 1000                      # start year of homogenised time coord mxy       = 2000                      # end year of homogenised time coord nyears    = np.min([600, mxy-mny])    # minimum length of each record #====================================================================  # filter for record length during target period df_proxy = uta.filter_record_length(df_proxy, nyears, mny, mxy)  # filter for resolution df_proxy = uta.filter_resolution(df_proxy, minres)  # plot coverage and resolution uplt.plot_coverage_analysis(df_proxy, np.arange(mny, mxy+minres, minres), key, col[at]) uplt.plot_resolution(df_proxy, key, col=col[at]) uplt.plot_length(df_proxy, key, col=col[at])  n_recs = len(df_proxy) # final number of records   print(df_proxy[['miny', 'maxy', 'originalDatabase']])  pca_rec[key] = df_proxy['datasetId'] <pre>Keep 61 records with nyears&gt;=600 during 1000-2000. Exclude 829 records.\nKeep 61 records with resolution &lt;=1. Exclude 0 records.\n</pre> <pre>       miny    maxy                   originalDatabase\n3    1360.0  1983.0  FE23 (Breitenmoser et al. (2014))\n132  1153.0  1986.0  FE23 (Breitenmoser et al. (2014))\n205   935.0  1993.0  FE23 (Breitenmoser et al. (2014))\n226  1155.0  1985.0  FE23 (Breitenmoser et al. (2014))\n227  1230.0  1980.0  FE23 (Breitenmoser et al. (2014))\n..      ...     ...                                ...\n842   850.0  1985.0  FE23 (Breitenmoser et al. (2014))\n843   850.0  1989.0  FE23 (Breitenmoser et al. (2014))\n847  1390.0  1998.0  FE23 (Breitenmoser et al. (2014))\n859  1236.0  1984.0  FE23 (Breitenmoser et al. (2014))\n888  1377.0  1999.0  FE23 (Breitenmoser et al. (2014))\n\n[61 rows x 3 columns]\n</pre> In\u00a0[8]: Copied! <pre># add 'z-scores' to dataframe and plot z-scores and values\ndf_proxy = uta.add_zscores_plot(df_proxy, key, plot_output=True)\n</pre> # add 'z-scores' to dataframe and plot z-scores and values df_proxy = uta.add_zscores_plot(df_proxy, key, plot_output=True)  In\u00a0[9]: Copied! <pre># define new homogenised time coordinate\ndf_proxy, years_hom = uta.homogenise_time(df_proxy, mny, mxy, minres)\ntime[key] = years_hom\n</pre> # define new homogenised time coordinate df_proxy, years_hom = uta.homogenise_time(df_proxy, mny, mxy, minres) time[key] = years_hom <pre>Homogenised time coordinate: 1000-2000 CE\nResolution: [1] years\nINTERSECT: 1400-1963\n</pre> In\u00a0[10]: Copied! <pre># assign the paleoData_values to the non-missing values in the homogenised data array\n\nout = uta.homogenise_data_dimensions(df_proxy, years_hom, key, plot_output=False) # returns list of homogenised paleoData_values and paleoData_zscores\npaleoData_values_hom, paleoData_zscores_hom, year_hom_avbl, zsco_hom_avbl = out\n\n# define new columns in df_filt\nnew_columns = {'year_hom': [years_hom]*n_recs, \n               'year_hom_avbl': year_hom_avbl, \n               'paleoData_values_hom': [paleoData_values_hom[ii, :] for ii in range(n_recs)], \n               'paleoData_zscores_hom': [paleoData_zscores_hom[ii, :] for ii in range(n_recs)], \n               'paleoData_zscores_hom_avbl': zsco_hom_avbl}\ndf_proxy = df_proxy.assign(**new_columns)\n\nprint('Real intersect after homogenising resolution: ')\nintrsct = uta.find_shared_period(df_proxy, minmax=(mny, mxy), time='year_hom_avbl', \n                             data='paleoData_zscores_hom_avbl')\ndata[key] = paleoData_zscores_hom\n</pre> # assign the paleoData_values to the non-missing values in the homogenised data array  out = uta.homogenise_data_dimensions(df_proxy, years_hom, key, plot_output=False) # returns list of homogenised paleoData_values and paleoData_zscores paleoData_values_hom, paleoData_zscores_hom, year_hom_avbl, zsco_hom_avbl = out  # define new columns in df_filt new_columns = {'year_hom': [years_hom]*n_recs,                 'year_hom_avbl': year_hom_avbl,                 'paleoData_values_hom': [paleoData_values_hom[ii, :] for ii in range(n_recs)],                 'paleoData_zscores_hom': [paleoData_zscores_hom[ii, :] for ii in range(n_recs)],                 'paleoData_zscores_hom_avbl': zsco_hom_avbl} df_proxy = df_proxy.assign(**new_columns)  print('Real intersect after homogenising resolution: ') intrsct = uta.find_shared_period(df_proxy, minmax=(mny, mxy), time='year_hom_avbl',                               data='paleoData_zscores_hom_avbl') data[key] = paleoData_zscores_hom  <pre>(61, 1001)\nReal intersect after homogenising resolution: \nINTERSECT: 1400-1963\n</pre> In\u00a0[11]: Copied! <pre>covariance, overlap = uta.calc_covariance_matrix(df_proxy)\n</pre> covariance, overlap = uta.calc_covariance_matrix(df_proxy) <pre>short records :  []\n</pre> In\u00a0[12]: Copied! <pre>eigenvalues, eigenvectors = uta.PCA(covariance)\n\nfoev[key] = uta.fraction_of_explained_var(covariance, eigenvalues, n_recs, key, df.name)\n</pre> eigenvalues, eigenvectors = uta.PCA(covariance)  foev[key] = uta.fraction_of_explained_var(covariance, eigenvalues, n_recs, key, df.name) <pre>saved figure in /figs/dod2k_v2.0_filtered_M/foev_Wood_ring width.pdf\n</pre> In\u00a0[13]: Copied! <pre>PCs[key], EOFs[key] = uplt.plot_PCs(years_hom, eigenvectors, paleoData_zscores_hom, key, df.name)\n</pre> PCs[key], EOFs[key] = uplt.plot_PCs(years_hom, eigenvectors, paleoData_zscores_hom, key, df.name) <pre>saved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0_filtered_M/PCs_Wood_ring width.pdf\nsaved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0_filtered_M/EOFloading_Wood_ring width.pdf\n</pre> In\u00a0[14]: Copied! <pre>print('Finished for %s'%key)\n</pre> print('Finished for %s'%key) <pre>Finished for Wood_ring width\n</pre> In\u00a0[15]: Copied! <pre># (1) filter for archiveType and/or paleoData_proxy: \n\nat = 'Wood'\npt = 'd18O'\nkey = '%s_%s'%(at, pt)\n\nkeys += [key]\n\ndf_proxy = df.copy().loc[(df['archiveType']==at)].loc[(df['paleoData_proxy']==pt)] # filter records for specific archive and proxy type\n\nn_recs = len(df_proxy) # number of records\nprint('n_records   : ', n_recs)\n\nprint('archive type: ', set(df_proxy['archiveType']))\nprint('proxy type:   ', set(df_proxy['paleoData_proxy']))\n\n# (2) plot the spatial distribution of records\ngeo_fig, col = uplt.geo_plot(df_proxy, return_col=True)\n\n# (3) plot the coverage for proxy types and plot resolution\n\nuta.convert_subannual_to_annual_res(df_proxy)\n\ndf_proxy = uta.add_auxvars_plot_summary(df_proxy, key, col=col[at])\n</pre> # (1) filter for archiveType and/or paleoData_proxy:   at = 'Wood' pt = 'd18O' key = '%s_%s'%(at, pt)  keys += [key]  df_proxy = df.copy().loc[(df['archiveType']==at)].loc[(df['paleoData_proxy']==pt)] # filter records for specific archive and proxy type  n_recs = len(df_proxy) # number of records print('n_records   : ', n_recs)  print('archive type: ', set(df_proxy['archiveType'])) print('proxy type:   ', set(df_proxy['paleoData_proxy']))  # (2) plot the spatial distribution of records geo_fig, col = uplt.geo_plot(df_proxy, return_col=True)  # (3) plot the coverage for proxy types and plot resolution  uta.convert_subannual_to_annual_res(df_proxy)  df_proxy = uta.add_auxvars_plot_summary(df_proxy, key, col=col[at])   <pre>n_records   :  27\narchive type:  {'Wood'}\nproxy type:    {'d18O'}\n0 Wood 3650\n1 Speleothem 567\n2 Coral 312\n3 GlacierIce 162\n4 MarineSediment 126\n5 LakeSediment 111\n6 Documents 13\n7 Sclerosponge 6\n8 GroundIce 4\n9 Borehole 3\n10 Other 2\n11 MolluskShell 1\n</pre> In\u00a0[16]: Copied! <pre>df_proxy.keys()\n</pre> df_proxy.keys() Out[16]: <pre>Index(['archiveType', 'dataSetName', 'datasetId', 'duplicateDetails',\n       'geo_meanElev', 'geo_meanLat', 'geo_meanLon', 'geo_siteName',\n       'interpretation_direction', 'interpretation_seasonality',\n       'interpretation_variable', 'interpretation_variableDetail',\n       'originalDataURL', 'originalDatabase', 'paleoData_notes',\n       'paleoData_proxy', 'paleoData_sensorSpecies', 'paleoData_units',\n       'paleoData_values', 'paleoData_variableName', 'year', 'yearUnits',\n       'length', 'miny', 'maxy', 'resolution'],\n      dtype='object')</pre> In\u00a0[17]: Copied! <pre>#========================= PROXY SPECIFIC: Wood d18O =========================\nminres    = 2                         # homogenised resolution\nmny       = 1700                      # start year of homogenised time coord\nmxy       = 2000                      # end year of homogenised time coord\nnyears    = np.min([50, mxy-mny])    # minimum length of each record\n#====================================================================\n\n# filter for record length during target period\ndf_proxy = uta.filter_record_length(df_proxy, nyears, mny, mxy)\n\n# filter for resolution\ndf_proxy = uta.filter_resolution(df_proxy, minres)\n\n# plot coverage and resolution\nuplt.plot_coverage_analysis(df_proxy, np.arange(mny, mxy+minres, minres), key, col[at])\nuplt.plot_resolution(df_proxy, key, col=col[at])\nuplt.plot_length(df_proxy, key, col=col[at])\n\nn_recs = len(df_proxy) # final number of records\n\n\nprint(df_proxy[['miny', 'maxy', 'originalDatabase']])\n\npca_rec[key] = df_proxy['datasetId']\n</pre> #========================= PROXY SPECIFIC: Wood d18O ========================= minres    = 2                         # homogenised resolution mny       = 1700                      # start year of homogenised time coord mxy       = 2000                      # end year of homogenised time coord nyears    = np.min([50, mxy-mny])    # minimum length of each record #====================================================================  # filter for record length during target period df_proxy = uta.filter_record_length(df_proxy, nyears, mny, mxy)  # filter for resolution df_proxy = uta.filter_resolution(df_proxy, minres)  # plot coverage and resolution uplt.plot_coverage_analysis(df_proxy, np.arange(mny, mxy+minres, minres), key, col[at]) uplt.plot_resolution(df_proxy, key, col=col[at]) uplt.plot_length(df_proxy, key, col=col[at])  n_recs = len(df_proxy) # final number of records   print(df_proxy[['miny', 'maxy', 'originalDatabase']])  pca_rec[key] = df_proxy['datasetId'] <pre>Keep 25 records with nyears&gt;=50 during 1700-2000. Exclude 2 records.\nKeep 23 records with resolution &lt;=2. Exclude 2 records.\n</pre> <pre>       miny    maxy originalDatabase\n906  1778.0  2000.0     Iso2k v1.1.2\n907  1780.0  2003.0     Iso2k v1.1.2\n918  1901.0  2009.0     Iso2k v1.1.2\n920  1901.0  2001.0     Iso2k v1.1.2\n921  1900.0  2002.0     Iso2k v1.1.2\n925  1000.0  1998.0     Iso2k v1.1.2\n926  1163.0  2005.0     Iso2k v1.1.2\n928  1820.0  2004.0     Iso2k v1.1.2\n936  1352.0  2012.0     Iso2k v1.1.2\n947  1901.0  2010.0     Iso2k v1.1.2\n949  1865.0  1969.0     Iso2k v1.1.2\n950  1830.0  2010.0     Iso2k v1.1.2\n951  1877.0  1998.0     Iso2k v1.1.2\n952  1877.0  1998.0     Iso2k v1.1.2\n953   489.0  2010.0     Iso2k v1.1.2\n954  1801.0  2000.0     Iso2k v1.1.2\n965  1767.0  2008.0     Iso2k v1.1.2\n969  1743.0  2011.0     Iso2k v1.1.2\n977  1705.0  2004.0     Iso2k v1.1.2\n978  1705.0  2004.0     Iso2k v1.1.2\n979  1705.0  2004.0     Iso2k v1.1.2\n991  1592.0  2011.0     Iso2k v1.1.2\n994  1850.0  2012.0     Iso2k v1.1.2\n</pre> In\u00a0[18]: Copied! <pre># add 'z-scores' to dataframe and plot z-scores and values\ndf_proxy = uta.add_zscores_plot(df_proxy, key, plot_output=True)\n</pre> # add 'z-scores' to dataframe and plot z-scores and values df_proxy = uta.add_zscores_plot(df_proxy, key, plot_output=True)  In\u00a0[19]: Copied! <pre># define new homogenised time coordinate\ndf_proxy, years_hom = uta.homogenise_time(df_proxy, mny, mxy, minres)\ntime[key] = years_hom\n</pre> # define new homogenised time coordinate df_proxy, years_hom = uta.homogenise_time(df_proxy, mny, mxy, minres) time[key] = years_hom <pre>Homogenised time coordinate: 1700-2000 CE\nResolution: [2] years\nINTERSECT: 1901-1969\n</pre> In\u00a0[20]: Copied! <pre># assign the paleoData_values to the non-missing values in the homogenised data array\n\nout = uta.homogenise_data_dimensions(df_proxy, years_hom, key, plot_output=False) # returns list of homogenised paleoData_values and paleoData_zscores\npaleoData_values_hom, paleoData_zscores_hom, year_hom_avbl, zsco_hom_avbl = out\n\n# define new columns in df_filt\nnew_columns = {'year_hom': [years_hom]*n_recs, \n               'year_hom_avbl': year_hom_avbl, \n               'paleoData_values_hom': [paleoData_values_hom[ii, :] for ii in range(n_recs)], \n               'paleoData_zscores_hom': [paleoData_zscores_hom[ii, :] for ii in range(n_recs)], \n               'paleoData_zscores_hom_avbl': zsco_hom_avbl}\ndf_proxy = df_proxy.assign(**new_columns)\n\nprint('Real intersect after homogenising resolution: ')\nintrsct = uta.find_shared_period(df_proxy, minmax=(mny, mxy), time='year_hom_avbl', \n                             data='paleoData_zscores_hom_avbl')\ndata[key] = paleoData_zscores_hom\n</pre> # assign the paleoData_values to the non-missing values in the homogenised data array  out = uta.homogenise_data_dimensions(df_proxy, years_hom, key, plot_output=False) # returns list of homogenised paleoData_values and paleoData_zscores paleoData_values_hom, paleoData_zscores_hom, year_hom_avbl, zsco_hom_avbl = out  # define new columns in df_filt new_columns = {'year_hom': [years_hom]*n_recs,                 'year_hom_avbl': year_hom_avbl,                 'paleoData_values_hom': [paleoData_values_hom[ii, :] for ii in range(n_recs)],                 'paleoData_zscores_hom': [paleoData_zscores_hom[ii, :] for ii in range(n_recs)],                 'paleoData_zscores_hom_avbl': zsco_hom_avbl} df_proxy = df_proxy.assign(**new_columns)  print('Real intersect after homogenising resolution: ') intrsct = uta.find_shared_period(df_proxy, minmax=(mny, mxy), time='year_hom_avbl',                               data='paleoData_zscores_hom_avbl') data[key] = paleoData_zscores_hom  <pre>(23, 151)\nReal intersect after homogenising resolution: \nINTERSECT: 1902-1970\n</pre> In\u00a0[21]: Copied! <pre>covariance, overlap = uta.calc_covariance_matrix(df_proxy)\n</pre> covariance, overlap = uta.calc_covariance_matrix(df_proxy) <pre>short records :  [35. 35. 36. 35. 35. 35. 36. 35.]\n</pre> In\u00a0[22]: Copied! <pre>eigenvalues, eigenvectors = uta.PCA(covariance)\n\nfoev[key] = uta.fraction_of_explained_var(covariance, eigenvalues, n_recs, key, df.name)\n</pre> eigenvalues, eigenvectors = uta.PCA(covariance)  foev[key] = uta.fraction_of_explained_var(covariance, eigenvalues, n_recs, key, df.name) <pre>saved figure in /figs/dod2k_v2.0_filtered_M/foev_Wood_d18O.pdf\n</pre> In\u00a0[23]: Copied! <pre>PCs[key], EOFs[key] = uplt.plot_PCs(years_hom, eigenvectors, paleoData_zscores_hom, key, df.name)\n</pre> PCs[key], EOFs[key] = uplt.plot_PCs(years_hom, eigenvectors, paleoData_zscores_hom, key, df.name) <pre>saved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0_filtered_M/PCs_Wood_d18O.pdf\nsaved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0_filtered_M/EOFloading_Wood_d18O.pdf\n</pre> In\u00a0[24]: Copied! <pre>print('Finished for %s'%key)\n</pre> print('Finished for %s'%key) <pre>Finished for Wood_d18O\n</pre> In\u00a0[25]: Copied! <pre># (1) filter for archiveType and/or paleoData_proxy: \n\nat = 'Coral'\npt = 'd18O'\nkey = '%s_%s'%(at, pt)\n\nkeys += [key]\n\ndf_proxy = df.copy().loc[(df['archiveType']==at)].loc[(df['paleoData_proxy']==pt)] # filter records for specific archive and proxy type\n\nn_recs = len(df_proxy) # number of records\nprint('n_records   : ', n_recs)\n\nprint('archive type: ', set(df_proxy['archiveType']))\nprint('proxy type:   ', set(df_proxy['paleoData_proxy']))\n\n# (2) plot the spatial distribution of records\ngeo_fig, col = uplt.geo_plot(df_proxy, return_col=True)\n\n# (3) plot the coverage for proxy types and plot resolution\n\nuta.convert_subannual_to_annual_res(df_proxy)\n\ndf_proxy = uta.add_auxvars_plot_summary(df_proxy, key, col=col[at])\n</pre> # (1) filter for archiveType and/or paleoData_proxy:   at = 'Coral' pt = 'd18O' key = '%s_%s'%(at, pt)  keys += [key]  df_proxy = df.copy().loc[(df['archiveType']==at)].loc[(df['paleoData_proxy']==pt)] # filter records for specific archive and proxy type  n_recs = len(df_proxy) # number of records print('n_records   : ', n_recs)  print('archive type: ', set(df_proxy['archiveType'])) print('proxy type:   ', set(df_proxy['paleoData_proxy']))  # (2) plot the spatial distribution of records geo_fig, col = uplt.geo_plot(df_proxy, return_col=True)  # (3) plot the coverage for proxy types and plot resolution  uta.convert_subannual_to_annual_res(df_proxy)  df_proxy = uta.add_auxvars_plot_summary(df_proxy, key, col=col[at])   <pre>n_records   :  7\narchive type:  {'Coral'}\nproxy type:    {'d18O'}\n0 Wood 3650\n1 Speleothem 567\n2 Coral 312\n3 GlacierIce 162\n4 MarineSediment 126\n5 LakeSediment 111\n6 Documents 13\n7 Sclerosponge 6\n8 GroundIce 4\n9 Borehole 3\n10 Other 2\n11 MolluskShell 1\n</pre> In\u00a0[26]: Copied! <pre>#========================= PROXY SPECIFIC: Coral d18O =========================\nminres    = 1                         # homogenised resolution\nmny       = 1750                      # start year of homogenised time coord\nmxy       = 2000                      # end year of homogenised time coord\nnyears    = np.min([90, mxy-mny])    # minimum length of each record\n#====================================================================\n# filter for record length during target period\ndf_proxy = uta.filter_record_length(df_proxy, nyears, mny, mxy)\n\n# filter for resolution\ndf_proxy = uta.filter_resolution(df_proxy, minres)\n\n# plot coverage and resolution\nuplt.plot_coverage_analysis(df_proxy, np.arange(mny, mxy+minres, minres), key, col[at])\nuplt.plot_resolution(df_proxy, key, col=col[at])\nuplt.plot_length(df_proxy, key, col=col[at])\n\nn_recs = len(df_proxy) # final number of records\n\n\nprint(df_proxy[['miny', 'maxy', 'originalDatabase']])\n\npca_rec[key] = df_proxy['datasetId']\n</pre> #========================= PROXY SPECIFIC: Coral d18O ========================= minres    = 1                         # homogenised resolution mny       = 1750                      # start year of homogenised time coord mxy       = 2000                      # end year of homogenised time coord nyears    = np.min([90, mxy-mny])    # minimum length of each record #==================================================================== # filter for record length during target period df_proxy = uta.filter_record_length(df_proxy, nyears, mny, mxy)  # filter for resolution df_proxy = uta.filter_resolution(df_proxy, minres)  # plot coverage and resolution uplt.plot_coverage_analysis(df_proxy, np.arange(mny, mxy+minres, minres), key, col[at]) uplt.plot_resolution(df_proxy, key, col=col[at]) uplt.plot_length(df_proxy, key, col=col[at])  n_recs = len(df_proxy) # final number of records   print(df_proxy[['miny', 'maxy', 'originalDatabase']])  pca_rec[key] = df_proxy['datasetId'] <pre>Keep 4 records with nyears&gt;=90 during 1750-2000. Exclude 3 records.\nKeep 4 records with resolution &lt;=1. Exclude 0 records.\n</pre> <pre>       miny    maxy originalDatabase\n913  1899.0  1996.0     Iso2k v1.1.2\n927  1886.0  1998.0     Iso2k v1.1.2\n931  1824.0  1985.0     Iso2k v1.1.2\n935  1751.0  1986.0     Iso2k v1.1.2\n</pre> In\u00a0[27]: Copied! <pre># add 'z-scores' to dataframe and plot z-scores and values\ndf_proxy = uta.add_zscores_plot(df_proxy, key, plot_output=True)\n</pre> # add 'z-scores' to dataframe and plot z-scores and values df_proxy = uta.add_zscores_plot(df_proxy, key, plot_output=True)  In\u00a0[28]: Copied! <pre># define new homogenised time coordinate\ndf_proxy, years_hom = uta.homogenise_time(df_proxy, mny, mxy, minres)\ntime[key] = years_hom\n</pre> # define new homogenised time coordinate df_proxy, years_hom = uta.homogenise_time(df_proxy, mny, mxy, minres) time[key] = years_hom <pre>Homogenised time coordinate: 1750-2000 CE\nResolution: [1] years\nINTERSECT: 1899-1985\n</pre> In\u00a0[29]: Copied! <pre># assign the paleoData_values to the non-missing values in the homogenised data array\n\nout = uta.homogenise_data_dimensions(df_proxy, years_hom, key, plot_output=False) # returns list of homogenised paleoData_values and paleoData_zscores\npaleoData_values_hom, paleoData_zscores_hom, year_hom_avbl, zsco_hom_avbl = out\n\n# define new columns in df_filt\nnew_columns = {'year_hom': [years_hom]*n_recs, \n               'year_hom_avbl': year_hom_avbl, \n               'paleoData_values_hom': [paleoData_values_hom[ii, :] for ii in range(n_recs)], \n               'paleoData_zscores_hom': [paleoData_zscores_hom[ii, :] for ii in range(n_recs)], \n               'paleoData_zscores_hom_avbl': zsco_hom_avbl}\ndf_proxy = df_proxy.assign(**new_columns)\n\nprint('Real intersect after homogenising resolution: ')\nintrsct = uta.find_shared_period(df_proxy, minmax=(mny, mxy), time='year_hom_avbl', \n                             data='paleoData_zscores_hom_avbl')\ndata[key] = paleoData_zscores_hom\n</pre> # assign the paleoData_values to the non-missing values in the homogenised data array  out = uta.homogenise_data_dimensions(df_proxy, years_hom, key, plot_output=False) # returns list of homogenised paleoData_values and paleoData_zscores paleoData_values_hom, paleoData_zscores_hom, year_hom_avbl, zsco_hom_avbl = out  # define new columns in df_filt new_columns = {'year_hom': [years_hom]*n_recs,                 'year_hom_avbl': year_hom_avbl,                 'paleoData_values_hom': [paleoData_values_hom[ii, :] for ii in range(n_recs)],                 'paleoData_zscores_hom': [paleoData_zscores_hom[ii, :] for ii in range(n_recs)],                 'paleoData_zscores_hom_avbl': zsco_hom_avbl} df_proxy = df_proxy.assign(**new_columns)  print('Real intersect after homogenising resolution: ') intrsct = uta.find_shared_period(df_proxy, minmax=(mny, mxy), time='year_hom_avbl',                               data='paleoData_zscores_hom_avbl') data[key] = paleoData_zscores_hom  <pre>(4, 251)\nReal intersect after homogenising resolution: \nINTERSECT: 1899-1985\n</pre> In\u00a0[30]: Copied! <pre>covariance, overlap = uta.calc_covariance_matrix(df_proxy)\n</pre> covariance, overlap = uta.calc_covariance_matrix(df_proxy) <pre>short records :  []\n</pre> In\u00a0[31]: Copied! <pre>eigenvalues, eigenvectors = uta.PCA(covariance)\n\nfoev[key] = uta.fraction_of_explained_var(covariance, eigenvalues, n_recs, key, df.name, col=col[at])\n</pre> eigenvalues, eigenvectors = uta.PCA(covariance)  foev[key] = uta.fraction_of_explained_var(covariance, eigenvalues, n_recs, key, df.name, col=col[at]) <pre>saved figure in /figs/dod2k_v2.0_filtered_M/foev_Coral_d18O.pdf\n</pre> In\u00a0[32]: Copied! <pre>PCs[key], EOFs[key] = uplt.plot_PCs(years_hom, eigenvectors, paleoData_zscores_hom, key, df.name, col=col[at])\n</pre> PCs[key], EOFs[key] = uplt.plot_PCs(years_hom, eigenvectors, paleoData_zscores_hom, key, df.name, col=col[at]) <pre>saved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0_filtered_M/PCs_Coral_d18O.pdf\nsaved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0_filtered_M/EOFloading_Coral_d18O.pdf\n</pre> In\u00a0[33]: Copied! <pre>print('Finished for %s'%key)\n</pre> print('Finished for %s'%key) <pre>Finished for Coral_d18O\n</pre> In\u00a0[34]: Copied! <pre># (1) filter for archiveType and/or paleoData_proxy: \n\nat = 'Speleothem'\npt = 'd18O'\nkey = '%s_%s'%(at, pt)\n\nkeys += [key]\n\ndf_proxy = df.copy().loc[(df['archiveType']==at)].loc[(df['paleoData_proxy']==pt)] # filter records for specific archive and proxy type\n\nn_recs = len(df_proxy) # number of records\nprint('n_records   : ', n_recs)\n\nprint('archive type: ', set(df_proxy['archiveType']))\nprint('proxy type:   ', set(df_proxy['paleoData_proxy']))\n\n# (2) plot the spatial distribution of records\ngeo_fig, col = uplt.geo_plot(df_proxy, return_col=True)\n\n# (3) plot the coverage for proxy types and plot resolution\n\nuta.convert_subannual_to_annual_res(df_proxy)\n\ndf_proxy = uta.add_auxvars_plot_summary(df_proxy, key, col=col[at])\n</pre> # (1) filter for archiveType and/or paleoData_proxy:   at = 'Speleothem' pt = 'd18O' key = '%s_%s'%(at, pt)  keys += [key]  df_proxy = df.copy().loc[(df['archiveType']==at)].loc[(df['paleoData_proxy']==pt)] # filter records for specific archive and proxy type  n_recs = len(df_proxy) # number of records print('n_records   : ', n_recs)  print('archive type: ', set(df_proxy['archiveType'])) print('proxy type:   ', set(df_proxy['paleoData_proxy']))  # (2) plot the spatial distribution of records geo_fig, col = uplt.geo_plot(df_proxy, return_col=True)  # (3) plot the coverage for proxy types and plot resolution  uta.convert_subannual_to_annual_res(df_proxy)  df_proxy = uta.add_auxvars_plot_summary(df_proxy, key, col=col[at])   <pre>n_records   :  25\narchive type:  {'Speleothem'}\nproxy type:    {'d18O'}\n0 Wood 3650\n1 Speleothem 567\n2 Coral 312\n3 GlacierIce 162\n4 MarineSediment 126\n5 LakeSediment 111\n6 Documents 13\n7 Sclerosponge 6\n8 GroundIce 4\n9 Borehole 3\n10 Other 2\n11 MolluskShell 1\n</pre> In\u00a0[35]: Copied! <pre>#========================= PROXY SPECIFIC: Speleothem d18O =========================\nminres    = 55                         # homogenised resolution\nmny       = 500                      # start year of homogenised time coord\nmxy       = 1600                      # end year of homogenised time coord\nnyears    = np.min([200, mxy-mny])    # minimum length of each record\n#====================================================================\n\n# filter for record length during target period\ndf_proxy = uta.filter_record_length(df_proxy, nyears, mny, mxy)\n\n# filter for resolution\ndf_proxy = uta.filter_resolution(df_proxy, minres)\n\n# plot coverage and resolution\nuplt.plot_coverage_analysis(df_proxy, np.arange(mny, mxy+minres, minres), key, col[at])\nuplt.plot_resolution(df_proxy, key, col=col[at])\nuplt.plot_length(df_proxy, key, col=col[at])\n\nn_recs = len(df_proxy) # final number of records\n\n\nprint(df_proxy[['miny', 'maxy', 'originalDatabase']])\n\npca_rec[key] = df_proxy['datasetId']\n</pre> #========================= PROXY SPECIFIC: Speleothem d18O ========================= minres    = 55                         # homogenised resolution mny       = 500                      # start year of homogenised time coord mxy       = 1600                      # end year of homogenised time coord nyears    = np.min([200, mxy-mny])    # minimum length of each record #====================================================================  # filter for record length during target period df_proxy = uta.filter_record_length(df_proxy, nyears, mny, mxy)  # filter for resolution df_proxy = uta.filter_resolution(df_proxy, minres)  # plot coverage and resolution uplt.plot_coverage_analysis(df_proxy, np.arange(mny, mxy+minres, minres), key, col[at]) uplt.plot_resolution(df_proxy, key, col=col[at]) uplt.plot_length(df_proxy, key, col=col[at])  n_recs = len(df_proxy) # final number of records   print(df_proxy[['miny', 'maxy', 'originalDatabase']])  pca_rec[key] = df_proxy['datasetId'] <pre>Keep 6 records with nyears&gt;=200 during 500-1600. Exclude 19 records.\nKeep 6 records with resolution &lt;=55. Exclude 0 records.\n</pre> <pre>      miny    maxy originalDatabase\n899    2.0  2002.0     Iso2k v1.1.2\n901  624.0  1562.0     Iso2k v1.1.2\n944  192.0  2003.0     Iso2k v1.1.2\n958    1.0  1997.0     Iso2k v1.1.2\n962    6.0  1998.0     Iso2k v1.1.2\n990    3.0  1716.0     Iso2k v1.1.2\n</pre> In\u00a0[36]: Copied! <pre># add 'z-scores' to dataframe and plot z-scores and values\ndf_proxy = uta.add_zscores_plot(df_proxy, key, plot_output=True)\n</pre> # add 'z-scores' to dataframe and plot z-scores and values df_proxy = uta.add_zscores_plot(df_proxy, key, plot_output=True)  In\u00a0[37]: Copied! <pre># define new homogenised time coordinate\ndf_proxy, years_hom = uta.homogenise_time(df_proxy, mny, mxy, minres)\ntime[key] = years_hom\n</pre> # define new homogenised time coordinate df_proxy, years_hom = uta.homogenise_time(df_proxy, mny, mxy, minres) time[key] = years_hom <pre>Homogenised time coordinate: 500-1600 CE\nResolution: [55] years\nINTERSECT: 638-1481\n</pre> In\u00a0[38]: Copied! <pre># assign the paleoData_values to the non-missing values in the homogenised data array\n\nout = uta.homogenise_data_dimensions(df_proxy, years_hom, key, plot_output=False) # returns list of homogenised paleoData_values and paleoData_zscores\npaleoData_values_hom, paleoData_zscores_hom, year_hom_avbl, zsco_hom_avbl = out\n\n# define new columns in df_filt\nnew_columns = {'year_hom': [years_hom]*n_recs, \n               'year_hom_avbl': year_hom_avbl, \n               'paleoData_values_hom': [paleoData_values_hom[ii, :] for ii in range(n_recs)], \n               'paleoData_zscores_hom': [paleoData_zscores_hom[ii, :] for ii in range(n_recs)], \n               'paleoData_zscores_hom_avbl': zsco_hom_avbl}\ndf_proxy = df_proxy.assign(**new_columns)\n\nprint('Real intersect after homogenising resolution: ')\nintrsct = uta.find_shared_period(df_proxy, minmax=(mny, mxy), time='year_hom_avbl', \n                             data='paleoData_zscores_hom_avbl')\ndata[key] = paleoData_zscores_hom\n</pre> # assign the paleoData_values to the non-missing values in the homogenised data array  out = uta.homogenise_data_dimensions(df_proxy, years_hom, key, plot_output=False) # returns list of homogenised paleoData_values and paleoData_zscores paleoData_values_hom, paleoData_zscores_hom, year_hom_avbl, zsco_hom_avbl = out  # define new columns in df_filt new_columns = {'year_hom': [years_hom]*n_recs,                 'year_hom_avbl': year_hom_avbl,                 'paleoData_values_hom': [paleoData_values_hom[ii, :] for ii in range(n_recs)],                 'paleoData_zscores_hom': [paleoData_zscores_hom[ii, :] for ii in range(n_recs)],                 'paleoData_zscores_hom_avbl': zsco_hom_avbl} df_proxy = df_proxy.assign(**new_columns)  print('Real intersect after homogenising resolution: ') intrsct = uta.find_shared_period(df_proxy, minmax=(mny, mxy), time='year_hom_avbl',                               data='paleoData_zscores_hom_avbl') data[key] = paleoData_zscores_hom  <pre>(6, 21)\nReal intersect after homogenising resolution: \nINTERSECT: 665-1600\n</pre> In\u00a0[39]: Copied! <pre>covariance, overlap = uta.calc_covariance_matrix(df_proxy)\n</pre> covariance, overlap = uta.calc_covariance_matrix(df_proxy) <pre>short records :  [21. 18. 21. 21. 21. 21. 18. 18. 18. 18. 18. 18. 21. 18. 21. 21. 21. 21.\n 21. 18. 21. 21. 21. 21. 21. 18. 21. 21. 21. 21. 21. 18. 21. 21. 21. 21.]\n</pre> In\u00a0[40]: Copied! <pre>eigenvalues, eigenvectors = uta.PCA(covariance)\n\nfoev[key] = uta.fraction_of_explained_var(covariance, eigenvalues, n_recs, key, df.name, col=col[at])\n</pre> eigenvalues, eigenvectors = uta.PCA(covariance)  foev[key] = uta.fraction_of_explained_var(covariance, eigenvalues, n_recs, key, df.name, col=col[at]) <pre>saved figure in /figs/dod2k_v2.0_filtered_M/foev_Speleothem_d18O.pdf\n</pre> In\u00a0[41]: Copied! <pre>PCs[key], EOFs[key] = uplt.plot_PCs(years_hom, eigenvectors, paleoData_zscores_hom, key, df.name, col=col[at])\n</pre> PCs[key], EOFs[key] = uplt.plot_PCs(years_hom, eigenvectors, paleoData_zscores_hom, key, df.name, col=col[at]) <pre>saved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0_filtered_M/PCs_Speleothem_d18O.pdf\nsaved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0_filtered_M/EOFloading_Speleothem_d18O.pdf\n</pre> In\u00a0[42]: Copied! <pre>print('Finished for %s'%key)\n</pre> print('Finished for %s'%key) <pre>Finished for Speleothem_d18O\n</pre> In\u00a0[43]: Copied! <pre># (1) filter for archiveType and/or paleoData_proxy: \n\nat = 'LakeSediment'\npt  = 'd18O+dD'\nkey = '%s_%s'%(at, pt)\n\nkeys += [key]\n\ndf_proxy = df.loc[(df['archiveType']==at)].loc[(df['paleoData_proxy']==pt.split('+')[0])|(df['paleoData_proxy']==pt.split('+')[1])]\n\nn_recs = len(df_proxy) # number of records\nprint('n_records   : ', n_recs)\n\nprint('archive type: ', set(df_proxy['archiveType']))\nprint('proxy type:   ', set(df_proxy['paleoData_proxy']))\n\n# (2) plot the spatial distribution of records\ngeo_fig, col = uplt.geo_plot(df_proxy, return_col=True)\n\n# (3) plot the coverage for proxy types and plot resolution\n\nuta.convert_subannual_to_annual_res(df_proxy)\n\ndf_proxy = uta.add_auxvars_plot_summary(df_proxy, key, col=col[at])\n</pre> # (1) filter for archiveType and/or paleoData_proxy:   at = 'LakeSediment' pt  = 'd18O+dD' key = '%s_%s'%(at, pt)  keys += [key]  df_proxy = df.loc[(df['archiveType']==at)].loc[(df['paleoData_proxy']==pt.split('+')[0])|(df['paleoData_proxy']==pt.split('+')[1])]  n_recs = len(df_proxy) # number of records print('n_records   : ', n_recs)  print('archive type: ', set(df_proxy['archiveType'])) print('proxy type:   ', set(df_proxy['paleoData_proxy']))  # (2) plot the spatial distribution of records geo_fig, col = uplt.geo_plot(df_proxy, return_col=True)  # (3) plot the coverage for proxy types and plot resolution  uta.convert_subannual_to_annual_res(df_proxy)  df_proxy = uta.add_auxvars_plot_summary(df_proxy, key, col=col[at])   <pre>n_records   :  35\narchive type:  {'LakeSediment'}\nproxy type:    {'d18O', 'dD'}\n0 Wood 3650\n1 Speleothem 567\n2 Coral 312\n3 GlacierIce 162\n4 MarineSediment 126\n5 LakeSediment 111\n6 Documents 13\n7 Sclerosponge 6\n8 GroundIce 4\n9 Borehole 3\n10 Other 2\n11 MolluskShell 1\n</pre> In\u00a0[44]: Copied! <pre>#========================= PROXY SPECIFIC: LakeSediment d18O+dD =========================\nminres    = 55                         # homogenised resolution\nmny       = 300                      # start year of homogenised time coord\nmxy       = 1800                      # end year of homogenised time coord\nnyears    = np.min([100, mxy-mny])    # minimum length of each record\n#====================================================================\n\n# filter for record length during target period\ndf_proxy = uta.filter_record_length(df_proxy, nyears, mny, mxy)\n\n# filter for resolution\ndf_proxy = uta.filter_resolution(df_proxy, minres)\n\n# plot coverage and resolution\nuplt.plot_coverage_analysis(df_proxy, np.arange(mny, mxy+minres, minres), key, col[at])\nuplt.plot_resolution(df_proxy, key, col=col[at])\nuplt.plot_length(df_proxy, key, col=col[at])\n\nn_recs = len(df_proxy) # final number of records\n\n\nprint(df_proxy[['miny', 'maxy', 'originalDatabase']])\n\npca_rec[key] = df_proxy['datasetId']\n</pre> #========================= PROXY SPECIFIC: LakeSediment d18O+dD ========================= minres    = 55                         # homogenised resolution mny       = 300                      # start year of homogenised time coord mxy       = 1800                      # end year of homogenised time coord nyears    = np.min([100, mxy-mny])    # minimum length of each record #====================================================================  # filter for record length during target period df_proxy = uta.filter_record_length(df_proxy, nyears, mny, mxy)  # filter for resolution df_proxy = uta.filter_resolution(df_proxy, minres)  # plot coverage and resolution uplt.plot_coverage_analysis(df_proxy, np.arange(mny, mxy+minres, minres), key, col[at]) uplt.plot_resolution(df_proxy, key, col=col[at]) uplt.plot_length(df_proxy, key, col=col[at])  n_recs = len(df_proxy) # final number of records   print(df_proxy[['miny', 'maxy', 'originalDatabase']])  pca_rec[key] = df_proxy['datasetId'] <pre>Keep 10 records with nyears&gt;=100 during 300-1800. Exclude 25 records.\nKeep 9 records with resolution &lt;=55. Exclude 1 records.\n</pre> <pre>      miny    maxy originalDatabase\n897    7.0  2007.0     Iso2k v1.1.2\n916    2.0  2009.0     Iso2k v1.1.2\n923  276.0  2001.0     Iso2k v1.1.2\n957  958.0  1940.0     Iso2k v1.1.2\n959  341.0  2004.0     Iso2k v1.1.2\n967    2.0  1842.0     Iso2k v1.1.2\n968   13.0  1963.0     Iso2k v1.1.2\n975  500.0  2000.0     Iso2k v1.1.2\n987  500.0  2000.0     Iso2k v1.1.2\n</pre> In\u00a0[45]: Copied! <pre># add 'z-scores' to dataframe and plot z-scores and values\ndf_proxy = uta.add_zscores_plot(df_proxy, key, plot_output=True)\n</pre> # add 'z-scores' to dataframe and plot z-scores and values df_proxy = uta.add_zscores_plot(df_proxy, key, plot_output=True)  In\u00a0[46]: Copied! <pre># define new homogenised time coordinate\ndf_proxy, years_hom = uta.homogenise_time(df_proxy, mny, mxy, minres)\ntime[key] = years_hom\n</pre> # define new homogenised time coordinate df_proxy, years_hom = uta.homogenise_time(df_proxy, mny, mxy, minres) time[key] = years_hom <pre>Homogenised time coordinate: 300-1840 CE\nResolution: [55] years\nNo shared period across all records.\n</pre> In\u00a0[47]: Copied! <pre># assign the paleoData_values to the non-missing values in the homogenised data array\n\nout = uta.homogenise_data_dimensions(df_proxy, years_hom, key, plot_output=False) # returns list of homogenised paleoData_values and paleoData_zscores\npaleoData_values_hom, paleoData_zscores_hom, year_hom_avbl, zsco_hom_avbl = out\n\n# define new columns in df_filt\nnew_columns = {'year_hom': [years_hom]*n_recs, \n               'year_hom_avbl': year_hom_avbl, \n               'paleoData_values_hom': [paleoData_values_hom[ii, :] for ii in range(n_recs)], \n               'paleoData_zscores_hom': [paleoData_zscores_hom[ii, :] for ii in range(n_recs)], \n               'paleoData_zscores_hom_avbl': zsco_hom_avbl}\ndf_proxy = df_proxy.assign(**new_columns)\n\nprint('Real intersect after homogenising resolution: ')\nintrsct = uta.find_shared_period(df_proxy, minmax=(mny, mxy), time='year_hom_avbl', \n                             data='paleoData_zscores_hom_avbl')\ndata[key] = paleoData_zscores_hom\n</pre> # assign the paleoData_values to the non-missing values in the homogenised data array  out = uta.homogenise_data_dimensions(df_proxy, years_hom, key, plot_output=False) # returns list of homogenised paleoData_values and paleoData_zscores paleoData_values_hom, paleoData_zscores_hom, year_hom_avbl, zsco_hom_avbl = out  # define new columns in df_filt new_columns = {'year_hom': [years_hom]*n_recs,                 'year_hom_avbl': year_hom_avbl,                 'paleoData_values_hom': [paleoData_values_hom[ii, :] for ii in range(n_recs)],                 'paleoData_zscores_hom': [paleoData_zscores_hom[ii, :] for ii in range(n_recs)],                 'paleoData_zscores_hom_avbl': zsco_hom_avbl} df_proxy = df_proxy.assign(**new_columns)  print('Real intersect after homogenising resolution: ') intrsct = uta.find_shared_period(df_proxy, minmax=(mny, mxy), time='year_hom_avbl',                               data='paleoData_zscores_hom_avbl') data[key] = paleoData_zscores_hom  <pre>(9, 29)\nReal intersect after homogenising resolution: \nINTERSECT: 960-1840\n</pre> In\u00a0[48]: Copied! <pre>covariance, overlap = uta.calc_covariance_matrix(df_proxy)\n</pre> covariance, overlap = uta.calc_covariance_matrix(df_proxy) <pre>short records :  [29. 29. 29. 17. 28. 29. 29. 25. 25. 29. 29. 29. 17. 28. 29. 29. 25. 25.\n 29. 29. 29. 17. 28. 29. 29. 25. 25. 17. 17. 17. 17. 17. 17. 17. 17. 17.\n 28. 28. 28. 17. 28. 28. 28. 25. 25. 29. 29. 29. 17. 28. 29. 29. 25. 25.\n 29. 29. 29. 17. 28. 29. 29. 25. 25. 25. 25. 25. 17. 25. 25. 25. 25. 25.\n 25. 25. 25. 17. 25. 25. 25. 25. 25.]\n</pre> In\u00a0[49]: Copied! <pre>eigenvalues, eigenvectors = uta.PCA(covariance)\n\nfoev[key] = uta.fraction_of_explained_var(covariance, eigenvalues, n_recs, key, df.name, col=col[at])\n</pre> eigenvalues, eigenvectors = uta.PCA(covariance)  foev[key] = uta.fraction_of_explained_var(covariance, eigenvalues, n_recs, key, df.name, col=col[at]) <pre>saved figure in /figs/dod2k_v2.0_filtered_M/foev_LakeSediment_d18O+dD.pdf\n</pre> In\u00a0[50]: Copied! <pre>PCs[key], EOFs[key] = uplt.plot_PCs(years_hom, eigenvectors, paleoData_zscores_hom, key, df.name, col=col[at])\n</pre> PCs[key], EOFs[key] = uplt.plot_PCs(years_hom, eigenvectors, paleoData_zscores_hom, key, df.name, col=col[at]) <pre>saved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0_filtered_M/PCs_LakeSediment_d18O+dD.pdf\nsaved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0_filtered_M/EOFloading_LakeSediment_d18O+dD.pdf\n</pre> In\u00a0[51]: Copied! <pre>print('Finished for %s'%key)\n</pre> print('Finished for %s'%key) <pre>Finished for LakeSediment_d18O+dD\n</pre> In\u00a0[52]: Copied! <pre># # (1) filter for archiveType and/or paleoData_proxy: \n\n# at = 'MarineSediment'\n# pt = 'd18O'\n# key = '%s_%s'%(at, pt)\n\n# keys += [key]\n\n# df_proxy = df.copy().loc[(df['archiveType']==at)].loc[(df['paleoData_proxy']==pt)] # filter records for specific archive and proxy type\n\n# n_recs = len(df_proxy) # number of records\n# print('n_records   : ', n_recs)\n\n# print('archive type: ', set(df_proxy['archiveType']))\n# print('proxy type:   ', set(df_proxy['paleoData_proxy']))\n\n# # (2) plot the spatial distribution of records\n# geo_fig, col = uplt.geo_plot(df_proxy, return_col=True)\n\n# # (3) plot the coverage for proxy types and plot resolution\n\n# uta.convert_subannual_to_annual_res(df_proxy)\n\n# df_proxy = uta.add_auxvars_plot_summary(df_proxy, key, col=col[at])\n</pre> # # (1) filter for archiveType and/or paleoData_proxy:   # at = 'MarineSediment' # pt = 'd18O' # key = '%s_%s'%(at, pt)  # keys += [key]  # df_proxy = df.copy().loc[(df['archiveType']==at)].loc[(df['paleoData_proxy']==pt)] # filter records for specific archive and proxy type  # n_recs = len(df_proxy) # number of records # print('n_records   : ', n_recs)  # print('archive type: ', set(df_proxy['archiveType'])) # print('proxy type:   ', set(df_proxy['paleoData_proxy']))  # # (2) plot the spatial distribution of records # geo_fig, col = uplt.geo_plot(df_proxy, return_col=True)  # # (3) plot the coverage for proxy types and plot resolution  # uta.convert_subannual_to_annual_res(df_proxy)  # df_proxy = uta.add_auxvars_plot_summary(df_proxy, key, col=col[at])   In\u00a0[53]: Copied! <pre># #========================= PROXY SPECIFIC: MarineSediment d18O =========================\n# minres    = 100                         # homogenised resolution\n# mny       = 100                      # start year of homogenised time coord\n# mxy       = 1800                      # end year of homogenised time coord\n# nyears    = np.min([100, mxy-mny])    # minimum length of each record\n# #====================================================================\n\n# # filter for record length during target period\n# df_proxy = uta.filter_record_length(df_proxy, nyears, mny, mxy)\n\n# # filter for resolution\n# df_proxy = uta.filter_resolution(df_proxy, minres)\n\n# # plot coverage and resolution\n# uplt.plot_coverage_analysis(df_proxy, np.arange(mny, mxy+minres, minres), key, col[at])\n# uplt.plot_resolution(df_proxy, key)\n# uplt.plot_length(df_proxy, key)\n\n# n_recs = len(df_proxy) # final number of records\n\n\n# print(df_proxy[['miny', 'maxy', 'originalDatabase']])\n\n# pca_rec[key] = df_proxy['datasetId']\n</pre> # #========================= PROXY SPECIFIC: MarineSediment d18O ========================= # minres    = 100                         # homogenised resolution # mny       = 100                      # start year of homogenised time coord # mxy       = 1800                      # end year of homogenised time coord # nyears    = np.min([100, mxy-mny])    # minimum length of each record # #====================================================================  # # filter for record length during target period # df_proxy = uta.filter_record_length(df_proxy, nyears, mny, mxy)  # # filter for resolution # df_proxy = uta.filter_resolution(df_proxy, minres)  # # plot coverage and resolution # uplt.plot_coverage_analysis(df_proxy, np.arange(mny, mxy+minres, minres), key, col[at]) # uplt.plot_resolution(df_proxy, key) # uplt.plot_length(df_proxy, key)  # n_recs = len(df_proxy) # final number of records   # print(df_proxy[['miny', 'maxy', 'originalDatabase']])  # pca_rec[key] = df_proxy['datasetId'] In\u00a0[54]: Copied! <pre># # add 'z-scores' to dataframe and plot z-scores and values\n# df_proxy = uta.add_zscores_plot(df_proxy, key, plot_output=True)\n</pre> # # add 'z-scores' to dataframe and plot z-scores and values # df_proxy = uta.add_zscores_plot(df_proxy, key, plot_output=True)  In\u00a0[55]: Copied! <pre># # define new homogenised time coordinate\n# df_proxy, years_hom = uta.homogenise_time(df_proxy, mny, mxy, minres)\n# time[key] = years_hom\n</pre> # # define new homogenised time coordinate # df_proxy, years_hom = uta.homogenise_time(df_proxy, mny, mxy, minres) # time[key] = years_hom In\u00a0[56]: Copied! <pre># # assign the paleoData_values to the non-missing values in the homogenised data array\n\n# out = uta.homogenise_data_dimensions(df_proxy, years_hom, key, plot_output=False) # returns list of homogenised paleoData_values and paleoData_zscores\n# paleoData_values_hom, paleoData_zscores_hom, year_hom_avbl, zsco_hom_avbl = out\n\n# # define new columns in df_filt\n# new_columns = {'year_hom': [years_hom]*n_recs, \n#                'year_hom_avbl': year_hom_avbl, \n#                'paleoData_values_hom': [paleoData_values_hom[ii, :] for ii in range(n_recs)], \n#                'paleoData_zscores_hom': [paleoData_zscores_hom[ii, :] for ii in range(n_recs)], \n#                'paleoData_zscores_hom_avbl': zsco_hom_avbl}\n# df_proxy = df_proxy.assign(**new_columns)\n\n# print('Real intersect after homogenising resolution: ')\n# intrsct = uta.find_shared_period(df_proxy, minmax=(mny, mxy), time='year_hom_avbl', \n#                              data='paleoData_zscores_hom_avbl')\n# data[key] = paleoData_zscores_hom\n</pre> # # assign the paleoData_values to the non-missing values in the homogenised data array  # out = uta.homogenise_data_dimensions(df_proxy, years_hom, key, plot_output=False) # returns list of homogenised paleoData_values and paleoData_zscores # paleoData_values_hom, paleoData_zscores_hom, year_hom_avbl, zsco_hom_avbl = out  # # define new columns in df_filt # new_columns = {'year_hom': [years_hom]*n_recs,  #                'year_hom_avbl': year_hom_avbl,  #                'paleoData_values_hom': [paleoData_values_hom[ii, :] for ii in range(n_recs)],  #                'paleoData_zscores_hom': [paleoData_zscores_hom[ii, :] for ii in range(n_recs)],  #                'paleoData_zscores_hom_avbl': zsco_hom_avbl} # df_proxy = df_proxy.assign(**new_columns)  # print('Real intersect after homogenising resolution: ') # intrsct = uta.find_shared_period(df_proxy, minmax=(mny, mxy), time='year_hom_avbl',  #                              data='paleoData_zscores_hom_avbl') # data[key] = paleoData_zscores_hom  In\u00a0[57]: Copied! <pre># covariance, overlap = uta.calc_covariance_matrix(df_proxy)\n</pre> # covariance, overlap = uta.calc_covariance_matrix(df_proxy) In\u00a0[58]: Copied! <pre># eigenvalues, eigenvectors = uta.PCA(covariance)\n\n# foev[key] = uta.fraction_of_explained_var(covariance, eigenvalues, n_recs, key, df.name)\n</pre> # eigenvalues, eigenvectors = uta.PCA(covariance)  # foev[key] = uta.fraction_of_explained_var(covariance, eigenvalues, n_recs, key, df.name) In\u00a0[59]: Copied! <pre># PCs[key], EOFs[key] = uplt.plot_PCs(years_hom, eigenvectors, paleoData_zscores_hom, key, df.name)\n</pre> # PCs[key], EOFs[key] = uplt.plot_PCs(years_hom, eigenvectors, paleoData_zscores_hom, key, df.name) In\u00a0[60]: Copied! <pre># print('Finished for %s'%key)\n</pre> # print('Finished for %s'%key) In\u00a0[69]: Copied! <pre>archive_colour, archives_sorted, proxy_marker = uplt.df_colours_markers()\n</pre> archive_colour, archives_sorted, proxy_marker = uplt.df_colours_markers() <pre>0 Wood 3650\n1 Speleothem 567\n2 Coral 312\n3 GlacierIce 162\n4 MarineSediment 126\n5 LakeSediment 111\n6 Documents 13\n7 Sclerosponge 6\n8 GroundIce 4\n9 Borehole 3\n10 Other 2\n11 MolluskShell 1\n</pre> In\u00a0[70]: Copied! <pre>key_colour = {}\nfor key in keys:\n    key_colour[key]=archive_colour[key.split('_')[0]]\n\nkey_colour['Wood_d18O']='k'\n</pre> key_colour = {} for key in keys:     key_colour[key]=archive_colour[key.split('_')[0]]  key_colour['Wood_d18O']='k' In\u00a0[71]: Copied! <pre># as a comparison plot the mean over all records\nfig = plt.figure(figsize=(9, 4), dpi=300)\nfor key in keys:\n    lw = 1 if key in ['Wood_ring width', 'Wood_d18O', 'Coral_d18O'] else 2\n    plt.plot(time[key], np.ma.mean(data[key], axis=0)-np.mean(np.ma.mean(data[key], axis=0)), \n             label=key, lw=lw, color=key_colour[key])\nplt.ylabel('paleoData_zscores')\nplt.xlabel('year CE')\nplt.legend(ncol=3, bbox_to_anchor=(0,-0.2), loc='upper left', framealpha=0)\nplt.show()\n</pre> # as a comparison plot the mean over all records fig = plt.figure(figsize=(9, 4), dpi=300) for key in keys:     lw = 1 if key in ['Wood_ring width', 'Wood_d18O', 'Coral_d18O'] else 2     plt.plot(time[key], np.ma.mean(data[key], axis=0)-np.mean(np.ma.mean(data[key], axis=0)),               label=key, lw=lw, color=key_colour[key]) plt.ylabel('paleoData_zscores') plt.xlabel('year CE') plt.legend(ncol=3, bbox_to_anchor=(0,-0.2), loc='upper left', framealpha=0) plt.show() In\u00a0[72]: Copied! <pre># as a comparison plot the mean over all records SMOOTHED\nfig = plt.figure(figsize=(9, 4), dpi=300)\nfor key in keys:\n    if key in ['Wood_ring width', 'Wood_d18O', 'Coral_d18O']:\n        tt, dd = uta.smooth(np.ma.mean(data[key], axis=0), time[key], 11)\n        label=key+' 11yr-mean'\n    else:\n        tt, dd = time[key], np.ma.mean(data[key], axis=0)\n        label=key\n    plt.plot(tt, dd, label=label, lw=2, color=key_colour[key])\nplt.ylabel('paleoData_zscores')\nplt.xlabel('year CE')    \nplt.legend(ncol=3, bbox_to_anchor=(0,-0.2), loc='upper left', framealpha=0)\nplt.show()\n</pre> # as a comparison plot the mean over all records SMOOTHED fig = plt.figure(figsize=(9, 4), dpi=300) for key in keys:     if key in ['Wood_ring width', 'Wood_d18O', 'Coral_d18O']:         tt, dd = uta.smooth(np.ma.mean(data[key], axis=0), time[key], 11)         label=key+' 11yr-mean'     else:         tt, dd = time[key], np.ma.mean(data[key], axis=0)         label=key     plt.plot(tt, dd, label=label, lw=2, color=key_colour[key]) plt.ylabel('paleoData_zscores') plt.xlabel('year CE')     plt.legend(ncol=3, bbox_to_anchor=(0,-0.2), loc='upper left', framealpha=0) plt.show() <pre>/home/jupyter-mnevans/.conda/envs/cfr-env/lib/python3.11/site-packages/numpy/core/shape_base.py:65: UserWarning: Warning: converting a masked element to nan.\n  ary = asanyarray(ary)\n</pre> In\u00a0[73]: Copied! <pre>fig = plt.figure(figsize=(7,3), dpi=300)\nax  = plt.gca()\nax2 = ax.twinx()\nfor key in keys:\n    print(key)\n    ax.plot(range(1,len(foev[key][:10])+1), foev[key][:10], label=key, color=key_colour[key])\n    ax2.plot(range(1,len(foev[key][:10])+1), np.cumsum(foev[key][:10]), label=key, ls=':', color=key_colour[key])\nax.set_ylabel('Fraction of explained variance')\n# ax2.set_ylabel('Cumulative fraction of explained variance')\nax.set_xlabel('PC')    \nax.legend(ncol=3, bbox_to_anchor=(0,-0.15), loc='upper left', framealpha=0)\n</pre> fig = plt.figure(figsize=(7,3), dpi=300) ax  = plt.gca() ax2 = ax.twinx() for key in keys:     print(key)     ax.plot(range(1,len(foev[key][:10])+1), foev[key][:10], label=key, color=key_colour[key])     ax2.plot(range(1,len(foev[key][:10])+1), np.cumsum(foev[key][:10]), label=key, ls=':', color=key_colour[key]) ax.set_ylabel('Fraction of explained variance') # ax2.set_ylabel('Cumulative fraction of explained variance') ax.set_xlabel('PC')     ax.legend(ncol=3, bbox_to_anchor=(0,-0.15), loc='upper left', framealpha=0) <pre>Wood_ring width\nWood_d18O\nCoral_d18O\nSpeleothem_d18O\nLakeSediment_d18O+dD\n</pre> Out[73]: <pre>&lt;matplotlib.legend.Legend at 0x7f315af3c150&gt;</pre> In\u00a0[74]: Copied! <pre># plot PCs of different proxy types on the same axis. \n# Note that these are plotted as calculated in the respective analysis- NOT standardised differently!\n\nfig = plt.figure(figsize=(8,5), dpi=150)\n\ngrid = GS(2,1)\nfor ii in range(2):\n    ax = plt.subplot(grid[ii,:])\n    for key in keys:\n        a=1 if key in ['Wood_ring width', 'Speleothem_d18O', 'LakeSediment_d18O+dD'] else -1\n        label = key+ '\\n ($\\\\ast(-1)$)' if key in ['Wood_ring width', 'Speleothem_d18O', 'LakeSediment_d18O+dD'] else key\n        plt.plot(time[key], a*PCs[key][ii], label=label, color=key_colour[key])\n    if ii==0: \n        ax.axes.xaxis.set_ticklabels([])\n    plt.ylabel('PC %d'%(ii+1)) \nplt.xlabel('year CE')\n\nplt.legend(ncol=3, bbox_to_anchor=(0,-0.2), \n           loc='upper left', framealpha=0)\ngrid.tight_layout(fig)\n</pre> # plot PCs of different proxy types on the same axis.  # Note that these are plotted as calculated in the respective analysis- NOT standardised differently!  fig = plt.figure(figsize=(8,5), dpi=150)  grid = GS(2,1) for ii in range(2):     ax = plt.subplot(grid[ii,:])     for key in keys:         a=1 if key in ['Wood_ring width', 'Speleothem_d18O', 'LakeSediment_d18O+dD'] else -1         label = key+ '\\n ($\\\\ast(-1)$)' if key in ['Wood_ring width', 'Speleothem_d18O', 'LakeSediment_d18O+dD'] else key         plt.plot(time[key], a*PCs[key][ii], label=label, color=key_colour[key])     if ii==0:          ax.axes.xaxis.set_ticklabels([])     plt.ylabel('PC %d'%(ii+1))  plt.xlabel('year CE')  plt.legend(ncol=3, bbox_to_anchor=(0,-0.2),             loc='upper left', framealpha=0) grid.tight_layout(fig)   <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[74], line 6\n      1 # plot PCs of different proxy types on the same axis. \n      2 # Note that these are plotted as calculated in the respective analysis- NOT standardised differently!\n      4 fig = plt.figure(figsize=(8,5), dpi=150)\n----&gt; 6 grid = GS(2,1)\n      7 for ii in range(2):\n      8     ax = plt.subplot(grid[ii,:])\n\nNameError: name 'GS' is not defined</pre> <pre>&lt;Figure size 1200x750 with 0 Axes&gt;</pre> In\u00a0[\u00a0]: Copied! <pre># plot PCs of different proxy types on the same axis. \n# SMOOTHED VIA 11YEAR RUNNING MEAN\nfig = plt.figure(figsize=(8,5), dpi=150)\n\ngrid = GS(2,1)\nfor ii in range(2):\n    ax = plt.subplot(grid[ii,:])\n    for key in keys:\n        label = key\n        if ((key in ['Wood_d18O', 'Coral_d18O'])):\n            a = -1 \n            label+= '\\n ($\\\\ast(-1)$)'\n        else:\n            a = 1\n        resolution = np.unique(np.diff(time[key]))[0]\n        if key in ['Wood_ring width', 'Wood_d18O', 'Coral_d18O']:\n            smooth = 11\n            tt, dd = uta.smooth(PCs[key][ii], time[key], int(smooth/resolution))\n            # label+=' %d-yr mean'%smooth\n        else:\n            tt, dd = time[key], PCs[key][ii]\n        plt.plot(tt, a*np.array(dd), label=label, color=key_colour[key])\n        \n    plt.ylabel('PC %d \\n (%d yr running mean)'%(ii+1, smooth))\n    plt.xlim(1750,2000)\n    if ii==0: \n        ax.axes.xaxis.set_ticklabels([])\n        hh, ll = ax.get_legend_handles_labels()\nplt.xlabel('year CE')\n\ngrid.tight_layout(fig)\n\nplt.legend(hh, ll, ncol=3, bbox_to_anchor=(0,-0.2), \n           loc='upper left', framealpha=0)\nplt.show()\n</pre> # plot PCs of different proxy types on the same axis.  # SMOOTHED VIA 11YEAR RUNNING MEAN fig = plt.figure(figsize=(8,5), dpi=150)  grid = GS(2,1) for ii in range(2):     ax = plt.subplot(grid[ii,:])     for key in keys:         label = key         if ((key in ['Wood_d18O', 'Coral_d18O'])):             a = -1              label+= '\\n ($\\\\ast(-1)$)'         else:             a = 1         resolution = np.unique(np.diff(time[key]))[0]         if key in ['Wood_ring width', 'Wood_d18O', 'Coral_d18O']:             smooth = 11             tt, dd = uta.smooth(PCs[key][ii], time[key], int(smooth/resolution))             # label+=' %d-yr mean'%smooth         else:             tt, dd = time[key], PCs[key][ii]         plt.plot(tt, a*np.array(dd), label=label, color=key_colour[key])              plt.ylabel('PC %d \\n (%d yr running mean)'%(ii+1, smooth))     plt.xlim(1750,2000)     if ii==0:          ax.axes.xaxis.set_ticklabels([])         hh, ll = ax.get_legend_handles_labels() plt.xlabel('year CE')  grid.tight_layout(fig)  plt.legend(hh, ll, ncol=3, bbox_to_anchor=(0,-0.2),             loc='upper left', framealpha=0) plt.show()  In\u00a0[\u00a0]: Copied! <pre>fig = plt.figure(figsize=(8,5), dpi=150)\n\ngrid = GS(2,1)\nfor key in keys:\n    n_recs = data[key].shape[0]\n    for ii in range(2):\n        ax = plt.subplot(grid[ii,:])\n        a = -1 if ((key in ['Wood_d18O', 'Coral_d18O'])) else 1\n        label = key+'\\n ($\\\\ast(-1)$)' if ((key in ['Wood_d18O', 'Coral_d18O'])) else key\n        plt.plot(range(n_recs), a*EOFs[key][ii], label=label, color=key_colour[key])\n        if ii==1: plt.xlabel('record')\n        plt.ylabel('EOF %d load'%(ii+1))\n        plt.axhline(0, color='k', alpha=0.5, lw=0.5)\n        if ii==0: \n            ax.axes.xaxis.set_ticklabels([])\n        if ii==0: plt.legend(bbox_to_anchor=(0,1.1), loc='lower left', ncol=3, framealpha=0)\ngrid.tight_layout(fig)\n</pre>  fig = plt.figure(figsize=(8,5), dpi=150)  grid = GS(2,1) for key in keys:     n_recs = data[key].shape[0]     for ii in range(2):         ax = plt.subplot(grid[ii,:])         a = -1 if ((key in ['Wood_d18O', 'Coral_d18O'])) else 1         label = key+'\\n ($\\\\ast(-1)$)' if ((key in ['Wood_d18O', 'Coral_d18O'])) else key         plt.plot(range(n_recs), a*EOFs[key][ii], label=label, color=key_colour[key])         if ii==1: plt.xlabel('record')         plt.ylabel('EOF %d load'%(ii+1))         plt.axhline(0, color='k', alpha=0.5, lw=0.5)         if ii==0:              ax.axes.xaxis.set_ticklabels([])         if ii==0: plt.legend(bbox_to_anchor=(0,1.1), loc='lower left', ncol=3, framealpha=0) grid.tight_layout(fig) In\u00a0[\u00a0]: Copied! <pre>fig = uplt.geo_EOF_plot(df, pca_rec, EOFs, keys, barlabel='EOF 1 load', which_EOF=0)\n#print(keys)\nutf.save_fig(fig, 'MT_spatial_EOF1', dir=df.name)\n</pre> fig = uplt.geo_EOF_plot(df, pca_rec, EOFs, keys, barlabel='EOF 1 load', which_EOF=0) #print(keys) utf.save_fig(fig, 'MT_spatial_EOF1', dir=df.name)  In\u00a0[75]: Copied! <pre>fig = uplt.geo_EOF_plot(df, pca_rec, EOFs, keys, barlabel='EOF 2 load', which_EOF=1)\n#print(keys)\nutf.save_fig(fig, 'MT_spatial_EOF2', dir=df.name)\n</pre> fig = uplt.geo_EOF_plot(df, pca_rec, EOFs, keys, barlabel='EOF 2 load', which_EOF=1) #print(keys) utf.save_fig(fig, 'MT_spatial_EOF2', dir=df.name)  <pre>{'Wood_ring width': 1, 'Wood_d18O': 1, 'Coral_d18O': 1, 'Speleothem_d18O': 1, 'LakeSediment_d18O+dD': 1}\n-0.6\n0.6\nsaved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0_filtered_M/MT_spatial_EOF2.pdf\n</pre> In\u00a0[76]: Copied! <pre>fig = uplt.geo_plot(df, fs=(13,8))\n\nutf.save_fig(fig, 'MT_spatial', dir=df.name)\n</pre> fig = uplt.geo_plot(df, fs=(13,8))  utf.save_fig(fig, 'MT_spatial', dir=df.name) <pre>0 Wood 3650\n1 Speleothem 567\n2 Coral 312\n3 GlacierIce 162\n4 MarineSediment 126\n5 LakeSediment 111\n6 Documents 13\n7 Sclerosponge 6\n8 GroundIce 4\n9 Borehole 3\n10 Other 2\n11 MolluskShell 1\nsaved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0_filtered_M/MT_spatial.pdf\n</pre> In\u00a0[77]: Copied! <pre># as a comparison plot the mean over all records SMOOTHED\nfig = plt.figure(figsize=(9, 5), dpi=300)\n\ngrid = GS(2,3)\n\nax=plt.subplot(grid[:,0])\nax  = plt.gca()\n# ax2 = ax.twinx()\nfor key in keys:\n    # ax.plot(range(1,9), foev[key][:8], label=key, color=key_colour[key])\n    ax.plot(range(1,len(foev[key][:10])+1), np.cumsum(foev[key][:10]), label=key, color=key_colour[key], lw=2.5)\nax.set_ylabel('Cumulative fraction of explained variance')\nax.set_xlabel('PC')    \nax.set_xlim(.9,10.1)\nplt.title('A', x=0.1, y=0.91)\n\ngrid.tight_layout(fig)\ngrid.update(hspace=0.1,wspace=0.3)\nplt.legend(ncol=1, bbox_to_anchor=(0.1,0.3), \n           loc='upper left', framealpha=0, fontsize=8, facecolor='white')\n# plt.grid()\n\nsmooth = 11\n            \nfor key in keys:\n    label = key\n    resolution = np.unique(np.diff(time[key]))[0]\n    #print(resolution, smooth, int(smooth/resolution))\n\n    if resolution&lt;smooth:\n        tt, dd = uta.smooth(np.ma.mean(data[key], axis=0), time[key], smooth)\n    else:\n        tt, dd = time[key], np.ma.mean(data[key], axis=0)\n    \n    for ii in range(2):\n        ax = plt.subplot(grid[ii,1:4])\n        # warmer and wetter is more negative in coral d18O; \n        # warmer is more positive and wetter is more negative (maybe?) in tree d18O\n        if ((key in ['Wood_d18O', 'Coral_d18O'])):\n            a = -1 # multiply PC sign by -1\n            label+= ' ($\\\\ast(-1)$)' # and label it as such\n        else:\n            a = 1\n        if resolution&lt;smooth:\n            tt, dd = uta.smooth(PCs[key][ii], time[key], int(smooth/resolution))\n        else:\n            tt, dd = time[key], PCs[key][ii]\n        plt.plot(tt, a*np.array(dd), label=label, color=key_colour[key], lw=1.5)\n        plt.ylabel('PC %d \\n (%d yr MA)'%(ii+1, smooth))\n        plt.grid()\n        if ii==0: \n            ax.axes.xaxis.set_ticklabels([])\n        plt.title('BC'[ii], x=0.05, y=0.8)\n        # plt.xlim(1750, 2010)\n        \nplt.xlabel('year CE')\n\n\nutf.save_fig(fig, 'MT_PCA', dir=df.name)\n</pre> # as a comparison plot the mean over all records SMOOTHED fig = plt.figure(figsize=(9, 5), dpi=300)  grid = GS(2,3)  ax=plt.subplot(grid[:,0]) ax  = plt.gca() # ax2 = ax.twinx() for key in keys:     # ax.plot(range(1,9), foev[key][:8], label=key, color=key_colour[key])     ax.plot(range(1,len(foev[key][:10])+1), np.cumsum(foev[key][:10]), label=key, color=key_colour[key], lw=2.5) ax.set_ylabel('Cumulative fraction of explained variance') ax.set_xlabel('PC')     ax.set_xlim(.9,10.1) plt.title('A', x=0.1, y=0.91)  grid.tight_layout(fig) grid.update(hspace=0.1,wspace=0.3) plt.legend(ncol=1, bbox_to_anchor=(0.1,0.3),             loc='upper left', framealpha=0, fontsize=8, facecolor='white') # plt.grid()  smooth = 11              for key in keys:     label = key     resolution = np.unique(np.diff(time[key]))[0]     #print(resolution, smooth, int(smooth/resolution))      if resolution <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[77], line 4\n      1 # as a comparison plot the mean over all records SMOOTHED\n      2 fig = plt.figure(figsize=(9, 5), dpi=300)\n----&gt; 4 grid = GS(2,3)\n      6 ax=plt.subplot(grid[:,0])\n      7 ax  = plt.gca()\n\nNameError: name 'GS' is not defined</pre> <pre>&lt;Figure size 2700x1500 with 0 Axes&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/analysis_moisture/#analyse-moisture-records","title":"Analyse moisture records\u00b6","text":""},{"location":"notebooks/analysis_moisture/#set-up-working-environment","title":"Set up working environment\u00b6","text":""},{"location":"notebooks/analysis_moisture/#read-already-filtered-dataframe","title":"Read already filtered dataframe\u00b6","text":""},{"location":"notebooks/analysis_moisture/#data-analysis","title":"data analysis\u00b6","text":""},{"location":"notebooks/analysis_moisture/#tree-trw","title":"tree TRW\u00b6","text":""},{"location":"notebooks/analysis_moisture/#filter-archive_type-and-paleodata_proxy","title":"filter archive_type and paleoData_proxy\u00b6","text":""},{"location":"notebooks/analysis_moisture/#filter-for-target-period-resolution-and-minimum-record-length","title":"filter for target period, resolution and minimum record length\u00b6","text":""},{"location":"notebooks/analysis_moisture/#homogenise-data-dimensions-across-the-records","title":"homogenise data dimensions across the records\u00b6","text":""},{"location":"notebooks/analysis_moisture/#pca","title":"PCA\u00b6","text":""},{"location":"notebooks/analysis_moisture/#tree-d18o","title":"tree d18O\u00b6","text":""},{"location":"notebooks/analysis_moisture/#filter-archive_type-and-paleodata_proxy","title":"filter archive_type and paleoData_proxy\u00b6","text":""},{"location":"notebooks/analysis_moisture/#filter-for-target-period-resolution-and-minimum-record-length","title":"filter for target period, resolution and minimum record length\u00b6","text":""},{"location":"notebooks/analysis_moisture/#homogenise-data-dimensions-across-the-records","title":"homogenise data dimensions across the records\u00b6","text":""},{"location":"notebooks/analysis_moisture/#pca","title":"PCA\u00b6","text":""},{"location":"notebooks/analysis_moisture/#coral-d18o","title":"coral d18O\u00b6","text":""},{"location":"notebooks/analysis_moisture/#filter-archive_type-and-paleodata_proxy","title":"filter archive_type and paleoData_proxy\u00b6","text":""},{"location":"notebooks/analysis_moisture/#filter-for-target-period-resolution-and-minimum-record-length","title":"filter for target period, resolution and minimum record length\u00b6","text":""},{"location":"notebooks/analysis_moisture/#homogenise-data-dimensions-across-the-records","title":"homogenise data dimensions across the records\u00b6","text":""},{"location":"notebooks/analysis_moisture/#pca","title":"PCA\u00b6","text":""},{"location":"notebooks/analysis_moisture/#speleothem-d18o","title":"speleothem d18O\u00b6","text":""},{"location":"notebooks/analysis_moisture/#filter-archive_type-and-paleodata_proxy","title":"filter archive_type and paleoData_proxy\u00b6","text":""},{"location":"notebooks/analysis_moisture/#filter-for-target-period-resolution-and-minimum-record-length","title":"filter for target period, resolution and minimum record length\u00b6","text":""},{"location":"notebooks/analysis_moisture/#homogenise-data-dimensions-across-the-records","title":"homogenise data dimensions across the records\u00b6","text":""},{"location":"notebooks/analysis_moisture/#pca","title":"PCA\u00b6","text":""},{"location":"notebooks/analysis_moisture/#lake-sediment-d18o-dd","title":"lake sediment d18O + dD\u00b6","text":""},{"location":"notebooks/analysis_moisture/#filter-archive_type-and-paleodata_proxy","title":"filter archive_type and paleoData_proxy\u00b6","text":""},{"location":"notebooks/analysis_moisture/#filter-for-target-period-resolution-and-minimum-record-length","title":"filter for target period, resolution and minimum record length\u00b6","text":""},{"location":"notebooks/analysis_moisture/#homogenise-data-dimensions-across-the-records","title":"homogenise data dimensions across the records\u00b6","text":""},{"location":"notebooks/analysis_moisture/#pca","title":"PCA\u00b6","text":""},{"location":"notebooks/analysis_moisture/#marine-sediment-d18o","title":"marine sediment d18O\u00b6","text":""},{"location":"notebooks/analysis_moisture/#filter-archive_type-and-paleodata_proxy","title":"filter archive_type and paleoData_proxy\u00b6","text":""},{"location":"notebooks/analysis_moisture/#filter-for-target-period-resolution-and-minimum-record-length","title":"filter for target period, resolution and minimum record length\u00b6","text":""},{"location":"notebooks/analysis_moisture/#homogenise-data-dimensions-across-the-records","title":"homogenise data dimensions across the records\u00b6","text":""},{"location":"notebooks/analysis_moisture/#pca","title":"PCA\u00b6","text":""},{"location":"notebooks/analysis_moisture/#summary-figures","title":"summary figures\u00b6","text":""},{"location":"notebooks/analysis_moisture/#mean-z-score","title":"mean z-score\u00b6","text":""},{"location":"notebooks/analysis_moisture/#fraction-of-explained-variance","title":"Fraction of Explained Variance\u00b6","text":""},{"location":"notebooks/analysis_moisture/#1st-and-2nd-pc","title":"1st and 2nd PC\u00b6","text":""},{"location":"notebooks/analysis_moisture/#1st-and-2nd-eof","title":"1st and 2nd EOF\u00b6","text":""},{"location":"notebooks/analysis_moisture/#spatial-eof-load","title":"spatial EOF load\u00b6","text":""},{"location":"notebooks/analysis_moisture/#spatial-distribution","title":"spatial distribution\u00b6","text":""},{"location":"notebooks/analysis_speleothem/","title":"Analyse speleothem records","text":"<p>Author: Kevin Fan, revised: MN Evans, Feng Zhu, Lucie Luecke Last edited: 2025/12/17 by LL Modified from code by: Lucie Luecke</p> <p>This notebook reads the DoD2k, filters for late 20th century speleothem $\\delta^{18}O$ records, and uses CRUTS4.07 gridded temperature and precipitated amount-weighted mean annual $\\delta^{18}O$ of precipitation estimated by Bowen and Ravenaugh (2003, updated) to simulate o18 of speleothem calcite and compare results across a spatial gradient, to observations.</p> <p>Note that to execute this notebook, you will need to first download and unzip source input data from Harris et al (2020) and Bowen and Ravenough (2003, updated) in the directory speleothem_modeling_inputs.  See the Bibliography cell at bottom of this notebook and the file Quickstart.md for URLs and more information.</p> <ul> <li>v1.3: cleaned up (MNE)</li> <li>v1.4: added nearest non-NaN search for CRUTS and d18Op data (FZ)</li> <li>v1.4d: General cleaning of code and comments up to cell 21 (LL)</li> <li>v1.5: move to lluecke for github push, clean up, move gcd and nearest2d functions to functions.py, revise for key elements and debug</li> <li>v1.6: brief cleanup, updated plots (LL, MNE, KF); tested with simpler dod2k-env and relative paths</li> <li>v1.7: implementing review suggestions (KF)</li> <li>v1.8: committed to statsmodels regression (KF)</li> <li>v2.0: tidied up and updated for dod2kv2.0 compatibility (LL)</li> </ul> <p>Make sure the repo_root is added correctly, it should be: <code>your_root_dir/dod2k</code> This should be the working directory throughout this notebook (and all other notebooks).</p> In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n\nimport sys\nimport os\nfrom pathlib import Path\n\n# Add parent directory to path (works from any notebook in notebooks/)\n# the repo_root should be the parent directory of the notebooks folder\ninit_dir = Path().resolve()\n# Determine repo root\nif init_dir.name == 'dod2k': repo_root = init_dir\nelif init_dir.parent.name == 'dod2k': repo_root = init_dir.parent\nelse: raise Exception('Please review the repo root structure (see first cell).')\n\n# Update cwd and path only if needed\nif os.getcwd() != str(repo_root):\n    os.chdir(repo_root)\nif str(repo_root) not in sys.path:\n    sys.path.insert(0, str(repo_root))\n\nprint(f\"Repo root: {repo_root}\")\nif str(os.getcwd())==str(repo_root):\n    print(f\"Working directory matches repo root. \")\n</pre> %load_ext autoreload %autoreload 2  import sys import os from pathlib import Path  # Add parent directory to path (works from any notebook in notebooks/) # the repo_root should be the parent directory of the notebooks folder init_dir = Path().resolve() # Determine repo root if init_dir.name == 'dod2k': repo_root = init_dir elif init_dir.parent.name == 'dod2k': repo_root = init_dir.parent else: raise Exception('Please review the repo root structure (see first cell).')  # Update cwd and path only if needed if os.getcwd() != str(repo_root):     os.chdir(repo_root) if str(repo_root) not in sys.path:     sys.path.insert(0, str(repo_root))  print(f\"Repo root: {repo_root}\") if str(os.getcwd())==str(repo_root):     print(f\"Working directory matches repo root. \") <pre>Repo root: /home/jupyter-lluecke/dod2k\nWorking directory matches repo root. \n</pre> In\u00a0[2]: Copied! <pre># Import packages\nimport numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport xarray as xr\nimport rioxarray\nfrom tqdm import tqdm\nimport psm\nimport statsmodels.api as sm\nimport scipy\n\nfrom dod2k_utilities import ut_functions as utf # contains utility functions\nfrom dod2k_utilities import ut_plot as uplt # contains plotting functions\nfrom dod2k_utilities import ut_analysis as uta # contains plotting functions\n</pre> # Import packages import numpy as np import pandas as pd import os import matplotlib.pyplot as plt import xarray as xr import rioxarray from tqdm import tqdm import psm import statsmodels.api as sm import scipy  from dod2k_utilities import ut_functions as utf # contains utility functions from dod2k_utilities import ut_plot as uplt # contains plotting functions from dod2k_utilities import ut_analysis as uta # contains plotting functions <p>Read compact dataframe.</p> <p>{db_name} refers to the database, including</p> <pre><code>- dod2k_v2.0</code></pre> <p>All compact dataframes are saved in {repo_root}/data/{db_name} as {db_name}_compact.csv.</p> In\u00a0[3]: Copied! <pre># read dataframe, choose from the list below, or specify your own\n\ndb_name = 'dod2k_v2.0'\n\n# load dataframe\norig_df = utf.load_compact_dataframe_from_csv(db_name)\nprint(orig_df.info())\norig_df.name = db_name\n</pre> # read dataframe, choose from the list below, or specify your own  db_name = 'dod2k_v2.0'  # load dataframe orig_df = utf.load_compact_dataframe_from_csv(db_name) print(orig_df.info()) orig_df.name = db_name  <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4957 entries, 0 to 4956\nData columns (total 22 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   archiveType                    4957 non-null   object \n 1   dataSetName                    4957 non-null   object \n 2   datasetId                      4957 non-null   object \n 3   duplicateDetails               4957 non-null   object \n 4   geo_meanElev                   4875 non-null   float32\n 5   geo_meanLat                    4957 non-null   float32\n 6   geo_meanLon                    4957 non-null   float32\n 7   geo_siteName                   4957 non-null   object \n 8   interpretation_direction       4957 non-null   object \n 9   interpretation_seasonality     4957 non-null   object \n 10  interpretation_variable        4957 non-null   object \n 11  interpretation_variableDetail  4957 non-null   object \n 12  originalDataURL                4957 non-null   object \n 13  originalDatabase               4957 non-null   object \n 14  paleoData_notes                4957 non-null   object \n 15  paleoData_proxy                4957 non-null   object \n 16  paleoData_sensorSpecies        4957 non-null   object \n 17  paleoData_units                4957 non-null   object \n 18  paleoData_values               4957 non-null   object \n 19  paleoData_variableName         4957 non-null   object \n 20  year                           4957 non-null   object \n 21  yearUnits                      4957 non-null   object \ndtypes: float32(3), object(19)\nmemory usage: 794.0+ KB\nNone\n</pre> In\u00a0[4]: Copied! <pre># Filter for desired data, in this case, speleothem calcite oxygen isotope measurements from the years min_year to max_year\nfiltered_df = orig_df.loc[(orig_df[\"paleoData_proxy\"] ==\"d18O\") &amp; (orig_df[\"archiveType\"] == \"Speleothem\") &amp; (orig_df[\"paleoData_notes\"] == \"calcite\")]\n\n# Filter data for time series that include records from or past the year 1960 AD\nmin_year = 1960 # Inclusive\nmax_year = 2006 # Exclusive\n\n# d18O data ranges from 1960 to 2005- pick data to match the same timeframe\nfiltered_df = filtered_df.loc[orig_df[\"year\"].apply(lambda x: np.max(x) &gt;= min_year and np.min(x) &lt; max_year)] #exclude post 2005 data\nfiltered_df.reset_index(inplace = True)\n\n# Filter the filtered records for data in the correct timeframe\nnew_year = []\nnew_values = []\n# For every row, apply above time conditions\nfor i in range (0, len(filtered_df)):\n    index = np.where(filtered_df.iloc[i][\"year\"] &gt;= min_year)\n    new_year.append(filtered_df.iloc[i][\"year\"][index]) # Adds values to lists\n    new_values.append(filtered_df.iloc[i][\"paleoData_values\"][index])\nfiltered_df[\"year\"] = new_year # Create a dataframe with the filtered data lists populating it\nfiltered_df[\"paleoData_values\"] = new_values\nfiltered_df.info()\n\nfiltered_df.name = 'dod2k_v2.0_speleothemcalciteoxygen'\n\nprint(filtered_df.name)\n</pre> # Filter for desired data, in this case, speleothem calcite oxygen isotope measurements from the years min_year to max_year filtered_df = orig_df.loc[(orig_df[\"paleoData_proxy\"] ==\"d18O\") &amp; (orig_df[\"archiveType\"] == \"Speleothem\") &amp; (orig_df[\"paleoData_notes\"] == \"calcite\")]  # Filter data for time series that include records from or past the year 1960 AD min_year = 1960 # Inclusive max_year = 2006 # Exclusive  # d18O data ranges from 1960 to 2005- pick data to match the same timeframe filtered_df = filtered_df.loc[orig_df[\"year\"].apply(lambda x: np.max(x) &gt;= min_year and np.min(x) &lt; max_year)] #exclude post 2005 data filtered_df.reset_index(inplace = True)  # Filter the filtered records for data in the correct timeframe new_year = [] new_values = [] # For every row, apply above time conditions for i in range (0, len(filtered_df)):     index = np.where(filtered_df.iloc[i][\"year\"] &gt;= min_year)     new_year.append(filtered_df.iloc[i][\"year\"][index]) # Adds values to lists     new_values.append(filtered_df.iloc[i][\"paleoData_values\"][index]) filtered_df[\"year\"] = new_year # Create a dataframe with the filtered data lists populating it filtered_df[\"paleoData_values\"] = new_values filtered_df.info()  filtered_df.name = 'dod2k_v2.0_speleothemcalciteoxygen'  print(filtered_df.name) <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 61 entries, 0 to 60\nData columns (total 23 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   index                          61 non-null     int64  \n 1   archiveType                    61 non-null     object \n 2   dataSetName                    61 non-null     object \n 3   datasetId                      61 non-null     object \n 4   duplicateDetails               61 non-null     object \n 5   geo_meanElev                   61 non-null     float32\n 6   geo_meanLat                    61 non-null     float32\n 7   geo_meanLon                    61 non-null     float32\n 8   geo_siteName                   61 non-null     object \n 9   interpretation_direction       61 non-null     object \n 10  interpretation_seasonality     61 non-null     object \n 11  interpretation_variable        61 non-null     object \n 12  interpretation_variableDetail  61 non-null     object \n 13  originalDataURL                61 non-null     object \n 14  originalDatabase               61 non-null     object \n 15  paleoData_notes                61 non-null     object \n 16  paleoData_proxy                61 non-null     object \n 17  paleoData_sensorSpecies        61 non-null     object \n 18  paleoData_units                61 non-null     object \n 19  paleoData_values               61 non-null     object \n 20  paleoData_variableName         61 non-null     object \n 21  year                           61 non-null     object \n 22  yearUnits                      61 non-null     object \ndtypes: float32(3), int64(1), object(19)\nmemory usage: 10.4+ KB\ndod2k_v2.0_speleothemcalciteoxygen\n</pre> In\u00a0[5]: Copied! <pre>#%% check data extraction for proxy_type and archive_type\nproxy_types   = filtered_df['paleoData_proxy'].unique()\narchive_types = filtered_df['archiveType'].unique()\ncalcite_types = filtered_df['paleoData_notes'].unique()\n\nprint('Proxy type: ', proxy_types)\nprint('Archive type: ', archive_types)\nprint('Carbonate type: ', calcite_types)\n# should give only ['d18O'], ['speleothem'], ['calcite']\n</pre> #%% check data extraction for proxy_type and archive_type proxy_types   = filtered_df['paleoData_proxy'].unique() archive_types = filtered_df['archiveType'].unique() calcite_types = filtered_df['paleoData_notes'].unique()  print('Proxy type: ', proxy_types) print('Archive type: ', archive_types) print('Carbonate type: ', calcite_types) # should give only ['d18O'], ['speleothem'], ['calcite'] <pre>Proxy type:  ['d18O']\nArchive type:  ['Speleothem']\nCarbonate type:  ['calcite']\n</pre> In\u00a0[6]: Copied! <pre># count archive types\narchive_count = {}\nfor ii, at in enumerate(set(filtered_df['archiveType'])):\n    archive_count[at] = filtered_df.loc[filtered_df['archiveType']==at, 'archiveType'].count()\n\nsort = np.argsort([cc for cc in archive_count.values()])\narchives_sorted = np.array([cc for cc in archive_count.keys()])[sort][::-1]\n\nfig = uplt.plot_geo_archive_proxy(filtered_df, {'Speleothem':'#EE6677'}, marker='^', figsize=(13/1.3, 8/1.3))\nutf.save_fig(fig, f'geo_{filtered_df.name}', dir=filtered_df.name)\n</pre> # count archive types archive_count = {} for ii, at in enumerate(set(filtered_df['archiveType'])):     archive_count[at] = filtered_df.loc[filtered_df['archiveType']==at, 'archiveType'].count()  sort = np.argsort([cc for cc in archive_count.values()]) archives_sorted = np.array([cc for cc in archive_count.keys()])[sort][::-1]  fig = uplt.plot_geo_archive_proxy(filtered_df, {'Speleothem':'#EE6677'}, marker='^', figsize=(13/1.3, 8/1.3)) utf.save_fig(fig, f'geo_{filtered_df.name}', dir=filtered_df.name) <pre>saved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0_speleothemcalciteoxygen/geo_dod2k_v2.0_speleothemcalciteoxygen.pdf\n</pre> In\u00a0[7]: Copied! <pre>fig = uplt.plot_coverage(filtered_df, archives_sorted, ['Speleothem'], [], {'Speleothem':'#EE6677'})\nplt.xlim(min_year-10, max_year+10)\nutf.save_fig(fig, f'time_{filtered_df.name}', dir=filtered_df.name)\n</pre> fig = uplt.plot_coverage(filtered_df, archives_sorted, ['Speleothem'], [], {'Speleothem':'#EE6677'}) plt.xlim(min_year-10, max_year+10) utf.save_fig(fig, f'time_{filtered_df.name}', dir=filtered_df.name) <pre>saved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0_speleothemcalciteoxygen/time_dod2k_v2.0_speleothemcalciteoxygen.pdf\n</pre> In\u00a0[8]: Copied! <pre># Choose relevant data for simulation comparison\ndf = filtered_df[['geo_meanLat', 'geo_meanLon', 'paleoData_values']]\n\n# extract average of paleoData_values\ndf.loc[:,'paleoData_values'] = df.loc[:,'paleoData_values'].apply(np.average)\n\n# create directory for speleo modeling inputs\nos.makedirs('data/speleo_modeling', exist_ok=True)\ndf.to_csv('data/speleo_modeling/df_dod2k_speleo_analysis.csv')\ndf.info()\n</pre> # Choose relevant data for simulation comparison df = filtered_df[['geo_meanLat', 'geo_meanLon', 'paleoData_values']]  # extract average of paleoData_values df.loc[:,'paleoData_values'] = df.loc[:,'paleoData_values'].apply(np.average)  # create directory for speleo modeling inputs os.makedirs('data/speleo_modeling', exist_ok=True) df.to_csv('data/speleo_modeling/df_dod2k_speleo_analysis.csv') df.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 61 entries, 0 to 60\nData columns (total 3 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   geo_meanLat       61 non-null     float32\n 1   geo_meanLon       61 non-null     float32\n 2   paleoData_values  61 non-null     object \ndtypes: float32(2), object(1)\nmemory usage: 1.1+ KB\n</pre> In\u00a0[9]: Copied! <pre># Load temperature data, source: CRU TS 4.07 tmp\nds = xr.open_dataset('data/speleo_modeling/cru_ts4.07.1901.2022.tmp.dat.nc.gz')\n</pre> # Load temperature data, source: CRU TS 4.07 tmp ds = xr.open_dataset('data/speleo_modeling/cru_ts4.07.1901.2022.tmp.dat.nc.gz') In\u00a0[10]: Copied! <pre>cruT_mon = ds['tmp']\n</pre> cruT_mon = ds['tmp'] In\u00a0[11]: Copied! <pre>#Reduce this DataArray\u2019s data by applying mean along some dimension(s).\ncruT=cruT_mon.mean(dim='time')\nprint(cruT)\n</pre> #Reduce this DataArray\u2019s data by applying mean along some dimension(s). cruT=cruT_mon.mean(dim='time') print(cruT) <pre>&lt;xarray.DataArray 'tmp' (lat: 360, lon: 720)&gt; Size: 1MB\narray([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       ...,\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]], dtype=float32)\nCoordinates:\n  * lon      (lon) float32 3kB -179.8 -179.2 -178.8 -178.2 ... 178.8 179.2 179.8\n  * lat      (lat) float32 1kB -89.75 -89.25 -88.75 -88.25 ... 88.75 89.25 89.75\n</pre> In\u00a0[12]: Copied! <pre>path = 'data/speleo_modeling/GlobalPrecip/d18o_MA.tif'\nd18Op_da = rioxarray.open_rasterio(path)\nif d18Op_da.sizes.get(\"band\", 1) == 1:\n    d18Op_da = d18Op_da.squeeze('band', drop=True)\nd18Op_da = d18Op_da.rename({'x': 'lon', 'y': 'lat'})\nd18Op_da = d18Op_da.where(np.abs(d18Op_da)&lt;1e3)  # mask NaN regions\nd18Op_da.attrs['units'] = 'permil'\nd18Op_da.attrs['long_name'] = 'd18Op'\nd18Op_da.name = 'd18Op'\n#d18Op_da\n</pre> path = 'data/speleo_modeling/GlobalPrecip/d18o_MA.tif' d18Op_da = rioxarray.open_rasterio(path) if d18Op_da.sizes.get(\"band\", 1) == 1:     d18Op_da = d18Op_da.squeeze('band', drop=True) d18Op_da = d18Op_da.rename({'x': 'lon', 'y': 'lat'}) d18Op_da = d18Op_da.where(np.abs(d18Op_da)&lt;1e3)  # mask NaN regions d18Op_da.attrs['units'] = 'permil' d18Op_da.attrs['long_name'] = 'd18Op' d18Op_da.name = 'd18Op' #d18Op_da In\u00a0[13]: Copied! <pre># create a dataframe to hold observed o18, T,o18p, pred o18c\npred_frame = pd.DataFrame()\npred_frame['geo_meanLon'] = filtered_df['geo_meanLon']\npred_frame['geo_meanLat'] = filtered_df['geo_meanLat']\npred_frame['d18Oc_vals'] = filtered_df['paleoData_values']\npred_frame['d18Op_vals'] = object\npred_frame['cruT_vals'] = object\npred_frame['d18Ocs_vals']= object #np.zeros(len['d18Op_vals'])#object #np.nan # object # np.nan(len(['d18Op_vals']))\nprint(len(pred_frame['d18Ocs_vals']))\n\n# KF: Finding nearest param data to observed data\nfor idx, row in tqdm(pred_frame.iterrows(), total=len(pred_frame)):\n    lon = row['geo_meanLon']\n    lat = row['geo_meanLat']\n\n    # temp\n    nearest_cruT = uta.find_nearest2d(cruT, lat, lon, lat_name='lat', lon_name='lon', new_dim='sites',r=6)\n    if np.isnan(nearest_cruT).all():\n        pred_frame.at[idx, 'cruT_vals'] = np.nan\n    else:\n        pred_frame.at[idx, 'cruT_vals'] = nearest_cruT.data\n        #pred_frame.at[idx, 'cruT_vals'] = np.repeat(nearest_cruT.data,46)\n\n    # d18Op\n    nearest_o18p = uta.find_nearest2d(d18Op_da, lat, lon, lat_name='lat', lon_name='lon', new_dim='sites', r=6)\n    if np.isnan(nearest_o18p).all():\n        pred_frame.at[idx, 'd18Op_vals'] = np.nan\n    else:\n        pred_frame.at[idx, 'd18Op_vals'] = nearest_o18p.data\n        pred_frame.at[idx, 'd18Op_vals'] = np.repeat(nearest_o18p.data,46)\n        \n#pred_frame\n# KF: Important to note that the simulated records use data collected from sites closest to, but not always at the same location as, the observed records\n# KF: This data collection is done above. Methods for finding the nearest gridpoint to an observation can be found in the functions.py file, function find_nearest2d, written by Feng Zhu\n</pre> # create a dataframe to hold observed o18, T,o18p, pred o18c pred_frame = pd.DataFrame() pred_frame['geo_meanLon'] = filtered_df['geo_meanLon'] pred_frame['geo_meanLat'] = filtered_df['geo_meanLat'] pred_frame['d18Oc_vals'] = filtered_df['paleoData_values'] pred_frame['d18Op_vals'] = object pred_frame['cruT_vals'] = object pred_frame['d18Ocs_vals']= object #np.zeros(len['d18Op_vals'])#object #np.nan # object # np.nan(len(['d18Op_vals'])) print(len(pred_frame['d18Ocs_vals']))  # KF: Finding nearest param data to observed data for idx, row in tqdm(pred_frame.iterrows(), total=len(pred_frame)):     lon = row['geo_meanLon']     lat = row['geo_meanLat']      # temp     nearest_cruT = uta.find_nearest2d(cruT, lat, lon, lat_name='lat', lon_name='lon', new_dim='sites',r=6)     if np.isnan(nearest_cruT).all():         pred_frame.at[idx, 'cruT_vals'] = np.nan     else:         pred_frame.at[idx, 'cruT_vals'] = nearest_cruT.data         #pred_frame.at[idx, 'cruT_vals'] = np.repeat(nearest_cruT.data,46)      # d18Op     nearest_o18p = uta.find_nearest2d(d18Op_da, lat, lon, lat_name='lat', lon_name='lon', new_dim='sites', r=6)     if np.isnan(nearest_o18p).all():         pred_frame.at[idx, 'd18Op_vals'] = np.nan     else:         pred_frame.at[idx, 'd18Op_vals'] = nearest_o18p.data         pred_frame.at[idx, 'd18Op_vals'] = np.repeat(nearest_o18p.data,46)          #pred_frame # KF: Important to note that the simulated records use data collected from sites closest to, but not always at the same location as, the observed records # KF: This data collection is done above. Methods for finding the nearest gridpoint to an observation can be found in the functions.py file, function find_nearest2d, written by Feng Zhu <pre>61\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61/61 [00:00&lt;00:00, 511.99it/s]\n</pre> In\u00a0[14]: Copied! <pre># d18Oc averages \nd18Oc=[] # create an empty list\nfor idx in range(len(pred_frame)):\n    #print(np.array(pred_frame['d18Oc_vals'][idx][0])) # this is what I want in my list.\n    # populate the list\n    d18Oc.append(pred_frame['d18Oc_vals'][idx][0])\n\nd18Oc=pd.DataFrame(d18Oc).values # convert list to df\nd18Oc=d18Oc.reshape(-1,1) # transpose list\n#print(d18Oc)\n# put these into the data frame in place of the raw values\npred_frame['d18Oc_vals'] = d18Oc\n</pre> # d18Oc averages  d18Oc=[] # create an empty list for idx in range(len(pred_frame)):     #print(np.array(pred_frame['d18Oc_vals'][idx][0])) # this is what I want in my list.     # populate the list     d18Oc.append(pred_frame['d18Oc_vals'][idx][0])  d18Oc=pd.DataFrame(d18Oc).values # convert list to df d18Oc=d18Oc.reshape(-1,1) # transpose list #print(d18Oc) # put these into the data frame in place of the raw values pred_frame['d18Oc_vals'] = d18Oc In\u00a0[15]: Copied! <pre># sanity check - nonmissing data found?\npred_frame.isna().sum()\n</pre> # sanity check - nonmissing data found? pred_frame.isna().sum()  Out[15]: <pre>geo_meanLon    0\ngeo_meanLat    0\nd18Oc_vals     0\nd18Op_vals     0\ncruT_vals      0\nd18Ocs_vals    0\ndtype: int64</pre> <p>PRYSM Speleothem Simulation Model Usage</p> <p>Speleothem Calcite [Sensor] Model Converts environmental signals to d18O calcite/dripwater using various models of karst aquifer recharge and calcification.</p> <p>INPUTS: dt        time step [years]                         (int or float, 1=1 year, 1./12. for months. etc.) t        time axis [years]                          (numpy array, length n) T        Average Annual Temperature     [K]         (numpy array, length n) d18O    delta-18-O (precip or soil water) [permil]  (numpy array, length n) NB: make sure these are precipitation weighted</p> <p>MODEL PARAMETERS:</p> <pre><code>  model:  aquifer recharge model. possible values 'Well-Mixed'[1,2] or 'Adv-Disp' [3]\n  tau0: mean transit time, [years]    (default = 0.5)\n  Pe: Peclet number [non-dimensional] (default = 1.0) ('Adv-Disp' only)</code></pre> <p>OUTPUTS: d18Oc  cave dripwater delta-18-O d18OK  delta-18-O of water after passage through karst h      transit time distribution</p> In\u00a0[16]: Copied! <pre># Simulation construction: using a series of 1960-2005 constant mean annual d18Op over time and a mean annual cruT to simulated d18Ocs\n# because we use a constant o18p over time, the results are for equilibrium, well-mixed conditions over the time interval of study, 1960-2005\ndt = 1\nt = np.arange(min_year, max_year)\npred_frame['d18Ocs_vals'] = pred_frame.apply(lambda row: np.mean(psm.speleo.sensor.speleo_sensor(t,np.array(row['d18Op_vals']),np.array(row['cruT_vals']) + 273.15,dt,model='Well-Mixed',tau0=1)[0]),axis=1)\npred_frame\n</pre> # Simulation construction: using a series of 1960-2005 constant mean annual d18Op over time and a mean annual cruT to simulated d18Ocs # because we use a constant o18p over time, the results are for equilibrium, well-mixed conditions over the time interval of study, 1960-2005 dt = 1 t = np.arange(min_year, max_year) pred_frame['d18Ocs_vals'] = pred_frame.apply(lambda row: np.mean(psm.speleo.sensor.speleo_sensor(t,np.array(row['d18Op_vals']),np.array(row['cruT_vals']) + 273.15,dt,model='Well-Mixed',tau0=1)[0]),axis=1) pred_frame Out[16]: geo_meanLon geo_meanLat d18Oc_vals d18Op_vals cruT_vals d18Ocs_vals 0 -55.450001 -4.066700 -5.930000 [-4.90709, -4.90709, -4.90709, -4.90709, -4.90... 26.539503 -7.223432 1 0.780000 45.430000 -4.559933 [-6.67699, -6.67699, -6.67699, -6.67699, -6.67... 11.989003 -5.773501 2 105.116699 33.583302 -7.730000 [-9.10825, -9.10825, -9.10825, -9.10825, -9.10... 10.181901 -7.772914 3 105.116699 33.583302 -8.874000 [-9.10825, -9.10825, -9.10825, -9.10825, -9.10... 10.181901 -7.772914 4 15.000000 67.000000 -7.062788 [-13.5065, -13.5065, -13.5065, -13.5065, -13.5... 1.8243847 -10.063785 ... ... ... ... ... ... ... 56 -9.490000 30.770000 -4.571000 [-5.79046, -5.79046, -5.79046, -5.79046, -5.79... 15.463603 -5.700161 57 115.690002 -31.547001 -3.180740 [-4.11277, -4.11277, -4.11277, -4.11277, -4.11... 18.29258 -4.664288 58 7.664700 51.367500 -5.273960 [-7.7014, -7.7014, -7.7014, -7.7014, -7.7014, ... 8.7618885 -6.016585 59 30.711700 37.232498 -3.770000 [-7.73942, -7.73942, -7.73942, -7.73942, -7.73... 15.118716 -7.570000 60 77.866699 30.600000 -8.510427 [-9.0451, -9.0451, -9.0451, -9.0451, -9.0451, ... 16.272415 -9.139685 <p>61 rows \u00d7 6 columns</p> In\u00a0[17]: Copied! <pre># get column vectors of data for regression estimation\n# 1. d18Ocs\nd18Ocs=pred_frame.iloc[:,5].values\nd18Ocs=d18Ocs.reshape(-1,1)\n#print(d18Ocs)\n</pre> # get column vectors of data for regression estimation # 1. d18Ocs d18Ocs=pred_frame.iloc[:,5].values d18Ocs=d18Ocs.reshape(-1,1) #print(d18Ocs) In\u00a0[18]: Copied! <pre># 2. d18Op \nd18Op=[] # create an empty list\nfor idx in range(len(pred_frame)):\n    #print(np.array(pred_frame['d18Op_vals'][idx][0])) # this is what I want in my list.\n    # populate the list\n    d18Op.append(pred_frame['d18Op_vals'][idx][0])\n\nd18Op=pd.DataFrame(d18Op).values # convert list to df\nd18Op=d18Op.reshape(-1,1) # transpose list\n#print(d18Op)\n</pre> # 2. d18Op  d18Op=[] # create an empty list for idx in range(len(pred_frame)):     #print(np.array(pred_frame['d18Op_vals'][idx][0])) # this is what I want in my list.     # populate the list     d18Op.append(pred_frame['d18Op_vals'][idx][0])  d18Op=pd.DataFrame(d18Op).values # convert list to df d18Op=d18Op.reshape(-1,1) # transpose list #print(d18Op) In\u00a0[19]: Copied! <pre># 3. d18Oc\n# is already created\n#print(d18Oc)\n</pre> # 3. d18Oc # is already created #print(d18Oc) In\u00a0[20]: Copied! <pre># 4. Temp\nTemp=pred_frame.iloc[:,4].values\nTemp=Temp.reshape(-1,1)\n#print(Temp)\n</pre> # 4. Temp Temp=pred_frame.iloc[:,4].values Temp=Temp.reshape(-1,1) #print(Temp) In\u00a0[21]: Copied! <pre># 1. get regression of o18p on o18c\n# Using statsmodels OLS, see: https://www.statsmodels.org/dev/examples/notebooks/generated/ols.html\nd18OpI = sm.add_constant(d18Op)\nmodel1 = sm.OLS(d18Oc, d18OpI)\nres1 = model1.fit()\nprint(res1.summary())\n\nplt.rcParams.update({'font.size': 20})\nfig = plt.figure(figsize=(4,4), dpi=400)\n#plt.plot(d18Op,d18Oc,'o',d18Op,d18Ochat,'r-')\nplt.plot(d18Op,d18Oc,'o', d18Op, res1.predict(d18OpI), 'r-')\n\nplt.xlabel(r\"$\\delta^{18}O_{p}$ (\" f\"\\u2030, V-PDB)\", fontsize=20) # MNE cleaned up the plot labeling\nplt.ylabel(r\"$\\delta^{18}O_{c,observed}$ (\" f\"\\u2030, V-PDB)\", fontsize=20)\nplt.xlim([-15, 5])\nplt.ylim([-12.5, 5])\nplt.title(r\"$\\delta^{18}O_{c,observed}$ vs. $\\delta^{18}O_{p}$\", fontsize=20)\n\nplt.grid(True)\nplt.show\n</pre> # 1. get regression of o18p on o18c # Using statsmodels OLS, see: https://www.statsmodels.org/dev/examples/notebooks/generated/ols.html d18OpI = sm.add_constant(d18Op) model1 = sm.OLS(d18Oc, d18OpI) res1 = model1.fit() print(res1.summary())  plt.rcParams.update({'font.size': 20}) fig = plt.figure(figsize=(4,4), dpi=400) #plt.plot(d18Op,d18Oc,'o',d18Op,d18Ochat,'r-') plt.plot(d18Op,d18Oc,'o', d18Op, res1.predict(d18OpI), 'r-')  plt.xlabel(r\"$\\delta^{18}O_{p}$ (\" f\"\\u2030, V-PDB)\", fontsize=20) # MNE cleaned up the plot labeling plt.ylabel(r\"$\\delta^{18}O_{c,observed}$ (\" f\"\\u2030, V-PDB)\", fontsize=20) plt.xlim([-15, 5]) plt.ylim([-12.5, 5]) plt.title(r\"$\\delta^{18}O_{c,observed}$ vs. $\\delta^{18}O_{p}$\", fontsize=20)  plt.grid(True) plt.show  <pre>                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.563\nModel:                            OLS   Adj. R-squared:                  0.556\nMethod:                 Least Squares   F-statistic:                     76.03\nDate:                Wed, 17 Dec 2025   Prob (F-statistic):           3.36e-12\nTime:                        10:36:32   Log-Likelihood:                -111.82\nNo. Observations:                  61   AIC:                             227.6\nDf Residuals:                      59   BIC:                             231.9\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -2.6536      0.417     -6.358      0.000      -3.489      -1.818\nx1             0.4714      0.054      8.719      0.000       0.363       0.580\n==============================================================================\nOmnibus:                        4.438   Durbin-Watson:                   1.860\nProb(Omnibus):                  0.109   Jarque-Bera (JB):                2.284\nSkew:                          -0.194   Prob(JB):                        0.319\nKurtosis:                       2.135   Cond. No.                         16.6\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n</pre> Out[21]: <pre>&lt;function matplotlib.pyplot.show(close=None, block=None)&gt;</pre> In\u00a0[22]: Copied! <pre># 2. get regression of o18c on T\n\nTempL = sm.add_constant(np.array(list(map(lambda x: x[0], Temp))))\nmodel2 = sm.OLS(d18Oc, TempL)\nres2 = model2.fit()\nprint(res2.summary())\n\nplt.rcParams.update({'font.size': 20})\nfig = plt.figure(figsize=(2,2), dpi=400)\nplt.plot(Temp, d18Oc,'o', Temp, res2.predict(TempL), 'r-')\n\nplt.ylabel(r\"$\\delta^{18}O_{c,o}$\" f\"(\\u2030, V-PDB)\",fontsize=20) # MNE cleaned up the plot labeling\nplt.xlabel(r\"T$_{surf}$($^{o}$C)\", fontsize=20)\nplt.title(r\"$\\delta^{18}O_{c,o}$ vs. T\", fontsize=20)\nplt.ylim([-12.5, 5])\nplt.xlim([-5, 30])\nplt.show\nplt.grid(True)\n</pre> # 2. get regression of o18c on T  TempL = sm.add_constant(np.array(list(map(lambda x: x[0], Temp)))) model2 = sm.OLS(d18Oc, TempL) res2 = model2.fit() print(res2.summary())  plt.rcParams.update({'font.size': 20}) fig = plt.figure(figsize=(2,2), dpi=400) plt.plot(Temp, d18Oc,'o', Temp, res2.predict(TempL), 'r-')  plt.ylabel(r\"$\\delta^{18}O_{c,o}$\" f\"(\\u2030, V-PDB)\",fontsize=20) # MNE cleaned up the plot labeling plt.xlabel(r\"T$_{surf}$($^{o}$C)\", fontsize=20) plt.title(r\"$\\delta^{18}O_{c,o}$ vs. T\", fontsize=20) plt.ylim([-12.5, 5]) plt.xlim([-5, 30]) plt.show plt.grid(True) <pre>                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.110\nModel:                            OLS   Adj. R-squared:                  0.094\nMethod:                 Least Squares   F-statistic:                     7.259\nDate:                Wed, 17 Dec 2025   Prob (F-statistic):            0.00917\nTime:                        10:36:32   Log-Likelihood:                -133.54\nNo. Observations:                  61   AIC:                             271.1\nDf Residuals:                      59   BIC:                             275.3\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -7.4639      0.658    -11.348      0.000      -8.780      -6.148\nx1             0.0991      0.037      2.694      0.009       0.025       0.173\n==============================================================================\nOmnibus:                        4.106   Durbin-Watson:                   1.751\nProb(Omnibus):                  0.128   Jarque-Bera (JB):                4.024\nSkew:                          -0.596   Prob(JB):                        0.134\nKurtosis:                       2.595   Cond. No.                         41.9\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n</pre> In\u00a0[23]: Copied! <pre># 3. get regression of o18cs on o18c \ncL = np.array(list(map(lambda x: x[0], d18Oc)))\nd18OcI = sm.add_constant(d18Oc)\nmodel3 = sm.OLS(d18Ocs, d18OcI)\nres3 = model3.fit()\nd18Ocshat = res3.predict(d18OcI)\nprint(res3.summary())\n\n#Plot line of slope 1 with same intercept for comparison\nplt.rcParams.update({'font.size': 20})\nd18Ocs_line = np.linspace(-15,5,100) # MNE adjusted 1:1 line range\nd18Oc_line = d18Ocs_line \nfig = plt.figure(figsize=(2,2), dpi=400)\nplt.plot(d18Oc,d18Ocs,'o', cL, d18Ocshat,'r-',d18Oc_line,d18Oc_line,'k--')\n\nplt.xlabel(r\"$\\delta^{18}O_{c,o}$\" f\"(\\u2030, V-PDB)\", fontsize=20) # MNE cleaned up the plot labeling\nplt.ylabel(r\"$\\delta^{18}O_{c,s}$\" f\"(\\u2030, V-PDB)\", fontsize=20)\nplt.title(r\"$\\delta^{18}O_{c,s}$ vs. $\\delta^{18}O_{c,o}$\", fontsize=20)\n\nplt.xlim([-12.5, 5])\nplt.ylim([-12.5, 5])\nplt.show\nplt.grid(True)\n\n#plt.savefig('figs/speleothem_modeling/speleo_d18Ocs_d18Oc_scatter.pdf', transparent=None, dpi=300, format='pdf',\n</pre> # 3. get regression of o18cs on o18c  cL = np.array(list(map(lambda x: x[0], d18Oc))) d18OcI = sm.add_constant(d18Oc) model3 = sm.OLS(d18Ocs, d18OcI) res3 = model3.fit() d18Ocshat = res3.predict(d18OcI) print(res3.summary())  #Plot line of slope 1 with same intercept for comparison plt.rcParams.update({'font.size': 20}) d18Ocs_line = np.linspace(-15,5,100) # MNE adjusted 1:1 line range d18Oc_line = d18Ocs_line  fig = plt.figure(figsize=(2,2), dpi=400) plt.plot(d18Oc,d18Ocs,'o', cL, d18Ocshat,'r-',d18Oc_line,d18Oc_line,'k--')  plt.xlabel(r\"$\\delta^{18}O_{c,o}$\" f\"(\\u2030, V-PDB)\", fontsize=20) # MNE cleaned up the plot labeling plt.ylabel(r\"$\\delta^{18}O_{c,s}$\" f\"(\\u2030, V-PDB)\", fontsize=20) plt.title(r\"$\\delta^{18}O_{c,s}$ vs. $\\delta^{18}O_{c,o}$\", fontsize=20)  plt.xlim([-12.5, 5]) plt.ylim([-12.5, 5]) plt.show plt.grid(True)  #plt.savefig('figs/speleothem_modeling/speleo_d18Ocs_d18Oc_scatter.pdf', transparent=None, dpi=300, format='pdf', <pre>                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.588\nModel:                            OLS   Adj. R-squared:                  0.581\nMethod:                 Least Squares   F-statistic:                     84.17\nDate:                Wed, 17 Dec 2025   Prob (F-statistic):           5.86e-13\nTime:                        10:36:32   Log-Likelihood:                -121.95\nNo. Observations:                  61   AIC:                             247.9\nDf Residuals:                      59   BIC:                             252.1\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -1.3469      0.639     -2.107      0.039      -2.626      -0.067\nx1             0.9321      0.102      9.174      0.000       0.729       1.135\n==============================================================================\nOmnibus:                       14.970   Durbin-Watson:                   1.141\nProb(Omnibus):                  0.001   Jarque-Bera (JB):               17.867\nSkew:                           1.026   Prob(JB):                     0.000132\nKurtosis:                       4.678   Cond. No.                         17.7\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n</pre> In\u00a0[24]: Copied! <pre># are the 1:1 and o18cs on d18Oc slopes significantly different?\nprint('slope of regression of o18cs vs o18co: ', np.round(res3.params[1],2))\nprint('standard error of regression of o18cs vs o18co: ', np.round(res3.bse[1],2))\nt_slope_1=res3.tvalues[1]\nprint('t value for slope of o18cs vs o18co vs 1:1 slope: ', np.round(t_slope_1,2))\np_value_slope =res3.pvalues[1]\nprint('p value: ', np.round(p_value_slope,8))\nmean_diff=(np.mean(d18Ocs)-np.mean(d18Oc))\nprint('mean difference (d18Oc - d18Ocs): ', np.round(mean_diff,2))\nse_mean_diff=np.sqrt(np.mean(np.std(d18Ocs)**2/len(d18Ocs)+np.std(d18Oc)**2/len(d18Oc)))\nprint('standard error of the mean difference: ', np.round(se_mean_diff,2))\nt_mean_diff=(np.mean(d18Oc)-np.mean(d18Ocs))/np.sqrt(np.mean(np.std(d18Ocs)**2/len(d18Ocs)+np.std(d18Oc)**2/len(d18Oc)))\nprint('t_mean_diff: ', np.round(t_mean_diff,2))\np_value_mean_diff=[2*(1-scipy.stats.t.cdf(t_mean_diff,(len(d18Oc)-2)))]\nprint('p value for difference of means: ', np.round(p_value_mean_diff,4))\n# simulated d18Oc is significantly lower than observed d18Oc\n# this difference is about \nfrac_diff=np.abs(mean_diff)/(np.max(d18Oc)-np.min(d18Oc))\nprint('percent bias: {}%'.format(np.round(frac_diff*100,1)))\n</pre> # are the 1:1 and o18cs on d18Oc slopes significantly different? print('slope of regression of o18cs vs o18co: ', np.round(res3.params[1],2)) print('standard error of regression of o18cs vs o18co: ', np.round(res3.bse[1],2)) t_slope_1=res3.tvalues[1] print('t value for slope of o18cs vs o18co vs 1:1 slope: ', np.round(t_slope_1,2)) p_value_slope =res3.pvalues[1] print('p value: ', np.round(p_value_slope,8)) mean_diff=(np.mean(d18Ocs)-np.mean(d18Oc)) print('mean difference (d18Oc - d18Ocs): ', np.round(mean_diff,2)) se_mean_diff=np.sqrt(np.mean(np.std(d18Ocs)**2/len(d18Ocs)+np.std(d18Oc)**2/len(d18Oc))) print('standard error of the mean difference: ', np.round(se_mean_diff,2)) t_mean_diff=(np.mean(d18Oc)-np.mean(d18Ocs))/np.sqrt(np.mean(np.std(d18Ocs)**2/len(d18Ocs)+np.std(d18Oc)**2/len(d18Oc))) print('t_mean_diff: ', np.round(t_mean_diff,2)) p_value_mean_diff=[2*(1-scipy.stats.t.cdf(t_mean_diff,(len(d18Oc)-2)))] print('p value for difference of means: ', np.round(p_value_mean_diff,4)) # simulated d18Oc is significantly lower than observed d18Oc # this difference is about  frac_diff=np.abs(mean_diff)/(np.max(d18Oc)-np.min(d18Oc)) print('percent bias: {}%'.format(np.round(frac_diff*100,1))) <pre>slope of regression of o18cs vs o18co:  0.93\nstandard error of regression of o18cs vs o18co:  0.1\nt value for slope of o18cs vs o18co vs 1:1 slope:  9.17\np value:  0.0\nmean difference (d18Oc - d18Ocs):  -0.95\nstandard error of the mean difference:  0.46\nt_mean_diff:  2.06\np value for difference of means:  [0.0441]\npercent bias: 9.0%\n</pre> In\u00a0[25]: Copied! <pre># clean up the dataframe to list only the mean value of o18p in the corresponding column of the data \npred_frame['d18Op_vals']=d18Op\npred_frame\n</pre> # clean up the dataframe to list only the mean value of o18p in the corresponding column of the data  pred_frame['d18Op_vals']=d18Op pred_frame Out[25]: geo_meanLon geo_meanLat d18Oc_vals d18Op_vals cruT_vals d18Ocs_vals 0 -55.450001 -4.066700 -5.930000 -4.90709 26.539503 -7.223432 1 0.780000 45.430000 -4.559933 -6.67699 11.989003 -5.773501 2 105.116699 33.583302 -7.730000 -9.10825 10.181901 -7.772914 3 105.116699 33.583302 -8.874000 -9.10825 10.181901 -7.772914 4 15.000000 67.000000 -7.062788 -13.50650 1.8243847 -10.063785 ... ... ... ... ... ... ... 56 -9.490000 30.770000 -4.571000 -5.79046 15.463603 -5.700161 57 115.690002 -31.547001 -3.180740 -4.11277 18.29258 -4.664288 58 7.664700 51.367500 -5.273960 -7.70140 8.7618885 -6.016585 59 30.711700 37.232498 -3.770000 -7.73942 15.118716 -7.570000 60 77.866699 30.600000 -8.510427 -9.04510 16.272415 -9.139685 <p>61 rows \u00d7 6 columns</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/analysis_speleothem/#analyse-speleothem-records","title":"Analyse speleothem records\u00b6","text":""},{"location":"notebooks/analysis_speleothem/#set-up-working-environment","title":"Set up working environment\u00b6","text":""},{"location":"notebooks/analysis_speleothem/#read-dataframe","title":"Read dataframe\u00b6","text":""},{"location":"notebooks/analysis_speleothem/#filter-dataframe-for-speleothem-type-records","title":"Filter dataframe for speleothem type records\u00b6","text":""},{"location":"notebooks/analysis_speleothem/#plot-data","title":"Plot data\u00b6","text":""},{"location":"notebooks/analysis_speleothem/#save-relevant-data-for-speleothem-analysis","title":"save relevant data for speleothem analysis\u00b6","text":""},{"location":"notebooks/analysis_speleothem/#load-observations","title":"load observations\u00b6","text":""},{"location":"notebooks/analysis_speleothem/#load-cru-ts","title":"load CRU TS\u00b6","text":""},{"location":"notebooks/analysis_speleothem/#load-observed-isotope-data","title":"Load observed isotope data\u00b6","text":""},{"location":"notebooks/analysis_speleothem/#join-data","title":"Join data\u00b6","text":""},{"location":"notebooks/analysis_speleothem/#modeling","title":"Modeling\u00b6","text":""},{"location":"notebooks/analysis_speleothem/#simulate-speleothem-data-with-prysm-model","title":"Simulate Speleothem Data with PRYSM Model\u00b6","text":""},{"location":"notebooks/analysis_speleothem/#regression-comparisons","title":"Regression Comparisons\u00b6","text":""},{"location":"notebooks/analysis_speleothem/#regress-and-plot","title":"Regress and plot\u00b6","text":""},{"location":"notebooks/analysis_speleothem/#regression-of-o18p-on-o18c","title":"regression of o18p on o18c\u00b6","text":""},{"location":"notebooks/analysis_speleothem/#regression-of-o18c-on-t","title":"regression of o18c on T\u00b6","text":""},{"location":"notebooks/analysis_speleothem/#regression-of-o18cs-on-o18c","title":"regression of o18cs on o18c\u00b6","text":""},{"location":"notebooks/analysis_speleothem/#measures-of-regression","title":"Measures of Regression\u00b6","text":""},{"location":"notebooks/analysis_speleothem/#bibliography","title":"Bibliography\u00b6","text":"<p>Bowen, G.J., Wassenaar, L.I., Hobson, K.A. (2005). Global application of stable hydrogen and oxygen isotopes to wildlife forensics. Oecologia 143, 337\u2013348. https://doi.org/10.1007/s00442-004-1813-y.  Data retrieved from:  https://wateriso.utah.edu/waterisotopes/media/ArcGrids/GlobalPrecip.zip, last accessed 2025/08/03.    Dee, S., J. Emile-Geay, M. N. Evans, A. Allam, E. J. Steig, and D. M. Thompson (2015). PRYSM: An open-source framework for PRoxY System Modeling, with applications to oxygen-isotope systems. J. Adv. Model. Earth Syst., 7, 1220\u20131247. https://doi.org/10.1002/2015MS000447.   Harris, I., Osborn, T.J., Jones, P. et al. (2020). Version 4 of the CRU TS monthly high-resolution gridded multivariate climate dataset. Sci Data 7, 109. https://doi.org/10.1038/s41597-020-0453-3.  Data retrieved from:  https://crudata.uea.ac.uk/cru/data/hrg/cru_ts_4.07/cruts.2304141047.v4.07/tmp/cru_ts4.07.1901.2022.tmp.dat.nc.gz, last accessed 2025/08/03.   Hu, J., J. Emile-Geay, J. Partin (2017). Correlation-based interpretations of paleoclimate data \u2013 where statistics meet past climates, Earth and Planetary Science Letters, 459, 362-371. https://doi.org/10.1016/j.epsl.2016.11.048.</p>"},{"location":"notebooks/analysis_temperature/","title":"Analyse temperature records","text":"<p>This notebook performs a PCA for a subset of records, filtered by archiveType/paleoData_proxy. For each subset, the following algorithm is being used:</p> <ol> <li>Filter archive_type and paleoData_proxy (defines subset) and produces summary plots of the data, in particular regarding: coverage, resolution and length of the records. This gives us information for the next step, in which we need to choose the parameters for the PCA</li> <li>Define proxy specific parameters for the PCA:<ul> <li>period (start and end year): choose a period of sufficient data density (all records chosen for the analysis need to at least overlap during this period)</li> <li>minimum resolution: records exceeding this resolution are being excluded from the analysis. Records with higher resolution will be subsampled to create homogeneous resolution across all the records.</li> <li>record length: records shorter than the record length are being excluded from the analysis.</li> <li>The choice of parameters will determine the success of the PCA. There is a trade-off between the number of records included and the quality (i.e. period/record length/resolution).</li> <li>Summary figures are being produced for the filtered data</li> <li>z-scores added to dataframe (mean=0 and std=1 over the entire record) as 'paleoData_zscores'</li> <li>note: z-scores may be biased if records are only partly overlapping in time, or increase in availability over time, or both.</li> </ul> </li> <li>Homogenise data dimensions across the records<ul> <li>defines a homogenised time variable over the target period and with the target resolution (as defined in the last step), which is saved as a new column in the dataframe named 'years_hom'</li> <li>creates a data matrix with dimensions n_records x n_time which is saved as a new column in df, named 'paleoData_values_hom' and 'paleoData_zscores_hom'.</li> <li>Note that this data is formatted as a np.ma.masked_array, where missing data is set to zero and masked out.</li> </ul> </li> <li>PCA<ul> <li>obtains covariance matrix of paleoData_zscores_hom (note that for every two records the covariance is calculated over their intersect of data availability)</li> <li>obtains eigenvectors and eigenvalues via SVD composition</li> <li>obtains and plots fraction of explained variance, first two PCs and load for first two EOFs vs ordering in the data frame.</li> </ul> </li> </ol> <p>2025/12/17: Created. T analysis is a copy of MT analysis!</p> <p>2025/12/17: Tidied up and updated for DoD2k v2.0</p> <p>Make sure the repo_root is added correctly, it should be: <code>your_root_dir/dod2k</code> This should be the working directory throughout this notebook (and all other notebooks).</p> In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n\nimport sys\nimport os\nfrom pathlib import Path\n\n# Add parent directory to path (works from any notebook in notebooks/)\n# the repo_root should be the parent directory of the notebooks folder\ninit_dir = Path().resolve()\n# Determine repo root\nif init_dir.name == 'dod2k': repo_root = init_dir\nelif init_dir.parent.name == 'dod2k': repo_root = init_dir.parent\nelse: raise Exception('Please review the repo root structure (see first cell).')\n\n# Update cwd and path only if needed\nif os.getcwd() != str(repo_root):\n    os.chdir(repo_root)\nif str(repo_root) not in sys.path:\n    sys.path.insert(0, str(repo_root))\n\nprint(f\"Repo root: {repo_root}\")\nif str(os.getcwd())==str(repo_root):\n    print(f\"Working directory matches repo root. \")\n</pre> %load_ext autoreload %autoreload 2  import sys import os from pathlib import Path  # Add parent directory to path (works from any notebook in notebooks/) # the repo_root should be the parent directory of the notebooks folder init_dir = Path().resolve() # Determine repo root if init_dir.name == 'dod2k': repo_root = init_dir elif init_dir.parent.name == 'dod2k': repo_root = init_dir.parent else: raise Exception('Please review the repo root structure (see first cell).')  # Update cwd and path only if needed if os.getcwd() != str(repo_root):     os.chdir(repo_root) if str(repo_root) not in sys.path:     sys.path.insert(0, str(repo_root))  print(f\"Repo root: {repo_root}\") if str(os.getcwd())==str(repo_root):     print(f\"Working directory matches repo root. \") <pre>Repo root: /home/jupyter-lluecke/dod2k\nWorking directory matches repo root. \n</pre> In\u00a0[2]: Copied! <pre># Import packages\nimport numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec as GS\n\n\nfrom dod2k_utilities import ut_functions as utf # contains utility functions\nfrom dod2k_utilities import ut_plot as uplt # contains plotting functions\nfrom dod2k_utilities import ut_analysis as uta # contains plotting functions\n</pre> # Import packages import numpy as np import pandas as pd import os import matplotlib.pyplot as plt from matplotlib.gridspec import GridSpec as GS   from dod2k_utilities import ut_functions as utf # contains utility functions from dod2k_utilities import ut_plot as uplt # contains plotting functions from dod2k_utilities import ut_analysis as uta # contains plotting functions <p>Read compact dataframe.</p> <p>{db_name} refers to the database, including</p> <pre><code>- dod2k_v2.0_filtered_M_TM (filtered for moisture and temperature+moisture sensitive records only</code></pre> <p>All compact dataframes are saved in {repo_root}/data/{db_name} as {db_name}_compact.csv.</p> In\u00a0[3]: Copied! <pre># read dataframe, choose from the list below, or specify your own\n\ndb_name = 'dod2k_v2.0_filtered_M_TM'\n\n# load dataframe\ndf = utf.load_compact_dataframe_from_csv(db_name)\nprint(df.info())\ndf.name = db_name\n</pre> # read dataframe, choose from the list below, or specify your own  db_name = 'dod2k_v2.0_filtered_M_TM'  # load dataframe df = utf.load_compact_dataframe_from_csv(db_name) print(df.info()) df.name = db_name  <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1416 entries, 0 to 1415\nData columns (total 22 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   archiveType                    1416 non-null   object \n 1   dataSetName                    1416 non-null   object \n 2   datasetId                      1416 non-null   object \n 3   duplicateDetails               1416 non-null   object \n 4   geo_meanElev                   1398 non-null   float32\n 5   geo_meanLat                    1416 non-null   float32\n 6   geo_meanLon                    1416 non-null   float32\n 7   geo_siteName                   1416 non-null   object \n 8   interpretation_direction       1416 non-null   object \n 9   interpretation_seasonality     1416 non-null   object \n 10  interpretation_variable        1416 non-null   object \n 11  interpretation_variableDetail  1416 non-null   object \n 12  originalDataURL                1416 non-null   object \n 13  originalDatabase               1416 non-null   object \n 14  paleoData_notes                1416 non-null   object \n 15  paleoData_proxy                1416 non-null   object \n 16  paleoData_sensorSpecies        1416 non-null   object \n 17  paleoData_units                1416 non-null   object \n 18  paleoData_values               1416 non-null   object \n 19  paleoData_variableName         1416 non-null   object \n 20  year                           1416 non-null   object \n 21  yearUnits                      1416 non-null   object \ndtypes: float32(3), object(19)\nmemory usage: 226.9+ KB\nNone\n</pre> In\u00a0[4]: Copied! <pre># cols = [ '#4477AA', '#EE6677', '#228833', '#CCBB44', '#66CCEE', '#AA3377', '#BBBBBB', '#44AA99', '#332288']\n</pre> # cols = [ '#4477AA', '#EE6677', '#228833', '#CCBB44', '#66CCEE', '#AA3377', '#BBBBBB', '#44AA99', '#332288'] In\u00a0[5]: Copied! <pre>time    = {}\ndata    = {}\nPCs     = {}\nEOFs    = {}\nfoev    = {}\npca_rec = {}\n\nkeys    = []\n</pre> time    = {} data    = {} PCs     = {} EOFs    = {} foev    = {} pca_rec = {}  keys    = [] In\u00a0[6]: Copied! <pre># (1) filter for archiveType and/or paleoData_proxy: \n\nat = 'Wood'\npt = 'ring width'\nkey = '%s_%s'%(at, pt)\n\nkeys += [key]\n\ndf_proxy = df.copy().loc[(df['archiveType']==at)].loc[(df['paleoData_proxy']==pt)] # filter records for specific archive and proxy type\n\nn_recs = len(df_proxy) # number of records\nprint('n_records   : ', n_recs)\n\nprint('archive type: ', set(df_proxy['archiveType']))\nprint('proxy type:   ', set(df_proxy['paleoData_proxy']))\n\n# (2) plot the spatial distribution of records\ngeo_fig, col = uplt.geo_plot(df_proxy, return_col=True)\n\n# (3) plot the coverage for proxy types and plot resolution\n\nuta.convert_subannual_to_annual_res(df_proxy)\n\ndf_proxy = uta.add_auxvars_plot_summary(df_proxy, key, col=col[at])\n</pre> # (1) filter for archiveType and/or paleoData_proxy:   at = 'Wood' pt = 'ring width' key = '%s_%s'%(at, pt)  keys += [key]  df_proxy = df.copy().loc[(df['archiveType']==at)].loc[(df['paleoData_proxy']==pt)] # filter records for specific archive and proxy type  n_recs = len(df_proxy) # number of records print('n_records   : ', n_recs)  print('archive type: ', set(df_proxy['archiveType'])) print('proxy type:   ', set(df_proxy['paleoData_proxy']))  # (2) plot the spatial distribution of records geo_fig, col = uplt.geo_plot(df_proxy, return_col=True)  # (3) plot the coverage for proxy types and plot resolution  uta.convert_subannual_to_annual_res(df_proxy)  df_proxy = uta.add_auxvars_plot_summary(df_proxy, key, col=col[at])   <pre>n_records   :  1065\narchive type:  {'Wood'}\nproxy type:    {'ring width'}\n0 Wood 3650\n1 Speleothem 567\n2 Coral 312\n3 GlacierIce 162\n4 MarineSediment 126\n5 LakeSediment 111\n6 Documents 13\n7 Sclerosponge 6\n8 GroundIce 4\n9 Borehole 3\n10 Other 2\n11 MolluskShell 1\n</pre> In\u00a0[7]: Copied! <pre>#========================= PROXY SPECIFIC: Wood ring width =========================\nminres    = 1                         # homogenised resolution\nmny       = 1000                      # start year of homogenised time coord\nmxy       = 2000                      # end year of homogenised time coord\nnyears    = np.min([600, mxy-mny])    # minimum length of each record\n#====================================================================\n\n# filter for record length during target period\ndf_proxy = uta.filter_record_length(df_proxy, nyears, mny, mxy)\n\n# filter for resolution\ndf_proxy = uta.filter_resolution(df_proxy, minres)\n\n# plot coverage and resolution\nuplt.plot_coverage_analysis(df_proxy, np.arange(mny, mxy+minres, minres), key, col[at])\nuplt.plot_resolution(df_proxy, key, col=col[at])\nuplt.plot_length(df_proxy, key, col=col[at])\n\nn_recs = len(df_proxy) # final number of records\n\n\nprint(df_proxy[['miny', 'maxy', 'originalDatabase']])\n\npca_rec[key] = df_proxy['datasetId']\n</pre> #========================= PROXY SPECIFIC: Wood ring width ========================= minres    = 1                         # homogenised resolution mny       = 1000                      # start year of homogenised time coord mxy       = 2000                      # end year of homogenised time coord nyears    = np.min([600, mxy-mny])    # minimum length of each record #====================================================================  # filter for record length during target period df_proxy = uta.filter_record_length(df_proxy, nyears, mny, mxy)  # filter for resolution df_proxy = uta.filter_resolution(df_proxy, minres)  # plot coverage and resolution uplt.plot_coverage_analysis(df_proxy, np.arange(mny, mxy+minres, minres), key, col[at]) uplt.plot_resolution(df_proxy, key, col=col[at]) uplt.plot_length(df_proxy, key, col=col[at])  n_recs = len(df_proxy) # final number of records   print(df_proxy[['miny', 'maxy', 'originalDatabase']])  pca_rec[key] = df_proxy['datasetId'] <pre>Keep 71 records with nyears&gt;=600 during 1000-2000. Exclude 994 records.\nKeep 71 records with resolution &lt;=1. Exclude 0 records.\n</pre> <pre>        miny    maxy                   originalDatabase\n3     1360.0  1983.0  FE23 (Breitenmoser et al. (2014))\n84    1357.0  1975.0  FE23 (Breitenmoser et al. (2014))\n174   1153.0  1986.0  FE23 (Breitenmoser et al. (2014))\n226   1123.0  2001.0  FE23 (Breitenmoser et al. (2014))\n231   1381.0  2000.0  FE23 (Breitenmoser et al. (2014))\n...      ...     ...                                ...\n1003   850.0  1989.0  FE23 (Breitenmoser et al. (2014))\n1007  1390.0  1998.0  FE23 (Breitenmoser et al. (2014))\n1023   980.0  1985.0  FE23 (Breitenmoser et al. (2014))\n1026  1236.0  1984.0  FE23 (Breitenmoser et al. (2014))\n1063  1377.0  1999.0  FE23 (Breitenmoser et al. (2014))\n\n[71 rows x 3 columns]\n</pre> In\u00a0[8]: Copied! <pre># add 'z-scores' to dataframe and plot z-scores and values\ndf_proxy = uta.add_zscores_plot(df_proxy, key, plot_output=True)\n</pre> # add 'z-scores' to dataframe and plot z-scores and values df_proxy = uta.add_zscores_plot(df_proxy, key, plot_output=True)  In\u00a0[9]: Copied! <pre># define new homogenised time coordinate\ndf_proxy, years_hom = uta.homogenise_time(df_proxy, mny, mxy, minres)\ntime[key] = years_hom\n</pre> # define new homogenised time coordinate df_proxy, years_hom = uta.homogenise_time(df_proxy, mny, mxy, minres) time[key] = years_hom <pre>Homogenised time coordinate: 1000-2000 CE\nResolution: [1] years\nINTERSECT: 1400-1963\n</pre> In\u00a0[10]: Copied! <pre># assign the paleoData_values to the non-missing values in the homogenised data array\n\nout = uta.homogenise_data_dimensions(df_proxy, years_hom, key, plot_output=False) # returns list of homogenised paleoData_values and paleoData_zscores\npaleoData_values_hom, paleoData_zscores_hom, year_hom_avbl, zsco_hom_avbl = out\n\n# define new columns in df_filt\nnew_columns = {'year_hom': [years_hom]*n_recs, \n               'year_hom_avbl': year_hom_avbl, \n               'paleoData_values_hom': [paleoData_values_hom[ii, :] for ii in range(n_recs)], \n               'paleoData_zscores_hom': [paleoData_zscores_hom[ii, :] for ii in range(n_recs)], \n               'paleoData_zscores_hom_avbl': zsco_hom_avbl}\ndf_proxy = df_proxy.assign(**new_columns)\n\nprint('Real intersect after homogenising resolution: ')\nintrsct = uta.find_shared_period(df_proxy, minmax=(mny, mxy), time='year_hom_avbl', \n                             data='paleoData_zscores_hom_avbl')\ndata[key] = paleoData_zscores_hom\n</pre> # assign the paleoData_values to the non-missing values in the homogenised data array  out = uta.homogenise_data_dimensions(df_proxy, years_hom, key, plot_output=False) # returns list of homogenised paleoData_values and paleoData_zscores paleoData_values_hom, paleoData_zscores_hom, year_hom_avbl, zsco_hom_avbl = out  # define new columns in df_filt new_columns = {'year_hom': [years_hom]*n_recs,                 'year_hom_avbl': year_hom_avbl,                 'paleoData_values_hom': [paleoData_values_hom[ii, :] for ii in range(n_recs)],                 'paleoData_zscores_hom': [paleoData_zscores_hom[ii, :] for ii in range(n_recs)],                 'paleoData_zscores_hom_avbl': zsco_hom_avbl} df_proxy = df_proxy.assign(**new_columns)  print('Real intersect after homogenising resolution: ') intrsct = uta.find_shared_period(df_proxy, minmax=(mny, mxy), time='year_hom_avbl',                               data='paleoData_zscores_hom_avbl') data[key] = paleoData_zscores_hom  <pre>(71, 1001)\nReal intersect after homogenising resolution: \nINTERSECT: 1400-1963\n</pre> In\u00a0[11]: Copied! <pre>covariance, overlap = uta.calc_covariance_matrix(df_proxy)\n</pre> covariance, overlap = uta.calc_covariance_matrix(df_proxy) <pre>short records :  []\n</pre> In\u00a0[12]: Copied! <pre>eigenvalues, eigenvectors = uta.PCA(covariance)\n\nfoev[key] = uta.fraction_of_explained_var(covariance, eigenvalues, n_recs, key, df.name)\n</pre> eigenvalues, eigenvectors = uta.PCA(covariance)  foev[key] = uta.fraction_of_explained_var(covariance, eigenvalues, n_recs, key, df.name) <pre>saved figure in /figs/dod2k_v2.0_filtered_M_TM/foev_Wood_ring width.pdf\n</pre> In\u00a0[13]: Copied! <pre>PCs[key], EOFs[key] = uplt.plot_PCs(years_hom, eigenvectors, paleoData_zscores_hom, key, df.name)\n</pre> PCs[key], EOFs[key] = uplt.plot_PCs(years_hom, eigenvectors, paleoData_zscores_hom, key, df.name) <pre>saved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0_filtered_M_TM/PCs_Wood_ring width.pdf\nsaved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0_filtered_M_TM/EOFloading_Wood_ring width.pdf\n</pre> In\u00a0[14]: Copied! <pre>print('Finished for %s'%key)\n</pre> print('Finished for %s'%key) <pre>Finished for Wood_ring width\n</pre> In\u00a0[15]: Copied! <pre># (1) filter for archiveType and/or paleoData_proxy: \n\nat = 'Wood'\npt = 'd18O'\nkey = '%s_%s'%(at, pt)\n\nkeys += [key]\n\ndf_proxy = df.copy().loc[(df['archiveType']==at)].loc[(df['paleoData_proxy']==pt)] # filter records for specific archive and proxy type\n\nn_recs = len(df_proxy) # number of records\nprint('n_records   : ', n_recs)\n\nprint('archive type: ', set(df_proxy['archiveType']))\nprint('proxy type:   ', set(df_proxy['paleoData_proxy']))\n\n# (2) plot the spatial distribution of records\ngeo_fig, col = uplt.geo_plot(df_proxy, return_col=True)\n\n# (3) plot the coverage for proxy types and plot resolution\n\nuta.convert_subannual_to_annual_res(df_proxy)\n\ndf_proxy = uta.add_auxvars_plot_summary(df_proxy, key, col=col[at])\n</pre> # (1) filter for archiveType and/or paleoData_proxy:   at = 'Wood' pt = 'd18O' key = '%s_%s'%(at, pt)  keys += [key]  df_proxy = df.copy().loc[(df['archiveType']==at)].loc[(df['paleoData_proxy']==pt)] # filter records for specific archive and proxy type  n_recs = len(df_proxy) # number of records print('n_records   : ', n_recs)  print('archive type: ', set(df_proxy['archiveType'])) print('proxy type:   ', set(df_proxy['paleoData_proxy']))  # (2) plot the spatial distribution of records geo_fig, col = uplt.geo_plot(df_proxy, return_col=True)  # (3) plot the coverage for proxy types and plot resolution  uta.convert_subannual_to_annual_res(df_proxy)  df_proxy = uta.add_auxvars_plot_summary(df_proxy, key, col=col[at])   <pre>n_records   :  27\narchive type:  {'Wood'}\nproxy type:    {'d18O'}\n0 Wood 3650\n1 Speleothem 567\n2 Coral 312\n3 GlacierIce 162\n4 MarineSediment 126\n5 LakeSediment 111\n6 Documents 13\n7 Sclerosponge 6\n8 GroundIce 4\n9 Borehole 3\n10 Other 2\n11 MolluskShell 1\n</pre> In\u00a0[16]: Copied! <pre>df_proxy.keys()\n</pre> df_proxy.keys() Out[16]: <pre>Index(['archiveType', 'dataSetName', 'datasetId', 'duplicateDetails',\n       'geo_meanElev', 'geo_meanLat', 'geo_meanLon', 'geo_siteName',\n       'interpretation_direction', 'interpretation_seasonality',\n       'interpretation_variable', 'interpretation_variableDetail',\n       'originalDataURL', 'originalDatabase', 'paleoData_notes',\n       'paleoData_proxy', 'paleoData_sensorSpecies', 'paleoData_units',\n       'paleoData_values', 'paleoData_variableName', 'year', 'yearUnits',\n       'length', 'miny', 'maxy', 'resolution'],\n      dtype='object')</pre> In\u00a0[17]: Copied! <pre>#========================= PROXY SPECIFIC: Wood d18O =========================\nminres    = 2                         # homogenised resolution\nmny       = 1700                      # start year of homogenised time coord\nmxy       = 2000                      # end year of homogenised time coord\nnyears    = np.min([100, mxy-mny])    # minimum length of each record\n#====================================================================\n\n# filter for record length during target period\ndf_proxy = uta.filter_record_length(df_proxy, nyears, mny, mxy)\n\n# filter for resolution\ndf_proxy = uta.filter_resolution(df_proxy, minres)\n\n# plot coverage and resolution\nuplt.plot_coverage_analysis(df_proxy, np.arange(mny, mxy+minres, minres), key, col[at])\nuplt.plot_resolution(df_proxy, key, col=col[at])\nuplt.plot_length(df_proxy, key, col=col[at])\n\nn_recs = len(df_proxy) # final number of records\n\n\nprint(df_proxy[['miny', 'maxy', 'originalDatabase']])\n\npca_rec[key] = df_proxy['datasetId']\n</pre> #========================= PROXY SPECIFIC: Wood d18O ========================= minres    = 2                         # homogenised resolution mny       = 1700                      # start year of homogenised time coord mxy       = 2000                      # end year of homogenised time coord nyears    = np.min([100, mxy-mny])    # minimum length of each record #====================================================================  # filter for record length during target period df_proxy = uta.filter_record_length(df_proxy, nyears, mny, mxy)  # filter for resolution df_proxy = uta.filter_resolution(df_proxy, minres)  # plot coverage and resolution uplt.plot_coverage_analysis(df_proxy, np.arange(mny, mxy+minres, minres), key, col[at]) uplt.plot_resolution(df_proxy, key, col=col[at]) uplt.plot_length(df_proxy, key, col=col[at])  n_recs = len(df_proxy) # final number of records   print(df_proxy[['miny', 'maxy', 'originalDatabase']])  pca_rec[key] = df_proxy['datasetId'] <pre>Keep 25 records with nyears&gt;=100 during 1700-2000. Exclude 2 records.\nKeep 23 records with resolution &lt;=2. Exclude 2 records.\n</pre> <pre>        miny    maxy originalDatabase\n1141  1778.0  2000.0     Iso2k v1.1.2\n1142  1780.0  2003.0     Iso2k v1.1.2\n1153  1901.0  2009.0     Iso2k v1.1.2\n1155  1901.0  2001.0     Iso2k v1.1.2\n1156  1900.0  2002.0     Iso2k v1.1.2\n1160  1000.0  1998.0     Iso2k v1.1.2\n1161  1163.0  2005.0     Iso2k v1.1.2\n1163  1820.0  2004.0     Iso2k v1.1.2\n1171  1352.0  2012.0     Iso2k v1.1.2\n1182  1901.0  2010.0     Iso2k v1.1.2\n1184  1865.0  1969.0     Iso2k v1.1.2\n1185  1830.0  2010.0     Iso2k v1.1.2\n1186  1877.0  1998.0     Iso2k v1.1.2\n1187  1877.0  1998.0     Iso2k v1.1.2\n1188   489.0  2010.0     Iso2k v1.1.2\n1189  1801.0  2000.0     Iso2k v1.1.2\n1200  1767.0  2008.0     Iso2k v1.1.2\n1204  1743.0  2011.0     Iso2k v1.1.2\n1212  1705.0  2004.0     Iso2k v1.1.2\n1213  1705.0  2004.0     Iso2k v1.1.2\n1214  1705.0  2004.0     Iso2k v1.1.2\n1226  1592.0  2011.0     Iso2k v1.1.2\n1229  1850.0  2012.0     Iso2k v1.1.2\n</pre> In\u00a0[18]: Copied! <pre># add 'z-scores' to dataframe and plot z-scores and values\ndf_proxy = uta.add_zscores_plot(df_proxy, key, plot_output=True)\n</pre> # add 'z-scores' to dataframe and plot z-scores and values df_proxy = uta.add_zscores_plot(df_proxy, key, plot_output=True)  In\u00a0[19]: Copied! <pre># define new homogenised time coordinate\ndf_proxy, years_hom = uta.homogenise_time(df_proxy, mny, mxy, minres)\ntime[key] = years_hom\n</pre> # define new homogenised time coordinate df_proxy, years_hom = uta.homogenise_time(df_proxy, mny, mxy, minres) time[key] = years_hom <pre>Homogenised time coordinate: 1700-2000 CE\nResolution: [2] years\nINTERSECT: 1901-1969\n</pre> In\u00a0[20]: Copied! <pre># assign the paleoData_values to the non-missing values in the homogenised data array\n\nout = uta.homogenise_data_dimensions(df_proxy, years_hom, key, plot_output=False) # returns list of homogenised paleoData_values and paleoData_zscores\npaleoData_values_hom, paleoData_zscores_hom, year_hom_avbl, zsco_hom_avbl = out\n\n# define new columns in df_filt\nnew_columns = {'year_hom': [years_hom]*n_recs, \n               'year_hom_avbl': year_hom_avbl, \n               'paleoData_values_hom': [paleoData_values_hom[ii, :] for ii in range(n_recs)], \n               'paleoData_zscores_hom': [paleoData_zscores_hom[ii, :] for ii in range(n_recs)], \n               'paleoData_zscores_hom_avbl': zsco_hom_avbl}\ndf_proxy = df_proxy.assign(**new_columns)\n\nprint('Real intersect after homogenising resolution: ')\nintrsct = uta.find_shared_period(df_proxy, minmax=(mny, mxy), time='year_hom_avbl', \n                             data='paleoData_zscores_hom_avbl')\ndata[key] = paleoData_zscores_hom\n</pre> # assign the paleoData_values to the non-missing values in the homogenised data array  out = uta.homogenise_data_dimensions(df_proxy, years_hom, key, plot_output=False) # returns list of homogenised paleoData_values and paleoData_zscores paleoData_values_hom, paleoData_zscores_hom, year_hom_avbl, zsco_hom_avbl = out  # define new columns in df_filt new_columns = {'year_hom': [years_hom]*n_recs,                 'year_hom_avbl': year_hom_avbl,                 'paleoData_values_hom': [paleoData_values_hom[ii, :] for ii in range(n_recs)],                 'paleoData_zscores_hom': [paleoData_zscores_hom[ii, :] for ii in range(n_recs)],                 'paleoData_zscores_hom_avbl': zsco_hom_avbl} df_proxy = df_proxy.assign(**new_columns)  print('Real intersect after homogenising resolution: ') intrsct = uta.find_shared_period(df_proxy, minmax=(mny, mxy), time='year_hom_avbl',                               data='paleoData_zscores_hom_avbl') data[key] = paleoData_zscores_hom  <pre>(23, 151)\nReal intersect after homogenising resolution: \nINTERSECT: 1902-1970\n</pre> In\u00a0[21]: Copied! <pre>covariance, overlap = uta.calc_covariance_matrix(df_proxy)\n</pre> covariance, overlap = uta.calc_covariance_matrix(df_proxy) <pre>short records :  [35. 35. 36. 35. 35. 35. 36. 35.]\n</pre> In\u00a0[22]: Copied! <pre>eigenvalues, eigenvectors = uta.PCA(covariance)\n\nfoev[key] = uta.fraction_of_explained_var(covariance, eigenvalues, n_recs, key, df.name)\n</pre> eigenvalues, eigenvectors = uta.PCA(covariance)  foev[key] = uta.fraction_of_explained_var(covariance, eigenvalues, n_recs, key, df.name) <pre>saved figure in /figs/dod2k_v2.0_filtered_M_TM/foev_Wood_d18O.pdf\n</pre> In\u00a0[23]: Copied! <pre>PCs[key], EOFs[key] = uplt.plot_PCs(years_hom, eigenvectors, paleoData_zscores_hom, key, df.name)\n</pre> PCs[key], EOFs[key] = uplt.plot_PCs(years_hom, eigenvectors, paleoData_zscores_hom, key, df.name) <pre>saved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0_filtered_M_TM/PCs_Wood_d18O.pdf\nsaved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0_filtered_M_TM/EOFloading_Wood_d18O.pdf\n</pre> In\u00a0[24]: Copied! <pre>print('Finished for %s'%key)\n</pre> print('Finished for %s'%key) <pre>Finished for Wood_d18O\n</pre> In\u00a0[25]: Copied! <pre># (1) filter for archiveType and/or paleoData_proxy: \n\nat = 'Coral'\npt = 'd18O'\nkey = '%s_%s'%(at, pt)\n\nkeys += [key]\n\ndf_proxy = df.copy().loc[(df['archiveType']==at)].loc[(df['paleoData_proxy']==pt)] # filter records for specific archive and proxy type\n\nn_recs = len(df_proxy) # number of records\nprint('n_records   : ', n_recs)\n\nprint('archive type: ', set(df_proxy['archiveType']))\nprint('proxy type:   ', set(df_proxy['paleoData_proxy']))\n\n# (2) plot the spatial distribution of records\ngeo_fig, col = uplt.geo_plot(df_proxy, return_col=True)\n\n# (3) plot the coverage for proxy types and plot resolution\n\nuta.convert_subannual_to_annual_res(df_proxy)\n\ndf_proxy = uta.add_auxvars_plot_summary(df_proxy, key, col=col[at])\n</pre> # (1) filter for archiveType and/or paleoData_proxy:   at = 'Coral' pt = 'd18O' key = '%s_%s'%(at, pt)  keys += [key]  df_proxy = df.copy().loc[(df['archiveType']==at)].loc[(df['paleoData_proxy']==pt)] # filter records for specific archive and proxy type  n_recs = len(df_proxy) # number of records print('n_records   : ', n_recs)  print('archive type: ', set(df_proxy['archiveType'])) print('proxy type:   ', set(df_proxy['paleoData_proxy']))  # (2) plot the spatial distribution of records geo_fig, col = uplt.geo_plot(df_proxy, return_col=True)  # (3) plot the coverage for proxy types and plot resolution  uta.convert_subannual_to_annual_res(df_proxy)  df_proxy = uta.add_auxvars_plot_summary(df_proxy, key, col=col[at])   <pre>n_records   :  67\narchive type:  {'Coral'}\nproxy type:    {'d18O'}\n0 Wood 3650\n1 Speleothem 567\n2 Coral 312\n3 GlacierIce 162\n4 MarineSediment 126\n5 LakeSediment 111\n6 Documents 13\n7 Sclerosponge 6\n8 GroundIce 4\n9 Borehole 3\n10 Other 2\n11 MolluskShell 1\n</pre> In\u00a0[26]: Copied! <pre>#========================= PROXY SPECIFIC: Coral d18O =========================\nminres    = 1                         # homogenised resolution\nmny       = 1750                      # start year of homogenised time coord\nmxy       = 2000                      # end year of homogenised time coord\nnyears    = np.min([90, mxy-mny])    # minimum length of each record\n#====================================================================\n# filter for record length during target period\ndf_proxy = uta.filter_record_length(df_proxy, nyears, mny, mxy)\n\n# filter for resolution\ndf_proxy = uta.filter_resolution(df_proxy, minres)\n\n# plot coverage and resolution\nuplt.plot_coverage_analysis(df_proxy, np.arange(mny, mxy+minres, minres), key, col[at])\nuplt.plot_resolution(df_proxy, key, col=col[at])\nuplt.plot_length(df_proxy, key, col=col[at])\n\nn_recs = len(df_proxy) # final number of records\n\n\nprint(df_proxy[['miny', 'maxy', 'originalDatabase']])\n\npca_rec[key] = df_proxy['datasetId']\n</pre> #========================= PROXY SPECIFIC: Coral d18O ========================= minres    = 1                         # homogenised resolution mny       = 1750                      # start year of homogenised time coord mxy       = 2000                      # end year of homogenised time coord nyears    = np.min([90, mxy-mny])    # minimum length of each record #==================================================================== # filter for record length during target period df_proxy = uta.filter_record_length(df_proxy, nyears, mny, mxy)  # filter for resolution df_proxy = uta.filter_resolution(df_proxy, minres)  # plot coverage and resolution uplt.plot_coverage_analysis(df_proxy, np.arange(mny, mxy+minres, minres), key, col[at]) uplt.plot_resolution(df_proxy, key, col=col[at]) uplt.plot_length(df_proxy, key, col=col[at])  n_recs = len(df_proxy) # final number of records   print(df_proxy[['miny', 'maxy', 'originalDatabase']])  pca_rec[key] = df_proxy['datasetId'] <pre>Keep 25 records with nyears&gt;=90 during 1750-2000. Exclude 42 records.\nKeep 24 records with resolution &lt;=1. Exclude 1 records.\n</pre> <pre>        miny    maxy     originalDatabase\n1068  1860.0  1990.0  CoralHydro2k v1.0.1\n1069  1887.0  2011.0  CoralHydro2k v1.0.1\n1072  1726.0  1996.0  CoralHydro2k v1.0.1\n1075  1905.0  2016.0  CoralHydro2k v1.0.1\n1084  1899.0  2008.0  CoralHydro2k v1.0.1\n1085  1780.0  1997.0  CoralHydro2k v1.0.1\n1086  1884.0  1993.0  CoralHydro2k v1.0.1\n1090  1824.0  2016.0  CoralHydro2k v1.0.1\n1094  1896.0  1998.0  CoralHydro2k v1.0.1\n1097  1819.0  1998.0  CoralHydro2k v1.0.1\n1102  1520.0  2011.0  CoralHydro2k v1.0.1\n1103  1873.0  1994.0  CoralHydro2k v1.0.1\n1110  1899.0  1996.0  CoralHydro2k v1.0.1\n1116  1751.0  1994.0  CoralHydro2k v1.0.1\n1118  1882.0  1994.0  CoralHydro2k v1.0.1\n1122  1781.0  1998.0  CoralHydro2k v1.0.1\n1123  1808.0  2009.0  CoralHydro2k v1.0.1\n1124  1852.0  1990.0  CoralHydro2k v1.0.1\n1125  1782.0  1990.0  CoralHydro2k v1.0.1\n1127  1846.0  1995.0  CoralHydro2k v1.0.1\n1148  1899.0  1996.0         Iso2k v1.1.2\n1162  1886.0  1998.0         Iso2k v1.1.2\n1166  1824.0  1985.0         Iso2k v1.1.2\n1170  1751.0  1986.0         Iso2k v1.1.2\n</pre> In\u00a0[27]: Copied! <pre># add 'z-scores' to dataframe and plot z-scores and values\ndf_proxy = uta.add_zscores_plot(df_proxy, key, plot_output=True)\n</pre> # add 'z-scores' to dataframe and plot z-scores and values df_proxy = uta.add_zscores_plot(df_proxy, key, plot_output=True)  In\u00a0[28]: Copied! <pre># define new homogenised time coordinate\ndf_proxy, years_hom = uta.homogenise_time(df_proxy, mny, mxy, minres)\ntime[key] = years_hom\n</pre> # define new homogenised time coordinate df_proxy, years_hom = uta.homogenise_time(df_proxy, mny, mxy, minres) time[key] = years_hom <pre>Homogenised time coordinate: 1750-2000 CE\nResolution: [1] years\nINTERSECT: 1905-1985\n</pre> In\u00a0[29]: Copied! <pre># assign the paleoData_values to the non-missing values in the homogenised data array\n\nout = uta.homogenise_data_dimensions(df_proxy, years_hom, key, plot_output=False) # returns list of homogenised paleoData_values and paleoData_zscores\npaleoData_values_hom, paleoData_zscores_hom, year_hom_avbl, zsco_hom_avbl = out\n\n# define new columns in df_filt\nnew_columns = {'year_hom': [years_hom]*n_recs, \n               'year_hom_avbl': year_hom_avbl, \n               'paleoData_values_hom': [paleoData_values_hom[ii, :] for ii in range(n_recs)], \n               'paleoData_zscores_hom': [paleoData_zscores_hom[ii, :] for ii in range(n_recs)], \n               'paleoData_zscores_hom_avbl': zsco_hom_avbl}\ndf_proxy = df_proxy.assign(**new_columns)\n\nprint('Real intersect after homogenising resolution: ')\nintrsct = uta.find_shared_period(df_proxy, minmax=(mny, mxy), time='year_hom_avbl', \n                             data='paleoData_zscores_hom_avbl')\ndata[key] = paleoData_zscores_hom\n</pre> # assign the paleoData_values to the non-missing values in the homogenised data array  out = uta.homogenise_data_dimensions(df_proxy, years_hom, key, plot_output=False) # returns list of homogenised paleoData_values and paleoData_zscores paleoData_values_hom, paleoData_zscores_hom, year_hom_avbl, zsco_hom_avbl = out  # define new columns in df_filt new_columns = {'year_hom': [years_hom]*n_recs,                 'year_hom_avbl': year_hom_avbl,                 'paleoData_values_hom': [paleoData_values_hom[ii, :] for ii in range(n_recs)],                 'paleoData_zscores_hom': [paleoData_zscores_hom[ii, :] for ii in range(n_recs)],                 'paleoData_zscores_hom_avbl': zsco_hom_avbl} df_proxy = df_proxy.assign(**new_columns)  print('Real intersect after homogenising resolution: ') intrsct = uta.find_shared_period(df_proxy, minmax=(mny, mxy), time='year_hom_avbl',                               data='paleoData_zscores_hom_avbl') data[key] = paleoData_zscores_hom  <pre>(24, 251)\nReal intersect after homogenising resolution: \nINTERSECT: 1905-1985\n</pre> In\u00a0[30]: Copied! <pre>covariance, overlap = uta.calc_covariance_matrix(df_proxy)\n</pre> covariance, overlap = uta.calc_covariance_matrix(df_proxy) <pre>short records :  []\n</pre> In\u00a0[31]: Copied! <pre>eigenvalues, eigenvectors = uta.PCA(covariance)\n\nfoev[key] = uta.fraction_of_explained_var(covariance, eigenvalues, n_recs, key, df.name, col=col[at])\n</pre> eigenvalues, eigenvectors = uta.PCA(covariance)  foev[key] = uta.fraction_of_explained_var(covariance, eigenvalues, n_recs, key, df.name, col=col[at]) <pre>saved figure in /figs/dod2k_v2.0_filtered_M_TM/foev_Coral_d18O.pdf\n</pre> In\u00a0[32]: Copied! <pre>PCs[key], EOFs[key] = uplt.plot_PCs(years_hom, eigenvectors, paleoData_zscores_hom, key, df.name, col=col[at])\n</pre> PCs[key], EOFs[key] = uplt.plot_PCs(years_hom, eigenvectors, paleoData_zscores_hom, key, df.name, col=col[at]) <pre>saved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0_filtered_M_TM/PCs_Coral_d18O.pdf\nsaved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0_filtered_M_TM/EOFloading_Coral_d18O.pdf\n</pre> In\u00a0[33]: Copied! <pre>print('Finished for %s'%key)\n</pre> print('Finished for %s'%key) <pre>Finished for Coral_d18O\n</pre> In\u00a0[59]: Copied! <pre># (1) filter for archiveType and/or paleoData_proxy: \n\nat = 'Speleothem'\npt = 'd18O'\nkey = '%s_%s'%(at, pt)\n\nkeys += [key]\n\ndf_proxy = df.copy().loc[(df['archiveType']==at)].loc[(df['paleoData_proxy']==pt)] # filter records for specific archive and proxy type\n\nn_recs = len(df_proxy) # number of records\nprint('n_records   : ', n_recs)\n\nprint('archive type: ', set(df_proxy['archiveType']))\nprint('proxy type:   ', set(df_proxy['paleoData_proxy']))\n\n# (2) plot the spatial distribution of records\ngeo_fig, col = uplt.geo_plot(df_proxy, return_col=True)\n\n# (3) plot the coverage for proxy types and plot resolution\n\nuta.convert_subannual_to_annual_res(df_proxy)\n\ndf_proxy = uta.add_auxvars_plot_summary(df_proxy, key, col=col[at])\n</pre> # (1) filter for archiveType and/or paleoData_proxy:   at = 'Speleothem' pt = 'd18O' key = '%s_%s'%(at, pt)  keys += [key]  df_proxy = df.copy().loc[(df['archiveType']==at)].loc[(df['paleoData_proxy']==pt)] # filter records for specific archive and proxy type  n_recs = len(df_proxy) # number of records print('n_records   : ', n_recs)  print('archive type: ', set(df_proxy['archiveType'])) print('proxy type:   ', set(df_proxy['paleoData_proxy']))  # (2) plot the spatial distribution of records geo_fig, col = uplt.geo_plot(df_proxy, return_col=True)  # (3) plot the coverage for proxy types and plot resolution  uta.convert_subannual_to_annual_res(df_proxy)  df_proxy = uta.add_auxvars_plot_summary(df_proxy, key, col=col[at])   <pre>n_records   :  210\narchive type:  {'Speleothem'}\nproxy type:    {'d18O'}\n0 Wood 3650\n1 Speleothem 567\n2 Coral 312\n3 GlacierIce 162\n4 MarineSediment 126\n5 LakeSediment 111\n6 Documents 13\n7 Sclerosponge 6\n8 GroundIce 4\n9 Borehole 3\n10 Other 2\n11 MolluskShell 1\n</pre> In\u00a0[60]: Copied! <pre>#========================= PROXY SPECIFIC: Speleothem d18O =========================\nminres    = 101                         # homogenised resolution\nmny       = 1000                      # start year of homogenised time coord\nmxy       = 1400                      # end year of homogenised time coord\nnyears    = np.min([200, mxy-mny])    # minimum length of each record\n#====================================================================\n\n# filter for record length during target period\ndf_proxy = uta.filter_record_length(df_proxy, nyears, mny, mxy)\n\n# filter for resolution\ndf_proxy = uta.filter_resolution(df_proxy, minres)\n\n# plot coverage and resolution\nuplt.plot_coverage_analysis(df_proxy, np.arange(mny, mxy+minres, minres), key, col[at])\nuplt.plot_resolution(df_proxy, key, col=col[at])\nuplt.plot_length(df_proxy, key, col=col[at])\n\nn_recs = len(df_proxy) # final number of records\n\n\nprint(df_proxy[['miny', 'maxy', 'originalDatabase']])\n\npca_rec[key] = df_proxy['datasetId']\n</pre> #========================= PROXY SPECIFIC: Speleothem d18O ========================= minres    = 101                         # homogenised resolution mny       = 1000                      # start year of homogenised time coord mxy       = 1400                      # end year of homogenised time coord nyears    = np.min([200, mxy-mny])    # minimum length of each record #====================================================================  # filter for record length during target period df_proxy = uta.filter_record_length(df_proxy, nyears, mny, mxy)  # filter for resolution df_proxy = uta.filter_resolution(df_proxy, minres)  # plot coverage and resolution uplt.plot_coverage_analysis(df_proxy, np.arange(mny, mxy+minres, minres), key, col[at]) uplt.plot_resolution(df_proxy, key, col=col[at]) uplt.plot_length(df_proxy, key, col=col[at])  n_recs = len(df_proxy) # final number of records   print(df_proxy[['miny', 'maxy', 'originalDatabase']])  pca_rec[key] = df_proxy['datasetId'] <pre>Keep 12 records with nyears&gt;=200 during 1000-1400. Exclude 198 records.\nKeep 12 records with resolution &lt;=101. Exclude 0 records.\n</pre> <pre>        miny    maxy originalDatabase\n1136   624.0  1562.0     Iso2k v1.1.2\n1256   790.0  1953.0         SISAL v3\n1261   491.0  1860.0         SISAL v3\n1279     1.0  2005.0         SISAL v3\n1290   949.0  1957.0         SISAL v3\n1296  1076.0  2007.0         SISAL v3\n1317  1123.0  2010.0         SISAL v3\n1325   746.0  2000.0         SISAL v3\n1352   851.0  2007.0         SISAL v3\n1390   920.0  1699.0         SISAL v3\n1414  1035.0  2006.0         SISAL v3\n1415     5.0  1357.0         SISAL v3\n</pre> In\u00a0[61]: Copied! <pre># add 'z-scores' to dataframe and plot z-scores and values\ndf_proxy = uta.add_zscores_plot(df_proxy, key, plot_output=True)\n</pre> # add 'z-scores' to dataframe and plot z-scores and values df_proxy = uta.add_zscores_plot(df_proxy, key, plot_output=True)  In\u00a0[62]: Copied! <pre># define new homogenised time coordinate\ndf_proxy, years_hom = uta.homogenise_time(df_proxy, mny, mxy, minres)\ntime[key] = years_hom\n</pre> # define new homogenised time coordinate df_proxy, years_hom = uta.homogenise_time(df_proxy, mny, mxy, minres) time[key] = years_hom <pre>Homogenised time coordinate: 1000-1404 CE\nResolution: [101] years\nINTERSECT: 1133-1326\n</pre> In\u00a0[63]: Copied! <pre># assign the paleoData_values to the non-missing values in the homogenised data array\n\nout = uta.homogenise_data_dimensions(df_proxy, years_hom, key, plot_output=False) # returns list of homogenised paleoData_values and paleoData_zscores\npaleoData_values_hom, paleoData_zscores_hom, year_hom_avbl, zsco_hom_avbl = out\n\n# define new columns in df_filt\nnew_columns = {'year_hom': [years_hom]*n_recs, \n               'year_hom_avbl': year_hom_avbl, \n               'paleoData_values_hom': [paleoData_values_hom[ii, :] for ii in range(n_recs)], \n               'paleoData_zscores_hom': [paleoData_zscores_hom[ii, :] for ii in range(n_recs)], \n               'paleoData_zscores_hom_avbl': zsco_hom_avbl}\ndf_proxy = df_proxy.assign(**new_columns)\n\nprint('Real intersect after homogenising resolution: ')\nintrsct = uta.find_shared_period(df_proxy, minmax=(mny, mxy), time='year_hom_avbl', \n                             data='paleoData_zscores_hom_avbl')\ndata[key] = paleoData_zscores_hom\n</pre> # assign the paleoData_values to the non-missing values in the homogenised data array  out = uta.homogenise_data_dimensions(df_proxy, years_hom, key, plot_output=False) # returns list of homogenised paleoData_values and paleoData_zscores paleoData_values_hom, paleoData_zscores_hom, year_hom_avbl, zsco_hom_avbl = out  # define new columns in df_filt new_columns = {'year_hom': [years_hom]*n_recs,                 'year_hom_avbl': year_hom_avbl,                 'paleoData_values_hom': [paleoData_values_hom[ii, :] for ii in range(n_recs)],                 'paleoData_zscores_hom': [paleoData_zscores_hom[ii, :] for ii in range(n_recs)],                 'paleoData_zscores_hom_avbl': zsco_hom_avbl} df_proxy = df_proxy.assign(**new_columns)  print('Real intersect after homogenising resolution: ') intrsct = uta.find_shared_period(df_proxy, minmax=(mny, mxy), time='year_hom_avbl',                               data='paleoData_zscores_hom_avbl') data[key] = paleoData_zscores_hom  <pre>(12, 5)\nReal intersect after homogenising resolution: \nINTERSECT: 1202-1404\n</pre> In\u00a0[64]: Copied! <pre>covariance, overlap = uta.calc_covariance_matrix(df_proxy)\n</pre> covariance, overlap = uta.calc_covariance_matrix(df_proxy) <pre>short records :  [5. 5. 5. 5. 5. 4. 3. 5. 5. 5. 4. 5. 5. 5. 5. 5. 5. 4. 3. 5. 5. 5. 4. 5.\n 5. 5. 5. 5. 5. 4. 3. 5. 5. 5. 4. 5. 5. 5. 5. 5. 5. 4. 3. 5. 5. 5. 4. 5.\n 5. 5. 5. 5. 5. 4. 3. 5. 5. 5. 4. 5. 4. 4. 4. 4. 4. 4. 3. 4. 4. 4. 4. 4.\n 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 5. 5. 5. 5. 5. 4. 3. 5. 5. 5. 4. 5.\n 5. 5. 5. 5. 5. 4. 3. 5. 5. 5. 4. 5. 5. 5. 5. 5. 5. 4. 3. 5. 5. 5. 4. 5.\n 4. 4. 4. 4. 4. 4. 3. 4. 4. 4. 4. 4. 5. 5. 5. 5. 5. 4. 3. 5. 5. 5. 4. 5.]\n</pre> In\u00a0[65]: Copied! <pre>eigenvalues, eigenvectors = uta.PCA(covariance)\n\nfoev[key] = uta.fraction_of_explained_var(covariance, eigenvalues, n_recs, key, df.name, col=col[at])\n</pre> eigenvalues, eigenvectors = uta.PCA(covariance)  foev[key] = uta.fraction_of_explained_var(covariance, eigenvalues, n_recs, key, df.name, col=col[at]) <pre>saved figure in /figs/dod2k_v2.0_filtered_M_TM/foev_Speleothem_d18O.pdf\n</pre> In\u00a0[66]: Copied! <pre>PCs[key], EOFs[key] = uplt.plot_PCs(years_hom, eigenvectors, paleoData_zscores_hom, key, df.name, col=col[at])\n</pre> PCs[key], EOFs[key] = uplt.plot_PCs(years_hom, eigenvectors, paleoData_zscores_hom, key, df.name, col=col[at]) <pre>saved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0_filtered_M_TM/PCs_Speleothem_d18O.pdf\nsaved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0_filtered_M_TM/EOFloading_Speleothem_d18O.pdf\n</pre> In\u00a0[67]: Copied! <pre>print('Finished for %s'%key)\n</pre> print('Finished for %s'%key) <pre>Finished for Speleothem_d18O\n</pre> In\u00a0[68]: Copied! <pre># (1) filter for archiveType and/or paleoData_proxy: \n\nat = 'LakeSediment'\npt  = 'd18O+dD'\nkey = '%s_%s'%(at, pt)\n\nkeys += [key]\n\ndf_proxy = df.loc[(df['archiveType']==at)].loc[(df['paleoData_proxy']==pt.split('+')[0])|(df['paleoData_proxy']==pt.split('+')[1])]\n\nn_recs = len(df_proxy) # number of records\nprint('n_records   : ', n_recs)\n\nprint('archive type: ', set(df_proxy['archiveType']))\nprint('proxy type:   ', set(df_proxy['paleoData_proxy']))\n\n# (2) plot the spatial distribution of records\ngeo_fig, col = uplt.geo_plot(df_proxy, return_col=True)\n\n# (3) plot the coverage for proxy types and plot resolution\n\nuta.convert_subannual_to_annual_res(df_proxy)\n\ndf_proxy = uta.add_auxvars_plot_summary(df_proxy, key, col=col[at])\n</pre> # (1) filter for archiveType and/or paleoData_proxy:   at = 'LakeSediment' pt  = 'd18O+dD' key = '%s_%s'%(at, pt)  keys += [key]  df_proxy = df.loc[(df['archiveType']==at)].loc[(df['paleoData_proxy']==pt.split('+')[0])|(df['paleoData_proxy']==pt.split('+')[1])]  n_recs = len(df_proxy) # number of records print('n_records   : ', n_recs)  print('archive type: ', set(df_proxy['archiveType'])) print('proxy type:   ', set(df_proxy['paleoData_proxy']))  # (2) plot the spatial distribution of records geo_fig, col = uplt.geo_plot(df_proxy, return_col=True)  # (3) plot the coverage for proxy types and plot resolution  uta.convert_subannual_to_annual_res(df_proxy)  df_proxy = uta.add_auxvars_plot_summary(df_proxy, key, col=col[at])   <pre>n_records   :  35\narchive type:  {'LakeSediment'}\nproxy type:    {'dD', 'd18O'}\n0 Wood 3650\n1 Speleothem 567\n2 Coral 312\n3 GlacierIce 162\n4 MarineSediment 126\n5 LakeSediment 111\n6 Documents 13\n7 Sclerosponge 6\n8 GroundIce 4\n9 Borehole 3\n10 Other 2\n11 MolluskShell 1\n</pre> In\u00a0[69]: Copied! <pre>#========================= PROXY SPECIFIC: LakeSediment d18O+dD =========================\nminres    = 55                         # homogenised resolution\nmny       = 300                      # start year of homogenised time coord\nmxy       = 1800                      # end year of homogenised time coord\nnyears    = np.min([100, mxy-mny])    # minimum length of each record\n#====================================================================\n\n# filter for record length during target period\ndf_proxy = uta.filter_record_length(df_proxy, nyears, mny, mxy)\n\n# filter for resolution\ndf_proxy = uta.filter_resolution(df_proxy, minres)\n\n# plot coverage and resolution\nuplt.plot_coverage_analysis(df_proxy, np.arange(mny, mxy+minres, minres), key, col[at])\nuplt.plot_resolution(df_proxy, key, col=col[at])\nuplt.plot_length(df_proxy, key, col=col[at])\n\nn_recs = len(df_proxy) # final number of records\n\n\nprint(df_proxy[['miny', 'maxy', 'originalDatabase']])\n\npca_rec[key] = df_proxy['datasetId']\n</pre> #========================= PROXY SPECIFIC: LakeSediment d18O+dD ========================= minres    = 55                         # homogenised resolution mny       = 300                      # start year of homogenised time coord mxy       = 1800                      # end year of homogenised time coord nyears    = np.min([100, mxy-mny])    # minimum length of each record #====================================================================  # filter for record length during target period df_proxy = uta.filter_record_length(df_proxy, nyears, mny, mxy)  # filter for resolution df_proxy = uta.filter_resolution(df_proxy, minres)  # plot coverage and resolution uplt.plot_coverage_analysis(df_proxy, np.arange(mny, mxy+minres, minres), key, col[at]) uplt.plot_resolution(df_proxy, key, col=col[at]) uplt.plot_length(df_proxy, key, col=col[at])  n_recs = len(df_proxy) # final number of records   print(df_proxy[['miny', 'maxy', 'originalDatabase']])  pca_rec[key] = df_proxy['datasetId'] <pre>Keep 10 records with nyears&gt;=100 during 300-1800. Exclude 25 records.\nKeep 9 records with resolution &lt;=55. Exclude 1 records.\n</pre> <pre>       miny    maxy originalDatabase\n1132    7.0  2007.0     Iso2k v1.1.2\n1151    2.0  2009.0     Iso2k v1.1.2\n1158  276.0  2001.0     Iso2k v1.1.2\n1192  958.0  1940.0     Iso2k v1.1.2\n1194  341.0  2004.0     Iso2k v1.1.2\n1202    2.0  1842.0     Iso2k v1.1.2\n1203   13.0  1963.0     Iso2k v1.1.2\n1210  500.0  2000.0     Iso2k v1.1.2\n1222  500.0  2000.0     Iso2k v1.1.2\n</pre> In\u00a0[70]: Copied! <pre># add 'z-scores' to dataframe and plot z-scores and values\ndf_proxy = uta.add_zscores_plot(df_proxy, key, plot_output=True)\n</pre> # add 'z-scores' to dataframe and plot z-scores and values df_proxy = uta.add_zscores_plot(df_proxy, key, plot_output=True)  In\u00a0[71]: Copied! <pre># define new homogenised time coordinate\ndf_proxy, years_hom = uta.homogenise_time(df_proxy, mny, mxy, minres)\ntime[key] = years_hom\n</pre> # define new homogenised time coordinate df_proxy, years_hom = uta.homogenise_time(df_proxy, mny, mxy, minres) time[key] = years_hom <pre>Homogenised time coordinate: 300-1840 CE\nResolution: [55] years\nNo shared period across all records.\n</pre> In\u00a0[72]: Copied! <pre># assign the paleoData_values to the non-missing values in the homogenised data array\n\nout = uta.homogenise_data_dimensions(df_proxy, years_hom, key, plot_output=False) # returns list of homogenised paleoData_values and paleoData_zscores\npaleoData_values_hom, paleoData_zscores_hom, year_hom_avbl, zsco_hom_avbl = out\n\n# define new columns in df_filt\nnew_columns = {'year_hom': [years_hom]*n_recs, \n               'year_hom_avbl': year_hom_avbl, \n               'paleoData_values_hom': [paleoData_values_hom[ii, :] for ii in range(n_recs)], \n               'paleoData_zscores_hom': [paleoData_zscores_hom[ii, :] for ii in range(n_recs)], \n               'paleoData_zscores_hom_avbl': zsco_hom_avbl}\ndf_proxy = df_proxy.assign(**new_columns)\n\nprint('Real intersect after homogenising resolution: ')\nintrsct = uta.find_shared_period(df_proxy, minmax=(mny, mxy), time='year_hom_avbl', \n                             data='paleoData_zscores_hom_avbl')\ndata[key] = paleoData_zscores_hom\n</pre> # assign the paleoData_values to the non-missing values in the homogenised data array  out = uta.homogenise_data_dimensions(df_proxy, years_hom, key, plot_output=False) # returns list of homogenised paleoData_values and paleoData_zscores paleoData_values_hom, paleoData_zscores_hom, year_hom_avbl, zsco_hom_avbl = out  # define new columns in df_filt new_columns = {'year_hom': [years_hom]*n_recs,                 'year_hom_avbl': year_hom_avbl,                 'paleoData_values_hom': [paleoData_values_hom[ii, :] for ii in range(n_recs)],                 'paleoData_zscores_hom': [paleoData_zscores_hom[ii, :] for ii in range(n_recs)],                 'paleoData_zscores_hom_avbl': zsco_hom_avbl} df_proxy = df_proxy.assign(**new_columns)  print('Real intersect after homogenising resolution: ') intrsct = uta.find_shared_period(df_proxy, minmax=(mny, mxy), time='year_hom_avbl',                               data='paleoData_zscores_hom_avbl') data[key] = paleoData_zscores_hom  <pre>(9, 29)\nReal intersect after homogenising resolution: \nINTERSECT: 960-1840\n</pre> In\u00a0[73]: Copied! <pre>covariance, overlap = uta.calc_covariance_matrix(df_proxy)\n</pre> covariance, overlap = uta.calc_covariance_matrix(df_proxy) <pre>short records :  [29. 29. 29. 17. 28. 29. 29. 25. 25. 29. 29. 29. 17. 28. 29. 29. 25. 25.\n 29. 29. 29. 17. 28. 29. 29. 25. 25. 17. 17. 17. 17. 17. 17. 17. 17. 17.\n 28. 28. 28. 17. 28. 28. 28. 25. 25. 29. 29. 29. 17. 28. 29. 29. 25. 25.\n 29. 29. 29. 17. 28. 29. 29. 25. 25. 25. 25. 25. 17. 25. 25. 25. 25. 25.\n 25. 25. 25. 17. 25. 25. 25. 25. 25.]\n</pre> In\u00a0[74]: Copied! <pre>eigenvalues, eigenvectors = uta.PCA(covariance)\n\nfoev[key] = uta.fraction_of_explained_var(covariance, eigenvalues, n_recs, key, df.name, col=col[at])\n</pre> eigenvalues, eigenvectors = uta.PCA(covariance)  foev[key] = uta.fraction_of_explained_var(covariance, eigenvalues, n_recs, key, df.name, col=col[at]) <pre>saved figure in /figs/dod2k_v2.0_filtered_M_TM/foev_LakeSediment_d18O+dD.pdf\n</pre> In\u00a0[75]: Copied! <pre>PCs[key], EOFs[key] = uplt.plot_PCs(years_hom, eigenvectors, paleoData_zscores_hom, key, df.name, col=col[at])\n</pre> PCs[key], EOFs[key] = uplt.plot_PCs(years_hom, eigenvectors, paleoData_zscores_hom, key, df.name, col=col[at]) <pre>saved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0_filtered_M_TM/PCs_LakeSediment_d18O+dD.pdf\nsaved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0_filtered_M_TM/EOFloading_LakeSediment_d18O+dD.pdf\n</pre> In\u00a0[76]: Copied! <pre>print('Finished for %s'%key)\n</pre> print('Finished for %s'%key) <pre>Finished for LakeSediment_d18O+dD\n</pre> In\u00a0[77]: Copied! <pre># # (1) filter for archiveType and/or paleoData_proxy: \n\n# at = 'MarineSediment'\n# pt = 'd18O'\n# key = '%s_%s'%(at, pt)\n\n# keys += [key]\n\n# df_proxy = df.copy().loc[(df['archiveType']==at)].loc[(df['paleoData_proxy']==pt)] # filter records for specific archive and proxy type\n\n# n_recs = len(df_proxy) # number of records\n# print('n_records   : ', n_recs)\n\n# print('archive type: ', set(df_proxy['archiveType']))\n# print('proxy type:   ', set(df_proxy['paleoData_proxy']))\n\n# # (2) plot the spatial distribution of records\n# geo_fig, col = uplt.geo_plot(df_proxy, return_col=True)\n\n# # (3) plot the coverage for proxy types and plot resolution\n\n# uta.convert_subannual_to_annual_res(df_proxy)\n\n# df_proxy = uta.add_auxvars_plot_summary(df_proxy, key, col=col[at])\n</pre> # # (1) filter for archiveType and/or paleoData_proxy:   # at = 'MarineSediment' # pt = 'd18O' # key = '%s_%s'%(at, pt)  # keys += [key]  # df_proxy = df.copy().loc[(df['archiveType']==at)].loc[(df['paleoData_proxy']==pt)] # filter records for specific archive and proxy type  # n_recs = len(df_proxy) # number of records # print('n_records   : ', n_recs)  # print('archive type: ', set(df_proxy['archiveType'])) # print('proxy type:   ', set(df_proxy['paleoData_proxy']))  # # (2) plot the spatial distribution of records # geo_fig, col = uplt.geo_plot(df_proxy, return_col=True)  # # (3) plot the coverage for proxy types and plot resolution  # uta.convert_subannual_to_annual_res(df_proxy)  # df_proxy = uta.add_auxvars_plot_summary(df_proxy, key, col=col[at])   In\u00a0[78]: Copied! <pre># #========================= PROXY SPECIFIC: MarineSediment d18O =========================\n# minres    = 100                         # homogenised resolution\n# mny       = 100                      # start year of homogenised time coord\n# mxy       = 1800                      # end year of homogenised time coord\n# nyears    = np.min([100, mxy-mny])    # minimum length of each record\n# #====================================================================\n\n# # filter for record length during target period\n# df_proxy = uta.filter_record_length(df_proxy, nyears, mny, mxy)\n\n# # filter for resolution\n# df_proxy = uta.filter_resolution(df_proxy, minres)\n\n# # plot coverage and resolution\n# uplt.plot_coverage_analysis(df_proxy, np.arange(mny, mxy+minres, minres), key, col[at])\n# uplt.plot_resolution(df_proxy, key)\n# uplt.plot_length(df_proxy, key)\n\n# n_recs = len(df_proxy) # final number of records\n\n\n# print(df_proxy[['miny', 'maxy', 'originalDatabase']])\n\n# pca_rec[key] = df_proxy['datasetId']\n</pre> # #========================= PROXY SPECIFIC: MarineSediment d18O ========================= # minres    = 100                         # homogenised resolution # mny       = 100                      # start year of homogenised time coord # mxy       = 1800                      # end year of homogenised time coord # nyears    = np.min([100, mxy-mny])    # minimum length of each record # #====================================================================  # # filter for record length during target period # df_proxy = uta.filter_record_length(df_proxy, nyears, mny, mxy)  # # filter for resolution # df_proxy = uta.filter_resolution(df_proxy, minres)  # # plot coverage and resolution # uplt.plot_coverage_analysis(df_proxy, np.arange(mny, mxy+minres, minres), key, col[at]) # uplt.plot_resolution(df_proxy, key) # uplt.plot_length(df_proxy, key)  # n_recs = len(df_proxy) # final number of records   # print(df_proxy[['miny', 'maxy', 'originalDatabase']])  # pca_rec[key] = df_proxy['datasetId'] In\u00a0[79]: Copied! <pre># # add 'z-scores' to dataframe and plot z-scores and values\n# df_proxy = uta.add_zscores_plot(df_proxy, key, plot_output=True)\n</pre> # # add 'z-scores' to dataframe and plot z-scores and values # df_proxy = uta.add_zscores_plot(df_proxy, key, plot_output=True)  In\u00a0[80]: Copied! <pre># # define new homogenised time coordinate\n# df_proxy, years_hom = uta.homogenise_time(df_proxy, mny, mxy, minres)\n# time[key] = years_hom\n</pre> # # define new homogenised time coordinate # df_proxy, years_hom = uta.homogenise_time(df_proxy, mny, mxy, minres) # time[key] = years_hom In\u00a0[81]: Copied! <pre># # assign the paleoData_values to the non-missing values in the homogenised data array\n\n# out = uta.homogenise_data_dimensions(df_proxy, years_hom, key, plot_output=False) # returns list of homogenised paleoData_values and paleoData_zscores\n# paleoData_values_hom, paleoData_zscores_hom, year_hom_avbl, zsco_hom_avbl = out\n\n# # define new columns in df_filt\n# new_columns = {'year_hom': [years_hom]*n_recs, \n#                'year_hom_avbl': year_hom_avbl, \n#                'paleoData_values_hom': [paleoData_values_hom[ii, :] for ii in range(n_recs)], \n#                'paleoData_zscores_hom': [paleoData_zscores_hom[ii, :] for ii in range(n_recs)], \n#                'paleoData_zscores_hom_avbl': zsco_hom_avbl}\n# df_proxy = df_proxy.assign(**new_columns)\n\n# print('Real intersect after homogenising resolution: ')\n# intrsct = uta.find_shared_period(df_proxy, minmax=(mny, mxy), time='year_hom_avbl', \n#                              data='paleoData_zscores_hom_avbl')\n# data[key] = paleoData_zscores_hom\n</pre> # # assign the paleoData_values to the non-missing values in the homogenised data array  # out = uta.homogenise_data_dimensions(df_proxy, years_hom, key, plot_output=False) # returns list of homogenised paleoData_values and paleoData_zscores # paleoData_values_hom, paleoData_zscores_hom, year_hom_avbl, zsco_hom_avbl = out  # # define new columns in df_filt # new_columns = {'year_hom': [years_hom]*n_recs,  #                'year_hom_avbl': year_hom_avbl,  #                'paleoData_values_hom': [paleoData_values_hom[ii, :] for ii in range(n_recs)],  #                'paleoData_zscores_hom': [paleoData_zscores_hom[ii, :] for ii in range(n_recs)],  #                'paleoData_zscores_hom_avbl': zsco_hom_avbl} # df_proxy = df_proxy.assign(**new_columns)  # print('Real intersect after homogenising resolution: ') # intrsct = uta.find_shared_period(df_proxy, minmax=(mny, mxy), time='year_hom_avbl',  #                              data='paleoData_zscores_hom_avbl') # data[key] = paleoData_zscores_hom  In\u00a0[82]: Copied! <pre># covariance, overlap = uta.calc_covariance_matrix(df_proxy)\n</pre> # covariance, overlap = uta.calc_covariance_matrix(df_proxy) In\u00a0[83]: Copied! <pre># eigenvalues, eigenvectors = uta.PCA(covariance)\n\n# foev[key] = uta.fraction_of_explained_var(covariance, eigenvalues, n_recs, key, df.name)\n</pre> # eigenvalues, eigenvectors = uta.PCA(covariance)  # foev[key] = uta.fraction_of_explained_var(covariance, eigenvalues, n_recs, key, df.name) In\u00a0[84]: Copied! <pre># PCs[key], EOFs[key] = uplt.plot_PCs(years_hom, eigenvectors, paleoData_zscores_hom, key, df.name)\n</pre> # PCs[key], EOFs[key] = uplt.plot_PCs(years_hom, eigenvectors, paleoData_zscores_hom, key, df.name) In\u00a0[85]: Copied! <pre># print('Finished for %s'%key)\n</pre> # print('Finished for %s'%key) In\u00a0[86]: Copied! <pre>archive_colour, archives_sorted, proxy_marker = uplt.df_colours_markers()\n</pre> archive_colour, archives_sorted, proxy_marker = uplt.df_colours_markers() <pre>0 Wood 3650\n1 Speleothem 567\n2 Coral 312\n3 GlacierIce 162\n4 MarineSediment 126\n5 LakeSediment 111\n6 Documents 13\n7 Sclerosponge 6\n8 GroundIce 4\n9 Borehole 3\n10 Other 2\n11 MolluskShell 1\n</pre> In\u00a0[87]: Copied! <pre>key_colour = {}\nfor key in keys:\n    key_colour[key]=archive_colour[key.split('_')[0]]\n\nkey_colour['Wood_d18O']='k'\n</pre> key_colour = {} for key in keys:     key_colour[key]=archive_colour[key.split('_')[0]]  key_colour['Wood_d18O']='k' In\u00a0[88]: Copied! <pre># as a comparison plot the mean over all records\nfig = plt.figure(figsize=(9, 4), dpi=300)\nfor key in keys:\n    lw = 1 if key in ['Wood_ring width', 'Wood_d18O', 'Coral_d18O'] else 2\n    plt.plot(time[key], np.ma.mean(data[key], axis=0)-np.mean(np.ma.mean(data[key], axis=0)), \n             label=key, lw=lw, color=key_colour[key])\nplt.ylabel('paleoData_zscores')\nplt.xlabel('year CE')\nplt.legend(ncol=3, bbox_to_anchor=(0,-0.2), loc='upper left', framealpha=0)\nplt.show()\n</pre> # as a comparison plot the mean over all records fig = plt.figure(figsize=(9, 4), dpi=300) for key in keys:     lw = 1 if key in ['Wood_ring width', 'Wood_d18O', 'Coral_d18O'] else 2     plt.plot(time[key], np.ma.mean(data[key], axis=0)-np.mean(np.ma.mean(data[key], axis=0)),               label=key, lw=lw, color=key_colour[key]) plt.ylabel('paleoData_zscores') plt.xlabel('year CE') plt.legend(ncol=3, bbox_to_anchor=(0,-0.2), loc='upper left', framealpha=0) plt.show() In\u00a0[89]: Copied! <pre># as a comparison plot the mean over all records SMOOTHED\nfig = plt.figure(figsize=(9, 4), dpi=300)\nfor key in keys:\n    if key in ['Wood_ring width', 'Wood_d18O', 'Coral_d18O']:\n        tt, dd = uta.smooth(np.ma.mean(data[key], axis=0), time[key], 11)\n        label=key+' 11yr-mean'\n    else:\n        tt, dd = time[key], np.ma.mean(data[key], axis=0)\n        label=key\n    plt.plot(tt, dd, label=label, lw=2, color=key_colour[key])\nplt.ylabel('paleoData_zscores')\nplt.xlabel('year CE')    \nplt.legend(ncol=3, bbox_to_anchor=(0,-0.2), loc='upper left', framealpha=0)\nplt.show()\n</pre> # as a comparison plot the mean over all records SMOOTHED fig = plt.figure(figsize=(9, 4), dpi=300) for key in keys:     if key in ['Wood_ring width', 'Wood_d18O', 'Coral_d18O']:         tt, dd = uta.smooth(np.ma.mean(data[key], axis=0), time[key], 11)         label=key+' 11yr-mean'     else:         tt, dd = time[key], np.ma.mean(data[key], axis=0)         label=key     plt.plot(tt, dd, label=label, lw=2, color=key_colour[key]) plt.ylabel('paleoData_zscores') plt.xlabel('year CE')     plt.legend(ncol=3, bbox_to_anchor=(0,-0.2), loc='upper left', framealpha=0) plt.show() In\u00a0[90]: Copied! <pre>fig = plt.figure(figsize=(7,3), dpi=300)\nax  = plt.gca()\nax2 = ax.twinx()\nfor key in keys:\n    print(key)\n    ax.plot(range(1,len(foev[key][:10])+1), foev[key][:10], label=key, color=key_colour[key])\n    ax2.plot(range(1,len(foev[key][:10])+1), np.cumsum(foev[key][:10]), label=key, ls=':', color=key_colour[key])\nax.set_ylabel('Fraction of explained variance')\n# ax2.set_ylabel('Cumulative fraction of explained variance')\nax.set_xlabel('PC')    \nax.legend(ncol=3, bbox_to_anchor=(0,-0.15), loc='upper left', framealpha=0)\n</pre> fig = plt.figure(figsize=(7,3), dpi=300) ax  = plt.gca() ax2 = ax.twinx() for key in keys:     print(key)     ax.plot(range(1,len(foev[key][:10])+1), foev[key][:10], label=key, color=key_colour[key])     ax2.plot(range(1,len(foev[key][:10])+1), np.cumsum(foev[key][:10]), label=key, ls=':', color=key_colour[key]) ax.set_ylabel('Fraction of explained variance') # ax2.set_ylabel('Cumulative fraction of explained variance') ax.set_xlabel('PC')     ax.legend(ncol=3, bbox_to_anchor=(0,-0.15), loc='upper left', framealpha=0) <pre>Wood_ring width\nWood_d18O\nCoral_d18O\nSpeleothem_d18O\nSpeleothem_d18O\nSpeleothem_d18O\nSpeleothem_d18O\nLakeSediment_d18O+dD\n</pre> Out[90]: <pre>&lt;matplotlib.legend.Legend at 0x7f3792b8d450&gt;</pre> In\u00a0[91]: Copied! <pre># plot PCs of different proxy types on the same axis. \n# Note that these are plotted as calculated in the respective analysis- NOT standardised differently!\n\nfig = plt.figure(figsize=(8,5), dpi=150)\n\ngrid = GS(2,1)\nfor ii in range(2):\n    ax = plt.subplot(grid[ii,:])\n    for key in keys:\n        a=1 if key in ['Wood_ring width', 'Speleothem_d18O', 'LakeSediment_d18O+dD'] else -1\n        label = key+ '\\n ($\\\\ast(-1)$)' if key in ['Wood_ring width', 'Speleothem_d18O', 'LakeSediment_d18O+dD'] else key\n        plt.plot(time[key], a*PCs[key][ii], label=label, color=key_colour[key])\n    if ii==0: \n        ax.axes.xaxis.set_ticklabels([])\n    plt.ylabel('PC %d'%(ii+1)) \nplt.xlabel('year CE')\n\nplt.legend(ncol=3, bbox_to_anchor=(0,-0.2), \n           loc='upper left', framealpha=0)\ngrid.tight_layout(fig)\n</pre> # plot PCs of different proxy types on the same axis.  # Note that these are plotted as calculated in the respective analysis- NOT standardised differently!  fig = plt.figure(figsize=(8,5), dpi=150)  grid = GS(2,1) for ii in range(2):     ax = plt.subplot(grid[ii,:])     for key in keys:         a=1 if key in ['Wood_ring width', 'Speleothem_d18O', 'LakeSediment_d18O+dD'] else -1         label = key+ '\\n ($\\\\ast(-1)$)' if key in ['Wood_ring width', 'Speleothem_d18O', 'LakeSediment_d18O+dD'] else key         plt.plot(time[key], a*PCs[key][ii], label=label, color=key_colour[key])     if ii==0:          ax.axes.xaxis.set_ticklabels([])     plt.ylabel('PC %d'%(ii+1))  plt.xlabel('year CE')  plt.legend(ncol=3, bbox_to_anchor=(0,-0.2),             loc='upper left', framealpha=0) grid.tight_layout(fig)   In\u00a0[92]: Copied! <pre># plot PCs of different proxy types on the same axis. \n# SMOOTHED VIA 11YEAR RUNNING MEAN\nfig = plt.figure(figsize=(8,5), dpi=150)\n\ngrid = GS(2,1)\nfor ii in range(2):\n    ax = plt.subplot(grid[ii,:])\n    for key in keys:\n        label = key\n        if ((key in ['Wood_d18O', 'Coral_d18O'])):\n            a = -1 \n            label+= '\\n ($\\\\ast(-1)$)'\n        else:\n            a = 1\n        resolution = np.unique(np.diff(time[key]))[0]\n        if key in ['Wood_ring width', 'Wood_d18O', 'Coral_d18O']:\n            smooth = 11\n            tt, dd = uta.smooth(PCs[key][ii], time[key], int(smooth/resolution))\n            # label+=' %d-yr mean'%smooth\n        else:\n            tt, dd = time[key], PCs[key][ii]\n        plt.plot(tt, a*np.array(dd), label=label, color=key_colour[key])\n        \n    plt.ylabel('PC %d \\n (%d yr running mean)'%(ii+1, smooth))\n    plt.xlim(1750,2000)\n    if ii==0: \n        ax.axes.xaxis.set_ticklabels([])\n        hh, ll = ax.get_legend_handles_labels()\nplt.xlabel('year CE')\n\ngrid.tight_layout(fig)\n\nplt.legend(hh, ll, ncol=3, bbox_to_anchor=(0,-0.2), \n           loc='upper left', framealpha=0)\nplt.show()\n</pre> # plot PCs of different proxy types on the same axis.  # SMOOTHED VIA 11YEAR RUNNING MEAN fig = plt.figure(figsize=(8,5), dpi=150)  grid = GS(2,1) for ii in range(2):     ax = plt.subplot(grid[ii,:])     for key in keys:         label = key         if ((key in ['Wood_d18O', 'Coral_d18O'])):             a = -1              label+= '\\n ($\\\\ast(-1)$)'         else:             a = 1         resolution = np.unique(np.diff(time[key]))[0]         if key in ['Wood_ring width', 'Wood_d18O', 'Coral_d18O']:             smooth = 11             tt, dd = uta.smooth(PCs[key][ii], time[key], int(smooth/resolution))             # label+=' %d-yr mean'%smooth         else:             tt, dd = time[key], PCs[key][ii]         plt.plot(tt, a*np.array(dd), label=label, color=key_colour[key])              plt.ylabel('PC %d \\n (%d yr running mean)'%(ii+1, smooth))     plt.xlim(1750,2000)     if ii==0:          ax.axes.xaxis.set_ticklabels([])         hh, ll = ax.get_legend_handles_labels() plt.xlabel('year CE')  grid.tight_layout(fig)  plt.legend(hh, ll, ncol=3, bbox_to_anchor=(0,-0.2),             loc='upper left', framealpha=0) plt.show()  In\u00a0[93]: Copied! <pre>fig = plt.figure(figsize=(8,5), dpi=150)\n\ngrid = GS(2,1)\nfor key in keys:\n    n_recs = data[key].shape[0]\n    for ii in range(2):\n        ax = plt.subplot(grid[ii,:])\n        a = -1 if ((key in ['Wood_d18O', 'Coral_d18O'])) else 1\n        label = key+'\\n ($\\\\ast(-1)$)' if ((key in ['Wood_d18O', 'Coral_d18O'])) else key\n        plt.plot(range(n_recs), a*EOFs[key][ii], label=label, color=key_colour[key])\n        if ii==1: plt.xlabel('record')\n        plt.ylabel('EOF %d load'%(ii+1))\n        plt.axhline(0, color='k', alpha=0.5, lw=0.5)\n        if ii==0: \n            ax.axes.xaxis.set_ticklabels([])\n        if ii==0: plt.legend(bbox_to_anchor=(0,1.1), loc='lower left', ncol=3, framealpha=0)\ngrid.tight_layout(fig)\n</pre>  fig = plt.figure(figsize=(8,5), dpi=150)  grid = GS(2,1) for key in keys:     n_recs = data[key].shape[0]     for ii in range(2):         ax = plt.subplot(grid[ii,:])         a = -1 if ((key in ['Wood_d18O', 'Coral_d18O'])) else 1         label = key+'\\n ($\\\\ast(-1)$)' if ((key in ['Wood_d18O', 'Coral_d18O'])) else key         plt.plot(range(n_recs), a*EOFs[key][ii], label=label, color=key_colour[key])         if ii==1: plt.xlabel('record')         plt.ylabel('EOF %d load'%(ii+1))         plt.axhline(0, color='k', alpha=0.5, lw=0.5)         if ii==0:              ax.axes.xaxis.set_ticklabels([])         if ii==0: plt.legend(bbox_to_anchor=(0,1.1), loc='lower left', ncol=3, framealpha=0) grid.tight_layout(fig) In\u00a0[94]: Copied! <pre>fig = uplt.geo_EOF_plot(df, pca_rec, EOFs, keys, barlabel='EOF 1 load', which_EOF=0)\n#print(keys)\nutf.save_fig(fig, 'MT_spatial_EOF1', dir=df.name)\n</pre> fig = uplt.geo_EOF_plot(df, pca_rec, EOFs, keys, barlabel='EOF 1 load', which_EOF=0) #print(keys) utf.save_fig(fig, 'MT_spatial_EOF1', dir=df.name)  <pre>{'Wood_ring width': 1, 'Wood_d18O': 1, 'Coral_d18O': 1, 'Speleothem_d18O': 1, 'LakeSediment_d18O+dD': 1}\n-0.6\n0.6\nsaved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0_filtered_M_TM/MT_spatial_EOF1.pdf\n</pre> In\u00a0[95]: Copied! <pre>fig = uplt.geo_EOF_plot(df, pca_rec, EOFs, keys, barlabel='EOF 2 load', which_EOF=1)\n#print(keys)\nutf.save_fig(fig, 'MT_spatial_EOF2', dir=df.name)\n</pre> fig = uplt.geo_EOF_plot(df, pca_rec, EOFs, keys, barlabel='EOF 2 load', which_EOF=1) #print(keys) utf.save_fig(fig, 'MT_spatial_EOF2', dir=df.name)  <pre>{'Wood_ring width': 1, 'Wood_d18O': 1, 'Coral_d18O': 1, 'Speleothem_d18O': 1, 'LakeSediment_d18O+dD': 1}\n-0.6\n0.6\nsaved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0_filtered_M_TM/MT_spatial_EOF2.pdf\n</pre> In\u00a0[96]: Copied! <pre>fig = uplt.geo_plot(df, fs=(13,8))\n\nutf.save_fig(fig, 'MT_spatial', dir=df.name)\n</pre> fig = uplt.geo_plot(df, fs=(13,8))  utf.save_fig(fig, 'MT_spatial', dir=df.name) <pre>0 Wood 3650\n1 Speleothem 567\n2 Coral 312\n3 GlacierIce 162\n4 MarineSediment 126\n5 LakeSediment 111\n6 Documents 13\n7 Sclerosponge 6\n8 GroundIce 4\n9 Borehole 3\n10 Other 2\n11 MolluskShell 1\nsaved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0_filtered_M_TM/MT_spatial.pdf\n</pre> In\u00a0[97]: Copied! <pre># as a comparison plot the mean over all records SMOOTHED\nfig = plt.figure(figsize=(9, 5), dpi=300)\n\ngrid = GS(2,3)\n\nax=plt.subplot(grid[:,0])\nax  = plt.gca()\n# ax2 = ax.twinx()\nfor key in keys:\n    # ax.plot(range(1,9), foev[key][:8], label=key, color=key_colour[key])\n    ax.plot(range(1,len(foev[key][:10])+1), np.cumsum(foev[key][:10]), label=key, color=key_colour[key], lw=2.5)\nax.set_ylabel('Cumulative fraction of explained variance')\nax.set_xlabel('PC')    \nax.set_xlim(.9,10.1)\nplt.title('A', x=0.1, y=0.91)\n\ngrid.tight_layout(fig)\ngrid.update(hspace=0.1,wspace=0.3)\nplt.legend(ncol=1, bbox_to_anchor=(0.1,0.3), \n           loc='upper left', framealpha=0, fontsize=8, facecolor='white')\n# plt.grid()\n\nsmooth = 11\n            \nfor key in keys:\n    label = key\n    resolution = np.unique(np.diff(time[key]))[0]\n    #print(resolution, smooth, int(smooth/resolution))\n\n    if resolution&lt;smooth:\n        tt, dd = uta.smooth(np.ma.mean(data[key], axis=0), time[key], smooth)\n    else:\n        tt, dd = time[key], np.ma.mean(data[key], axis=0)\n    \n    for ii in range(2):\n        ax = plt.subplot(grid[ii,1:4])\n        # warmer and wetter is more negative in coral d18O; \n        # warmer is more positive and wetter is more negative (maybe?) in tree d18O\n        if ((key in ['Wood_d18O', 'Coral_d18O'])):\n            a = -1 # multiply PC sign by -1\n            label+= ' ($\\\\ast(-1)$)' # and label it as such\n        else:\n            a = 1\n        if resolution&lt;smooth:\n            tt, dd = uta.smooth(PCs[key][ii], time[key], int(smooth/resolution))\n        else:\n            tt, dd = time[key], PCs[key][ii]\n        plt.plot(tt, a*np.array(dd), label=label, color=key_colour[key], lw=1.5)\n        plt.ylabel('PC %d \\n (%d yr MA)'%(ii+1, smooth))\n        plt.grid()\n        if ii==0: \n            ax.axes.xaxis.set_ticklabels([])\n        plt.title('BC'[ii], x=0.05, y=0.8)\n        # plt.xlim(1750, 2010)\n        \nplt.xlabel('year CE')\n\n\nutf.save_fig(fig, 'MT_PCA', dir=df.name)\n</pre> # as a comparison plot the mean over all records SMOOTHED fig = plt.figure(figsize=(9, 5), dpi=300)  grid = GS(2,3)  ax=plt.subplot(grid[:,0]) ax  = plt.gca() # ax2 = ax.twinx() for key in keys:     # ax.plot(range(1,9), foev[key][:8], label=key, color=key_colour[key])     ax.plot(range(1,len(foev[key][:10])+1), np.cumsum(foev[key][:10]), label=key, color=key_colour[key], lw=2.5) ax.set_ylabel('Cumulative fraction of explained variance') ax.set_xlabel('PC')     ax.set_xlim(.9,10.1) plt.title('A', x=0.1, y=0.91)  grid.tight_layout(fig) grid.update(hspace=0.1,wspace=0.3) plt.legend(ncol=1, bbox_to_anchor=(0.1,0.3),             loc='upper left', framealpha=0, fontsize=8, facecolor='white') # plt.grid()  smooth = 11              for key in keys:     label = key     resolution = np.unique(np.diff(time[key]))[0]     #print(resolution, smooth, int(smooth/resolution))      if resolution <pre>saved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0_filtered_M_TM/MT_PCA.pdf\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/analysis_temperature/#analyse-temperature-records","title":"Analyse temperature records\u00b6","text":""},{"location":"notebooks/analysis_temperature/#set-up-working-environment","title":"Set up working environment\u00b6","text":""},{"location":"notebooks/analysis_temperature/#read-already-filtered-dataframe","title":"Read already filtered dataframe\u00b6","text":""},{"location":"notebooks/analysis_temperature/#data-analysis","title":"data analysis\u00b6","text":""},{"location":"notebooks/analysis_temperature/#tree-trw","title":"tree TRW\u00b6","text":""},{"location":"notebooks/analysis_temperature/#filter-archive_type-and-paleodata_proxy","title":"filter archive_type and paleoData_proxy\u00b6","text":""},{"location":"notebooks/analysis_temperature/#filter-for-target-period-resolution-and-minimum-record-length","title":"filter for target period, resolution and minimum record length\u00b6","text":""},{"location":"notebooks/analysis_temperature/#homogenise-data-dimensions-across-the-records","title":"homogenise data dimensions across the records\u00b6","text":""},{"location":"notebooks/analysis_temperature/#pca","title":"PCA\u00b6","text":""},{"location":"notebooks/analysis_temperature/#tree-d18o","title":"tree d18O\u00b6","text":""},{"location":"notebooks/analysis_temperature/#filter-archive_type-and-paleodata_proxy","title":"filter archive_type and paleoData_proxy\u00b6","text":""},{"location":"notebooks/analysis_temperature/#filter-for-target-period-resolution-and-minimum-record-length","title":"filter for target period, resolution and minimum record length\u00b6","text":""},{"location":"notebooks/analysis_temperature/#homogenise-data-dimensions-across-the-records","title":"homogenise data dimensions across the records\u00b6","text":""},{"location":"notebooks/analysis_temperature/#pca","title":"PCA\u00b6","text":""},{"location":"notebooks/analysis_temperature/#coral-d18o","title":"coral d18O\u00b6","text":""},{"location":"notebooks/analysis_temperature/#filter-archive_type-and-paleodata_proxy","title":"filter archive_type and paleoData_proxy\u00b6","text":""},{"location":"notebooks/analysis_temperature/#filter-for-target-period-resolution-and-minimum-record-length","title":"filter for target period, resolution and minimum record length\u00b6","text":""},{"location":"notebooks/analysis_temperature/#homogenise-data-dimensions-across-the-records","title":"homogenise data dimensions across the records\u00b6","text":""},{"location":"notebooks/analysis_temperature/#pca","title":"PCA\u00b6","text":""},{"location":"notebooks/analysis_temperature/#speleothem-d18o","title":"speleothem d18O\u00b6","text":""},{"location":"notebooks/analysis_temperature/#filter-archive_type-and-paleodata_proxy","title":"filter archive_type and paleoData_proxy\u00b6","text":""},{"location":"notebooks/analysis_temperature/#filter-for-target-period-resolution-and-minimum-record-length","title":"filter for target period, resolution and minimum record length\u00b6","text":""},{"location":"notebooks/analysis_temperature/#homogenise-data-dimensions-across-the-records","title":"homogenise data dimensions across the records\u00b6","text":""},{"location":"notebooks/analysis_temperature/#pca","title":"PCA\u00b6","text":""},{"location":"notebooks/analysis_temperature/#lake-sediment-d18o-dd","title":"lake sediment d18O + dD\u00b6","text":""},{"location":"notebooks/analysis_temperature/#filter-archive_type-and-paleodata_proxy","title":"filter archive_type and paleoData_proxy\u00b6","text":""},{"location":"notebooks/analysis_temperature/#filter-for-target-period-resolution-and-minimum-record-length","title":"filter for target period, resolution and minimum record length\u00b6","text":""},{"location":"notebooks/analysis_temperature/#homogenise-data-dimensions-across-the-records","title":"homogenise data dimensions across the records\u00b6","text":""},{"location":"notebooks/analysis_temperature/#pca","title":"PCA\u00b6","text":""},{"location":"notebooks/analysis_temperature/#marine-sediment-d18o","title":"marine sediment d18O\u00b6","text":""},{"location":"notebooks/analysis_temperature/#filter-archive_type-and-paleodata_proxy","title":"filter archive_type and paleoData_proxy\u00b6","text":""},{"location":"notebooks/analysis_temperature/#filter-for-target-period-resolution-and-minimum-record-length","title":"filter for target period, resolution and minimum record length\u00b6","text":""},{"location":"notebooks/analysis_temperature/#homogenise-data-dimensions-across-the-records","title":"homogenise data dimensions across the records\u00b6","text":""},{"location":"notebooks/analysis_temperature/#pca","title":"PCA\u00b6","text":""},{"location":"notebooks/analysis_temperature/#summary-figures","title":"summary figures\u00b6","text":""},{"location":"notebooks/analysis_temperature/#mean-z-score","title":"mean z-score\u00b6","text":""},{"location":"notebooks/analysis_temperature/#fraction-of-explained-variance","title":"Fraction of Explained Variance\u00b6","text":""},{"location":"notebooks/analysis_temperature/#1st-and-2nd-pc","title":"1st and 2nd PC\u00b6","text":""},{"location":"notebooks/analysis_temperature/#1st-and-2nd-eof","title":"1st and 2nd EOF\u00b6","text":""},{"location":"notebooks/analysis_temperature/#spatial-eof-load","title":"spatial EOF load\u00b6","text":""},{"location":"notebooks/analysis_temperature/#spatial-distribution","title":"spatial distribution\u00b6","text":""},{"location":"notebooks/df_filter/","title":"Filter compact dataframe","text":"<p>This file reads the compact dataframes and filters for specific records (e.g. for moisture sensitive records). The filtered dataset is saved in a separate directory and can be loaded for further analysis or plotting etc.</p> <p>Author: Lucie Luecke</p> <p>Date produced: 21/01/2025</p> <p>Input: reads dataframe with the following keys:</p> <ul> <li><code>archiveType</code></li> <li><code>dataSetName</code></li> <li><code>datasetId</code></li> <li><code>geo_meanElev</code></li> <li><code>geo_meanLat</code></li> <li><code>geo_meanLon</code></li> <li><code>geo_siteName</code></li> <li><code>interpretation_direction</code> (new in v2.0)</li> <li><code>interpretation_variable</code></li> <li><code>interpretation_variableDetail</code></li> <li><code>interpretation_seasonality</code> (new in v2.0)</li> <li><code>originalDataURL</code></li> <li><code>originalDatabase</code></li> <li><code>paleoData_notes</code></li> <li><code>paleoData_proxy</code></li> <li><code>paleoData_sensorSpecies</code></li> <li><code>paleoData_units</code></li> <li><code>paleoData_values</code></li> <li><code>paleoData_variableName</code></li> <li><code>year</code></li> <li><code>yearUnits</code></li> <li>(optional: <code>DuplicateDetails</code>)</li> </ul> <p>Make sure the repo_root is added correctly, it should be: your_root_dir/dod2k This should be the working directory throughout this notebook (and all other notebooks).</p> In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n\nimport sys\nimport os\nfrom pathlib import Path\n\n# Add parent directory to path (works from any notebook in notebooks/)\n# the repo_root should be the parent directory of the notebooks folder\ncurrent_dir = Path().resolve()\n# Determine repo root\nif current_dir.name == 'dod2k':\n    repo_root = current_dir\nelif current_dir.parent.name == 'dod2k':\n    repo_root = current_dir.parent\nelse:\n    raise Exception('Please review the repo root structure (see first cell).')\n\n# Update cwd and path only if needed\nif os.getcwd() != str(repo_root):\n    os.chdir(repo_root)\nif str(repo_root) not in sys.path:\n    sys.path.insert(0, str(repo_root))\n\nprint(f\"Repo root: {repo_root}\")\nif str(os.getcwd())==str(repo_root):\n    print(f\"Working directory matches repo root. \")\n</pre> %load_ext autoreload %autoreload 2  import sys import os from pathlib import Path  # Add parent directory to path (works from any notebook in notebooks/) # the repo_root should be the parent directory of the notebooks folder current_dir = Path().resolve() # Determine repo root if current_dir.name == 'dod2k':     repo_root = current_dir elif current_dir.parent.name == 'dod2k':     repo_root = current_dir.parent else:     raise Exception('Please review the repo root structure (see first cell).')  # Update cwd and path only if needed if os.getcwd() != str(repo_root):     os.chdir(repo_root) if str(repo_root) not in sys.path:     sys.path.insert(0, str(repo_root))  print(f\"Repo root: {repo_root}\") if str(os.getcwd())==str(repo_root):     print(f\"Working directory matches repo root. \") <pre>Repo root: /home/jupyter-lluecke/dod2k\nWorking directory matches repo root. \n</pre> In\u00a0[2]: Copied! <pre>import pandas as pd\nimport numpy as np\n\nfrom dod2k_utilities import ut_functions as utf # contains utility functions\n</pre> import pandas as pd import numpy as np  from dod2k_utilities import ut_functions as utf # contains utility functions  <p>Read compact dataframe.</p> <p>{db_name} refers to the database, including e.g.</p> <ul> <li>database of databases:<ul> <li>dod2k_v2.0 (dod2k: duplicate free, merged database)</li> <li>all_merged (NOT filtered for duplicates, only fusion of the input databases)</li> </ul> </li> <li>original databases:<ul> <li>fe23</li> <li>ch2k</li> <li>sisal</li> <li>pages2k</li> <li>iso2k</li> </ul> </li> </ul> <p>All compact dataframes are saved in {repo_root}/data/{db_name} as {db_name}_compact.csv.</p> In\u00a0[3]: Copied! <pre>db_name = 'dod2k_v2.0'\n\ndf = utf.load_compact_dataframe_from_csv(db_name)\nprint(df.originalDatabase.unique())\ndf.name = db_name\nprint(df.info())\n</pre>  db_name = 'dod2k_v2.0'  df = utf.load_compact_dataframe_from_csv(db_name) print(df.originalDatabase.unique()) df.name = db_name print(df.info()) <pre>['PAGES 2k v2.2.0' 'FE23 (Breitenmoser et al. (2014))'\n 'CoralHydro2k v1.0.1' 'Iso2k v1.1.2' 'SISAL v3' 'dod2k_composite_z']\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4957 entries, 0 to 4956\nData columns (total 22 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   archiveType                    4957 non-null   object \n 1   dataSetName                    4957 non-null   object \n 2   datasetId                      4957 non-null   object \n 3   duplicateDetails               4957 non-null   object \n 4   geo_meanElev                   4875 non-null   float32\n 5   geo_meanLat                    4957 non-null   float32\n 6   geo_meanLon                    4957 non-null   float32\n 7   geo_siteName                   4957 non-null   object \n 8   interpretation_direction       4957 non-null   object \n 9   interpretation_seasonality     4957 non-null   object \n 10  interpretation_variable        4957 non-null   object \n 11  interpretation_variableDetail  4957 non-null   object \n 12  originalDataURL                4957 non-null   object \n 13  originalDatabase               4957 non-null   object \n 14  paleoData_notes                4957 non-null   object \n 15  paleoData_proxy                4957 non-null   object \n 16  paleoData_sensorSpecies        4957 non-null   object \n 17  paleoData_units                4957 non-null   object \n 18  paleoData_values               4957 non-null   object \n 19  paleoData_variableName         4957 non-null   object \n 20  year                           4957 non-null   object \n 21  yearUnits                      4957 non-null   object \ndtypes: float32(3), object(19)\nmemory usage: 794.0+ KB\nNone\n</pre> <p>Here you can filter the dataframe for specific record types. Below is an example where we filter for interpretation_variable=temperature.</p> <p>This could be done with any column and any value (e.g. for a specific archive type, etc.)</p> <p>Please look at the examples below which are commented out for future use</p> In\u00a0[4]: Copied! <pre># if you want to filter for specific metadata, e.g. temperature or moisture records, run this:\n\n\n# ---&gt; interpretation_variable\n# e.g.\n\n# # filter for &gt;&gt;moisture&lt;&lt; sensitive records only (also include records which are moisture and temperature sensitive)\n# df_filter = df.loc[(df['interpretation_variable']=='moisture')|(df['interpretation_variable']=='temperature+moisture')]\n# df_filter.name = db_name + \"_filtered_M_TM\" \n\n\n# filter for &gt;&gt;exclusively moisture&lt;&lt; sensitive records only (without t+m)\ndf_filter = df.loc[(df['interpretation_variable']=='moisture')]\ndf_filter.name = db_name + \"_filtered_M\" \n\n\n# # filter for &gt;&gt;temperature&lt;&lt; sensitive records only (also include records which are moisture and temperature sensitive)\n# df_filter = df.loc[(df['interpretation_variable']=='temperature')|(df['interpretation_variable']=='temperature+moisture')]\n# df_filter.name = db_name + \"_filtered_T_TM\" \n\n# # filter for &gt;&gt;exclusively temperature&lt;&lt; sensitive records only (without t+m)\n# df_filter = df.loc[(df['interpretation_variable']=='temperature')]\n# df_filter.name = db_name + \"_filtered_T\" \n\n\n# ---&gt; archiveType and paleoData_proxy\n# e.g.\n\n# # filter for specific proxy type, e.g. archiveType='speleothem' and paleoData_proxy='d18O'\n# df_filter = df.loc[(df['archiveType']=='speleothem')&amp;(df['paleoData_proxy']=='d18O')]\n# df_filter.name = db_name + \"_filtered_speleo_d18O\" \n\n# # filter for specific proxy type, e.g. archiveType='speleothem' only\n# df_filter = df.loc[(df['archiveType']=='speleothem')]\n# df_filter.name = db_name + \"_filtered_speleothem\" \n\n\n# ---&gt; paleoData_proxy only\n# e.g. \n\n# df_filter = df.loc[(df['paleoData_proxy']=='MXD')]\n\n# etc.\n</pre> # if you want to filter for specific metadata, e.g. temperature or moisture records, run this:   # ---&gt; interpretation_variable # e.g.  # # filter for &gt;&gt;moisture&lt;&lt; sensitive records only (also include records which are moisture and temperature sensitive) # df_filter = df.loc[(df['interpretation_variable']=='moisture')|(df['interpretation_variable']=='temperature+moisture')] # df_filter.name = db_name + \"_filtered_M_TM\"    # filter for &gt;&gt;exclusively moisture&lt;&lt; sensitive records only (without t+m) df_filter = df.loc[(df['interpretation_variable']=='moisture')] df_filter.name = db_name + \"_filtered_M\"    # # filter for &gt;&gt;temperature&lt;&lt; sensitive records only (also include records which are moisture and temperature sensitive) # df_filter = df.loc[(df['interpretation_variable']=='temperature')|(df['interpretation_variable']=='temperature+moisture')] # df_filter.name = db_name + \"_filtered_T_TM\"   # # filter for &gt;&gt;exclusively temperature&lt;&lt; sensitive records only (without t+m) # df_filter = df.loc[(df['interpretation_variable']=='temperature')] # df_filter.name = db_name + \"_filtered_T\"    # ---&gt; archiveType and paleoData_proxy # e.g.  # # filter for specific proxy type, e.g. archiveType='speleothem' and paleoData_proxy='d18O' # df_filter = df.loc[(df['archiveType']=='speleothem')&amp;(df['paleoData_proxy']=='d18O')] # df_filter.name = db_name + \"_filtered_speleo_d18O\"   # # filter for specific proxy type, e.g. archiveType='speleothem' only # df_filter = df.loc[(df['archiveType']=='speleothem')] # df_filter.name = db_name + \"_filtered_speleothem\"    # ---&gt; paleoData_proxy only # e.g.   # df_filter = df.loc[(df['paleoData_proxy']=='MXD')]  # etc. <p>IMPORTANT: the database name needs to be adjusted according to the filtering.</p> <p>Please add an identifier to the dataframe name which will be used for displaying and savng the data.</p> <p>Make sure it is different from the original db_name.</p> <p>As df.name is used for saving the filtered data it is crucial that it differs from the original db_name otherwise the data will get overwritten!</p> In\u00a0[5]: Copied! <pre># df needs name reassigned as it gets lost otherwise after assigning new value to df (through the filtering above)\n\n# for the M+T filtered example, revise df.name to _filtered_MT\nprint(df_filter.name)\n\nassert df_filter.name!=db_name\n</pre> # df needs name reassigned as it gets lost otherwise after assigning new value to df (through the filtering above)  # for the M+T filtered example, revise df.name to _filtered_MT print(df_filter.name)  assert df_filter.name!=db_name <pre>dod2k_v2.0_filtered_M\n</pre> <p>Display the filtered dataframe</p> In\u00a0[6]: Copied! <pre>print(df_filter.info())\n</pre> print(df_filter.info()) <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 996 entries, 303 to 4421\nData columns (total 22 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   archiveType                    996 non-null    object \n 1   dataSetName                    996 non-null    object \n 2   datasetId                      996 non-null    object \n 3   duplicateDetails               996 non-null    object \n 4   geo_meanElev                   986 non-null    float32\n 5   geo_meanLat                    996 non-null    float32\n 6   geo_meanLon                    996 non-null    float32\n 7   geo_siteName                   996 non-null    object \n 8   interpretation_direction       996 non-null    object \n 9   interpretation_seasonality     996 non-null    object \n 10  interpretation_variable        996 non-null    object \n 11  interpretation_variableDetail  996 non-null    object \n 12  originalDataURL                996 non-null    object \n 13  originalDatabase               996 non-null    object \n 14  paleoData_notes                996 non-null    object \n 15  paleoData_proxy                996 non-null    object \n 16  paleoData_sensorSpecies        996 non-null    object \n 17  paleoData_units                996 non-null    object \n 18  paleoData_values               996 non-null    object \n 19  paleoData_variableName         996 non-null    object \n 20  year                           996 non-null    object \n 21  yearUnits                      996 non-null    object \ndtypes: float32(3), object(19)\nmemory usage: 167.3+ KB\nNone\n</pre> <p>Saves the filtered dataframe in:</p> <p>{repo_root}/data/{df_filter.name}</p> In\u00a0[7]: Copied! <pre># create new directory if dir does not exist\npath = '/data/'+df_filter.name\nos.makedirs(os.getcwd()+path, exist_ok = True)\n</pre> # create new directory if dir does not exist path = '/data/'+df_filter.name os.makedirs(os.getcwd()+path, exist_ok = True) In\u00a0[8]: Copied! <pre># save as pickle\ndf_filter.to_pickle(f'data/{df_filter.name}/{df_filter.name}_compact.pkl')\n</pre> # save as pickle df_filter.to_pickle(f'data/{df_filter.name}/{df_filter.name}_compact.pkl') In\u00a0[9]: Copied! <pre># save csv\nutf.write_compact_dataframe_to_csv(df_filter)\n</pre> # save csv utf.write_compact_dataframe_to_csv(df_filter) <pre>METADATA: datasetId, archiveType, dataSetName, duplicateDetails, geo_meanElev, geo_meanLat, geo_meanLon, geo_siteName, interpretation_direction, interpretation_seasonality, interpretation_variable, interpretation_variableDetail, originalDataURL, originalDatabase, paleoData_notes, paleoData_proxy, paleoData_sensorSpecies, paleoData_units, paleoData_variableName, yearUnits\nSaved to /home/jupyter-lluecke/dod2k/data/dod2k_v2.0_filtered_M/dod2k_v2.0_filtered_M_compact_%s.csv\n</pre> In\u00a0[10]: Copied! <pre># load dataframe\nutf.load_compact_dataframe_from_csv(df_filter.name).info()\n</pre> # load dataframe utf.load_compact_dataframe_from_csv(df_filter.name).info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 996 entries, 0 to 995\nData columns (total 22 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   archiveType                    996 non-null    object \n 1   dataSetName                    996 non-null    object \n 2   datasetId                      996 non-null    object \n 3   duplicateDetails               996 non-null    object \n 4   geo_meanElev                   986 non-null    float32\n 5   geo_meanLat                    996 non-null    float32\n 6   geo_meanLon                    996 non-null    float32\n 7   geo_siteName                   996 non-null    object \n 8   interpretation_direction       996 non-null    object \n 9   interpretation_seasonality     996 non-null    object \n 10  interpretation_variable        996 non-null    object \n 11  interpretation_variableDetail  996 non-null    object \n 12  originalDataURL                996 non-null    object \n 13  originalDatabase               996 non-null    object \n 14  paleoData_notes                996 non-null    object \n 15  paleoData_proxy                996 non-null    object \n 16  paleoData_sensorSpecies        996 non-null    object \n 17  paleoData_units                996 non-null    object \n 18  paleoData_values               996 non-null    object \n 19  paleoData_variableName         996 non-null    object \n 20  year                           996 non-null    object \n 21  yearUnits                      996 non-null    object \ndtypes: float32(3), object(19)\nmemory usage: 159.6+ KB\n</pre>"},{"location":"notebooks/df_filter/#filter-compact-dataframe","title":"Filter compact dataframe\u00b6","text":""},{"location":"notebooks/df_filter/#set-up-working-environment","title":"Set up working environment\u00b6","text":""},{"location":"notebooks/df_filter/#read-dataframe","title":"read dataframe\u00b6","text":""},{"location":"notebooks/df_filter/#filter-dataframe-for-specific-record-types","title":"filter dataframe for specific record types\u00b6","text":""},{"location":"notebooks/df_filter/#save-filtered-dataframe","title":"save filtered dataframe\u00b6","text":""},{"location":"notebooks/df_info/","title":"Display entries of compact dataframe column by column","text":"<p>Author: Lucie Luecke, 2024</p> <p>This notebook goes through the columns of a compact dataframe (original databases or output database of databases) and displays the (meta)data.</p> <p>Use this to familiarise yourself with the contents of a compact dataframe.</p> <p>A compact dataframe has standardised columns and data formats for:</p> <ul> <li><code>archiveType</code></li> <li><code>dataSetName</code></li> <li><code>datasetId</code></li> <li><code>geo_meanElev</code></li> <li><code>geo_meanLat</code></li> <li><code>geo_meanLon</code></li> <li><code>geo_siteName</code></li> <li><code>interpretation_direction</code> (new in v2.0)</li> <li><code>interpretation_variable</code></li> <li><code>interpretation_variableDetail</code></li> <li><code>interpretation_seasonality</code> (new in v2.0)</li> <li><code>originalDataURL</code></li> <li><code>originalDatabase</code></li> <li><code>paleoData_notes</code></li> <li><code>paleoData_proxy</code></li> <li><code>paleoData_sensorSpecies</code></li> <li><code>paleoData_units</code></li> <li><code>paleoData_values</code></li> <li><code>paleoData_variableName</code></li> <li><code>year</code></li> <li><code>yearUnits</code></li> <li>(optional: <code>DuplicateDetails</code>)</li> </ul> <p>Make sure the repo_root is added correctly, it should be: your_root_dir/dod2k This should be the working directory throughout this notebook (and all other notebooks).</p> In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n\nimport sys\nimport os\nfrom pathlib import Path\n\n# Add parent directory to path (works from any notebook in notebooks/)\n# the repo_root should be the parent directory of the notebooks folder\ncurrent_dir = Path().resolve()\n# Determine repo root\nif current_dir.name == 'dod2k': repo_root = current_dir\nelif current_dir.parent.name == 'dod2k': repo_root = current_dir.parent\nelse: raise Exception('Please review the repo root structure (see first cell).')\n\n# Update cwd and path only if needed\nif os.getcwd() != str(repo_root):\n    os.chdir(repo_root)\nif str(repo_root) not in sys.path:\n    sys.path.insert(0, str(repo_root))\n\nprint(f\"Repo root: {repo_root}\")\nif str(os.getcwd())==str(repo_root):\n    print(f\"Working directory matches repo root. \")\n</pre> %load_ext autoreload %autoreload 2  import sys import os from pathlib import Path  # Add parent directory to path (works from any notebook in notebooks/) # the repo_root should be the parent directory of the notebooks folder current_dir = Path().resolve() # Determine repo root if current_dir.name == 'dod2k': repo_root = current_dir elif current_dir.parent.name == 'dod2k': repo_root = current_dir.parent else: raise Exception('Please review the repo root structure (see first cell).')  # Update cwd and path only if needed if os.getcwd() != str(repo_root):     os.chdir(repo_root) if str(repo_root) not in sys.path:     sys.path.insert(0, str(repo_root))  print(f\"Repo root: {repo_root}\") if str(os.getcwd())==str(repo_root):     print(f\"Working directory matches repo root. \") <pre>Repo root: /home/jupyter-lluecke/dod2k\nWorking directory matches repo root. \n</pre> In\u00a0[2]: Copied! <pre>import pandas as pd\nimport numpy as np\n\nfrom dod2k_utilities import ut_functions as utf # contains utility functions\n</pre> import pandas as pd import numpy as np  from dod2k_utilities import ut_functions as utf # contains utility functions  <p>Read compact dataframe.</p> <p>{db_name} refers to the database, including e.g.</p> <ul> <li>database of databases:<ul> <li>dod2k_v2.0 (dod2k: duplicate free, merged database)</li> <li>dod2k_v2.0_filtered_M (filtered for M sensitive proxies only)</li> <li>dod2k_v2.0_filtered_M_TM (filtered for M and TM sensitive proxies only)</li> <li>dod2k_v2.0_filtered_speleo (filtered for speleothem proxies only)</li> <li>all_merged (NOT filtered for duplicates, only fusion of the input databases)</li> </ul> </li> <li>original databases:<ul> <li>fe23</li> <li>ch2k</li> <li>sisal</li> <li>pages2k</li> <li>iso2k</li> </ul> </li> </ul> <p>All compact dataframes are saved in {repo_root}/data/{db_name} as {db_name}_compact.csv.</p> In\u00a0[3]: Copied! <pre># read dataframe, choose from the list below, or specify your own\n\ndb_name = 'dod2k_v2.0'\n\n# load dataframe\ndf = utf.load_compact_dataframe_from_csv(db_name)\nprint(df.info())\ndf.name = db_name\n</pre> # read dataframe, choose from the list below, or specify your own  db_name = 'dod2k_v2.0'  # load dataframe df = utf.load_compact_dataframe_from_csv(db_name) print(df.info()) df.name = db_name  <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4957 entries, 0 to 4956\nData columns (total 22 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   archiveType                    4957 non-null   object \n 1   dataSetName                    4957 non-null   object \n 2   datasetId                      4957 non-null   object \n 3   duplicateDetails               4957 non-null   object \n 4   geo_meanElev                   4875 non-null   float32\n 5   geo_meanLat                    4957 non-null   float32\n 6   geo_meanLon                    4957 non-null   float32\n 7   geo_siteName                   4957 non-null   object \n 8   interpretation_direction       4957 non-null   object \n 9   interpretation_seasonality     4957 non-null   object \n 10  interpretation_variable        4957 non-null   object \n 11  interpretation_variableDetail  4957 non-null   object \n 12  originalDataURL                4957 non-null   object \n 13  originalDatabase               4957 non-null   object \n 14  paleoData_notes                4957 non-null   object \n 15  paleoData_proxy                4957 non-null   object \n 16  paleoData_sensorSpecies        4957 non-null   object \n 17  paleoData_units                4957 non-null   object \n 18  paleoData_values               4957 non-null   object \n 19  paleoData_variableName         4957 non-null   object \n 20  year                           4957 non-null   object \n 21  yearUnits                      4957 non-null   object \ndtypes: float32(3), object(19)\nmemory usage: 794.0+ KB\nNone\n</pre> In\u00a0[4]: Copied! <pre># # check index\nprint(df.index)\n</pre> # # check index print(df.index) <pre>RangeIndex(start=0, stop=4957, step=1)\n</pre> In\u00a0[5]: Copied! <pre># # check dataSetName\nkey = 'dataSetName'\nprint('%s: '%key)\nprint(df[key].values)\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # # check dataSetName key = 'dataSetName' print('%s: '%key) print(df[key].values) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>dataSetName: \n['NAm-MtLemon.Briffa.2002' 'NAm-MtLemon.Briffa.2002'\n 'NAm-MtLemon.Briffa.2002' ... 'Sahiya cave'\n 'Ocn-ArabianSea.Doose-Rolinski.2001, Ocn-ArabianSea.Doose-Rolinski.2001'\n 'europe_swed019w, europe_swed021w']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 3845/4957\n</pre> In\u00a0[6]: Copied! <pre># # check datasetId\n\nprint(len(df.datasetId.unique()))\nprint(len(df))\nkey = 'datasetId'\nprint('%s (starts with): '%key)\nprint(df[key].values)\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint('datasetId starts with: ', np.unique([str(dd.split('_')[0]) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # # check datasetId  print(len(df.datasetId.unique())) print(len(df)) key = 'datasetId' print('%s (starts with): '%key) print(df[key].values) print(np.unique([str(type(dd)) for dd in df[key]])) print('datasetId starts with: ', np.unique([str(dd.split('_')[0]) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>4957\n4957\ndatasetId (starts with): \n['pages2k_5' 'pages2k_7' 'pages2k_8' ... 'sisal_901.0_545'\n 'dod2k_composite_z_pages2k_1686_pages2k_1688'\n 'dod2k_composite_z_FE23_europe_swed019w_FE23_europe_swed021w']\n[\"&lt;class 'str'&gt;\"]\ndatasetId starts with:  ['FE23' 'ch2k' 'dod2k' 'iso2k' 'pages2k' 'sisal']\nNo. of unique values: 4957/4957\n</pre> In\u00a0[7]: Copied! <pre># originalDataURL\nkey = 'originalDataURL'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([kk for kk in df[key] if 'this' in kk]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n# 'this study' should point to the correct URL (PAGES2k)\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # originalDataURL key = 'originalDataURL' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([kk for kk in df[key] if 'this' in kk])) print(np.unique([str(type(dd)) for dd in df[key]])) # 'this study' should point to the correct URL (PAGES2k) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>originalDataURL: \n['FE23_europe_swed019w: https://www.ncei.noaa.gov/pub/data/paleo/treering/measurements/europe/swed019w-noaa.rwl, FE23_europe_swed021w: https://www.ncei.noaa.gov/pub/data/paleo/treering/measurements/europe/swed021w-noaa.rwl'\n 'This compilation' \"['10.1002/2015GL063826']\" ... 'this compilation'\n 'www.ncdc.noaa.gov/paleo-search/study/27330'\n 'www.ncdc.noaa.gov/paleo/study/2474']\n['this compilation']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 3776/4957\n</pre> In\u00a0[8]: Copied! <pre># # originalDataSet\nkey = 'originalDatabase'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n# Note: the last two records have missing URLs\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # # originalDataSet key = 'originalDatabase' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) # Note: the last two records have missing URLs print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>originalDatabase: \n['CoralHydro2k v1.0.1' 'FE23 (Breitenmoser et al. (2014))' 'Iso2k v1.1.2'\n 'PAGES 2k v2.2.0' 'SISAL v3' 'dod2k_composite_z']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 6/4957\n</pre> In\u00a0[9]: Copied! <pre># check Elevation\nkey = 'geo_meanElev'\nprint('%s: '%key)\nprint(df[key])\nprint(np.unique(['%d'%kk for kk in df[key] if np.isfinite(kk)]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # check Elevation key = 'geo_meanElev' print('%s: '%key) print(df[key]) print(np.unique(['%d'%kk for kk in df[key] if np.isfinite(kk)])) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>geo_meanElev: \n0       2700.0\n1       2700.0\n2       2700.0\n3       2700.0\n4       2700.0\n         ...  \n4952    1190.0\n4953    1190.0\n4954    1190.0\n4955    -695.0\n4956     400.0\nName: geo_meanElev, Length: 4957, dtype: float32\n['-1' '-10' '-1011' ... '991' '994' '995']\n[\"&lt;class 'float'&gt;\"]\nNo. of unique values: 1091/4957\n</pre> In\u00a0[10]: Copied! <pre># # Latitude\nkey = 'geo_meanLat'\nprint('%s: '%key)\nprint(np.unique(['%d'%kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # # Latitude key = 'geo_meanLat' print('%s: '%key) print(np.unique(['%d'%kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>geo_meanLat: \n['-1' '-10' '-11' '-12' '-13' '-14' '-15' '-16' '-17' '-18' '-19' '-20'\n '-21' '-22' '-23' '-24' '-25' '-26' '-27' '-28' '-29' '-3' '-31' '-32'\n '-33' '-34' '-35' '-36' '-37' '-38' '-39' '-4' '-40' '-41' '-42' '-43'\n '-44' '-45' '-46' '-47' '-5' '-50' '-51' '-53' '-54' '-6' '-64' '-66'\n '-69' '-7' '-70' '-71' '-72' '-73' '-74' '-75' '-76' '-77' '-78' '-79'\n '-8' '-82' '-84' '-89' '-9' '0' '1' '10' '11' '12' '13' '15' '16' '17'\n '18' '19' '2' '20' '21' '22' '23' '24' '25' '26' '27' '28' '29' '3' '30'\n '31' '32' '33' '34' '35' '36' '37' '38' '39' '4' '40' '41' '42' '43' '44'\n '45' '46' '47' '48' '49' '5' '50' '51' '52' '53' '54' '55' '56' '57' '58'\n '59' '6' '60' '61' '62' '63' '64' '65' '66' '67' '68' '69' '7' '70' '71'\n '72' '73' '75' '76' '77' '78' '79' '8' '80' '81' '82' '9']\n[\"&lt;class 'float'&gt;\"]\nNo. of unique values: 2169/4957\n</pre> In\u00a0[11]: Copied! <pre># # Longitude \nkey = 'geo_meanLon'\nprint('%s: '%key)\nprint(np.unique(['%d'%kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # # Longitude  key = 'geo_meanLon' print('%s: '%key) print(np.unique(['%d'%kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>geo_meanLon: \n['-1' '-10' '-100' '-101' '-102' '-103' '-104' '-105' '-106' '-107' '-108'\n '-109' '-110' '-111' '-112' '-113' '-114' '-115' '-116' '-117' '-118'\n '-119' '-12' '-120' '-121' '-122' '-123' '-124' '-125' '-126' '-127'\n '-128' '-129' '-13' '-130' '-131' '-132' '-133' '-134' '-135' '-136'\n '-137' '-138' '-139' '-140' '-141' '-142' '-143' '-144' '-145' '-146'\n '-147' '-148' '-149' '-150' '-151' '-152' '-153' '-154' '-157' '-159'\n '-16' '-160' '-161' '-162' '-163' '-169' '-17' '-174' '-18' '-19' '-2'\n '-22' '-24' '-26' '-27' '-3' '-33' '-35' '-36' '-37' '-38' '-39' '-4'\n '-41' '-42' '-43' '-44' '-45' '-46' '-47' '-49' '-5' '-50' '-51' '-54'\n '-55' '-56' '-57' '-58' '-6' '-60' '-61' '-62' '-63' '-64' '-65' '-66'\n '-67' '-68' '-69' '-7' '-70' '-71' '-72' '-73' '-74' '-75' '-76' '-77'\n '-78' '-79' '-8' '-80' '-81' '-82' '-83' '-84' '-85' '-86' '-87' '-88'\n '-89' '-9' '-90' '-91' '-92' '-93' '-94' '-95' '-96' '-97' '-98' '-99'\n '0' '1' '10' '100' '101' '102' '103' '104' '105' '106' '107' '108' '109'\n '11' '110' '111' '112' '113' '114' '115' '116' '117' '118' '119' '12'\n '120' '121' '122' '123' '124' '125' '126' '127' '128' '129' '13' '130'\n '132' '133' '134' '136' '137' '138' '14' '141' '142' '143' '144' '145'\n '146' '147' '148' '149' '15' '150' '151' '152' '153' '154' '155' '158'\n '159' '16' '160' '162' '163' '165' '166' '167' '168' '169' '17' '170'\n '171' '172' '173' '174' '175' '176' '177' '179' '18' '19' '2' '20' '21'\n '22' '23' '24' '25' '26' '27' '28' '29' '3' '30' '31' '32' '33' '34' '35'\n '36' '37' '38' '39' '4' '40' '41' '42' '43' '44' '45' '46' '49' '5' '50'\n '51' '53' '54' '55' '56' '57' '58' '59' '6' '60' '63' '64' '65' '68' '69'\n '7' '70' '71' '72' '74' '75' '76' '77' '78' '79' '8' '80' '81' '82' '83'\n '84' '85' '86' '87' '88' '89' '9' '90' '91' '92' '93' '94' '95' '96' '97'\n '98' '99']\n[\"&lt;class 'float'&gt;\"]\nNo. of unique values: 2641/4957\n</pre> In\u00a0[12]: Copied! <pre># Site Name \nkey = 'geo_siteName'\nprint('%s: '%key)\nprint(df[key].values)\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # Site Name  key = 'geo_siteName' print('%s: '%key) print(df[key].values) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>geo_siteName: \n['Mt. Lemon' 'Mt. Lemon' 'Mt. Lemon' ... 'Sahiya cave' 'Arabian Sea'\n 'COMPOSITE: Tornetr\u00e4skr+f.,Bartoli + Tornetr\u00e4skfos.,Bartoli']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 3527/4957\n</pre> In\u00a0[13]: Copied! <pre># archiveType\nkey = 'archiveType'\nprint('%s: '%key)\nprint(np.unique(df[key]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # archiveType key = 'archiveType' print('%s: '%key) print(np.unique(df[key])) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>archiveType: \n['Borehole' 'Coral' 'Documents' 'GlacierIce' 'GroundIce' 'LakeSediment'\n 'MarineSediment' 'MolluskShell' 'Other' 'Sclerosponge' 'Speleothem'\n 'Wood' 'speleothem']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 13/4957\n</pre> In\u00a0[14]: Copied! <pre># paleoData_proxy\nkey = 'paleoData_proxy'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # paleoData_proxy key = 'paleoData_proxy' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>paleoData_proxy: \n['ARSTAN' 'Mg/Ca' 'Sr/Ca' 'TEX86' 'Uk37' 'accumulation rate' 'alkenone'\n 'borehole' 'calcification rate' 'chironomid' 'chloride'\n 'chrysophyte assemblage' 'concentration' 'count' 'd13C' 'd18O' 'dD'\n 'diatom' 'dinocyst' 'dust' 'effective precipitation' 'foraminifera'\n 'growth rate' 'historical' 'humidification index' 'ice melt'\n 'maximum latewood density' 'multiproxy' 'nitrate' 'pollen' 'reflectance'\n 'residual chronology' 'ring width' 'sodium' 'sulfate' 'temperature'\n 'thickness' 'varve thickness']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 38/4957\n</pre> In\u00a0[15]: Copied! <pre># climate_interpretation\nkey = 'paleoData_sensorSpecies'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # climate_interpretation key = 'paleoData_sensorSpecies' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')  <pre>paleoData_sensorSpecies: \n['ABAL' 'ABAM' 'ABBA' 'ABBO' 'ABCE' 'ABCI' 'ABCO' 'ABLA' 'ABMA' 'ABPI'\n 'ABPN' 'ABPR' 'ABSB' 'ABSP' 'ACRU' 'ACSH' 'ADHO' 'ADUS' 'AGAU' 'ARAR'\n 'ATCU' 'ATSE' 'AUCH' 'BEPU' 'CABU' 'CADE' 'CADN' 'CARO' 'CDAT' 'CDBR'\n 'CDDE' 'CDLI' 'CEAN' 'CESP' 'CHLA' 'CHNO' 'Ceratoporella nicholsoni'\n 'DABI' 'DACO' 'Diploria labyrinthiformis' 'Diploria strigosa' 'FAGR'\n 'FASY' 'FICU' 'FRNI' 'HABI' 'Hydnophora microconos, Porites lobata'\n 'JGAU' 'JUEX' 'JUFO' 'JUOC' 'JUPH' 'JUPR' 'JURE' 'JUSC' 'JUSP' 'JUVI'\n 'LADE' 'LAGM' 'LALA' 'LALY' 'LAOC' 'LASI' 'LGFR' 'LIBI' 'LITU'\n 'Montastraea faveolata' 'N/A' 'NA' 'NOBE' 'NOGU' 'NOME' 'NOPU' 'NOSO'\n 'NaN' 'Orbicella faveolata' 'P. australiensis, possibly P. lobata' 'PCAB'\n 'PCEN' 'PCGL' 'PCGN' 'PCMA' 'PCOB' 'PCOM' 'PCPU' 'PCRU' 'PCSH' 'PCSI'\n 'PCSM' 'PCSP' 'PHAL' 'PHAS' 'PHGL' 'PHTR' 'PIAL' 'PIAM' 'PIAR' 'PIBA'\n 'PIBN' 'PIBR' 'PICE' 'PICL' 'PICO' 'PIEC' 'PIED' 'PIFL' 'PIHA' 'PIHR'\n 'PIJE' 'PIKO' 'PILA' 'PILE' 'PILO' 'PIMO' 'PIMU' 'PIMZ' 'PINI' 'PIPA'\n 'PIPE' 'PIPI' 'PIPN' 'PIPO' 'PIPU' 'PIRE' 'PIRI' 'PIRO' 'PISF' 'PISI'\n 'PISP' 'PIST' 'PISY' 'PITA' 'PITO' 'PIUN' 'PIVI' 'PIWA' 'PLRA' 'PLUV'\n 'PPDE' 'PPSP' 'PRMA' 'PSMA' 'PSME' 'PTAN' 'Pavona clavus' 'Porites'\n 'Porites austraiensis' 'Porites lobata' 'Porites lutea' 'Porites solida'\n 'Porites sp.' 'Pseudodiploria strigosa' 'QUAL' 'QUDG' 'QUFR' 'QUHA'\n 'QUKE' 'QULO' 'QULY' 'QUMA' 'QUMC' 'QUPE' 'QUPR' 'QURO' 'QURU' 'QUSP'\n 'QUST' 'QUVE' 'Siderastrea radians' 'Siderastrea siderea'\n 'Siderastrea stellata' 'Solenastrea bournoni' 'TABA' 'TADI' 'TAMU' 'TEGR'\n 'THOC' 'THPL' 'TSCA' 'TSCR' 'TSDU' 'TSHE' 'TSME' 'ULSP' 'VIKE' 'WICE'\n 'bournoni' 'heliopora' 'labyrinthiformis' 'lamellina' 'lobata' 'lutea'\n 'nan' 'siderea']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 194/4957\n</pre> In\u00a0[16]: Copied! <pre># # paleoData_notes\nkey = 'paleoData_notes'\nprint('%s: '%key)\nprint(df[key].values)\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # # paleoData_notes key = 'paleoData_notes' print('%s: '%key) print(df[key].values) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>paleoData_notes: \n['nan' 'nan' 'nan' ... 'calcite'\n 'pages2k_1686: SON1997: T(sediments) = (Uk37-0.316)/(0.023); paleoData_units changed - was originally deg C; climateInterpretation_seasonality changed - was originally Annual, pages2k_1688: All O2K-LR records have been quality-controlled according to protocols published in Nature Geoscience supplement.; climateInterpretation_seasonality changed - was originally Annual'\n 'FE23_europe_swed019w: Investigator: Schweingruber, FE23_europe_swed021w: Investigator: Schweingruber']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 426/4957\n</pre> In\u00a0[17]: Copied! <pre># paleoData_variableName\nkey = 'paleoData_variableName'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n</pre> # paleoData_variableName key = 'paleoData_variableName' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) <pre>paleoData_variableName: \n['ARSTAN' 'MAR' 'Mg/Ca' 'R650/R700' 'RABD660670' 'Sr/Ca' 'TEX86' 'Uk37'\n 'calcification rate' 'chloride' 'composite' 'concentration' 'count'\n 'd13C' 'd18O' 'd2H' 'dD' 'dust' 'effective precipitation' 'growth rate'\n 'humidification index' 'ice melt' 'maximum latewood density' 'nan'\n 'nitrate' 'precipitation' 'reflectance' 'residual chronology'\n 'ring width' 'sodium' 'sulfate' 'temperature' 'thickness' 'year']\n[\"&lt;class 'str'&gt;\"]\n</pre> In\u00a0[18]: Copied! <pre># climate_interpretation\nkey = 'interpretation_direction'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # climate_interpretation key = 'interpretation_direction' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>interpretation_direction: \n['Increase' 'N/A' 'NaN' 'None' 'T_air (positive), P_amount (negative)'\n 'T_air (positive), P_amount (negative), SPEI (negative)' 'decrease'\n 'decrease/increase'\n 'depends (orbital timescale: More Indian Monsoon moisture--&gt;more enriched. Since 3ka: Indian source has been stable, so amount effect dominates: more rainfall, more intense hydrological cycle --&gt;More depleted)'\n 'increase' 'negaitive' 'negative' 'positive'\n 'positive for d18O-temperature relation, negative for d13C-precipiation amount']\nNo. of unique values: 14/4957\n</pre> In\u00a0[19]: Copied! <pre># climate_interpretation\nkey = 'interpretation_seasonality'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # climate_interpretation key = 'interpretation_seasonality' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>interpretation_seasonality: \n['Annual' 'Apr' 'Apr-Jul' 'Apr-Jun' 'Apr-Sep' 'Aug' 'Aug-Jul' 'Dec-Feb'\n 'Dec-Mar' 'Dec-May' 'Feb' 'Feb-Aug' 'Growing Season' 'Jan' 'Jan-Apr'\n 'Jan-Jun' 'Jan-Mar' 'Jul' 'Jul-Dec' 'Jul-Sep' 'Jun' 'Jun-Aug' 'Jun-Jul'\n 'Jun-Sep' 'Mar' 'Mar-Aug' 'Mar-May' 'Mar-Nov' 'Mar-Oct' 'May' 'May-Apr'\n 'May-Dec' 'May-Jul' 'May-Oct' 'May-Sep' 'N/A' 'None' 'Nov-Apr' 'Nov-Feb'\n 'Nov-Jan' 'Oct-Apr' 'Oct-Dec' 'Oct-Sep' 'Sep-Apr' 'Sep-Aug' 'Sep-Nov'\n 'Sep-Oct' 'Spr-Sum' 'Summer' 'Wet Season' 'Winter' 'deleteMe' 'subannual']\nNo. of unique values: 53/4957\n</pre> In\u00a0[20]: Copied! <pre># climate_interpretation\nkey = 'interpretation_variable'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # climate_interpretation key = 'interpretation_variable' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>interpretation_variable: \n['N/A' 'NOT temperature NOT moisture' 'None' 'circulationIndex'\n 'circulationVariable' 'deleteMe' 'effectivePrecipitation' 'evaporation'\n 'hydrologicBalance' 'moisture' 'precipitation' 'precipitationIsotope'\n 'salinity' 'seasonality' 'streamflow' 'temperature'\n 'temperature+moisture']\nNo. of unique values: 17/4957\n</pre> In\u00a0[21]: Copied! <pre># climate_interpretation\nkey = 'interpretation_variableDetail'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # climate_interpretation key = 'interpretation_variableDetail' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>interpretation_variableDetail: \n['0.58 +- 0.11ppt/degrees C' 'Aleutian Low/westerly storm trajectories'\n 'Amount of rainfall change'\n 'Australian-Indonesian Summer monsoon; More negative d18O values correspond to stronger amount'\n 'Australian-Indonesian monsoon rainfall'\n 'Competing influence of polar and maritime airmasses'\n 'Continental Sweden' 'E:P lake water' 'ENSO/PDO'\n 'East Asian Monsoon Strength; more negative values of d18O are interpreted as indicative of increased monsoon strength'\n 'East Asian Monsoon rainfall'\n 'FE23_europe_swed019w: N/A, FE23_europe_swed021w: N/A'\n 'Indian Monsoon Strength'\n 'Indian Summer Monsoon; more negative values of d18O are interpreted as indicative of increased monsoon strength'\n 'Indian monsoon'\n 'Lower precipitation produces higher d13C and Sr/Ca values'\n 'Maximum air temperature, seasonal' 'Maximum temperature'\n 'Monsoon strength'\n 'More negative d18O values correspond to stronger amount' 'N/A' 'NaN'\n 'None' 'Precipitation' 'SAM' 'Seasonal' 'Seasonal, annual'\n 'South China Sea' 'Southern Tibet'\n 'The interpretation is made for an older section of the sample. Last 2k data was not the focus of the manuscript'\n 'Variations in NAO (related to the amount of rainfall. Season not specified)'\n 'air' 'air-surface' 'air@600m' 'air@condensationLevel' 'air@surface'\n 'amount of rainfall'\n 'changes of d18O in speleothems reflect changes of the average d18O of rainfall in the region related to rainfall seasonality'\n 'd18O changes of speleothems reflect effects of temperature on raifnall d18O, rainfall amounts affect cave hydrology and biomass density above the cave, which is recorded in d13C of speleothems'\n 'eff' 'ground@surface' 'groundwater'\n 'higher values are related to less rainfall - this can be realted to less moisture influex from the Caribbean due to a southward shift of the ITCZ in phases when high amounts of meltwater enter the cooling north Atlantic Ocean; after ~4.3 ka the connection to the north Atalatic is lost and ENSO becomes more important with warm ENSO events (El Nino) causing higher d18O'\n 'ice@surface' 'in the southern tropical Andes' 'lake level'\n 'lake surface' 'lake water' 'lake, winds in eastern Patagonia'\n 'lake@surface' 'minimum temperature' 'moisture'\n 'more negative values of d18O are interpreted as indicative of increased summer monsoon precipitation'\n 'more positive d18O values are interpreted to represent wetter conditions'\n 'more positive values of d13C indicate a spread of C4 prairy grasses and decline of C3 forest plants, more positive d18O indicates evaporation of soil water which is stronger in the prairy environment than in the forsest'\n 'near sea surface' 'of precipitation'\n 'pages2k_1686: sea@surface, pages2k_1688: sea@surface' 'precipitation'\n 'precipitation amount' 'rain' 'regional and hemispheric temperature'\n 'relative amount of winter snowfall' 'relative humidity'\n 'relative portion of summer (SAM) vs winter rainfall' 'sea surface'\n 'sea@surface' 'sea_surface' 'soil moisture' 'sub surface (30m)'\n 'sub surface (~50 m)' 'subsurface (60-80m)' 'subsurface, 136 m'\n 'subsurface, 143 m' 'summer monsoon' 'surface'\n 'temperature - manually assigned by DoD2k authors for paleoData_proxy = Mg/Ca'\n 'temperature - manually assigned by DoD2k authors for paleoData_proxy = Sr/Ca'\n 'temperature+moisture - manually assigned by DoD2k authors for paleoData_proxy = d18O'\n 'temperature+moisture - manually assigned by DoD2k authors for paleoData_proxy = d18O.'\n 'tropical or North Pacific moisture'\n 'variations in air temperature due to large-scale atmospheric patterns'\n 'variations in paleoprecipitation amount on a multi-annual timescale (On longer timescales, however, the flowstone?s growth dynamics have to be considered)'\n 'variations in winter temperature in the Alps']\nNo. of unique values: 84/4957\n</pre> In\u00a0[22]: Copied! <pre># # paleoData_values\nkey = 'paleoData_values'\n\nprint('%s: '%key)\nfor ii, vv in enumerate(df[key][:20]):\n    try: \n        print('%-30s: %s -- %s'%(df['dataSetName'].iloc[ii][:30], str(np.nanmin(vv)), str(np.nanmax(vv))))\n        print(type(vv))\n    except: print(df['dataSetName'].iloc[ii], 'NaNs detected.')\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n</pre> # # paleoData_values key = 'paleoData_values'  print('%s: '%key) for ii, vv in enumerate(df[key][:20]):     try:          print('%-30s: %s -- %s'%(df['dataSetName'].iloc[ii][:30], str(np.nanmin(vv)), str(np.nanmax(vv))))         print(type(vv))     except: print(df['dataSetName'].iloc[ii], 'NaNs detected.') print(np.unique([str(type(dd)) for dd in df[key]])) <pre>paleoData_values: \nNAm-MtLemon.Briffa.2002       : 0.154 -- 2.91\n&lt;class 'numpy.ndarray'&gt;\nNAm-MtLemon.Briffa.2002       : 0.245 -- 1.655\n&lt;class 'numpy.ndarray'&gt;\nNAm-MtLemon.Briffa.2002       : 0.283 -- 1.666\n&lt;class 'numpy.ndarray'&gt;\nNAm-MtLemon.Briffa.2002       : 0.574 -- 0.951\n&lt;class 'numpy.ndarray'&gt;\nNAm-MtLemon.Briffa.2002       : 0.707 -- 1.118\n&lt;class 'numpy.ndarray'&gt;\nNAm-MtLemon.Briffa.2002       : 0.789 -- 1.102\n&lt;class 'numpy.ndarray'&gt;\nNAm-MtLemon.Briffa.2002       : 0.757 -- 1.114\n&lt;class 'numpy.ndarray'&gt;\nArc-Arjeplog.Bjorklund.2014   : -3.532171 -- 2.5670047\n&lt;class 'numpy.ndarray'&gt;\nArc-Arjeplog.Bjorklund.2014   : -4.1141653 -- 2.6139\n&lt;class 'numpy.ndarray'&gt;\nAsi-CHIN019.Li.2010           : 0.298 -- 1.664\n&lt;class 'numpy.ndarray'&gt;\nNAm-Landslide.Luckman.2006    : 0.057 -- 0.76\n&lt;class 'numpy.ndarray'&gt;\nNAm-Landslide.Luckman.2006    : 0.164 -- 1.781\n&lt;class 'numpy.ndarray'&gt;\nNAm-Landslide.Luckman.2006    : 0.125 -- 1.813\n&lt;class 'numpy.ndarray'&gt;\nNAm-Landslide.Luckman.2006    : 0.116 -- 1.889\n&lt;class 'numpy.ndarray'&gt;\nNAm-SmithersSkiArea.Schweingru: 0.319 -- 1.73\n&lt;class 'numpy.ndarray'&gt;\nNAm-SmithersSkiArea.Schweingru: 0.628 -- 1.56\n&lt;class 'numpy.ndarray'&gt;\nNAm-SmithersSkiArea.Schweingru: 0.472 -- 1.576\n&lt;class 'numpy.ndarray'&gt;\nNAm-SmithersSkiArea.Schweingru: 0.44 -- 0.83\n&lt;class 'numpy.ndarray'&gt;\nNAm-SmithersSkiArea.Schweingru: 0.655 -- 1.176\n&lt;class 'numpy.ndarray'&gt;\nNAm-SmithersSkiArea.Schweingru: 0.636 -- 1.179\n&lt;class 'numpy.ndarray'&gt;\n[\"&lt;class 'numpy.ndarray'&gt;\"]\n</pre> In\u00a0[23]: Copied! <pre># paleoData_units\nkey = 'paleoData_units'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # paleoData_units key = 'paleoData_units' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>paleoData_units: \n['cm' 'cm/yr' 'count' 'count/mL' 'degC' 'g/cm/yr' 'g/cm2/yr' 'g/cm3' 'mm'\n 'mm/year' 'mm/yr' 'mmol/mol' 'nan' 'needsToBeChanged' 'ng/g' 'percent'\n 'permil' 'ppb' 'standardized_anomalies' 'unitless' 'yr AD' 'z score'\n 'z-scores']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 23/4957\n</pre> In\u00a0[24]: Copied! <pre># # year\nkey = 'year'\nprint('%s: '%key)\nfor ii, vv in enumerate(df[key][:20]):\n    try: print('%-30s: %s -- %s'%(df['dataSetName'].iloc[ii][:30], str(np.nanmin(vv)), str(np.nanmax(vv))))\n    except: print('NaNs detected.', vv)\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n</pre> # # year key = 'year' print('%s: '%key) for ii, vv in enumerate(df[key][:20]):     try: print('%-30s: %s -- %s'%(df['dataSetName'].iloc[ii][:30], str(np.nanmin(vv)), str(np.nanmax(vv))))     except: print('NaNs detected.', vv) print(np.unique([str(type(dd)) for dd in df[key]])) <pre>year: \nNAm-MtLemon.Briffa.2002       : 1568.0 -- 1983.0\nNAm-MtLemon.Briffa.2002       : 1568.0 -- 1983.0\nNAm-MtLemon.Briffa.2002       : 1568.0 -- 1983.0\nNAm-MtLemon.Briffa.2002       : 1568.0 -- 1983.0\nNAm-MtLemon.Briffa.2002       : 1568.0 -- 1983.0\nNAm-MtLemon.Briffa.2002       : 1568.0 -- 1983.0\nNAm-MtLemon.Briffa.2002       : 1568.0 -- 1983.0\nArc-Arjeplog.Bjorklund.2014   : 1200.0 -- 2010.0\nArc-Arjeplog.Bjorklund.2014   : 1200.0 -- 2010.0\nAsi-CHIN019.Li.2010           : 1509.0 -- 2006.0\nNAm-Landslide.Luckman.2006    : 913.0 -- 2001.0\nNAm-Landslide.Luckman.2006    : 913.0 -- 2001.0\nNAm-Landslide.Luckman.2006    : 913.0 -- 2001.0\nNAm-Landslide.Luckman.2006    : 913.0 -- 2001.0\nNAm-SmithersSkiArea.Schweingru: 1680.0 -- 1983.0\nNAm-SmithersSkiArea.Schweingru: 1680.0 -- 1983.0\nNAm-SmithersSkiArea.Schweingru: 1680.0 -- 1983.0\nNAm-SmithersSkiArea.Schweingru: 1680.0 -- 1983.0\nNAm-SmithersSkiArea.Schweingru: 1680.0 -- 1983.0\nNAm-SmithersSkiArea.Schweingru: 1680.0 -- 1983.0\n[\"&lt;class 'numpy.ndarray'&gt;\"]\n</pre> In\u00a0[25]: Copied! <pre># yearUnits\nkey = 'yearUnits'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # yearUnits key = 'yearUnits' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>yearUnits: \n['CE']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 1/4957\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/df_info/#display-entries-of-compact-dataframe-column-by-column","title":"Display entries of compact dataframe column by column\u00b6","text":""},{"location":"notebooks/df_info/#set-up-working-environment","title":"Set up working environment\u00b6","text":""},{"location":"notebooks/df_info/#read-dataframe","title":"Read dataframe\u00b6","text":""},{"location":"notebooks/df_info/#display-dataframe","title":"Display dataframe\u00b6","text":""},{"location":"notebooks/df_info/#display-identification-metadata-datasetname-datasetid-originaldataurl-originaldatabase","title":"Display identification metadata: dataSetName, datasetId, originalDataURL, originalDatabase\u00b6","text":""},{"location":"notebooks/df_info/#index","title":"index\u00b6","text":""},{"location":"notebooks/df_info/#datasetname-associated-with-each-record-may-not-be-unique","title":"dataSetName (associated with each record, may not be unique)\u00b6","text":""},{"location":"notebooks/df_info/#datasetid-unique-identifier-as-given-by-original-authors-includes-original-database-token","title":"datasetId (unique identifier, as given by original authors, includes original database token)\u00b6","text":""},{"location":"notebooks/df_info/#originaldataurl-urldoi-of-original-published-record-where-available","title":"originalDataURL (URL/DOI of original published record where available)\u00b6","text":""},{"location":"notebooks/df_info/#originaldatabase-original-database-used-as-input-for-dataframe","title":"originalDatabase (original database used as input for dataframe)\u00b6","text":""},{"location":"notebooks/df_info/#geographical-metadata-elevation-latitude-longitude-site-name","title":"geographical metadata: elevation, latitude, longitude, site name\u00b6","text":""},{"location":"notebooks/df_info/#geo_meanelev-mean-elevation-in-m","title":"geo_meanElev (mean elevation in m)\u00b6","text":""},{"location":"notebooks/df_info/#geo_meanlat-mean-latitude-in-degrees-n","title":"geo_meanLat (mean latitude in degrees N)\u00b6","text":""},{"location":"notebooks/df_info/#geo_meanlon-mean-longitude","title":"geo_meanLon (mean longitude)\u00b6","text":""},{"location":"notebooks/df_info/#geo_sitename-name-of-collection-site","title":"geo_siteName (name of collection site)\u00b6","text":""},{"location":"notebooks/df_info/#proxy-metadata-archive-type-proxy-type-interpretation","title":"proxy metadata: archive type, proxy type, interpretation\u00b6","text":""},{"location":"notebooks/df_info/#archivetype-archive-type","title":"archiveType (archive type)\u00b6","text":""},{"location":"notebooks/df_info/#paleodata_proxy-proxy-type","title":"paleoData_proxy (proxy type)\u00b6","text":""},{"location":"notebooks/df_info/#paleodata_sensorspecies-further-information-on-proxy-type-species","title":"paleoData_sensorSpecies (further information on proxy type: species)\u00b6","text":""},{"location":"notebooks/df_info/#paleodata_notes-notes","title":"paleoData_notes (notes)\u00b6","text":""},{"location":"notebooks/df_info/#paleodata_variablename","title":"paleoData_variableName\u00b6","text":""},{"location":"notebooks/df_info/#climate-metadata-interpretation-variable-direction-seasonality","title":"climate metadata: interpretation variable, direction, seasonality\u00b6","text":""},{"location":"notebooks/df_info/#interpretation_direction","title":"interpretation_direction\u00b6","text":""},{"location":"notebooks/df_info/#interpretation_seasonality","title":"interpretation_seasonality\u00b6","text":""},{"location":"notebooks/df_info/#interpretation_variable","title":"interpretation_variable\u00b6","text":""},{"location":"notebooks/df_info/#interpretation_variabledetail","title":"interpretation_variableDetail\u00b6","text":""},{"location":"notebooks/df_info/#data","title":"data\u00b6","text":""},{"location":"notebooks/df_info/#paleodata_values","title":"paleoData_values\u00b6","text":""},{"location":"notebooks/df_info/#paleodata_units","title":"paleoData_units\u00b6","text":""},{"location":"notebooks/df_info/#year","title":"year\u00b6","text":""},{"location":"notebooks/df_info/#yearunits","title":"yearUnits\u00b6","text":""},{"location":"notebooks/df_plot_dod2k/","title":"Visualise DoD2k","text":"<p>This notebook reads the resulting compact dataframe for dod2k produces a series of plots which show some information on the dataframe entries (reproduces figures and supplementary figures as in the publication).</p> <p>This can also be checked for sanity against the original publications.</p> <p>The notebook can also be modified to visualise the original dataframes (PAGES 2k, SISAL, FE23, CH2k, Iso2k).</p> <p>Author: Lucie Luecke Date produced: 18/11/2024</p> <p>Input: reads dataframe with the following keys:</p> <ul> <li><code>archiveType</code></li> <li><code>dataSetName</code></li> <li><code>datasetId</code></li> <li><code>geo_meanElev</code></li> <li><code>geo_meanLat</code></li> <li><code>geo_meanLon</code></li> <li><code>geo_siteName</code></li> <li><code>interpretation_direction</code> (new in v2.0)</li> <li><code>interpretation_variable</code></li> <li><code>interpretation_variableDetail</code></li> <li><code>interpretation_seasonality</code> (new in v2.0)</li> <li><code>originalDataURL</code></li> <li><code>originalDatabase</code></li> <li><code>paleoData_notes</code></li> <li><code>paleoData_proxy</code></li> <li><code>paleoData_sensorSpecies</code></li> <li><code>paleoData_units</code></li> <li><code>paleoData_values</code></li> <li><code>paleoData_variableName</code></li> <li><code>year</code></li> <li><code>yearUnits</code></li> <li>(optional: <code>DuplicateDetails</code>)</li> </ul> <p>Make sure the repo_root is added correctly, it should be: your_root_dir/dod2k This should be the working directory throughout this notebook (and all other notebooks).</p> In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n\nimport sys\nimport os\nfrom pathlib import Path\n\n# Add parent directory to path (works from any notebook in notebooks/)\n# the repo_root should be the parent directory of the notebooks folder\ncurrent_dir = Path().resolve()\n# Determine repo root\nif current_dir.name == 'dod2k': repo_root = current_dir\nelif current_dir.parent.name == 'dod2k': repo_root = current_dir.parent\nelse: raise Exception('Please review the repo root structure (see first cell).')\n\n# Update cwd and path only if needed\nif os.getcwd() != str(repo_root):\n    os.chdir(repo_root)\nif str(repo_root) not in sys.path:\n    sys.path.insert(0, str(repo_root))\n\nprint(f\"Repo root: {repo_root}\")\nif str(os.getcwd())==str(repo_root):\n    print(f\"Working directory matches repo root. \")\n</pre> %load_ext autoreload %autoreload 2  import sys import os from pathlib import Path  # Add parent directory to path (works from any notebook in notebooks/) # the repo_root should be the parent directory of the notebooks folder current_dir = Path().resolve() # Determine repo root if current_dir.name == 'dod2k': repo_root = current_dir elif current_dir.parent.name == 'dod2k': repo_root = current_dir.parent else: raise Exception('Please review the repo root structure (see first cell).')  # Update cwd and path only if needed if os.getcwd() != str(repo_root):     os.chdir(repo_root) if str(repo_root) not in sys.path:     sys.path.insert(0, str(repo_root))  print(f\"Repo root: {repo_root}\") if str(os.getcwd())==str(repo_root):     print(f\"Working directory matches repo root. \") <pre>Repo root: /home/jupyter-lluecke/dod2k\nWorking directory matches repo root. \n</pre> In\u00a0[2]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature \nfrom matplotlib.gridspec import GridSpec as GS\nfrom copy import deepcopy as dc\n\nfrom dod2k_utilities import ut_functions as utf # contains utility functions\nfrom dod2k_utilities import ut_plot as uplt # contains plotting functions\n</pre> import pandas as pd import numpy as np import matplotlib.pyplot as plt import cartopy.crs as ccrs import cartopy.feature as cfeature  from matplotlib.gridspec import GridSpec as GS from copy import deepcopy as dc  from dod2k_utilities import ut_functions as utf # contains utility functions from dod2k_utilities import ut_plot as uplt # contains plotting functions <p>Read compact dataframe.</p> <p>{db_name} refers to the database, including</p> <ul> <li>database of databases:<ul> <li>dod2k_v2.0 (dod2k: duplicate free, merged database)</li> <li>dod2k_v2.0_MT (filtered for MT sensitive proxies only)</li> <li>all_merged (NOT filtered for duplicates, only fusion of the input databases)</li> </ul> </li> <li>original databases:<ul> <li>fe23</li> <li>ch2k</li> <li>sisal</li> <li>pages2k</li> <li>iso2k</li> </ul> </li> </ul> <p>All compact dataframes are saved in {repo_root}/data/{db_name} as {db_name}_compact.csv.</p> In\u00a0[3]: Copied! <pre># read dataframe, choose from the list below, or specify your own\n\ndb_name = 'dod2k_v2.0'\n# db_name = 'pages2k'\n# load dataframe\ndf = utf.load_compact_dataframe_from_csv(db_name)\nprint(df.info())\ndf.name = db_name\n</pre> # read dataframe, choose from the list below, or specify your own  db_name = 'dod2k_v2.0' # db_name = 'pages2k' # load dataframe df = utf.load_compact_dataframe_from_csv(db_name) print(df.info()) df.name = db_name  <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4957 entries, 0 to 4956\nData columns (total 22 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   archiveType                    4957 non-null   object \n 1   dataSetName                    4957 non-null   object \n 2   datasetId                      4957 non-null   object \n 3   duplicateDetails               4957 non-null   object \n 4   geo_meanElev                   4875 non-null   float32\n 5   geo_meanLat                    4957 non-null   float32\n 6   geo_meanLon                    4957 non-null   float32\n 7   geo_siteName                   4957 non-null   object \n 8   interpretation_direction       4957 non-null   object \n 9   interpretation_seasonality     4957 non-null   object \n 10  interpretation_variable        4957 non-null   object \n 11  interpretation_variableDetail  4957 non-null   object \n 12  originalDataURL                4957 non-null   object \n 13  originalDatabase               4957 non-null   object \n 14  paleoData_notes                4957 non-null   object \n 15  paleoData_proxy                4957 non-null   object \n 16  paleoData_sensorSpecies        4957 non-null   object \n 17  paleoData_units                4957 non-null   object \n 18  paleoData_values               4957 non-null   object \n 19  paleoData_variableName         4957 non-null   object \n 20  year                           4957 non-null   object \n 21  yearUnits                      4957 non-null   object \ndtypes: float32(3), object(19)\nmemory usage: 794.0+ KB\nNone\n</pre> <p>define colours</p> In\u00a0[4]: Copied! <pre>cols = [ '#4477AA', '#EE6677', '#228833', '#CCBB44', '#66CCEE', '#AA3377', '#BBBBBB', '#44AA99', '#332288']\n</pre> cols = [ '#4477AA', '#EE6677', '#228833', '#CCBB44', '#66CCEE', '#AA3377', '#BBBBBB', '#44AA99', '#332288'] <p>How many entries does each archive type have?</p> In\u00a0[5]: Copied! <pre># count archive types\narchive_count = {}\nfor ii, at in enumerate(set(df['archiveType'])):\n    archive_count[at] = df.loc[df['archiveType']==at, 'archiveType'].count()\n</pre> # count archive types archive_count = {} for ii, at in enumerate(set(df['archiveType'])):     archive_count[at] = df.loc[df['archiveType']==at, 'archiveType'].count() <p>Specify colour for each archive (smaller archives get grouped into the same colour)</p> In\u00a0[6]: Copied! <pre>sort = np.argsort([cc for cc in archive_count.values()])\narchives_sorted = np.array([cc for cc in archive_count.keys()])[sort][::-1]\n\narchive_colour, major_archives, other_archives = uplt.get_archive_colours(archives_sorted, archive_count, cols)\n</pre> sort = np.argsort([cc for cc in archive_count.values()]) archives_sorted = np.array([cc for cc in archive_count.keys()])[sort][::-1]  archive_colour, major_archives, other_archives = uplt.get_archive_colours(archives_sorted, archive_count, cols)  <pre>0 Wood 3650\n1 Speleothem 567\n2 Coral 312\n3 GlacierIce 162\n4 MarineSediment 126\n5 LakeSediment 111\n6 Documents 13\n7 Sclerosponge 6\n8 GroundIce 4\n9 Borehole 3\n10 Other 2\n11 MolluskShell 1\n</pre> <p>How many entries does each proxy type have, per archive?</p> <p>Results in dictionary with two levels: archive_proxy_count[archive][proxy]</p> In\u00a0[7]: Copied! <pre>archive_proxy_count = {}\narchive_proxy_ticks = []\nfor ii, at in enumerate(set(df['archiveType'])):\n    proxy_types   = df['paleoData_proxy'][df['archiveType']==at].unique()\n    for pt in proxy_types:\n        cc = df['paleoData_proxy'][(df['paleoData_proxy']==pt)&amp;(df['archiveType']==at)].count()\n        archive_proxy_count['%s: %s'%(at, pt)] = cc\n        archive_proxy_ticks += [at+': '+pt]\n</pre> archive_proxy_count = {} archive_proxy_ticks = [] for ii, at in enumerate(set(df['archiveType'])):     proxy_types   = df['paleoData_proxy'][df['archiveType']==at].unique()     for pt in proxy_types:         cc = df['paleoData_proxy'][(df['paleoData_proxy']==pt)&amp;(df['archiveType']==at)].count()         archive_proxy_count['%s: %s'%(at, pt)] = cc         archive_proxy_ticks += [at+': '+pt] <p>How many entries does each proxy type have per archive? When bundling the rare ones into 'other'</p> <p>Plot the major proxy types per archive in descending order. Omit the rare archive types (less than ten records)</p> In\u00a0[8]: Copied! <pre># plot a bar chart of the number of proxy types included in the dataset\n\nfig = uplt.plot_count_proxy_by_archive_short(df, archive_proxy_count, archive_proxy_ticks, archive_colour) \n\nutf.save_fig(fig, '%s_proxy_count_short'%df.name, dir=f'{df.name}')\n</pre> # plot a bar chart of the number of proxy types included in the dataset  fig = uplt.plot_count_proxy_by_archive_short(df, archive_proxy_count, archive_proxy_ticks, archive_colour)   utf.save_fig(fig, '%s_proxy_count_short'%df.name, dir=f'{df.name}')  <pre>saved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0/dod2k_v2.0_proxy_count_short.pdf\n</pre> <p>Plot all counts per archive per proxy types, including the rare ones, in descending order.</p> In\u00a0[9]: Copied! <pre># plot a bar chart of the number of proxy types included in the dataset\n\nfig = uplt.plot_count_proxy_by_archive_all(df, archive_proxy_count, archive_proxy_ticks, archive_colour)\n\nutf.save_fig(fig, '%s_proxy_count_long'%df.name, dir=f'{df.name}')\n</pre> # plot a bar chart of the number of proxy types included in the dataset  fig = uplt.plot_count_proxy_by_archive_all(df, archive_proxy_count, archive_proxy_ticks, archive_colour)  utf.save_fig(fig, '%s_proxy_count_long'%df.name, dir=f'{df.name}')  <pre>saved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0/dod2k_v2.0_proxy_count_long.pdf\n</pre> <p>Visualise the availability of archive types over time (rare archive types are bundled together as other archives)</p> <p>Plot the coverage over the Common Era</p> <ol> <li>logarithmic</li> <li>linear</li> </ol> In\u00a0[10]: Copied! <pre>for ysc in ['log', 'linear']:\n    fig, years, coverage, coverage_by_archive = uplt.plot_coverage(df, archives_sorted, major_archives, other_archives, archive_colour, \n                                              ysc=ysc, return_data=True)\n</pre> for ysc in ['log', 'linear']:     fig, years, coverage, coverage_by_archive = uplt.plot_coverage(df, archives_sorted, major_archives, other_archives, archive_colour,                                                ysc=ysc, return_data=True)  <p>Compare the availability over the Common Era to PAGES 2k- compute the coverage etc as above but for PAGES 2k</p> In\u00a0[11]: Copied! <pre>#%% plot the coverage of all records and coverage per archive \n# FOR PAGES2k!!!!\n\ndf_pages = utf.load_compact_dataframe_from_csv('pages2k')\n\n\nfor ysc in ['log', 'linear']:\n    fig, years_pages, coverage_pages, coverage_by_archive_pages = uplt.plot_coverage(df_pages, archives_sorted, \n                                                                                     major_archives, other_archives, \n                                                                                     archive_colour, ysc=ysc, \n                                                                                     return_data=True)\n\n\n# MinY_pages    = np.array([min([float(sy) for sy in yy])  for yy in df_pages['year']]) # find minimum year for each record\n# MaxY_pages    = np.array([max([float(sy) for sy in yy])  for yy in df_pages['year']]) # find maximum year for each record\n# years_pages   = np.arange(min(MinY_pages), max(MaxY_pages)+1)\n\n# # generate array of coverage (how many records are available each year, in total)\n# coverage_pages = np.zeros(years_pages.shape[0])\n# for ii in range(len(df_pages['year'])):\n#     coverage_pages[(years_pages&gt;=MinY_pages[ii])&amp;(years_pages&lt;=MaxY_pages[ii])] += 1\n</pre> #%% plot the coverage of all records and coverage per archive  # FOR PAGES2k!!!!  df_pages = utf.load_compact_dataframe_from_csv('pages2k')   for ysc in ['log', 'linear']:     fig, years_pages, coverage_pages, coverage_by_archive_pages = uplt.plot_coverage(df_pages, archives_sorted,                                                                                       major_archives, other_archives,                                                                                       archive_colour, ysc=ysc,                                                                                       return_data=True)   # MinY_pages    = np.array([min([float(sy) for sy in yy])  for yy in df_pages['year']]) # find minimum year for each record # MaxY_pages    = np.array([max([float(sy) for sy in yy])  for yy in df_pages['year']]) # find maximum year for each record # years_pages   = np.arange(min(MinY_pages), max(MaxY_pages)+1)  # # generate array of coverage (how many records are available each year, in total) # coverage_pages = np.zeros(years_pages.shape[0]) # for ii in range(len(df_pages['year'])): #     coverage_pages[(years_pages&gt;=MinY_pages[ii])&amp;(years_pages&lt;=MaxY_pages[ii])] += 1 In\u00a0[12]: Copied! <pre># # generate array of coverage for each archive type\n# # FOR PAGES2k!!!!\n\n# coverage_by_archive_pages = {arch: np.zeros(years_pages.shape[0]) for arch in major_archives+['other'] }\n# for arch in set(df_pages['archiveType']):\n#     arch_mask = df_pages['archiveType']==arch \n#     for ii in range(len(df_pages[arch_mask]['year'])):\n#         if arch not in major_archives: arch='other'\n#         cc = coverage_by_archive_pages[arch]\n#         coverage_by_archive_pages[arch][(years_pages&gt;=MinY_pages[arch_mask][ii])&amp;(years_pages&lt;=MaxY_pages[arch_mask][ii])] += 1\n        \n</pre> # # generate array of coverage for each archive type # # FOR PAGES2k!!!!  # coverage_by_archive_pages = {arch: np.zeros(years_pages.shape[0]) for arch in major_archives+['other'] } # for arch in set(df_pages['archiveType']): #     arch_mask = df_pages['archiveType']==arch  #     for ii in range(len(df_pages[arch_mask]['year'])): #         if arch not in major_archives: arch='other' #         cc = coverage_by_archive_pages[arch] #         coverage_by_archive_pages[arch][(years_pages&gt;=MinY_pages[arch_mask][ii])&amp;(years_pages&lt;=MaxY_pages[arch_mask][ii])] += 1          <p>Plot the coverage of PAGES 2k and DoD2k over the Common Era</p> In\u00a0[13]: Copied! <pre>ysc='log'\n\nfig = plt.figure(figsize=(8, 5), dpi=300)\ngrid = GS(9,2)\nax1 = plt.subplot(grid[1:,0])\nax2 = plt.subplot(grid[1:,1])\nymask  = (years&gt;=1)&amp;(years&lt;=2010)\nypmask = (years_pages&gt;=1)&amp;(years_pages&lt;=2010)\nax1.step(years[ymask], coverage[ymask], color='k', label='all', lw=3)\nax2.plot(years_pages[ypmask], coverage_pages[ypmask], color='k', label='all', lw=3)#, dashes=[2,1,1,1])\n\nax1.set_title('DoD 2k', y=0.9)\nax2.set_title('PAGES 2k', y=0.9)\n\nfor ax in [ax1, ax2]:\n    ax.set_xlabel('year')\n    \n    ax.set_xlim(-50, 2030)\n    ax.grid(False)\n\nlinestyles=['-','--',':','-.', (5, (10, 1)),  (0, (5, 1)),  (0, (3, 1, 1, 1)), (0, (3, 1, 1, 1, 1, 1)) ]*2\n            \nfor ii, arch in enumerate(major_archives+['other']):\n    ax1.step(years, coverage_by_archive[arch], color=archive_colour[arch],\n             label=arch+'', lw=1.8, ls=linestyles[ii])\n    ax2.step(years_pages[ypmask], \n             coverage_by_archive_pages[arch][ypmask], \n             color=archive_colour[arch], ls=linestyles[ii],#dashes=[2,1,1,1],#ls='--',\n             label=arch+'', lw=1.8)\n    \nfor ax in [ax1, ax2]:\n    ax.set_yscale(ysc)\n\nax1.set_ylabel('# of records')\nax2.yaxis.set_ticks([])\nyl = ax1.get_ylim()\nax2.set_ylim(yl)\n\ngrid.tight_layout(fig)\nh1, l1 = ax1.get_legend_handles_labels()\nh2, l2 = ax2.get_legend_handles_labels()\nl_ax = plt.subplot(grid[0,:])\nl_ax.legend(h1,l1, ncol=4, framealpha=0, bbox_to_anchor=(0,0), loc='lower left' )\nl_ax.set_axis_off()\n\nl_ax.set_axis_off()\n\ngrid.tight_layout(fig)\ngrid.update(hspace=0.03,wspace=0.03)\n\nutf.save_fig(fig, f'{df.name}_temporal_avail_compared_to_PAGES2k_{ysc.upper()}', dir=df.name)\n</pre> ysc='log'  fig = plt.figure(figsize=(8, 5), dpi=300) grid = GS(9,2) ax1 = plt.subplot(grid[1:,0]) ax2 = plt.subplot(grid[1:,1]) ymask  = (years&gt;=1)&amp;(years&lt;=2010) ypmask = (years_pages&gt;=1)&amp;(years_pages&lt;=2010) ax1.step(years[ymask], coverage[ymask], color='k', label='all', lw=3) ax2.plot(years_pages[ypmask], coverage_pages[ypmask], color='k', label='all', lw=3)#, dashes=[2,1,1,1])  ax1.set_title('DoD 2k', y=0.9) ax2.set_title('PAGES 2k', y=0.9)  for ax in [ax1, ax2]:     ax.set_xlabel('year')          ax.set_xlim(-50, 2030)     ax.grid(False)  linestyles=['-','--',':','-.', (5, (10, 1)),  (0, (5, 1)),  (0, (3, 1, 1, 1)), (0, (3, 1, 1, 1, 1, 1)) ]*2              for ii, arch in enumerate(major_archives+['other']):     ax1.step(years, coverage_by_archive[arch], color=archive_colour[arch],              label=arch+'', lw=1.8, ls=linestyles[ii])     ax2.step(years_pages[ypmask],               coverage_by_archive_pages[arch][ypmask],               color=archive_colour[arch], ls=linestyles[ii],#dashes=[2,1,1,1],#ls='--',              label=arch+'', lw=1.8)      for ax in [ax1, ax2]:     ax.set_yscale(ysc)  ax1.set_ylabel('# of records') ax2.yaxis.set_ticks([]) yl = ax1.get_ylim() ax2.set_ylim(yl)  grid.tight_layout(fig) h1, l1 = ax1.get_legend_handles_labels() h2, l2 = ax2.get_legend_handles_labels() l_ax = plt.subplot(grid[0,:]) l_ax.legend(h1,l1, ncol=4, framealpha=0, bbox_to_anchor=(0,0), loc='lower left' ) l_ax.set_axis_off()  l_ax.set_axis_off()  grid.tight_layout(fig) grid.update(hspace=0.03,wspace=0.03)  utf.save_fig(fig, f'{df.name}_temporal_avail_compared_to_PAGES2k_{ysc.upper()}', dir=df.name) <pre>saved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0/dod2k_v2.0_temporal_avail_compared_to_PAGES2k_LOG.pdf\n</pre> In\u00a0[14]: Copied! <pre># plot outcome on two axis (left: total records, right: coverage per archive)\n# FOR PAGES2k!!!!\n\nysc='log'\n\nfig = plt.figure(figsize=(8, 5), dpi=300)\ngrid = GS(9,2)\nax1 = plt.subplot(grid[1:,0])\nax2 = plt.subplot(grid[1:,1])\n\nax1.set_title('DoD 2k', y=0.9)\nax2.set_title('PAGES 2k', y=0.9)\n\nfor ax in [ax1, ax2]:\n    ax.set_xlabel('year')    \n    ax.set_xlim(-50, 2030)\n    ax.grid(False)\n\n\nbaseline = np.zeros(years.shape[0])\nbaseline_p = np.zeros(years_pages.shape[0])\nfor ii, arch in enumerate((major_archives+['other'])[::-1]):\n    \n    ax1.fill_between(years, baseline, baseline+coverage_by_archive[arch], \n                     step='pre', label=arch,#alpha=0.5,\n                    color=archive_colour[arch])\n    ax2.fill_between(years_pages, baseline_p, baseline_p+coverage_by_archive_pages[arch], \n                     step='pre', #alpha=0.5,\n                    color=archive_colour[arch])\n    \n    baseline+=coverage_by_archive[arch]\n    baseline_p+=coverage_by_archive_pages[arch]\nfor ax in [ax1, ax2]:\n    ax.set_yscale(ysc)\n\nax1.set_ylabel('# of records')\nax2.yaxis.set_ticks([])\n\nyl = ax1.get_ylim()\nax2.set_ylim(yl)\n\n\ngrid.tight_layout(fig)\nh1, l1 = ax1.get_legend_handles_labels()\nl_ax = plt.subplot(grid[0,:])\nl_ax.legend(h1,l1, ncol=4, framealpha=0, bbox_to_anchor=(0,0), loc='lower left' )\nl_ax.set_axis_off()\n\ngrid.tight_layout(fig)\ngrid.update(hspace=0.03,wspace=0.03)\n\nutf.save_fig(fig, f'{df.name}_temporal_avail_compared_to_PAGES2k_cumulative_{ysc.upper()}', \n             dir=df.name)\n</pre> # plot outcome on two axis (left: total records, right: coverage per archive) # FOR PAGES2k!!!!  ysc='log'  fig = plt.figure(figsize=(8, 5), dpi=300) grid = GS(9,2) ax1 = plt.subplot(grid[1:,0]) ax2 = plt.subplot(grid[1:,1])  ax1.set_title('DoD 2k', y=0.9) ax2.set_title('PAGES 2k', y=0.9)  for ax in [ax1, ax2]:     ax.set_xlabel('year')         ax.set_xlim(-50, 2030)     ax.grid(False)   baseline = np.zeros(years.shape[0]) baseline_p = np.zeros(years_pages.shape[0]) for ii, arch in enumerate((major_archives+['other'])[::-1]):          ax1.fill_between(years, baseline, baseline+coverage_by_archive[arch],                       step='pre', label=arch,#alpha=0.5,                     color=archive_colour[arch])     ax2.fill_between(years_pages, baseline_p, baseline_p+coverage_by_archive_pages[arch],                       step='pre', #alpha=0.5,                     color=archive_colour[arch])          baseline+=coverage_by_archive[arch]     baseline_p+=coverage_by_archive_pages[arch] for ax in [ax1, ax2]:     ax.set_yscale(ysc)  ax1.set_ylabel('# of records') ax2.yaxis.set_ticks([])  yl = ax1.get_ylim() ax2.set_ylim(yl)   grid.tight_layout(fig) h1, l1 = ax1.get_legend_handles_labels() l_ax = plt.subplot(grid[0,:]) l_ax.legend(h1,l1, ncol=4, framealpha=0, bbox_to_anchor=(0,0), loc='lower left' ) l_ax.set_axis_off()  grid.tight_layout(fig) grid.update(hspace=0.03,wspace=0.03)  utf.save_fig(fig, f'{df.name}_temporal_avail_compared_to_PAGES2k_cumulative_{ysc.upper()}',               dir=df.name)  <pre>saved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0/dod2k_v2.0_temporal_avail_compared_to_PAGES2k_cumulative_LOG.pdf\n</pre> <p>Plot the distriobution of the records over the globe, colour coded by archive, marker type refers to proxy type. Count in legend.</p> <p>Bundle the rare archives into 'other' and do not distinguish procy types for these.</p> In\u00a0[15]: Copied! <pre>archive_proxy_count_short = {}\nfor ii, at in enumerate(archives_sorted):\n    proxy_types   = df['paleoData_proxy'][df['archiveType']==at].unique()\n    if at not in archive_proxy_count_short:\n        archive_proxy_count_short[at]={}\n    for pt in proxy_types:\n        cc = df['paleoData_proxy'][(df['paleoData_proxy']==pt)&amp;(df['archiveType']==at)].count()\n        \n        if cc&lt;=10:\n            if 'other %s'%at not in archive_proxy_count_short[at]:\n                archive_proxy_count_short[at]['other %s'%at]=0\n            archive_proxy_count_short[at]['other %s'%at] += cc\n        else:\n            archive_proxy_count_short[at]['%s: %s'%(at, pt)] = cc\n</pre> archive_proxy_count_short = {} for ii, at in enumerate(archives_sorted):     proxy_types   = df['paleoData_proxy'][df['archiveType']==at].unique()     if at not in archive_proxy_count_short:         archive_proxy_count_short[at]={}     for pt in proxy_types:         cc = df['paleoData_proxy'][(df['paleoData_proxy']==pt)&amp;(df['archiveType']==at)].count()                  if cc&lt;=10:             if 'other %s'%at not in archive_proxy_count_short[at]:                 archive_proxy_count_short[at]['other %s'%at]=0             archive_proxy_count_short[at]['other %s'%at] += cc         else:             archive_proxy_count_short[at]['%s: %s'%(at, pt)] = cc In\u00a0[16]: Copied! <pre>#%% plot the spatial distribution of all records, group short records together\n\nfig = uplt.plot_geo_archive_proxy_short(df, archives_sorted, archive_proxy_count_short, archive_colour)\n\nutf.save_fig(fig, f'{df.name}_spatial_short', dir=df.name)\n</pre> #%% plot the spatial distribution of all records, group short records together  fig = uplt.plot_geo_archive_proxy_short(df, archives_sorted, archive_proxy_count_short, archive_colour)  utf.save_fig(fig, f'{df.name}_spatial_short', dir=df.name)  <pre>['Wood: ARSTAN', 'Wood: d18O', 'Wood: maximum latewood density', 'Wood: residual chronology', 'Wood: ring width', 'other Wood']\n['Speleothem: Mg/Ca', 'Speleothem: d13C', 'Speleothem: d18O', 'Speleothem: growth rate']\n['Coral: Sr/Ca', 'Coral: calcification rate', 'Coral: d13C', 'Coral: d18O']\n['GlacierIce: d18O', 'GlacierIce: dD', 'other GlacierIce']\n['MarineSediment: Mg/Ca', 'MarineSediment: alkenone', 'MarineSediment: count', 'MarineSediment: d18O', 'MarineSediment: temperature', 'other MarineSediment']\n['LakeSediment: d18O', 'LakeSediment: dD', 'LakeSediment: pollen', 'other LakeSediment']\n['Documents: historical']\n['other Sclerosponge']\n['other GroundIce']\n['other Borehole']\n['other Other']\n['other MolluskShell']\nsaved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0/dod2k_v2.0_spatial_short.pdf\n</pre> <p>Plot the distribution of the records over the globe, colour coded by archive, marker type refers to proxy type. Count in legend.</p> <p>Show all archive and proxy types.</p> In\u00a0[17]: Copied! <pre>#%% plot the spatial distribution of all records\n\nfig = uplt.plot_geo_archive_proxy(df, archive_colour, highlight_archives=major_archives)\nutf.save_fig(fig, f'{df.name}_spatial_all', dir=df.name)\n</pre> #%% plot the spatial distribution of all records  fig = uplt.plot_geo_archive_proxy(df, archive_colour, highlight_archives=major_archives) utf.save_fig(fig, f'{df.name}_spatial_all', dir=df.name)  <pre>saved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0/dod2k_v2.0_spatial_all.pdf\n</pre> <p>Visualise the geographical distribution of the climate interpretation of the records where this information is available.</p> In\u00a0[18]: Copied! <pre>[ci for ci in set(df['interpretation_variable']) if ci not in ['N/A', 'nan']]\n</pre> [ci for ci in set(df['interpretation_variable']) if ci not in ['N/A', 'nan']] Out[18]: <pre>['NOT temperature NOT moisture',\n 'moisture',\n 'temperature',\n 'temperature+moisture']</pre> In\u00a0[19]: Copied! <pre>#%% plot the spatial distribution of all records\nproxy_lats = df['geo_meanLat'].values\nproxy_lons = df['geo_meanLon'].values\n\n# plots the map\nfig = plt.figure(figsize=(8, 6), dpi=350)\ngrid = GS(1, 3)\n\nax = plt.subplot(grid[:, :], projection=ccrs.Robinson()) # create axis with Robinson projection of globe\n\nax.add_feature(cfeature.LAND, alpha=0.5) # adds land features\nax.add_feature(cfeature.OCEAN, alpha=0.3, facecolor='#C5DEEA') # adds ocean features\nax.coastlines() # adds coastline features\n\nax.set_global()\nmt = 'sDo^s&lt;&gt;pP*XDdh'*10 # generates string of marker types\ncolors  = ['tab:grey', \n           'tab:purple', 'tab:blue', 'tab:red']\n\nfor jj, ci in enumerate([ci for ci in set(df['interpretation_variable']) if ci in ['NOT temperature NOT moisture', 'temperature+moisture','temperature', 'moisture']]):\n    ci_mask = df['interpretation_variable']==ci\n    plt.scatter(proxy_lons[ci_mask], proxy_lats[ci_mask], \n                transform=ccrs.PlateCarree(), zorder=999,\n                marker=mt[jj], color=colors[jj], \n                label=ci, lw=.3, ec='k', s=70)\n\nplt.legend(bbox_to_anchor=(0.03,-0.01), loc='upper left', ncol=2, \n           fontsize=12, framealpha=0)\ngrid.tight_layout(fig)\n\nutf.save_fig(fig, '%s_spatial_climateInterpretation'%df.name, dir=df.name)\n</pre> #%% plot the spatial distribution of all records proxy_lats = df['geo_meanLat'].values proxy_lons = df['geo_meanLon'].values  # plots the map fig = plt.figure(figsize=(8, 6), dpi=350) grid = GS(1, 3)  ax = plt.subplot(grid[:, :], projection=ccrs.Robinson()) # create axis with Robinson projection of globe  ax.add_feature(cfeature.LAND, alpha=0.5) # adds land features ax.add_feature(cfeature.OCEAN, alpha=0.3, facecolor='#C5DEEA') # adds ocean features ax.coastlines() # adds coastline features  ax.set_global() mt = 'sDo^s&lt;&gt;pP*XDdh'*10 # generates string of marker types colors  = ['tab:grey',             'tab:purple', 'tab:blue', 'tab:red']  for jj, ci in enumerate([ci for ci in set(df['interpretation_variable']) if ci in ['NOT temperature NOT moisture', 'temperature+moisture','temperature', 'moisture']]):     ci_mask = df['interpretation_variable']==ci     plt.scatter(proxy_lons[ci_mask], proxy_lats[ci_mask],                  transform=ccrs.PlateCarree(), zorder=999,                 marker=mt[jj], color=colors[jj],                  label=ci, lw=.3, ec='k', s=70)  plt.legend(bbox_to_anchor=(0.03,-0.01), loc='upper left', ncol=2,             fontsize=12, framealpha=0) grid.tight_layout(fig)  utf.save_fig(fig, '%s_spatial_climateInterpretation'%df.name, dir=df.name)  <pre>saved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0/dod2k_v2.0_spatial_climateInterpretation.pdf\n</pre> <p>Visualise the raw data for each archive.</p> <p>Note this is purely for sanity purposes and might show different units on the same axis!!</p> <p>So not a correct scientific visualisation.</p> <p>Better to convert to z-scores first (as in the next figure).</p> In\u00a0[20]: Copied! <pre>archive_types = np.unique(df['archiveType'])\nfig = plt.figure(figsize=(15, len(archive_types)*3), dpi=200)\ngrid = GS(len(archive_types), 1)\n\n# loop through the data to generate a scatter plot of each data record:\n# 1st loop: go through archive types individually (determines marker type)\n# 2nd loop: through paleo proxy types attributed to the specific archive, which is colour coded\nfor jj, at in enumerate(archive_types):\n    ax = plt.subplot(grid[jj, :])\n    arch_mask = df['archiveType']==at\n    arch_proxy_types = np.unique(df['paleoData_proxy'][arch_mask])\n    # col = f.get_colours(range(len(arch_proxy_types)), 'tab10', 0, len(arch_proxy_types))\n    for ii, pt in enumerate(arch_proxy_types):\n        pt_mask = df['paleoData_proxy']==pt\n        at_mask = df['archiveType']==at\n        df_slice = df[(df['paleoData_proxy']==pt)&amp;(df['archiveType']==at)]\n        for ijk in range(df_slice.shape[0]):\n            label=f'{pt} ({df_slice.iloc[ijk]['paleoData_units']})' if ijk==0 else None\n            x = np.array(df_slice.iloc[ijk]['year'], dtype=float)\n            sort = np.argsort(x)\n            y = np.array(df_slice.iloc[ijk]['paleoData_values'], dtype=float)\n            ax.plot(x[sort], y[sort],  lw=.7, alpha=.99, label=label, color=archive_colour[at])\n        ax.set_xlabel('year (%s)'%df_slice.iloc[0]['yearUnits'])\n        ax.set_ylabel(at)\n    ax.legend(ncol=3)\ngrid.tight_layout(fig)\n\nutf.save_fig(fig, '%s_data'%df.name, dir=df.name)\n</pre> archive_types = np.unique(df['archiveType']) fig = plt.figure(figsize=(15, len(archive_types)*3), dpi=200) grid = GS(len(archive_types), 1)  # loop through the data to generate a scatter plot of each data record: # 1st loop: go through archive types individually (determines marker type) # 2nd loop: through paleo proxy types attributed to the specific archive, which is colour coded for jj, at in enumerate(archive_types):     ax = plt.subplot(grid[jj, :])     arch_mask = df['archiveType']==at     arch_proxy_types = np.unique(df['paleoData_proxy'][arch_mask])     # col = f.get_colours(range(len(arch_proxy_types)), 'tab10', 0, len(arch_proxy_types))     for ii, pt in enumerate(arch_proxy_types):         pt_mask = df['paleoData_proxy']==pt         at_mask = df['archiveType']==at         df_slice = df[(df['paleoData_proxy']==pt)&amp;(df['archiveType']==at)]         for ijk in range(df_slice.shape[0]):             label=f'{pt} ({df_slice.iloc[ijk]['paleoData_units']})' if ijk==0 else None             x = np.array(df_slice.iloc[ijk]['year'], dtype=float)             sort = np.argsort(x)             y = np.array(df_slice.iloc[ijk]['paleoData_values'], dtype=float)             ax.plot(x[sort], y[sort],  lw=.7, alpha=.99, label=label, color=archive_colour[at])         ax.set_xlabel('year (%s)'%df_slice.iloc[0]['yearUnits'])         ax.set_ylabel(at)     ax.legend(ncol=3) grid.tight_layout(fig)  utf.save_fig(fig, '%s_data'%df.name, dir=df.name) <pre>/tmp/ipykernel_2490791/2112895674.py:26: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n  grid.tight_layout(fig)\n</pre> <pre>saved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0/dod2k_v2.0_data.pdf\n</pre> <pre>/home/jupyter-lluecke/.local/lib/python3.13/site-packages/IPython/core/events.py:82: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n  func(*args, **kwargs)\n/home/jupyter-lluecke/.local/lib/python3.13/site-packages/IPython/core/pylabtools.py:170: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n  fig.canvas.print_figure(bytes_io, **kw)\n</pre> In\u00a0[21]: Copied! <pre>fig = plt.figure(figsize=(15, 5), dpi=200)\n\n# loop through the data to generate a scatter plot of each data record:\n# 1st loop: go through archive types individually (determines marker type)\n# 2nd loop: through paleo proxy types attributed to the specific archive, which is colour coded\nx0, x1 = 0, 2020\nax   = plt.gca()\nax2  = ax.twinx()\nfor jj, at in enumerate(archive_types):\n    arch_mask = df['archiveType']==at\n    arch_proxy_types = np.unique(df['paleoData_proxy'][arch_mask])\n    col = uplt.get_colours(range(len(arch_proxy_types)), 'tab10', 0, len(arch_proxy_types))\n    for ii, pt in enumerate(arch_proxy_types):\n        pt_mask = df['paleoData_proxy']==pt\n        at_mask = df['archiveType']==at\n        df_slice = df[(df['paleoData_proxy']==pt)&amp;(df['archiveType']==at)]\n        for ijk in range(df_slice.shape[0]):\n            label=at if ((ijk==0)&amp;(ii==0)) else None\n            x = np.array(df_slice.iloc[ijk]['year'], dtype=float)\n            sort = np.argsort(x)\n            x = x[sort]\n            y = np.array(df_slice.iloc[ijk]['paleoData_values'], dtype=float)\n            y = y[sort]\n            mask = ((x&gt;x0)&amp;(x&lt;=x1))\n            y -= np.mean(y[mask])\n            y /= np.std(y[mask])\n            # ax = ax1 if ~np.any(y&lt;-5000) else ax2\n            ax.plot(x[mask], y[mask], lw=.5, \n                     alpha=.5, label=label, color=archive_colour[at])\n        ax.set_xlabel('year (%s)'%df_slice.iloc[0]['yearUnits'])\n        ax.set_ylabel('z-scores')\n    leg = ax.legend(ncol=7, bbox_to_anchor=(0,1), loc='lower left')\n    for line in leg.get_lines():\n        line.set_lw(2)\n    # ax.set_xlim(0,2020)\n    #ax.set_ylim(-1000,2000)\n#grid.tight_layout(fig)\nax2.plot(years[(years&gt;=x0)&amp;(years&lt;=x1)], coverage[(years&gt;=x0)&amp;(years&lt;=x1)], lw=2, color='k')\nax2.set_ylabel('coverage (# records)')\nutf.save_fig(fig, f'{df.name}_data_z-scores', dir=df.name)\nplt.show()\n</pre> fig = plt.figure(figsize=(15, 5), dpi=200)  # loop through the data to generate a scatter plot of each data record: # 1st loop: go through archive types individually (determines marker type) # 2nd loop: through paleo proxy types attributed to the specific archive, which is colour coded x0, x1 = 0, 2020 ax   = plt.gca() ax2  = ax.twinx() for jj, at in enumerate(archive_types):     arch_mask = df['archiveType']==at     arch_proxy_types = np.unique(df['paleoData_proxy'][arch_mask])     col = uplt.get_colours(range(len(arch_proxy_types)), 'tab10', 0, len(arch_proxy_types))     for ii, pt in enumerate(arch_proxy_types):         pt_mask = df['paleoData_proxy']==pt         at_mask = df['archiveType']==at         df_slice = df[(df['paleoData_proxy']==pt)&amp;(df['archiveType']==at)]         for ijk in range(df_slice.shape[0]):             label=at if ((ijk==0)&amp;(ii==0)) else None             x = np.array(df_slice.iloc[ijk]['year'], dtype=float)             sort = np.argsort(x)             x = x[sort]             y = np.array(df_slice.iloc[ijk]['paleoData_values'], dtype=float)             y = y[sort]             mask = ((x&gt;x0)&amp;(x&lt;=x1))             y -= np.mean(y[mask])             y /= np.std(y[mask])             # ax = ax1 if ~np.any(y&lt;-5000) else ax2             ax.plot(x[mask], y[mask], lw=.5,                       alpha=.5, label=label, color=archive_colour[at])         ax.set_xlabel('year (%s)'%df_slice.iloc[0]['yearUnits'])         ax.set_ylabel('z-scores')     leg = ax.legend(ncol=7, bbox_to_anchor=(0,1), loc='lower left')     for line in leg.get_lines():         line.set_lw(2)     # ax.set_xlim(0,2020)     #ax.set_ylim(-1000,2000) #grid.tight_layout(fig) ax2.plot(years[(years&gt;=x0)&amp;(years&lt;=x1)], coverage[(years&gt;=x0)&amp;(years&lt;=x1)], lw=2, color='k') ax2.set_ylabel('coverage (# records)') utf.save_fig(fig, f'{df.name}_data_z-scores', dir=df.name) plt.show() <pre>/tmp/ipykernel_2490791/3134449642.py:26: RuntimeWarning: invalid value encountered in divide\n  y /= np.std(y[mask])\n</pre> <pre>saved figure in /home/jupyter-lluecke/dod2k/figs/dod2k_v2.0/dod2k_v2.0_data_z-scores.pdf\n</pre> <p>How many entries does each original database have?</p> In\u00a0[22]: Copied! <pre># count archive types by originalDatabase (added by MNE 2025/01/07)\nprint(df.name)\noriginalDatabase_count  = {}\noriginalDatabase_labels = []\nfor ii, od in enumerate(set(df['originalDatabase'])):\n    originalDatabase_count[od] = df.loc[df['originalDatabase']==od, 'paleoData_proxy'].count()\n    originalDatabase_labels += [od]\n    print(od, originalDatabase_count[od])\n</pre> # count archive types by originalDatabase (added by MNE 2025/01/07) print(df.name) originalDatabase_count  = {} originalDatabase_labels = [] for ii, od in enumerate(set(df['originalDatabase'])):     originalDatabase_count[od] = df.loc[df['originalDatabase']==od, 'paleoData_proxy'].count()     originalDatabase_labels += [od]     print(od, originalDatabase_count[od]) <pre>dod2k_v2.0\nFE23 (Breitenmoser et al. (2014)) 2728\ndod2k_composite_z 2\nIso2k v1.1.2 413\nSISAL v3 528\nCoralHydro2k v1.0.1 145\nPAGES 2k v2.2.0 1141\n</pre> In\u00a0[23]: Copied! <pre>fig = plt.figure(figsize=(8, 5), dpi=500)\nax  = plt.gca()\n\nplt.bar(originalDatabase_labels, [originalDatabase_count[od] for od in originalDatabase_labels])\n\nax.set_xticks(np.arange(len(originalDatabase_labels)), \n              originalDatabase_labels, \n              rotation=45, ha='right', fontsize=10)\nplt.xlabel('original database')\nplt.ylabel('count')\n</pre>  fig = plt.figure(figsize=(8, 5), dpi=500) ax  = plt.gca()  plt.bar(originalDatabase_labels, [originalDatabase_count[od] for od in originalDatabase_labels])  ax.set_xticks(np.arange(len(originalDatabase_labels)),                originalDatabase_labels,                rotation=45, ha='right', fontsize=10) plt.xlabel('original database') plt.ylabel('count') Out[23]: <pre>Text(0, 0.5, 'count')</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/df_plot_dod2k/#visualise-dod2k","title":"Visualise DoD2k\u00b6","text":""},{"location":"notebooks/df_plot_dod2k/#set-up-working-environment","title":"Set up working environment\u00b6","text":""},{"location":"notebooks/df_plot_dod2k/#read-dataframe","title":"read dataframe\u00b6","text":""},{"location":"notebooks/df_plot_dod2k/#visualise","title":"visualise\u00b6","text":""},{"location":"notebooks/df_plot_dod2k/#bar-chart-of-dataset-composition-for-archive-and-proxy-types","title":"bar chart of dataset composition (for archive and proxy types)\u00b6","text":""},{"location":"notebooks/df_plot_dod2k/#temporal-availability-of-database-by-archive-type","title":"temporal availability of database, by archive type\u00b6","text":""},{"location":"notebooks/df_plot_dod2k/#geographical-coverage","title":"geographical coverage\u00b6","text":""},{"location":"notebooks/df_plot_dod2k/#climate-interpretation","title":"climate interpretation\u00b6","text":""},{"location":"notebooks/df_plot_dod2k/#data","title":"data\u00b6","text":""},{"location":"notebooks/df_plot_dod2k/#visualise-original-database","title":"Visualise original database\u00b6","text":""},{"location":"notebooks/dup_decision/","title":"Duplicate detection - step 2: review and decide on candidate pairs","text":"<p>This notebook runs the second part of the duplicate detection algorithm on a dataframe with the following columns:</p> <ul> <li><code>archiveType</code>       (used for duplicate detection algorithm)</li> <li><code>dataSetName</code></li> <li><code>datasetId</code></li> <li><code>geo_meanElev</code>      (used for duplicate detection algorithm)</li> <li><code>geo_meanLat</code>       (used for duplicate detection algorithm)</li> <li><code>geo_meanLon</code>       (used for duplicate detection algorithm)</li> <li><code>geo_siteName</code>      (used for duplicate detection algorithm)</li> <li><code>interpretation_direction</code></li> <li><code>interpretation_seasonality</code></li> <li><code>interpretation_variable</code></li> <li><code>interpretation_variableDetails</code></li> <li><code>originalDataURL</code></li> <li><code>originalDatabase</code></li> <li><code>paleoData_notes</code></li> <li><code>paleoData_proxy</code>   (used for duplicate detection algorithm)</li> <li><code>paleoData_units</code></li> <li><code>paleoData_values</code>  (used for duplicate detection algorithm, test for correlation, RMSE, correlation of 1st difference, RMSE of 1st difference)</li> <li><code>paleoData_variableName</code></li> <li><code>year</code>              (used for duplicate detection algorithm)</li> <li><code>yearUnits</code></li> </ul> <p>This interactive notebook runs a duplicate decision algorithm for a specific database, following the identification of the potential duplicate candidate pairs. The algorithm walks the operator through each of the detected duplicate candidate pairs from <code>dup_detection.ipynb</code> and runs a decision process to decide whether to keep or reject the identified records.</p> <p>The confirmed 'true' duplicates are saved in</p> <p><code>data/DATABASENAME/duplicate_detection/duplicate_decisions_DATABASENAME_AUTHORINITIALS_YY-MM-DD.csv</code></p> <p>10/11/2025 LL: tidied up with revised data organisation and prepared for documentation 27/11/2024 LL: Changed hierarchy FE23&gt;PAGES 2k 22/10/2024 v1: Updated the decision process: - created backup decision file which is intermediately saved - outputs URL which can be copied and pasted into browser - implemented a composite option in the decision process, to create a composite of two records</p> <p>Author: Lucie Luecke, created 27/9/2024</p> <p>Note: The algorithm can be either started from scratch or from a backup file:</p> <p>Make sure the repo_root is added correctly, it should be: your_root_dir/dod2k This should be the working directory throughout this notebook (and all other notebooks).</p> In\u00a0[10]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n\nimport sys\nimport os\nfrom pathlib import Path\n\n# Add parent directory to path (works from any notebook in notebooks/)\n# the repo_root should be the parent directory of the notebooks folder\ncurrent_dir = Path().resolve()\n# Determine repo root\nif current_dir.name == 'dod2k': repo_root = current_dir\nelif current_dir.parent.name == 'dod2k': repo_root = current_dir.parent\nelse: raise Exception('Please review the repo root structure (see first cell).')\n\n# Update cwd and path only if needed\nif os.getcwd() != str(repo_root):\n    os.chdir(repo_root)\nif str(repo_root) not in sys.path:\n    sys.path.insert(0, str(repo_root))\n\nprint(f\"Repo root: {repo_root}\")\nif str(os.getcwd())==str(repo_root):\n    print(f\"Working directory matches repo root. \")\n</pre> %load_ext autoreload %autoreload 2  import sys import os from pathlib import Path  # Add parent directory to path (works from any notebook in notebooks/) # the repo_root should be the parent directory of the notebooks folder current_dir = Path().resolve() # Determine repo root if current_dir.name == 'dod2k': repo_root = current_dir elif current_dir.parent.name == 'dod2k': repo_root = current_dir.parent else: raise Exception('Please review the repo root structure (see first cell).')  # Update cwd and path only if needed if os.getcwd() != str(repo_root):     os.chdir(repo_root) if str(repo_root) not in sys.path:     sys.path.insert(0, str(repo_root))  print(f\"Repo root: {repo_root}\") if str(os.getcwd())==str(repo_root):     print(f\"Working directory matches repo root. \") <pre>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\nRepo root: /home/jupyter-lluecke/dod2k\nWorking directory matches repo root. \n</pre> In\u00a0[11]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport datetime\n\nfrom dod2k_utilities import ut_functions as utf # contains utility functions\nfrom dod2k_utilities import ut_duplicate_search as dup # contains utility functions\n</pre> import pandas as pd import numpy as np import datetime  from dod2k_utilities import ut_functions as utf # contains utility functions from dod2k_utilities import ut_duplicate_search as dup # contains utility functions <p>Define the dataset which needs to be screened for duplicates. Input files for the duplicate detection mechanism need to be compact dataframes (<code>pandas</code> dataframes with standardised columns and entry formatting).</p> <p>The function <code>load_compact_dataframe_from_csv</code> loads the dataframe from a <code>csv</code> file from <code>data\\DB\\</code>, with <code>DB</code> the name of the database. The database name (<code>db_name</code>) can be</p> <ul> <li><code>pages2k</code></li> <li><code>ch2k</code></li> <li><code>iso2k</code></li> <li><code>sisal</code></li> <li><code>fe23</code></li> </ul> <p>for the individual databases,</p> <p>or use</p> <ul> <li><code>all_merged</code></li> </ul> <p>to load the merged database of all individual databases, or can be any user defined compact dataframe.</p> In\u00a0[12]: Copied! <pre># load dataframe\ndb_name='all_merged' \n# db_name='dup_test' \ndf = utf.load_compact_dataframe_from_csv(db_name)\n\nprint(df.info())\ndf.name = db_name\n</pre> # load dataframe db_name='all_merged'  # db_name='dup_test'  df = utf.load_compact_dataframe_from_csv(db_name)  print(df.info()) df.name = db_name  <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5147 entries, 0 to 5146\nData columns (total 21 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   archiveType                    5147 non-null   object \n 1   dataSetName                    5147 non-null   object \n 2   datasetId                      5147 non-null   object \n 3   geo_meanElev                   5048 non-null   float32\n 4   geo_meanLat                    5147 non-null   float32\n 5   geo_meanLon                    5147 non-null   float32\n 6   geo_siteName                   5147 non-null   object \n 7   interpretation_direction       5147 non-null   object \n 8   interpretation_seasonality     5147 non-null   object \n 9   interpretation_variable        5147 non-null   object \n 10  interpretation_variableDetail  5147 non-null   object \n 11  originalDataURL                5147 non-null   object \n 12  originalDatabase               5147 non-null   object \n 13  paleoData_notes                5147 non-null   object \n 14  paleoData_proxy                5147 non-null   object \n 15  paleoData_sensorSpecies        5147 non-null   object \n 16  paleoData_units                5147 non-null   object \n 17  paleoData_values               5147 non-null   object \n 18  paleoData_variableName         5147 non-null   object \n 19  year                           5147 non-null   object \n 20  yearUnits                      5147 non-null   object \ndtypes: float32(3), object(18)\nmemory usage: 784.2+ KB\nNone\n</pre> <p>In order to keep maximum transparency and reproduceability, put in the operator's credentials here.</p> <p>These details are used to flag the intermediate output files and provided along with the final duplicate free dataset.</p> In\u00a0[13]: Copied! <pre>initials = 'LL'\nfullname = 'Lucie Luecke'\nemail    = 'ljluec1@st-andrews.ac.uk'\n\n# initials = 'MNE'\n# fullname = 'Michael Evans'\n# email    = 'mnevans@umd.edu'\n\noperator_details = [initials, fullname, email]\n</pre> initials = 'LL' fullname = 'Lucie Luecke' email    = 'ljluec1@st-andrews.ac.uk'  # initials = 'MNE' # fullname = 'Michael Evans' # email    = 'mnevans@umd.edu'  operator_details = [initials, fullname, email] <p>We now start the duplicate decision process.</p> <p>For automated decisions, which apply to identical duplicates, we now define a hierarchy of databases, which decides which record should be kept.</p> <p>First, list all the original databases:</p> In\u00a0[14]: Copied! <pre>for db in df.originalDatabase.unique():\n    print(db)\n</pre> for db in df.originalDatabase.unique():     print(db) <pre>PAGES 2k v2.2.0\nFE23 (Breitenmoser et al. (2014))\nCoralHydro2k v1.0.1\nIso2k v1.1.2\nSISAL v3\n</pre> <p>Now assign a hierarchy (importance level) to the original databases. For $n$ original databases the hierarchy ranges from 1, the highest hierarchical value (most important, should always be kept), to the lowest value $n$ (least important, the lowest in the hierarchy).</p> <p>The operator can choose its own hierarchy by passing a dictionary as a kwarg to the function <code>dup.define_hierarchy</code>, where each database is assigned an interger number according to its importance (0 the most important).</p> <p>By default the hierarchy uses the novelty of the databases for determining the importance level:</p> <p><code>PAGES 2k v2.2.0</code> &gt; <code>SISAL v3</code> &gt; <code>CoralHydro2k v1.0.1</code> &gt; <code>Iso2k v1.1.2</code> &gt; <code>FE23 (Breitenmoser et al. (2014))</code></p> In\u00a0[26]: Copied! <pre># implement hierarchy for automated decisions for identical records\n\ndf = dup.define_hierarchy(df, hierarchy='default')\n</pre> # implement hierarchy for automated decisions for identical records  df = dup.define_hierarchy(df, hierarchy='default') <pre>Chosen hierarchy:\n1. PAGES 2k v2.2.0 (highest)\n2. SISAL v3\n3. CoralHydro2k v1.0.1\n4. Iso2k v1.1.2\n5. FE23 (Breitenmoser et al. (2014)) (lowest)\n</pre> In\u00a0[23]: Copied! <pre># make sure all datasetIds are unique.\nassert len(set(df.datasetId))==len(df.datasetId)\nassert (df.index==range(0, len(df), 1)).all()\n</pre> # make sure all datasetIds are unique. assert len(set(df.datasetId))==len(df.datasetId) assert (df.index==range(0, len(df), 1)).all() <p>You now have the option to implement an automatic choice for specific database combinations. Please also specify a reason!</p> <p>This is for records which do not satisfy the hierarchy criterion, i.e. records with different data but identical metadata, such as updated records.</p> <p>If you do not wish to do this, delete <code>automate_db_choice</code> from kwargs or set to <code>False</code> (default).</p> In\u00a0[8]: Copied! <pre>automate_db_choice = {'preferred_db': 'FE23 (Breitenmoser et al. (2014))', \n                      'rejected_db': 'PAGES 2k v2.2.0', \n                      'reason': 'conservative replication requirement'}\n</pre> automate_db_choice = {'preferred_db': 'FE23 (Breitenmoser et al. (2014))',                        'rejected_db': 'PAGES 2k v2.2.0',                        'reason': 'conservative replication requirement'} In\u00a0[\u00a0]: Copied! <pre># remove_identicals = True if you want to automatically remove identical duplicates, without operator input\ndup.duplicate_decisions_multiple(df, operator_details=operator_details, choose_recollection=True, \n                                 remove_identicals=True, backup=True, comment=True, automate_db_choice=automate_db_choice)\n</pre> # remove_identicals = True if you want to automatically remove identical duplicates, without operator input dup.duplicate_decisions_multiple(df, operator_details=operator_details, choose_recollection=True,                                   remove_identicals=True, backup=True, comment=True, automate_db_choice=automate_db_choice) <pre>No back up.\n------------------------------------------------------------\nDetected MULTIPLE duplicates, including:\npages2k_0 3 ['iso2k_296', 'iso2k_298', 'iso2k_299']\npages2k_81 2 ['ch2k_HE08LRA01_76', 'iso2k_1813']\npages2k_225 2 ['FE23_northamerica_usa_nv512', 'FE23_northamerica_usa_nv521']\npages2k_242 2 ['ch2k_LI06FIJ01_582', 'iso2k_353']\npages2k_267 2 ['iso2k_58', 'iso2k_1068']\npages2k_271 2 ['ch2k_FE18RUS01_492', 'iso2k_1861']\npages2k_317 2 ['ch2k_NA09MAL01_84', 'iso2k_1754']\npages2k_385 2 ['ch2k_FE09OGA01_304', 'iso2k_1922']\npages2k_395 2 ['ch2k_CA07FLI01_400', 'iso2k_1057']\npages2k_409 2 ['ch2k_QU96ESV01_422', 'iso2k_218']\npages2k_444 2 ['pages2k_445', 'pages2k_446']\npages2k_462 2 ['ch2k_OS14UCP01_236', 'iso2k_350']\npages2k_468 2 ['pages2k_3550', 'FE23_asia_russ137w']\npages2k_472 2 ['pages2k_474', 'pages2k_477']\npages2k_495 2 ['ch2k_LI06RAR01_12', 'iso2k_1502']\npages2k_500 2 ['ch2k_AS05GUA01_302', 'iso2k_1559']\npages2k_592 2 ['ch2k_LI06RAR02_270', 'iso2k_1500']\npages2k_831 2 ['pages2k_2220', 'FE23_asia_russ127w']\npages2k_893 2 ['pages2k_895', 'pages2k_900']\npages2k_940 3 ['ch2k_DR99ABR01_264', 'ch2k_DR99ABR01_266', 'iso2k_91']\npages2k_1089 2 ['FE23_northamerica_usa_mt112', 'FE23_northamerica_usa_mt113']\npages2k_1147 3 ['ch2k_DA06MAF01_78', 'ch2k_DA06MAF02_104', 'iso2k_1748']\npages2k_1153 2 ['pages2k_1156', 'pages2k_1160']\npages2k_1360 3 ['ch2k_UR00MAI01_22', 'iso2k_94', 'iso2k_98']\npages2k_1488 4 ['pages2k_1628', 'ch2k_NU11PAL01_52', 'iso2k_505', 'iso2k_579']\npages2k_1703 2 ['ch2k_MO06PED01_226', 'iso2k_629']\npages2k_1750 2 ['iso2k_1856', 'sisal_294.0_194']\npages2k_1859 2 ['ch2k_HE10GUA01_244', 'iso2k_1735']\npages2k_1942 2 ['ch2k_ZI04IFR01_26', 'iso2k_257']\npages2k_2042 2 ['ch2k_TU95MAD01_24', 'iso2k_20']\npages2k_2094 2 ['ch2k_TU01DEP01_450', 'iso2k_1201']\npages2k_2146 2 ['pages2k_2149', 'pages2k_2150']\npages2k_2604 2 ['pages2k_2606', 'iso2k_1481']\npages2k_2607 2 ['pages2k_2609', 'pages2k_2612']\npages2k_2752 2 ['pages2k_2755', 'pages2k_2759']\npages2k_2793 2 ['pages2k_2795', 'pages2k_2798']\npages2k_3028 2 ['pages2k_3030', 'pages2k_3033']\npages2k_3068 2 ['ch2k_ZI14IFR02_522', 'ch2k_ZI14IFR02_524']\npages2k_3085 3 ['ch2k_KU00NIN01_150', 'iso2k_1554', 'iso2k_1556']\npages2k_3132 2 ['ch2k_QU06RAB01_144', 'iso2k_1311']\npages2k_3234 2 ['pages2k_3236', 'pages2k_3239']\npages2k_3266 2 ['ch2k_GO12SBV01_396', 'iso2k_870']\npages2k_3352 3 ['ch2k_ZI14TUR01_480', 'ch2k_ZI14TUR01_482', 'iso2k_302']\npages2k_3372 2 ['ch2k_KI04MCV01_366', 'iso2k_155']\npages2k_3554 2 ['ch2k_LI94SEC01_436', 'iso2k_1124']\npages2k_3599 2 ['iso2k_1069', 'iso2k_1660']\nch2k_KU99HOU01_40 2 ['iso2k_786', 'iso2k_788']\nch2k_XI17HAI01_128 2 ['ch2k_XI17HAI01_136', 'iso2k_1762']\nch2k_HE13MIS01_194 2 ['iso2k_211', 'iso2k_213']\nch2k_PF04PBA01_204 2 ['iso2k_1701', 'iso2k_1704']\nch2k_GU99NAU01_314 2 ['iso2k_702', 'iso2k_705']\nch2k_DE13HAI01_424 2 ['ch2k_DE13HAI01_432', 'iso2k_1643']\niso2k_399 2 ['iso2k_806', 'iso2k_811']\niso2k_1107 2 ['iso2k_1817', 'sisal_271.0_174']\nPLEASE PAY ATTENTION WHEN MAKING DECISIONS FOR THESE DUPLICATES!\nThe decision process will go through the duplicates on a PAIR-BY-PAIR basis, which is not optimised for multiple duplicates.\nThe multiples will be highlighted throughout the decision process.\nShould the operator want to go back and revise a previous decision based on the presentation of a new candidate pair, they can manually modify the backup file to alter any previous decisions.\n------------------------------------------------------------\n&gt; 1/429,pages2k_0,iso2k_296,0.0,0.9999999995947852\n====================================================================\n=== POTENTIAL DUPLICATE 0/429: pages2k_0+iso2k_296 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ant-WDC05A.Steig.2013.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/22531   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_0, keep iso2k_296.\nwrite decision to backup file\n&gt; 2/429,pages2k_0,iso2k_298,0.0,0.9999999995947852\n====================================================================\n=== POTENTIAL DUPLICATE 1/429: pages2k_0+iso2k_298 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ant-WDC05A.Steig.2013.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/22531   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_0, keep iso2k_298.\nwrite decision to backup file\n&gt; 3/429,pages2k_0,iso2k_299,0.0,0.9999999995947852\n====================================================================\n=== POTENTIAL DUPLICATE 2/429: pages2k_0+iso2k_299 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ant-WDC05A.Steig.2013.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/22531   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_0, keep iso2k_299.\nwrite decision to backup file\n&gt; 4/429,pages2k_6,FE23_northamerica_usa_az555,5.775408685862238,0.978353859816631\n====================================================================\n=== POTENTIAL DUPLICATE 3/429: pages2k_6+FE23_northamerica_usa_az555 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-MtLemon.Briffa.2002-1.txt   ===\n=== URL 2: https://www.ncei.noaa.gov/pub/data/paleo/treering/measurements/northamerica/usa/az555-noaa.rwl   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False False\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\nAutomated choice. Metadata identical, automatically choose FE23 (Breitenmoser et al. (2014)) over PAGES 2k v2.2.0. conservative replication requirement\nKEEP RED CROSSES: remove pages2k_6, keep FE23_northamerica_usa_az555.\nwrite decision to backup file\n&gt; 5/429,pages2k_50,FE23_northamerica_canada_cana091,3.197082790629511,0.9674400553180403\n====================================================================\n=== POTENTIAL DUPLICATE 4/429: pages2k_50+FE23_northamerica_canada_cana091 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-SmithersSkiArea.Schweingruber.1996-1.txt   ===\n=== URL 2: https://www.ncei.noaa.gov/pub/data/paleo/treering/measurements/northamerica/canada/cana091-noaa.rwl   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False False\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\nAutomated choice. Metadata identical, automatically choose FE23 (Breitenmoser et al. (2014)) over PAGES 2k v2.2.0. conservative replication requirement\nKEEP RED CROSSES: remove pages2k_50, keep FE23_northamerica_canada_cana091.\nwrite decision to backup file\n&gt; 6/429,pages2k_62,pages2k_63,0.0,0.9442037258051723\n====================================================================\n=== POTENTIAL DUPLICATE 5/429: pages2k_62+pages2k_63 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-SmithersSkiArea.Schweingruber.1996-2.txt   ===\n=== URL 2: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-SmithersSkiArea.Schweingruber.1996-2.txt   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False False\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  True\ndata_identical:  False\ncorrelation_perfect:  False\n</pre> <pre>**Decision required for this duplicate pair (see figure above).**\nBefore inputting your decision. Would you like to leave a comment on your decision process?\n</pre> <pre>saved figure in /home/jupyter-lluecke/dod2k/figs//dup_detection/all_merged/005_pages2k_62_pages2k_63__20_21.jpg\nKEEP BLUE CIRCLES: keep pages2k_62, remove pages2k_63.\nwrite decision to backup file\n&gt; 7/429,pages2k_81,ch2k_HE08LRA01_76,0.0,0.9999999922133574\n====================================================================\n=== POTENTIAL DUPLICATE 6/429: pages2k_81+ch2k_HE08LRA01_76 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-LosRoques.Hetzinger.2008.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/12891   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_81, keep ch2k_HE08LRA01_76.\nwrite decision to backup file\n&gt; 8/429,pages2k_81,iso2k_1813,0.0,0.9999999922133574\n====================================================================\n=== POTENTIAL DUPLICATE 7/429: pages2k_81+iso2k_1813 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-LosRoques.Hetzinger.2008.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/12891   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_81, keep iso2k_1813.\nwrite decision to backup file\n&gt; 9/429,pages2k_83,iso2k_1916,0.0,0.9999999999999999\n====================================================================\n=== POTENTIAL DUPLICATE 8/429: pages2k_83+iso2k_1916 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-Agassiz.Vinther.2008.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo-search/study/2431   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  True\ncorrelation_perfect:  True\nRECORDS IDENTICAL (identical data) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_83, keep iso2k_1916.\nwrite decision to backup file\n&gt; 10/429,pages2k_85,pages2k_88,0.0,0.999999996118143\n====================================================================\n=== POTENTIAL DUPLICATE 9/429: pages2k_85+pages2k_88 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/SAm-LagunaChepical.deJong.2013.txt   ===\n=== URL 2: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/SAm-LagunaChepical.deJong.2013.txt   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  True\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation). Automatically choose #1.\nKEEP BLUE CIRCLES: keep pages2k_85, remove pages2k_88.\nwrite decision to backup file\n&gt; 11/429,pages2k_94,FE23_northamerica_canada_cana153,3.775253335744946,0.996675589148305\n====================================================================\n=== POTENTIAL DUPLICATE 10/429: pages2k_94+FE23_northamerica_canada_cana153 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-CoppermineRiver.Jacoby.1989.txt   ===\n=== URL 2: https://www.ncei.noaa.gov/pub/data/paleo/treering/measurements/northamerica/canada/cana153-noaa.rwl   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\nAutomated choice. Metadata identical, automatically choose FE23 (Breitenmoser et al. (2014)) over PAGES 2k v2.2.0. conservative replication requirement\nKEEP RED CROSSES: remove pages2k_94, keep FE23_northamerica_canada_cana153.\nwrite decision to backup file\n&gt; 12/429,pages2k_107,FE23_northamerica_usa_ak046,3.776010469650001,0.9805747494703706\n====================================================================\n=== POTENTIAL DUPLICATE 11/429: pages2k_107+FE23_northamerica_usa_ak046 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-KobukNoatak.King.2003.txt   ===\n=== URL 2: https://www.ncei.noaa.gov/pub/data/paleo/treering/measurements/northamerica/usa/ak046-noaa.rwl   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\nAutomated choice. Metadata identical, automatically choose FE23 (Breitenmoser et al. (2014)) over PAGES 2k v2.2.0. conservative replication requirement\nKEEP RED CROSSES: remove pages2k_107, keep FE23_northamerica_usa_ak046.\nwrite decision to backup file\n&gt; 13/429,pages2k_121,pages2k_122,0.0,0.9815589923346684\n====================================================================\n=== POTENTIAL DUPLICATE 12/429: pages2k_121+pages2k_122 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/SAm-CentralAndes9.Mundo.2014.txt   ===\n=== URL 2: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/SAm-CentralAndes9.Mundo.2014.txt   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  True\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation). Automatically choose #1.\nKEEP BLUE CIRCLES: keep pages2k_121, remove pages2k_122.\nwrite decision to backup file\n&gt; 14/429,pages2k_132,FE23_northamerica_canada_cana225,3.8914450331074146,0.9956052895756172\n====================================================================\n=== POTENTIAL DUPLICATE 13/429: pages2k_132+FE23_northamerica_canada_cana225 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-MeadowMountain.Wilson.2005-1.txt   ===\n=== URL 2: https://www.ncei.noaa.gov/pub/data/paleo/treering/measurements/northamerica/canada/cana225-noaa.rwl   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\nAutomated choice. Metadata identical, automatically choose FE23 (Breitenmoser et al. (2014)) over PAGES 2k v2.2.0. conservative replication requirement\nKEEP RED CROSSES: remove pages2k_132, keep FE23_northamerica_canada_cana225.\nwrite decision to backup file\n&gt; 15/429,pages2k_158,FE23_northamerica_usa_wa069,3.6691109671311533,0.9970316184150801\n====================================================================\n=== POTENTIAL DUPLICATE 14/429: pages2k_158+FE23_northamerica_usa_wa069 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-wa069.Peterson.1994.txt   ===\n=== URL 2: https://www.ncei.noaa.gov/pub/data/paleo/treering/measurements/northamerica/usa/wa069-noaa.rwl   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\nAutomated choice. Metadata identical, automatically choose FE23 (Breitenmoser et al. (2014)) over PAGES 2k v2.2.0. conservative replication requirement\nKEEP RED CROSSES: remove pages2k_158, keep FE23_northamerica_usa_wa069.\nwrite decision to backup file\n&gt; 16/429,pages2k_171,FE23_northamerica_usa_wy021,6.841598008457665,0.9852616141387495\n====================================================================\n=== POTENTIAL DUPLICATE 15/429: pages2k_171+FE23_northamerica_usa_wy021 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-PowderRiverPass.Briffa.1996-1.txt   ===\n=== URL 2: https://www.ncei.noaa.gov/pub/data/paleo/treering/measurements/northamerica/usa/wy021-noaa.rwl   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\nAutomated choice. Metadata identical, automatically choose FE23 (Breitenmoser et al. (2014)) over PAGES 2k v2.2.0. conservative replication requirement\nKEEP RED CROSSES: remove pages2k_171, keep FE23_northamerica_usa_wy021.\nwrite decision to backup file\n&gt; 17/429,pages2k_203,iso2k_826,0.0,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 16/429: pages2k_203+iso2k_826 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-BunakenIsland.Charles.2003.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/1903   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  False\ndata_identical:  True\ncorrelation_perfect:  True\nRECORDS IDENTICAL (identical data) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_203, keep iso2k_826.\nwrite decision to backup file\n&gt; 18/429,pages2k_225,FE23_northamerica_usa_nv512,4.66349518850797,0.9654504918783551\n====================================================================\n=== POTENTIAL DUPLICATE 17/429: pages2k_225+FE23_northamerica_usa_nv512 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-PearlPeak.Graybill.1994.txt   ===\n=== URL 2: https://www.ncei.noaa.gov/pub/data/paleo/treering/measurements/northamerica/usa/nv512-noaa.rwl   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False False\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\nAutomated choice. Metadata identical, automatically choose FE23 (Breitenmoser et al. (2014)) over PAGES 2k v2.2.0. conservative replication requirement\nKEEP RED CROSSES: remove pages2k_225, keep FE23_northamerica_usa_nv512.\nwrite decision to backup file\n&gt; 19/429,pages2k_238,iso2k_1044,0.0,0.9999999998644115\n====================================================================\n=== POTENTIAL DUPLICATE 18/429: pages2k_238+iso2k_1044 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ant-PlateauRemote.Mosley-Thompson.2013.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo-search/study/22479   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_238, keep iso2k_1044.\nwrite decision to backup file\n&gt; 20/429,pages2k_242,ch2k_LI06FIJ01_582,0.5087587517213885,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 19/429: pages2k_242+ch2k_LI06FIJ01_582 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-SavusavuBayAB.Linsley.2006.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/1003973   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_242, keep ch2k_LI06FIJ01_582.\nwrite decision to backup file\n&gt; 21/429,pages2k_242,iso2k_353,0.0,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 20/429: pages2k_242+iso2k_353 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-SavusavuBayAB.Linsley.2006.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/16216   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  True\ncorrelation_perfect:  True\nRECORDS IDENTICAL (identical data) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_242, keep iso2k_353.\nwrite decision to backup file\n&gt; 22/429,pages2k_258,iso2k_1498,0.0,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 21/429: pages2k_258+iso2k_1498 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Rarotongad18O2R.Linsley.2006.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/6089   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  True\ncorrelation_perfect:  True\nRECORDS IDENTICAL (identical data) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_258, keep iso2k_1498.\nwrite decision to backup file\n&gt; 23/429,pages2k_263,iso2k_1322,1.1119762877768709,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 22/429: pages2k_263+iso2k_1322 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Lombok.Charles.2003.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/1903   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  True\ncorrelation_perfect:  True\nRECORDS IDENTICAL (identical data) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_263, keep iso2k_1322.\nwrite decision to backup file\n&gt; 24/429,pages2k_267,iso2k_58,0.5087587517215614,0.9997675035800243\n====================================================================\n=== POTENTIAL DUPLICATE 23/429: pages2k_267+iso2k_58 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-SavusavuBayFiji.Bagnato.2005.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/1881   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\n</pre> <pre>**Decision required for this duplicate pair (see figure above).**\n---------------------------------------------------------------------------------------------------------\n***ATTENTION*** THIS RECORD IS ASSOCIATED WITH MULTIPLE DUPLICATES! PLEASE PAY SPECIAL ATTENTION WHEN MAKING DECISIONS FOR THIS RECORD!\nThe potential duplicates also associated with this record are:\n............................................................\n     - Dataset ID          : iso2k_1068\n     - URL                 : https://www.ncdc.noaa.gov/paleo/study/1916\n</pre> <pre>---------------------------------------------------------------------------------------------------------\nBefore inputting your decision. Would you like to leave a comment on your decision process?\n</pre> <pre>saved figure in /home/jupyter-lluecke/dod2k/figs//dup_detection/all_merged/023_pages2k_267_iso2k_58__99_4352.jpg\nKEEP BLUE CIRCLES: keep pages2k_267, remove iso2k_58.\nwrite decision to backup file\n&gt; 25/429,pages2k_267,iso2k_1068,0.5087587517215614,0.9999999999999999\n====================================================================\n=== POTENTIAL DUPLICATE 24/429: pages2k_267+iso2k_1068 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-SavusavuBayFiji.Bagnato.2005.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/1916   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  False\ndata_identical:  True\ncorrelation_perfect:  True\nRECORDS IDENTICAL (identical data) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_267, keep iso2k_1068.\nwrite decision to backup file\n&gt; 26/429,pages2k_271,ch2k_FE18RUS01_492,1.0009991654937371,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 25/429: pages2k_271+ch2k_FE18RUS01_492 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-RedSea.Felis.2000.txt   ===\n=== URL 2: https://doi.pangaea.de/10.1594/PANGAEA.891094   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  False\nlat True lon True elevation False archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_271, keep ch2k_FE18RUS01_492.\nwrite decision to backup file\n&gt; 27/429,pages2k_271,iso2k_1861,0.0,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 26/429: pages2k_271+iso2k_1861 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-RedSea.Felis.2000.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/1861   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  True\ncorrelation_perfect:  True\nRECORDS IDENTICAL (identical data) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_271, keep iso2k_1861.\nwrite decision to backup file\n&gt; 28/429,pages2k_273,FE23_asia_russ130w,0.43810000685217965,0.9680483685544976\n====================================================================\n=== POTENTIAL DUPLICATE 27/429: pages2k_273+FE23_asia_russ130w ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-AltaiJablonsky.Cook.2000.txt   ===\n=== URL 2: https://www.ncei.noaa.gov/pub/data/paleo/treering/measurements/asia/russ130w-noaa.rwl   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False False\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\nAutomated choice. Metadata identical, automatically choose FE23 (Breitenmoser et al. (2014)) over PAGES 2k v2.2.0. conservative replication requirement\nKEEP RED CROSSES: remove pages2k_273, keep FE23_asia_russ130w.\nwrite decision to backup file\n&gt; 29/429,pages2k_281,FE23_northamerica_canada_cana155,4.046549942741809,0.9890405986205609\n====================================================================\n=== POTENTIAL DUPLICATE 28/429: pages2k_281+FE23_northamerica_canada_cana155 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-HornbyCabin.Jacoby.1989.txt   ===\n=== URL 2: https://www.ncei.noaa.gov/pub/data/paleo/treering/measurements/northamerica/canada/cana155-noaa.rwl   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\nAutomated choice. Metadata identical, automatically choose FE23 (Breitenmoser et al. (2014)) over PAGES 2k v2.2.0. conservative replication requirement\nKEEP RED CROSSES: remove pages2k_281, keep FE23_northamerica_canada_cana155.\nwrite decision to backup file\n&gt; 30/429,pages2k_294,FE23_northamerica_usa_ak021,0.9263994682764632,0.9872961006283816\n====================================================================\n=== POTENTIAL DUPLICATE 29/429: pages2k_294+FE23_northamerica_usa_ak021 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-MinersWell.Wiles.2000.txt   ===\n=== URL 2: https://www.ncei.noaa.gov/pub/data/paleo/treering/measurements/northamerica/usa/ak021-noaa.rwl   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\nAutomated choice. Metadata identical, automatically choose FE23 (Breitenmoser et al. (2014)) over PAGES 2k v2.2.0. conservative replication requirement\nKEEP RED CROSSES: remove pages2k_294, keep FE23_northamerica_usa_ak021.\nwrite decision to backup file\n&gt; 31/429,pages2k_305,pages2k_309,0.0,0.9999999999999999\n====================================================================\n=== POTENTIAL DUPLICATE 30/429: pages2k_305+pages2k_309 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-EmeraldBasin.Keigwin.2007-1.txt   ===\n=== URL 2: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-EmeraldBasin.Keigwin.2007-2.txt   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\n</pre> <pre>**Decision required for this duplicate pair (see figure above).**\nBefore inputting your decision. Would you like to leave a comment on your decision process?\n</pre> <pre>saved figure in /home/jupyter-lluecke/dod2k/figs//dup_detection/all_merged/030_pages2k_305_pages2k_309__113_115.jpg\nKEEP RED CROSSES: remove pages2k_305, keep pages2k_309.\nwrite decision to backup file\n&gt; 32/429,pages2k_307,pages2k_311,0.0,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 31/429: pages2k_307+pages2k_311 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-EmeraldBasin.Keigwin.2007-1.txt   ===\n=== URL 2: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-EmeraldBasin.Keigwin.2007-2.txt   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\n</pre> <pre>**Decision required for this duplicate pair (see figure above).**\nBefore inputting your decision. Would you like to leave a comment on your decision process?\n</pre> <pre>saved figure in /home/jupyter-lluecke/dod2k/figs//dup_detection/all_merged/031_pages2k_307_pages2k_311__114_116.jpg\nKEEP RED CROSSES: remove pages2k_307, keep pages2k_311.\nwrite decision to backup file\n&gt; 33/429,pages2k_315,iso2k_362,0.392681883222941,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 32/429: pages2k_315+iso2k_362 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-Austfonna.Isaksson.2005.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo-search/study/11173   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  False\ndata_identical:  True\ncorrelation_perfect:  True\nRECORDS IDENTICAL (identical data) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_315, keep iso2k_362.\nwrite decision to backup file\n&gt; 34/429,pages2k_317,ch2k_NA09MAL01_84,0.0,0.9999999999999999\n====================================================================\n=== POTENTIAL DUPLICATE 33/429: pages2k_317+ch2k_NA09MAL01_84 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Malindi.Nakamura.2009.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/12994   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  False\nlat True lon True elevation False archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_317, keep ch2k_NA09MAL01_84.\nwrite decision to backup file\n&gt; 35/429,pages2k_317,iso2k_1754,0.0,0.9999999999999999\n====================================================================\n=== POTENTIAL DUPLICATE 34/429: pages2k_317+iso2k_1754 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Malindi.Nakamura.2009.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/12994   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  False\ndata_identical:  True\ncorrelation_perfect:  True\nRECORDS IDENTICAL (identical data) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_317, keep iso2k_1754.\nwrite decision to backup file\n&gt; 36/429,pages2k_323,FE23_northamerica_canada_cana210,1.8532231170308289,0.9489902863469498\n====================================================================\n=== POTENTIAL DUPLICATE 35/429: pages2k_323+FE23_northamerica_canada_cana210 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-MedusaBay.Buckley.2003.txt   ===\n=== URL 2: https://www.ncei.noaa.gov/pub/data/paleo/treering/measurements/northamerica/canada/cana210-noaa.rwl   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False False\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\nAutomated choice. Metadata identical, automatically choose FE23 (Breitenmoser et al. (2014)) over PAGES 2k v2.2.0. conservative replication requirement\nKEEP RED CROSSES: remove pages2k_323, keep FE23_northamerica_canada_cana210.\nwrite decision to backup file\n&gt; 37/429,pages2k_385,ch2k_FE09OGA01_304,0.0,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 36/429: pages2k_385+ch2k_FE09OGA01_304 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Miyanohama.Felis.2009.txt   ===\n=== URL 2: https://doi.pangaea.de/10.1594/PANGAEA.743953   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  False\nlat True lon True elevation False archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_385, keep ch2k_FE09OGA01_304.\nwrite decision to backup file\n&gt; 38/429,pages2k_385,iso2k_1922,0.0,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 37/429: pages2k_385+iso2k_1922 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Miyanohama.Felis.2009.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/8608   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  False\ndata_identical:  True\ncorrelation_perfect:  True\nRECORDS IDENTICAL (identical data) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_385, keep iso2k_1922.\nwrite decision to backup file\n&gt; 39/429,pages2k_387,ch2k_FE09OGA01_306,0.0,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 38/429: pages2k_387+ch2k_FE09OGA01_306 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Miyanohama.Felis.2009.txt   ===\n=== URL 2: https://doi.pangaea.de/10.1594/PANGAEA.743953   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  False\nlat True lon True elevation False archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_387, keep ch2k_FE09OGA01_306.\nwrite decision to backup file\n&gt; 40/429,pages2k_395,ch2k_CA07FLI01_400,0.0,0.9999999999999998\n====================================================================\n=== POTENTIAL DUPLICATE 39/429: pages2k_395+ch2k_CA07FLI01_400 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-CoralSea.Calvo.2007.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/6087   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_395, keep ch2k_CA07FLI01_400.\nwrite decision to backup file\n&gt; 41/429,pages2k_395,iso2k_1057,0.0,0.9999999999999998\n====================================================================\n=== POTENTIAL DUPLICATE 40/429: pages2k_395+iso2k_1057 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-CoralSea.Calvo.2007.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/6087   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  False\ndata_identical:  True\ncorrelation_perfect:  True\nRECORDS IDENTICAL (identical data) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_395, keep iso2k_1057.\nwrite decision to backup file\n&gt; 42/429,pages2k_397,ch2k_CA07FLI01_402,0.0,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 41/429: pages2k_397+ch2k_CA07FLI01_402 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-CoralSea.Calvo.2007.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/6087   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_397, keep ch2k_CA07FLI01_402.\nwrite decision to backup file\n&gt; 43/429,pages2k_409,ch2k_QU96ESV01_422,1.0734720312673933,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 42/429: pages2k_409+ch2k_QU96ESV01_422 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Vanuatu.Quinn.1996.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/1839   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_409, keep ch2k_QU96ESV01_422.\nwrite decision to backup file\n&gt; 44/429,pages2k_409,iso2k_218,1.0734720312673933,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 43/429: pages2k_409+iso2k_218 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Vanuatu.Quinn.1996.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/1839   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  True\ncorrelation_perfect:  True\nRECORDS IDENTICAL (identical data) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_409, keep iso2k_218.\nwrite decision to backup file\n&gt; 45/429,pages2k_414,pages2k_418,0.0,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 44/429: pages2k_414+pages2k_418 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-WestSpitzberg.Bonnet.2010-2.txt   ===\n=== URL 2: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-WestSpitzberg.Bonnet.2010-3.txt   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  False\ndata_identical:  True\ncorrelation_perfect:  True\nRECORDS IDENTICAL (identical data) except for metadata. Automatically choose #1.\nKEEP BLUE CIRCLES: keep pages2k_414, remove pages2k_418.\nwrite decision to backup file\n&gt; 46/429,pages2k_417,pages2k_421,0.0,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 45/429: pages2k_417+pages2k_421 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-WestSpitzberg.Bonnet.2010-2.txt   ===\n=== URL 2: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-WestSpitzberg.Bonnet.2010-3.txt   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  False\ndata_identical:  True\ncorrelation_perfect:  True\nRECORDS IDENTICAL (identical data) except for metadata. Automatically choose #1.\nKEEP BLUE CIRCLES: keep pages2k_417, remove pages2k_421.\nwrite decision to backup file\n&gt; 47/429,pages2k_427,pages2k_433,0.0,0.9830221452780444\n====================================================================\n=== POTENTIAL DUPLICATE 46/429: pages2k_427+pages2k_433 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-WestSpitzberg.Bonnet.2010-3.txt   ===\n=== URL 2: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-WestSpitzberg.Bonnet.2010-3.txt   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  True\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation). Automatically choose #1.\nKEEP BLUE CIRCLES: keep pages2k_427, remove pages2k_433.\nwrite decision to backup file\n&gt; 48/429,pages2k_435,pages2k_842,0.0,0.9855993305813949\n====================================================================\n=== POTENTIAL DUPLICATE 47/429: pages2k_435+pages2k_842 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-UULLWD.Schweingruber.2002.txt   ===\n=== URL 2: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-UULMXD.Schweingruber.2002.txt   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation) except for metadata. Automatically choose #1.\nKEEP BLUE CIRCLES: keep pages2k_435, remove pages2k_842.\nwrite decision to backup file\n&gt; 49/429,pages2k_444,pages2k_445,0.0,0.9999996722608564\n====================================================================\n=== POTENTIAL DUPLICATE 48/429: pages2k_444+pages2k_445 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/SAm-LagunaAculeo.vonGunten.2009.txt   ===\n=== URL 2: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/SAm-LagunaAculeo.vonGunten.2009.txt   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  True\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation). Automatically choose #1.\nKEEP BLUE CIRCLES: keep pages2k_444, remove pages2k_445.\nwrite decision to backup file\n&gt; 50/429,pages2k_444,pages2k_446,0.0,0.9999998768494976\n====================================================================\n=== POTENTIAL DUPLICATE 49/429: pages2k_444+pages2k_446 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/SAm-LagunaAculeo.vonGunten.2009.txt   ===\n=== URL 2: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/SAm-LagunaAculeo.vonGunten.2009.txt   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  True\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation). Automatically choose #1.\nKEEP BLUE CIRCLES: keep pages2k_444, remove pages2k_446.\nwrite decision to backup file\n&gt; 51/429,pages2k_445,pages2k_446,0.0,0.9999997978711366\n====================================================================\n=== POTENTIAL DUPLICATE 50/429: pages2k_445+pages2k_446 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/SAm-LagunaAculeo.vonGunten.2009.txt   ===\n=== URL 2: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/SAm-LagunaAculeo.vonGunten.2009.txt   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  True\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation). Automatically choose #1.\nKEEP BLUE CIRCLES: keep pages2k_445, remove pages2k_446.\nwrite decision to backup file\n&gt; 52/429,pages2k_462,ch2k_OS14UCP01_236,0.0,0.9999999861930808\n====================================================================\n=== POTENTIAL DUPLICATE 51/429: pages2k_462+ch2k_OS14UCP01_236 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-PalauUlongChannel.Osborne.2014.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/16339   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_462, keep ch2k_OS14UCP01_236.\nwrite decision to backup file\n&gt; 53/429,pages2k_462,iso2k_350,0.0,0.9999999861930808\n====================================================================\n=== POTENTIAL DUPLICATE 52/429: pages2k_462+iso2k_350 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-PalauUlongChannel.Osborne.2014.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/16339   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_462, keep iso2k_350.\nwrite decision to backup file\n&gt; 54/429,pages2k_468,pages2k_3550,0.0,0.987098513022764\n====================================================================\n=== POTENTIAL DUPLICATE 53/429: pages2k_468+pages2k_3550 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-UULTRW.Schweingruber.2002.txt   ===\n=== URL 2: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-UULEWW.Schweingruber.2002.txt   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation) except for metadata. Automatically choose #1.\nKEEP BLUE CIRCLES: keep pages2k_468, remove pages2k_3550.\nwrite decision to backup file\n&gt; 55/429,pages2k_468,FE23_asia_russ137w,0.37072945852236205,0.9989192723158098\n====================================================================\n=== POTENTIAL DUPLICATE 54/429: pages2k_468+FE23_asia_russ137w ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-UULTRW.Schweingruber.2002.txt   ===\n=== URL 2: https://www.ncei.noaa.gov/pub/data/paleo/treering/measurements/asia/russ137w-noaa.rwl   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\nAutomated choice. Metadata identical, automatically choose FE23 (Breitenmoser et al. (2014)) over PAGES 2k v2.2.0. conservative replication requirement\nKEEP RED CROSSES: remove pages2k_468, keep FE23_asia_russ137w.\nwrite decision to backup file\n&gt; 56/429,pages2k_472,pages2k_474,0.0,0.9990780776141117\n====================================================================\n=== POTENTIAL DUPLICATE 55/429: pages2k_472+pages2k_474 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-FiskBasin.Richey.2009-1.txt   ===\n=== URL 2: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-FiskBasin.Richey.2009-1.txt   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  True\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation). Automatically choose #1.\nKEEP BLUE CIRCLES: keep pages2k_472, remove pages2k_474.\nwrite decision to backup file\n&gt; 57/429,pages2k_472,pages2k_477,0.0,0.9991742362392582\n====================================================================\n=== POTENTIAL DUPLICATE 56/429: pages2k_472+pages2k_477 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-FiskBasin.Richey.2009-1.txt   ===\n=== URL 2: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-FiskBasin.Richey.2009-2.txt   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\n</pre> <pre>**Decision required for this duplicate pair (see figure above).**\n---------------------------------------------------------------------------------------------------------\n***ATTENTION*** THIS RECORD IS ASSOCIATED WITH MULTIPLE DUPLICATES! PLEASE PAY SPECIAL ATTENTION WHEN MAKING DECISIONS FOR THIS RECORD!\nThe potential duplicates also associated with this record are:\n............................................................\n     - Dataset ID          : pages2k_474\n     - URL                 : https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-FiskBasin.Richey.2009-1.txt\n</pre> <pre>---------------------------------------------------------------------------------------------------------\nBefore inputting your decision. Would you like to leave a comment on your decision process?\n</pre> <pre>saved figure in /home/jupyter-lluecke/dod2k/figs//dup_detection/all_merged/056_pages2k_472_pages2k_477__189_192.jpg\nKEEP BLUE CIRCLES: keep pages2k_472, remove pages2k_477.\nwrite decision to backup file\n&gt; 58/429,pages2k_474,pages2k_477,0.0,0.9999463761497801\n====================================================================\n=== POTENTIAL DUPLICATE 57/429: pages2k_474+pages2k_477 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-FiskBasin.Richey.2009-1.txt   ===\n=== URL 2: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-FiskBasin.Richey.2009-2.txt   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\n</pre> <pre>**Decision required for this duplicate pair (see figure above).**\n---------------------------------------------------------------------------------------------------------\n***ATTENTION*** THIS RECORD IS ASSOCIATED WITH MULTIPLE DUPLICATES! PLEASE PAY SPECIAL ATTENTION WHEN MAKING DECISIONS FOR THIS RECORD!\nThe potential duplicates also associated with this record are:\n-  pages2k_472\n---------------------------------------------------------------------------------------------------------\nBefore inputting your decision. Would you like to leave a comment on your decision process?\n</pre> <pre>saved figure in /home/jupyter-lluecke/dod2k/figs//dup_detection/all_merged/057_pages2k_474_pages2k_477__190_192.jpg\nREMOVE BOTH: remove pages2k_477, remove pages2k_474.\nwrite decision to backup file\n&gt; 59/429,pages2k_478,iso2k_1846,0.0,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 58/429: pages2k_478+iso2k_1846 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-Dye.Vinther.2010.txt   ===\n=== URL 2: http://www.iceandclimate.nbi.ku.dk/data/Vinther_etal_2010_data_02feb2010.xls   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  True\ncorrelation_perfect:  True\nRECORDS IDENTICAL (identical data) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_478, keep iso2k_1846.\nwrite decision to backup file\n&gt; 60/429,pages2k_486,FE23_northamerica_usa_ca609,1.8532231170308289,0.9741227085165554\n====================================================================\n=== POTENTIAL DUPLICATE 59/429: pages2k_486+FE23_northamerica_usa_ca609 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-FishCreekTrail.Biondi.2001.txt   ===\n=== URL 2: https://www.ncei.noaa.gov/pub/data/paleo/treering/measurements/northamerica/usa/ca609-noaa.rwl   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False False\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\nAutomated choice. Metadata identical, automatically choose FE23 (Breitenmoser et al. (2014)) over PAGES 2k v2.2.0. conservative replication requirement\nKEEP RED CROSSES: remove pages2k_486, keep FE23_northamerica_usa_ca609.\nwrite decision to backup file\n&gt; 61/429,pages2k_495,ch2k_LI06RAR01_12,0.0,0.9999999999999999\n====================================================================\n=== POTENTIAL DUPLICATE 60/429: pages2k_495+ch2k_LI06RAR01_12 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Rarotongad18O99.Linsley.2006.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/6089   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_495, keep ch2k_LI06RAR01_12.\nwrite decision to backup file\n&gt; 62/429,pages2k_495,iso2k_1502,0.0,0.9999999999999999\n====================================================================\n=== POTENTIAL DUPLICATE 61/429: pages2k_495+iso2k_1502 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Rarotongad18O99.Linsley.2006.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/6089   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  True\ncorrelation_perfect:  True\nRECORDS IDENTICAL (identical data) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_495, keep iso2k_1502.\nwrite decision to backup file\n&gt; 63/429,pages2k_500,ch2k_AS05GUA01_302,0.0243685480819044,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 62/429: pages2k_500+ch2k_AS05GUA01_302 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-DoubleReef.Asami.2005.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/1915   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_500, keep ch2k_AS05GUA01_302.\nwrite decision to backup file\n&gt; 64/429,pages2k_500,iso2k_1559,0.0243685480819044,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 63/429: pages2k_500+iso2k_1559 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-DoubleReef.Asami.2005.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/1915   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  True\ncorrelation_perfect:  True\nRECORDS IDENTICAL (identical data) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_500, keep iso2k_1559.\nwrite decision to backup file\n&gt; 65/429,pages2k_541,iso2k_404,0.5619803469445591,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 64/429: pages2k_541+iso2k_404 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ant-MES.Rhodes.2012.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/13175   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\n</pre> <pre>**Decision required for this duplicate pair (see figure above).**\nBefore inputting your decision. Would you like to leave a comment on your decision process?\n</pre> <pre>saved figure in /home/jupyter-lluecke/dod2k/figs//dup_detection/all_merged/064_pages2k_541_iso2k_404__216_4431.jpg\nKEEP RED CROSSES: remove pages2k_541, keep iso2k_404.\nwrite decision to backup file\n&gt; 66/429,pages2k_543,pages2k_976,0.0,0.9932155933107505\n====================================================================\n=== POTENTIAL DUPLICATE 65/429: pages2k_543+pages2k_976 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-UKHMXD.Schweingruber.2002.txt   ===\n=== URL 2: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-UKHLWD.Schweingruber.2002.txt   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation) except for metadata. Automatically choose #1.\nKEEP BLUE CIRCLES: keep pages2k_543, remove pages2k_976.\nwrite decision to backup file\n&gt; 67/429,pages2k_565,iso2k_998,0.0,0.9999999999999998\n====================================================================\n=== POTENTIAL DUPLICATE 66/429: pages2k_565+iso2k_998 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-PennyIceCapP96.Fisher.1998.txt   ===\n=== URL 2: www.ncdc.noaa.gov/paleo/study/2474   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  False\nlat True lon True elevation False archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  True\ncorrelation_perfect:  True\nRECORDS IDENTICAL (identical data) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_565, keep iso2k_998.\nwrite decision to backup file\n&gt; 68/429,pages2k_583,FE23_northamerica_usa_mt116,4.265038441214223,0.9940277703282503\n====================================================================\n=== POTENTIAL DUPLICATE 67/429: pages2k_583+FE23_northamerica_usa_mt116 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-FlintCreekRange.Hughes.2005.txt   ===\n=== URL 2: https://www.ncei.noaa.gov/pub/data/paleo/treering/measurements/northamerica/usa/mt116-noaa.rwl   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\nAutomated choice. Metadata identical, automatically choose FE23 (Breitenmoser et al. (2014)) over PAGES 2k v2.2.0. conservative replication requirement\nKEEP RED CROSSES: remove pages2k_583, keep FE23_northamerica_usa_mt116.\nwrite decision to backup file\n&gt; 69/429,pages2k_592,ch2k_LI06RAR02_270,0.0,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 68/429: pages2k_592+ch2k_LI06RAR02_270 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Rarotongad18O3R.Linsley.2006.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/6089   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_592, keep ch2k_LI06RAR02_270.\nwrite decision to backup file\n&gt; 70/429,pages2k_592,iso2k_1500,0.0,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 69/429: pages2k_592+iso2k_1500 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Rarotongad18O3R.Linsley.2006.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/6089   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  False\nlat True lon True elevation False archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  True\ncorrelation_perfect:  True\nRECORDS IDENTICAL (identical data) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_592, keep iso2k_1500.\nwrite decision to backup file\n&gt; 71/429,pages2k_610,iso2k_1199,0.0,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 70/429: pages2k_610+iso2k_1199 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ant-Ferrigno.Thomas.2013.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/22477   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  False\ndata_identical:  True\ncorrelation_perfect:  True\nRECORDS IDENTICAL (identical data) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_610, keep iso2k_1199.\nwrite decision to backup file\n&gt; 72/429,pages2k_626,FE23_northamerica_usa_wa071,3.6691109671311533,0.9790599696794638\n====================================================================\n=== POTENTIAL DUPLICATE 71/429: pages2k_626+FE23_northamerica_usa_wa071 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-wa071.Peterson.1994.txt   ===\n=== URL 2: https://www.ncei.noaa.gov/pub/data/paleo/treering/measurements/northamerica/usa/wa071-noaa.rwl   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False False\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\nAutomated choice. Metadata identical, automatically choose FE23 (Breitenmoser et al. (2014)) over PAGES 2k v2.2.0. conservative replication requirement\nKEEP RED CROSSES: remove pages2k_626, keep FE23_northamerica_usa_wa071.\nwrite decision to backup file\n&gt; 73/429,pages2k_691,FE23_northamerica_canada_cana062,3.8470954123399754,0.935513152519886\n====================================================================\n=== POTENTIAL DUPLICATE 72/429: pages2k_691+FE23_northamerica_canada_cana062 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-LacRomanel.Schweingruber.1996-1.txt   ===\n=== URL 2: https://www.ncei.noaa.gov/pub/data/paleo/treering/measurements/northamerica/canada/cana062-noaa.rwl   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False False\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\nAutomated choice. Metadata identical, automatically choose FE23 (Breitenmoser et al. (2014)) over PAGES 2k v2.2.0. conservative replication requirement\nKEEP RED CROSSES: remove pages2k_691, keep FE23_northamerica_canada_cana062.\nwrite decision to backup file\n&gt; 74/429,pages2k_730,iso2k_396,5.414343588593776,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 73/429: pages2k_730+iso2k_396 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Eur-SpannagelCave.Mangini.2005.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/5433   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  False\nlat True lon True elevation False archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  True\ncorrelation_perfect:  True\nRECORDS IDENTICAL (identical data) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_730, keep iso2k_396.\nwrite decision to backup file\n&gt; 75/429,pages2k_736,FE23_northamerica_usa_wy024,3.2176480591281234,0.9937765458543829\n====================================================================\n=== POTENTIAL DUPLICATE 74/429: pages2k_736+FE23_northamerica_usa_wy024 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-GranitePassHuntMountain.Briffa.1996-1.txt   ===\n=== URL 2: https://www.ncei.noaa.gov/pub/data/paleo/treering/measurements/northamerica/usa/wy024-noaa.rwl   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\nAutomated choice. Metadata identical, automatically choose FE23 (Breitenmoser et al. (2014)) over PAGES 2k v2.2.0. conservative replication requirement\nKEEP RED CROSSES: remove pages2k_736, keep FE23_northamerica_usa_wy024.\nwrite decision to backup file\n&gt; 76/429,pages2k_800,FE23_northamerica_canada_cana234,4.413706727317208,0.9890384832658766\n====================================================================\n=== POTENTIAL DUPLICATE 75/429: pages2k_800+FE23_northamerica_canada_cana234 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-BigWhite2.Wilson.2005-2.txt   ===\n=== URL 2: https://www.ncei.noaa.gov/pub/data/paleo/treering/measurements/northamerica/canada/cana234-noaa.rwl   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\nAutomated choice. Metadata identical, automatically choose FE23 (Breitenmoser et al. (2014)) over PAGES 2k v2.2.0. conservative replication requirement\nKEEP RED CROSSES: remove pages2k_800, keep FE23_northamerica_canada_cana234.\nwrite decision to backup file\n&gt; 77/429,pages2k_818,iso2k_488,0.0,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 76/429: pages2k_818+iso2k_488 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-Puruogangri.Thompson.2006.txt   ===\n=== URL 2: nan   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  True\ncorrelation_perfect:  True\nRECORDS IDENTICAL (identical data) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_818, keep iso2k_488.\nwrite decision to backup file\n&gt; 78/429,pages2k_827,pages2k_830,0.0,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 77/429: pages2k_827+pages2k_830 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-CH07-98-MC-22.Saenger.2011-1.txt   ===\n=== URL 2: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-CH07-98-MC-22.Saenger.2011-2.txt   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  False\ndata_identical:  True\ncorrelation_perfect:  True\nRECORDS IDENTICAL (identical data) except for metadata. Automatically choose #1.\nKEEP BLUE CIRCLES: keep pages2k_827, remove pages2k_830.\nwrite decision to backup file\n&gt; 79/429,pages2k_831,pages2k_2220,0.0,0.9910287300605741\n====================================================================\n=== POTENTIAL DUPLICATE 78/429: pages2k_831+pages2k_2220 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-UKHTRW.Schweingruber.2002.txt   ===\n=== URL 2: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-UKHEWW.Schweingruber.2002.txt   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation) except for metadata. Automatically choose #1.\nKEEP BLUE CIRCLES: keep pages2k_831, remove pages2k_2220.\nwrite decision to backup file\n&gt; 80/429,pages2k_831,FE23_asia_russ127w,0.23755598128809655,0.9938897185424319\n====================================================================\n=== POTENTIAL DUPLICATE 79/429: pages2k_831+FE23_asia_russ127w ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-UKHTRW.Schweingruber.2002.txt   ===\n=== URL 2: https://www.ncei.noaa.gov/pub/data/paleo/treering/measurements/asia/russ127w-noaa.rwl   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\nAutomated choice. Metadata identical, automatically choose FE23 (Breitenmoser et al. (2014)) over PAGES 2k v2.2.0. conservative replication requirement\nKEEP RED CROSSES: remove pages2k_831, keep FE23_asia_russ127w.\nwrite decision to backup file\n&gt; 81/429,pages2k_857,FE23_northamerica_usa_ut511,4.779603504757881,0.9930030347228277\n====================================================================\n=== POTENTIAL DUPLICATE 80/429: pages2k_857+FE23_northamerica_usa_ut511 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-CeaderBreaks.Briffa.1996-1.txt   ===\n=== URL 2: https://www.ncei.noaa.gov/pub/data/paleo/treering/measurements/northamerica/usa/ut511-noaa.rwl   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\nAutomated choice. Metadata identical, automatically choose FE23 (Breitenmoser et al. (2014)) over PAGES 2k v2.2.0. conservative replication requirement\nKEEP RED CROSSES: remove pages2k_857, keep FE23_northamerica_usa_ut511.\nwrite decision to backup file\n&gt; 82/429,pages2k_881,iso2k_1010,7.831443798552019,0.9999999999999999\n====================================================================\n=== POTENTIAL DUPLICATE 81/429: pages2k_881+iso2k_1010 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Malindi.Cole.2000.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/1855   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  True\ncorrelation_perfect:  True\nRECORDS IDENTICAL (identical data) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_881, keep iso2k_1010.\nwrite decision to backup file\n&gt; 83/429,pages2k_893,pages2k_895,0.0,0.997878731888924\n====================================================================\n=== POTENTIAL DUPLICATE 82/429: pages2k_893+pages2k_895 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-FeniDrift.Richter.2009-1.txt   ===\n=== URL 2: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-FeniDrift.Richter.2009-1.txt   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  True\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation). Automatically choose #1.\nKEEP BLUE CIRCLES: keep pages2k_893, remove pages2k_895.\nwrite decision to backup file\n&gt; 84/429,pages2k_893,pages2k_900,0.0,0.997878731888924\n====================================================================\n=== POTENTIAL DUPLICATE 83/429: pages2k_893+pages2k_900 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-FeniDrift.Richter.2009-1.txt   ===\n=== URL 2: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-FeniDrift.Richter.2009-2.txt   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation) except for metadata. Automatically choose #1.\nKEEP BLUE CIRCLES: keep pages2k_893, remove pages2k_900.\nwrite decision to backup file\n&gt; 85/429,pages2k_895,pages2k_900,0.0,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 84/429: pages2k_895+pages2k_900 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-FeniDrift.Richter.2009-1.txt   ===\n=== URL 2: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-FeniDrift.Richter.2009-2.txt   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  False\ndata_identical:  True\ncorrelation_perfect:  True\nRECORDS IDENTICAL (identical data) except for metadata. Automatically choose #1.\nKEEP BLUE CIRCLES: keep pages2k_895, remove pages2k_900.\nwrite decision to backup file\n&gt; 86/429,pages2k_940,ch2k_DR99ABR01_264,0.0,0.9999868748284537\n====================================================================\n=== POTENTIAL DUPLICATE 85/429: pages2k_940+ch2k_DR99ABR01_264 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-AbrahamReef.Druffel.1999.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/1911   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  False\nlat True lon True elevation False archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_940, keep ch2k_DR99ABR01_264.\nwrite decision to backup file\n&gt; 87/429,pages2k_940,ch2k_DR99ABR01_266,0.0,0.9999868748284537\n====================================================================\n=== POTENTIAL DUPLICATE 86/429: pages2k_940+ch2k_DR99ABR01_266 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-AbrahamReef.Druffel.1999.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/1911   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  False\nlat True lon True elevation False archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_940, keep ch2k_DR99ABR01_266.\nwrite decision to backup file\n&gt; 88/429,pages2k_940,iso2k_91,0.0,0.9999868748284537\n====================================================================\n=== POTENTIAL DUPLICATE 87/429: pages2k_940+iso2k_91 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-AbrahamReef.Druffel.1999.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/1911   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  False\nlat True lon True elevation False archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_940, keep iso2k_91.\nwrite decision to backup file\n&gt; 89/429,pages2k_945,iso2k_100,0.0,0.999999999878647\n====================================================================\n=== POTENTIAL DUPLICATE 88/429: pages2k_945+iso2k_100 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ant-CoastalDML.Thamban.2012.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo-search/study/22589   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  False\nlat True lon True elevation False archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_945, keep iso2k_100.\nwrite decision to backup file\n&gt; 90/429,pages2k_960,iso2k_641,1.4788881660628725,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 89/429: pages2k_960+iso2k_641 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-FloridaBay.Swart.1996.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/1886   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  False\nlat True lon True elevation False archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  True\ncorrelation_perfect:  True\nRECORDS IDENTICAL (identical data) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_960, keep iso2k_641.\nwrite decision to backup file\n&gt; 91/429,pages2k_982,FE23_northamerica_usa_or042,4.592357698872727,0.9964512838580512\n====================================================================\n=== POTENTIAL DUPLICATE 90/429: pages2k_982+FE23_northamerica_usa_or042 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-CraterLakeNE.Briffa.2002-1.txt   ===\n=== URL 2: https://www.ncei.noaa.gov/pub/data/paleo/treering/measurements/northamerica/usa/or042-noaa.rwl   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\nAutomated choice. Metadata identical, automatically choose FE23 (Breitenmoser et al. (2014)) over PAGES 2k v2.2.0. conservative replication requirement\nKEEP RED CROSSES: remove pages2k_982, keep FE23_northamerica_usa_or042.\nwrite decision to backup file\n&gt; 92/429,pages2k_1004,iso2k_644,0.0,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 91/429: pages2k_1004+iso2k_644 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ant-DomeF1993.Uemura.2014.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo-search/study/22471   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_1004, keep iso2k_644.\nwrite decision to backup file\n&gt; 93/429,pages2k_1026,FE23_northamerica_usa_az553,3.706446234061719,0.989435703033374\n====================================================================\n=== POTENTIAL DUPLICATE 92/429: pages2k_1026+FE23_northamerica_usa_az553 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-SnowBowlSanFranciscoPeak.Briffa.2002-1.txt   ===\n=== URL 2: https://www.ncei.noaa.gov/pub/data/paleo/treering/measurements/northamerica/usa/az553-noaa.rwl   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\nAutomated choice. Metadata identical, automatically choose FE23 (Breitenmoser et al. (2014)) over PAGES 2k v2.2.0. conservative replication requirement\nKEEP RED CROSSES: remove pages2k_1026, keep FE23_northamerica_usa_az553.\nwrite decision to backup file\n&gt; 94/429,pages2k_1048,iso2k_1212,0.08120329462182123,0.9999999999444483\n====================================================================\n=== POTENTIAL DUPLICATE 93/429: pages2k_1048+iso2k_1212 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ant-VLG.Bertler.2011.txt   ===\n=== URL 2: https://doi.pangaea.de/10.1594/PANGAEA.866368   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_1048, keep iso2k_1212.\nwrite decision to backup file\n&gt; 95/429,pages2k_1089,FE23_northamerica_usa_mt112,1.3032489760048571,0.9906780912896159\n====================================================================\n=== POTENTIAL DUPLICATE 94/429: pages2k_1089+FE23_northamerica_usa_mt112 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-YellowMountainRidge.King.2002.txt   ===\n=== URL 2: https://www.ncei.noaa.gov/pub/data/paleo/treering/measurements/northamerica/usa/mt112-noaa.rwl   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\nAutomated choice. Metadata identical, automatically choose FE23 (Breitenmoser et al. (2014)) over PAGES 2k v2.2.0. conservative replication requirement\nKEEP RED CROSSES: remove pages2k_1089, keep FE23_northamerica_usa_mt112.\nwrite decision to backup file\n&gt; 96/429,pages2k_1089,FE23_northamerica_usa_mt113,1.3032489760048571,0.8759857906162318\n====================================================================\n=== POTENTIAL DUPLICATE 95/429: pages2k_1089+FE23_northamerica_usa_mt113 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-YellowMountainRidge.King.2002.txt   ===\n=== URL 2: https://www.ncei.noaa.gov/pub/data/paleo/treering/measurements/northamerica/usa/mt113-noaa.rwl   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False False\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\nAutomated choice. Metadata identical, automatically choose FE23 (Breitenmoser et al. (2014)) over PAGES 2k v2.2.0. conservative replication requirement\nKEEP RED CROSSES: remove pages2k_1089, keep FE23_northamerica_usa_mt113.\nwrite decision to backup file\n&gt; 97/429,pages2k_1108,iso2k_1060,0.0,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 96/429: pages2k_1108+iso2k_1060 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-BermudaSouthShore.Goodkin.2008-1.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/6115   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  False\ndata_identical:  True\ncorrelation_perfect:  True\nRECORDS IDENTICAL (identical data) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_1108, keep iso2k_1060.\nwrite decision to backup file\n&gt; 98/429,pages2k_1116,FE23_northamerica_canada_cana170w,2.963034008750589,0.9012973226223439\n====================================================================\n=== POTENTIAL DUPLICATE 97/429: pages2k_1116+FE23_northamerica_canada_cana170w ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-Athabasca.Schweingruber.1996-1.txt   ===\n=== URL 2: https://www.ncei.noaa.gov/pub/data/paleo/treering/measurements/northamerica/canada/cana170w-noaa.rwl   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False False\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\nAutomated choice. Metadata identical, automatically choose FE23 (Breitenmoser et al. (2014)) over PAGES 2k v2.2.0. conservative replication requirement\nKEEP RED CROSSES: remove pages2k_1116, keep FE23_northamerica_canada_cana170w.\nwrite decision to backup file\n&gt; 99/429,pages2k_1147,ch2k_DA06MAF01_78,0.0,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 98/429: pages2k_1147+ch2k_DA06MAF01_78 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Mafia.Damassa.2006.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/10808   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\n</pre> <pre>**Decision required for this duplicate pair (see figure above).**\n---------------------------------------------------------------------------------------------------------\n***ATTENTION*** THIS RECORD IS ASSOCIATED WITH MULTIPLE DUPLICATES! PLEASE PAY SPECIAL ATTENTION WHEN MAKING DECISIONS FOR THIS RECORD!\nThe potential duplicates also associated with this record are:\n............................................................\n     - Dataset ID          : ch2k_DA06MAF02_104\n     - URL                 : https://www.ncdc.noaa.gov/paleo/study/10808\n</pre> <pre>............................................................\n     - Dataset ID          : iso2k_1748\n     - URL                 : https://www.ncdc.noaa.gov/paleo/study/10808\n</pre> <pre>---------------------------------------------------------------------------------------------------------\nBefore inputting your decision. Would you like to leave a comment on your decision process?\n</pre> <pre>saved figure in /home/jupyter-lluecke/dod2k/figs//dup_detection/all_merged/098_pages2k_1147_ch2k_DA06MAF01_78__428_4147.jpg\nKEEP BLUE CIRCLES: keep pages2k_1147, remove ch2k_DA06MAF01_78.\nwrite decision to backup file\n&gt; 100/429,pages2k_1147,ch2k_DA06MAF02_104,0.0,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 99/429: pages2k_1147+ch2k_DA06MAF02_104 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Mafia.Damassa.2006.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/10808   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\n</pre> <pre>**Decision required for this duplicate pair (see figure above).**\n---------------------------------------------------------------------------------------------------------\n***ATTENTION*** THIS RECORD IS ASSOCIATED WITH MULTIPLE DUPLICATES! PLEASE PAY SPECIAL ATTENTION WHEN MAKING DECISIONS FOR THIS RECORD!\nThe potential duplicates also associated with this record are:\n............................................................\n     - Dataset ID          : ch2k_DA06MAF01_78\n     - URL                 : https://www.ncdc.noaa.gov/paleo/study/10808\n</pre> <pre>............................................................\n     - Dataset ID          : iso2k_1748\n     - URL                 : https://www.ncdc.noaa.gov/paleo/study/10808\n</pre> <pre>---------------------------------------------------------------------------------------------------------\nBefore inputting your decision. Would you like to leave a comment on your decision process?\n</pre> <pre>saved figure in /home/jupyter-lluecke/dod2k/figs//dup_detection/all_merged/099_pages2k_1147_ch2k_DA06MAF02_104__428_4153.jpg\nKEEP BLUE CIRCLES: keep pages2k_1147, remove ch2k_DA06MAF02_104.\nwrite decision to backup file\n&gt; 101/429,pages2k_1147,iso2k_1748,0.0,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 100/429: pages2k_1147+iso2k_1748 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Mafia.Damassa.2006.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/10808   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  True\ncorrelation_perfect:  True\nRECORDS IDENTICAL (identical data) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_1147, keep iso2k_1748.\nwrite decision to backup file\n&gt; 102/429,pages2k_1153,pages2k_1156,0.0,0.9967946398784733\n====================================================================\n=== POTENTIAL DUPLICATE 101/429: pages2k_1153+pages2k_1156 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-ODP984.Came.2007-1.txt   ===\n=== URL 2: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-ODP984.Came.2007-1.txt   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  True\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation). Automatically choose #1.\nKEEP BLUE CIRCLES: keep pages2k_1153, remove pages2k_1156.\nwrite decision to backup file\n&gt; 103/429,pages2k_1153,pages2k_1160,0.0,0.9969118556506823\n====================================================================\n=== POTENTIAL DUPLICATE 102/429: pages2k_1153+pages2k_1160 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-ODP984.Came.2007-1.txt   ===\n=== URL 2: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-ODP984.Came.2007-2.txt   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation) except for metadata. Automatically choose #1.\nKEEP BLUE CIRCLES: keep pages2k_1153, remove pages2k_1160.\nwrite decision to backup file\n&gt; 104/429,pages2k_1156,pages2k_1160,0.0,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 103/429: pages2k_1156+pages2k_1160 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-ODP984.Came.2007-1.txt   ===\n=== URL 2: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-ODP984.Came.2007-2.txt   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation) except for metadata. Automatically choose #1.\nKEEP BLUE CIRCLES: keep pages2k_1156, remove pages2k_1160.\nwrite decision to backup file\n&gt; 105/429,pages2k_1209,FE23_northamerica_usa_co553,4.686190390758007,0.9848164514971338\n====================================================================\n=== POTENTIAL DUPLICATE 104/429: pages2k_1209+FE23_northamerica_usa_co553 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-PikePeaks.Harlan.1996-1.txt   ===\n=== URL 2: https://www.ncei.noaa.gov/pub/data/paleo/treering/measurements/northamerica/usa/co553-noaa.rwl   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\nAutomated choice. Metadata identical, automatically choose FE23 (Breitenmoser et al. (2014)) over PAGES 2k v2.2.0. conservative replication requirement\nKEEP RED CROSSES: remove pages2k_1209, keep FE23_northamerica_usa_co553.\nwrite decision to backup file\n&gt; 106/429,pages2k_1252,FE23_northamerica_canada_cana096,5.559669351093426,0.9976191515076486\n====================================================================\n=== POTENTIAL DUPLICATE 105/429: pages2k_1252+FE23_northamerica_canada_cana096 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-SunwaptaPass.Schweingruber.1996-1.txt   ===\n=== URL 2: https://www.ncei.noaa.gov/pub/data/paleo/treering/measurements/northamerica/canada/cana096-noaa.rwl   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\nAutomated choice. Metadata identical, automatically choose FE23 (Breitenmoser et al. (2014)) over PAGES 2k v2.2.0. conservative replication requirement\nKEEP RED CROSSES: remove pages2k_1252, keep FE23_northamerica_canada_cana096.\nwrite decision to backup file\n&gt; 107/429,pages2k_1274,iso2k_1577,0.0,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 106/429: pages2k_1274+iso2k_1577 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-GRIP.Vinther.2010.txt   ===\n=== URL 2: https://doi.pangaea.de/10.1594/PANGAEA.786354   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  False\nlat True lon True elevation False archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  False\ndata_identical:  True\ncorrelation_perfect:  True\nRECORDS IDENTICAL (identical data) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_1274, keep iso2k_1577.\nwrite decision to backup file\n&gt; 108/429,pages2k_1293,iso2k_821,0.0,0.9999999993592802\n====================================================================\n=== POTENTIAL DUPLICATE 107/429: pages2k_1293+iso2k_821 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ant-TALDICE.Stenni.2010.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/22502   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_1293, keep iso2k_821.\nwrite decision to backup file\n&gt; 109/429,pages2k_1325,FE23_northamerica_usa_wy030,4.634071826744492,0.9841375865267528\n====================================================================\n=== POTENTIAL DUPLICATE 108/429: pages2k_1325+FE23_northamerica_usa_wy030 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-SheepTrail.Brown.2005-1.txt   ===\n=== URL 2: https://www.ncei.noaa.gov/pub/data/paleo/treering/measurements/northamerica/usa/wy030-noaa.rwl   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\nAutomated choice. Metadata identical, automatically choose FE23 (Breitenmoser et al. (2014)) over PAGES 2k v2.2.0. conservative replication requirement\nKEEP RED CROSSES: remove pages2k_1325, keep FE23_northamerica_usa_wy030.\nwrite decision to backup file\n&gt; 110/429,pages2k_1360,ch2k_UR00MAI01_22,0.0,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 109/429: pages2k_1360+ch2k_UR00MAI01_22 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Maiana.Urban.2000.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/1859   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_1360, keep ch2k_UR00MAI01_22.\nwrite decision to backup file\n&gt; 111/429,pages2k_1360,iso2k_94,7.450067481225913,0.999999993382913\n====================================================================\n=== POTENTIAL DUPLICATE 110/429: pages2k_1360+iso2k_94 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Maiana.Urban.2000.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/1859   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  False\nlat True lon True elevation False archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_1360, keep iso2k_94.\nwrite decision to backup file\n&gt; 112/429,pages2k_1360,iso2k_98,7.450067481225913,0.999999993382913\n====================================================================\n=== POTENTIAL DUPLICATE 111/429: pages2k_1360+iso2k_98 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Maiana.Urban.2000.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/1859   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  False\nlat True lon True elevation False archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_1360, keep iso2k_98.\nwrite decision to backup file\n&gt; 113/429,pages2k_1362,pages2k_1365,0.0,0.9992389529344313\n====================================================================\n=== POTENTIAL DUPLICATE 112/429: pages2k_1362+pages2k_1365 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-GulfofGuinea.Weldeab.2007-1.txt   ===\n=== URL 2: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-GulfofGuinea.Weldeab.2007-1.txt   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  True\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation). Automatically choose #1.\nKEEP BLUE CIRCLES: keep pages2k_1362, remove pages2k_1365.\nwrite decision to backup file\n&gt; 114/429,pages2k_1370,iso2k_1619,0.0,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 113/429: pages2k_1370+iso2k_1619 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-NGRIP1.Vinther.2006.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/8700   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  True\ncorrelation_perfect:  True\nRECORDS IDENTICAL (identical data) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_1370, keep iso2k_1619.\nwrite decision to backup file\n&gt; 115/429,pages2k_1420,FE23_northamerica_canada_cana111,2.210833453585409,0.9185079638747947\n====================================================================\n=== POTENTIAL DUPLICATE 114/429: pages2k_1420+FE23_northamerica_canada_cana111 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-VancouverCyprusProvincialPark.Briffa.1996-1.txt   ===\n=== URL 2: https://www.ncei.noaa.gov/pub/data/paleo/treering/measurements/northamerica/canada/cana111-noaa.rwl   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False False\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\nAutomated choice. Metadata identical, automatically choose FE23 (Breitenmoser et al. (2014)) over PAGES 2k v2.2.0. conservative replication requirement\nKEEP RED CROSSES: remove pages2k_1420, keep FE23_northamerica_canada_cana111.\nwrite decision to backup file\n&gt; 116/429,pages2k_1442,pages2k_1444,0.0,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 115/429: pages2k_1442+pages2k_1444 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-LaurentianFan.Keigwin.2005-1.txt   ===\n=== URL 2: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-LaurentianFan.Keigwin.2005-2.txt   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  False\ndata_identical:  True\ncorrelation_perfect:  True\nRECORDS IDENTICAL (identical data) except for metadata. Automatically choose #1.\nKEEP BLUE CIRCLES: keep pages2k_1442, remove pages2k_1444.\nwrite decision to backup file\n&gt; 117/429,pages2k_1488,pages2k_1628,0.0,0.9999312398195644\n====================================================================\n=== POTENTIAL DUPLICATE 116/429: pages2k_1488+pages2k_1628 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Palmyra.Nurhati.2011-1.txt   ===\n=== URL 2: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Palmyra.Cobb.2003.txt   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) False\nmetadata_identical:  False\nlat True lon True elevation False archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\n</pre> <pre>**Decision required for this duplicate pair (see figure above).**\n---------------------------------------------------------------------------------------------------------\n***ATTENTION*** THIS RECORD IS ASSOCIATED WITH MULTIPLE DUPLICATES! PLEASE PAY SPECIAL ATTENTION WHEN MAKING DECISIONS FOR THIS RECORD!\nThe potential duplicates also associated with this record are:\n............................................................\n     - Dataset ID          : ch2k_NU11PAL01_52\n     - URL                 : https://www.ncdc.noaa.gov/paleo/study/10373\n</pre> <pre>............................................................\n     - Dataset ID          : iso2k_505\n     - URL                 : https://www.ncdc.noaa.gov/paleo/study/1875\n</pre> <pre>............................................................\n     - Dataset ID          : iso2k_579\n     - URL                 : https://www.ncdc.noaa.gov/paleo/study/10373\n</pre> <pre>---------------------------------------------------------------------------------------------------------\nBefore inputting your decision. Would you like to leave a comment on your decision process?\n</pre> <pre>saved figure in /home/jupyter-lluecke/dod2k/figs//dup_detection/all_merged/116_pages2k_1488_pages2k_1628__542_595.jpg\nKEEP RED CROSSES: remove pages2k_1488, keep pages2k_1628.\nwrite decision to backup file\n&gt; 118/429,pages2k_1488,ch2k_NU11PAL01_52,0.4697858835846662,0.9992710430546085\n====================================================================\n=== POTENTIAL DUPLICATE 117/429: pages2k_1488+ch2k_NU11PAL01_52 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Palmyra.Nurhati.2011-1.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/10373   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_1488, keep ch2k_NU11PAL01_52.\nwrite decision to backup file\n&gt; 119/429,pages2k_1488,iso2k_505,0.0,0.9976024758754877\n====================================================================\n=== POTENTIAL DUPLICATE 118/429: pages2k_1488+iso2k_505 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Palmyra.Nurhati.2011-1.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/1875   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) False\nmetadata_identical:  False\nlat True lon True elevation False archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\n</pre> <pre>**Decision required for this duplicate pair (see figure above).**\n---------------------------------------------------------------------------------------------------------\n***ATTENTION*** THIS RECORD IS ASSOCIATED WITH MULTIPLE DUPLICATES! PLEASE PAY SPECIAL ATTENTION WHEN MAKING DECISIONS FOR THIS RECORD!\nThe potential duplicates also associated with this record are:\n............................................................\n     - Dataset ID          : pages2k_1628\n     - URL                 : https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Palmyra.Cobb.2003.txt\n</pre> <pre>............................................................\n     - Dataset ID          : ch2k_NU11PAL01_52\n     - URL                 : https://www.ncdc.noaa.gov/paleo/study/10373\n</pre> <pre>............................................................\n     - Dataset ID          : iso2k_579\n     - URL                 : https://www.ncdc.noaa.gov/paleo/study/10373\n</pre> <pre>---------------------------------------------------------------------------------------------------------\nBefore inputting your decision. Would you like to leave a comment on your decision process?\n</pre> <pre>saved figure in /home/jupyter-lluecke/dod2k/figs//dup_detection/all_merged/118_pages2k_1488_iso2k_505__542_4456.jpg\nREMOVE BOTH: remove iso2k_505, remove pages2k_1488.\nwrite decision to backup file\n&gt; 120/429,pages2k_1488,iso2k_579,0.0,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 119/429: pages2k_1488+iso2k_579 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Palmyra.Nurhati.2011-1.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/10373   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  False\ndata_identical:  True\ncorrelation_perfect:  True\nRECORDS IDENTICAL (identical data) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_1488, keep iso2k_579.\nwrite decision to backup file\n&gt; 121/429,pages2k_1490,ch2k_NU11PAL01_54,0.4697858835846662,0.999992275602027\n====================================================================\n=== POTENTIAL DUPLICATE 120/429: pages2k_1490+ch2k_NU11PAL01_54 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Palmyra.Nurhati.2011-1.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/10373   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  True\nRECORDS IDENTICAL (perfect correlation) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_1490, keep ch2k_NU11PAL01_54.\nwrite decision to backup file\n&gt; 122/429,pages2k_1491,iso2k_575,0.0,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 121/429: pages2k_1491+iso2k_575 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Palmyra.Nurhati.2011-2.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/10373   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  False\ndata_identical:  True\ncorrelation_perfect:  True\nRECORDS IDENTICAL (identical data) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_1491, keep iso2k_575.\nwrite decision to backup file\n&gt; 123/429,pages2k_1497,iso2k_1885,0.0,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 122/429: pages2k_1497+iso2k_1885 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/SAm-QuelccayaIceCap.Thompson.2013.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/14174   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  False\ndata_identical:  True\ncorrelation_perfect:  True\nRECORDS IDENTICAL (identical data) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_1497, keep iso2k_1885.\nwrite decision to backup file\n&gt; 124/429,pages2k_1515,pages2k_1519,0.0,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 123/429: pages2k_1515+pages2k_1519 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-SouthChinaSea.Zhao.2006-1.txt   ===\n=== URL 2: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-SouthChinaSea.Zhao.2006-2.txt   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  False\ndata_identical:  True\ncorrelation_perfect:  True\nRECORDS IDENTICAL (identical data) except for metadata. Automatically choose #1.\nKEEP BLUE CIRCLES: keep pages2k_1515, remove pages2k_1519.\nwrite decision to backup file\n&gt; 125/429,pages2k_1520,pages2k_1522,0.0,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 124/429: pages2k_1520+pages2k_1522 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-SubTropicalEasternNorthAtlantic.deMenocal.2000-1.txt   ===\n=== URL 2: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-SubTropicalEasternNorthAtlantic.deMenocal.2000-2.txt   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\n</pre> <pre>**Decision required for this duplicate pair (see figure above).**\nBefore inputting your decision. Would you like to leave a comment on your decision process?\n</pre> <pre>saved figure in /home/jupyter-lluecke/dod2k/figs//dup_detection/all_merged/124_pages2k_1520_pages2k_1522__553_554.jpg\nKEEP RED CROSSES: remove pages2k_1520, keep pages2k_1522.\nwrite decision to backup file\n&gt; 126/429,pages2k_1547,iso2k_259,0.0,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 125/429: pages2k_1547+iso2k_259 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Kiritimati.Evans.1998.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/1847   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) True\nmetadata_identical:  False\nlat True lon True elevation False archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  False\ndata_identical:  True\ncorrelation_perfect:  True\nRECORDS IDENTICAL (identical data) except for metadata. Automatically choose #2.\nKEEP RED CROSSES: remove pages2k_1547, keep iso2k_259.\nwrite decision to backup file\n&gt; 127/429,pages2k_1566,FE23_northamerica_canada_cana231,4.390485654920984,0.9977556148240536\n====================================================================\n=== POTENTIAL DUPLICATE 126/429: pages2k_1566+FE23_northamerica_canada_cana231 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-ParkMountain.Wilson.2005-1.txt   ===\n=== URL 2: https://www.ncei.noaa.gov/pub/data/paleo/treering/measurements/northamerica/canada/cana231-noaa.rwl   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\nAutomated choice. Metadata identical, automatically choose FE23 (Breitenmoser et al. (2014)) over PAGES 2k v2.2.0. conservative replication requirement\nKEEP RED CROSSES: remove pages2k_1566, keep FE23_northamerica_canada_cana231.\nwrite decision to backup file\n&gt; 128/429,pages2k_1605,FE23_northamerica_usa_ca606,3.985226443144479,0.9693885470026882\n====================================================================\n=== POTENTIAL DUPLICATE 127/429: pages2k_1605+FE23_northamerica_usa_ca606 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-SpillwayLakeYosemiteNationalPark.King.2000.txt   ===\n=== URL 2: https://www.ncei.noaa.gov/pub/data/paleo/treering/measurements/northamerica/usa/ca606-noaa.rwl   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False False\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\nAutomated choice. Metadata identical, automatically choose FE23 (Breitenmoser et al. (2014)) over PAGES 2k v2.2.0. conservative replication requirement\nKEEP RED CROSSES: remove pages2k_1605, keep FE23_northamerica_usa_ca606.\nwrite decision to backup file\n&gt; 129/429,pages2k_1619,pages2k_1623,0.0,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 128/429: pages2k_1619+pages2k_1623 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-JacafFjord.Seplveda.2009-1.txt   ===\n=== URL 2: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-JacafFjord.Seplveda.2009-2.txt   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\n</pre> <pre>**Decision required for this duplicate pair (see figure above).**\nBefore inputting your decision. Would you like to leave a comment on your decision process?\n</pre> <pre>saved figure in /home/jupyter-lluecke/dod2k/figs//dup_detection/all_merged/128_pages2k_1619_pages2k_1623__590_592.jpg\nKEEP RED CROSSES: remove pages2k_1619, keep pages2k_1623.\nwrite decision to backup file\n&gt; 130/429,pages2k_1628,ch2k_NU11PAL01_52,0.4697858835846662,0.9999312398195646\n====================================================================\n=== POTENTIAL DUPLICATE 129/429: pages2k_1628+ch2k_NU11PAL01_52 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Palmyra.Cobb.2003.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/10373   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) False\nmetadata_identical:  False\nlat True lon True elevation False archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\n</pre> <pre>**Decision required for this duplicate pair (see figure above).**\n---------------------------------------------------------------------------------------------------------\n***ATTENTION*** THIS RECORD IS ASSOCIATED WITH MULTIPLE DUPLICATES! PLEASE PAY SPECIAL ATTENTION WHEN MAKING DECISIONS FOR THIS RECORD!\nThe potential duplicates also associated with this record are:\n-  pages2k_1488\n............................................................\n     - Dataset ID          : iso2k_505\n     - URL                 : https://www.ncdc.noaa.gov/paleo/study/1875\n</pre> <pre>............................................................\n     - Dataset ID          : iso2k_579\n     - URL                 : https://www.ncdc.noaa.gov/paleo/study/10373\n</pre> <pre>---------------------------------------------------------------------------------------------------------\nBefore inputting your decision. Would you like to leave a comment on your decision process?\n</pre> <pre>saved figure in /home/jupyter-lluecke/dod2k/figs//dup_detection/all_merged/129_pages2k_1628_ch2k_NU11PAL01_52__595_4138.jpg\nKEEP BLUE CIRCLES: keep pages2k_1628, remove ch2k_NU11PAL01_52.\nwrite decision to backup file\n&gt; 131/429,pages2k_1628,iso2k_505,0.0,0.9973624471178902\n====================================================================\n=== POTENTIAL DUPLICATE 130/429: pages2k_1628+iso2k_505 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Palmyra.Cobb.2003.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/1875   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\n</pre> <pre>**Decision required for this duplicate pair (see figure above).**\n---------------------------------------------------------------------------------------------------------\n***ATTENTION*** THIS RECORD IS ASSOCIATED WITH MULTIPLE DUPLICATES! PLEASE PAY SPECIAL ATTENTION WHEN MAKING DECISIONS FOR THIS RECORD!\nThe potential duplicates also associated with this record are:\n-  pages2k_1488\n............................................................\n     - Dataset ID          : ch2k_NU11PAL01_52\n     - URL                 : https://www.ncdc.noaa.gov/paleo/study/10373\n</pre> <pre>............................................................\n     - Dataset ID          : iso2k_579\n     - URL                 : https://www.ncdc.noaa.gov/paleo/study/10373\n</pre> <pre>---------------------------------------------------------------------------------------------------------\nBefore inputting your decision. Would you like to leave a comment on your decision process?\n</pre> <pre>saved figure in /home/jupyter-lluecke/dod2k/figs//dup_detection/all_merged/130_pages2k_1628_iso2k_505__595_4456.jpg\nKEEP BLUE CIRCLES: keep pages2k_1628, remove iso2k_505.\nwrite decision to backup file\n&gt; 132/429,pages2k_1628,iso2k_579,0.0,0.9999312398195646\n====================================================================\n=== POTENTIAL DUPLICATE 131/429: pages2k_1628+iso2k_579 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Palmyra.Cobb.2003.txt   ===\n=== URL 2: https://www.ncdc.noaa.gov/paleo/study/10373   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) False\nmetadata_identical:  False\nlat True lon True elevation False archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\n</pre> <pre>**Decision required for this duplicate pair (see figure above).**\n---------------------------------------------------------------------------------------------------------\n***ATTENTION*** THIS RECORD IS ASSOCIATED WITH MULTIPLE DUPLICATES! PLEASE PAY SPECIAL ATTENTION WHEN MAKING DECISIONS FOR THIS RECORD!\nThe potential duplicates also associated with this record are:\n-  pages2k_1488\n............................................................\n     - Dataset ID          : ch2k_NU11PAL01_52\n     - URL                 : https://www.ncdc.noaa.gov/paleo/study/10373\n</pre> <pre>............................................................\n     - Dataset ID          : iso2k_505\n     - URL                 : https://www.ncdc.noaa.gov/paleo/study/1875\n</pre> <pre>---------------------------------------------------------------------------------------------------------\nBefore inputting your decision. Would you like to leave a comment on your decision process?\n</pre> <pre>saved figure in /home/jupyter-lluecke/dod2k/figs//dup_detection/all_merged/131_pages2k_1628_iso2k_579__595_4482.jpg\nKEEP BLUE CIRCLES: keep pages2k_1628, remove iso2k_579.\nwrite decision to backup file\n&gt; 133/429,pages2k_1636,FE23_northamerica_usa_wa081,6.762315219649745,0.9980630247518526\n====================================================================\n=== POTENTIAL DUPLICATE 132/429: pages2k_1636+FE23_northamerica_usa_wa081 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-MtStHelens.Briffa.1996-1.txt   ===\n=== URL 2: https://www.ncei.noaa.gov/pub/data/paleo/treering/measurements/northamerica/usa/wa081-noaa.rwl   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  False\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\nAutomated choice. Metadata identical, automatically choose FE23 (Breitenmoser et al. (2014)) over PAGES 2k v2.2.0. conservative replication requirement\nKEEP RED CROSSES: remove pages2k_1636, keep FE23_northamerica_usa_wa081.\nwrite decision to backup file\n&gt; 134/429,pages2k_1686,pages2k_1688,0.0,1.0\n====================================================================\n=== POTENTIAL DUPLICATE 133/429: pages2k_1686+pages2k_1688 ===\n=== URL 1: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-ArabianSea.Doose-Rolinski.2001-1.txt   ===\n=== URL 2: https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-ArabianSea.Doose-Rolinski.2001-2.txt   ===\nTrue if pot_dup_corrs[i_pot_dups]&gt;=0.98 else False True\n(len(time_1)==len(time_2)) False\nmetadata_identical:  True\nlat True lon True elevation True archivetype True paleodata_proxy True\nsites_identical:  True\nURL_identical:  False\ndata_identical:  False\ncorrelation_perfect:  False\n</pre> <pre>**Decision required for this duplicate pair (see figure above).**\nBefore inputting your decision. Would you like to leave a comment on your decision process?\n</pre> In\u00a0[\u00a0]: Copied! <pre>date = str(datetime.datetime.utcnow())[2:10]\nfn   = utf.find(f'dup_decisions_{df.name}_{initials}_{date}.csv', f'data/{df.name}/dup_detection')\n\nif fn != []:\n    print('----------------------------------------------------')\n    print('Successfully finished the duplicate decision process!'.upper())\n    print('----------------------------------------------------')\n    print('Saved the decision output file in:')\n    print()\n    for ff in fn:\n        print('%s.'%ff)\n    print()\n    print('You are now able to proceed with the next notebook: dup_removal.ipynb')\nelse:\n    print('!!!!!!!!!!!!WARNING!!!!!!!!!!!')\n    print('Final output file is missing.')\n    print()\n    print('Please re-run the notebook to complete duplicate decision process.')\n</pre> date = str(datetime.datetime.utcnow())[2:10] fn   = utf.find(f'dup_decisions_{df.name}_{initials}_{date}.csv', f'data/{df.name}/dup_detection')  if fn != []:     print('----------------------------------------------------')     print('Successfully finished the duplicate decision process!'.upper())     print('----------------------------------------------------')     print('Saved the decision output file in:')     print()     for ff in fn:         print('%s.'%ff)     print()     print('You are now able to proceed with the next notebook: dup_removal.ipynb') else:     print('!!!!!!!!!!!!WARNING!!!!!!!!!!!')     print('Final output file is missing.')     print()     print('Please re-run the notebook to complete duplicate decision process.') In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/dup_decision/#duplicate-detection-step-2-review-and-decide-on-candidate-pairs","title":"Duplicate detection - step 2: review and decide on candidate pairs\u00b6","text":""},{"location":"notebooks/dup_decision/#intialisation","title":"Intialisation\u00b6","text":""},{"location":"notebooks/dup_decision/#set-up-working-environment","title":"Set up working environment\u00b6","text":""},{"location":"notebooks/dup_decision/#load-dataset","title":"Load dataset\u00b6","text":""},{"location":"notebooks/dup_decision/#input-operators-credentials","title":"Input operator's credentials\u00b6","text":""},{"location":"notebooks/dup_decision/#duplicate-decision-process","title":"Duplicate decision process\u00b6","text":""},{"location":"notebooks/dup_decision/#hierarchy-for-duplicate-removal-for-identical-duplicates","title":"Hierarchy for duplicate removal for identical duplicates\u00b6","text":""},{"location":"notebooks/dup_decision/#duplicate-decision-process","title":"Duplicate decision process\u00b6","text":"<p>The following cell takes you through the potential duplicate candidate pairs and lets you decide whether to</p> <ul> <li>keep both records</li> <li>keep just one record</li> <li>delete both records</li> <li>create composite of both records.</li> </ul> <p>Recollections and updates of duplicates are automatically selected, as well as identical duplicates following the hierarchy defined above.</p> <p>The output is saved in <code>data/DATABASENAME/dup_detection/dup_decisions_DATABASENAME_INITIALS_DATE.csv</code></p> <p>Summary figures are saved in <code>figs/DATABASENAME/dup_detection/</code>, also linked in the output csv file.</p> <p>Note: The operator has the option to restart the decision process from a backup file in the directory <code>data/DATABASENAME/dup_detection</code>. This can be especially useful should the connection be interrupted during the process.</p>"},{"location":"notebooks/dup_detection/","title":"Duplicate detection - step 1: find the potential duplicates","text":"<p>This notebook runs the first part of the duplicate detection algorithm on a dataframe with the following columns:</p> <ul> <li><code>archiveType</code>       (used for duplicate detection algorithm)</li> <li><code>dataSetName</code></li> <li><code>datasetId</code></li> <li><code>geo_meanElev</code>      (used for duplicate detection algorithm)</li> <li><code>geo_meanLat</code>       (used for duplicate detection algorithm)</li> <li><code>geo_meanLon</code>       (used for duplicate detection algorithm)</li> <li><code>geo_siteName</code>      (used for duplicate detection algorithm)</li> <li><code>interpretation_direction</code></li> <li><code>interpretation_seasonality</code></li> <li><code>interpretation_variable</code></li> <li><code>interpretation_variableDetails</code></li> <li><code>originalDataURL</code></li> <li><code>originalDatabase</code></li> <li><code>paleoData_notes</code></li> <li><code>paleoData_proxy</code>   (used for duplicate detection algorithm)</li> <li><code>paleoData_units</code></li> <li><code>paleoData_values</code>  (used for duplicate detection algorithm, test for correlation, RMSE, correlation of 1st difference, RMSE of 1st difference)</li> <li><code>paleoData_variableName</code></li> <li><code>year</code>              (used for duplicate detection algorithm)</li> <li><code>yearUnits</code></li> </ul> <p>The key function for duplicate detection is <code>find_duplicates</code> in <code>f_duplicate_search.py</code></p> <p>The output is saved as csvs in the directory <code>data/DATABASENAME/dup_detection</code>, which are used again for step 2 (<code>dup_decisions.py</code>):</p> <ul> <li><code>pot_dup_correlations_DATABASENAME.csv</code><ul> <li>matrix of correlations between each pair</li> </ul> </li> <li><code>pot_dup_distances_km_DATABASENAME.csv</code><ul> <li>matrix of distances between each pair</li> </ul> </li> <li><code>pot_dup_IDs_DATABASENAME.csv</code><ul> <li>saves the IDs of each pair</li> </ul> </li> <li><code>pot_dup_indices_DATABASENAME.csv</code><ul> <li>saves the dataframe indices of each pair</li> </ul> </li> </ul> <p>Summary figures of the potential duplicate pairs are created and the plots are saved in the same directory, following: duplicatenumber_ID1_ID2_index1_index2.jpg</p> <p>Updates:</p> <ul> <li>06/11/2025 by LL: Tidied up and updated for DoD2k v2.0</li> <li>27/11/2024 by LL: Fixed a bug in find_duplicates (in f_duplicate_search) and relaxed site criteria.</li> </ul> <p>27/9/2024 created by LL</p> <p>Author: Lucie J. Luecke</p> <p>Make sure the repo_root is added correctly, it should be: your_root_dir/dod2k This should be the working directory throughout this notebook (and all other notebooks).</p> In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n\nimport sys\nimport os\nfrom pathlib import Path\n\n# Add parent directory to path (works from any notebook in notebooks/)\n# the repo_root should be the parent directory of the notebooks folder\ncurrent_dir = Path().resolve()\n# Determine repo root\nif current_dir.name == 'dod2k': repo_root = current_dir\nelif current_dir.parent.name == 'dod2k': repo_root = current_dir.parent\nelse: raise Exception('Please review the repo root structure (see first cell).')\n\n# Update cwd and path only if needed\nif os.getcwd() != str(repo_root):\n    os.chdir(repo_root)\nif str(repo_root) not in sys.path:\n    sys.path.insert(0, str(repo_root))\n\nprint(f\"Repo root: {repo_root}\")\nif str(os.getcwd())==str(repo_root):\n    print(f\"Working directory matches repo root. \")\n</pre> %load_ext autoreload %autoreload 2  import sys import os from pathlib import Path  # Add parent directory to path (works from any notebook in notebooks/) # the repo_root should be the parent directory of the notebooks folder current_dir = Path().resolve() # Determine repo root if current_dir.name == 'dod2k': repo_root = current_dir elif current_dir.parent.name == 'dod2k': repo_root = current_dir.parent else: raise Exception('Please review the repo root structure (see first cell).')  # Update cwd and path only if needed if os.getcwd() != str(repo_root):     os.chdir(repo_root) if str(repo_root) not in sys.path:     sys.path.insert(0, str(repo_root))  print(f\"Repo root: {repo_root}\") if str(os.getcwd())==str(repo_root):     print(f\"Working directory matches repo root. \") <pre>Repo root: /home/jupyter-lluecke/dod2k\nWorking directory matches repo root. \n</pre> In\u00a0[2]: Copied! <pre>import pandas as pd\nimport numpy as np\n\nfrom dod2k_utilities import ut_functions as utf # contains utility functions\nfrom dod2k_utilities import ut_duplicate_search as dup # contains utility functions\n</pre> import pandas as pd import numpy as np  from dod2k_utilities import ut_functions as utf # contains utility functions from dod2k_utilities import ut_duplicate_search as dup # contains utility functions <p>Define the dataset which needs to be screened for duplicates. Input files for the duplicate detection mechanism need to be compact dataframes (<code>pandas</code> dataframes with standardised columns and entry formatting).</p> <p>The function <code>load_compact_dataframe_from_csv</code> loads the dataframe from a <code>csv</code> file from <code>data\\DB\\</code>, with <code>DB</code> the name of the database. The database name (<code>db_name</code>) can be</p> <ul> <li><code>pages2k</code></li> <li><code>ch2k</code></li> <li><code>iso2k</code></li> <li><code>sisal</code></li> <li><code>fe23</code></li> </ul> <p>for the individual databases, or</p> <ul> <li><code>all_merged</code></li> </ul> <p>to load the merged database of all individual databases, or can be any user defined compact dataframe.</p> In\u00a0[3]: Copied! <pre># load dataframe\ndb_name='all_merged' \n# db_name = 'dup_test'\n# db_name='ch2k' \ndf = utf.load_compact_dataframe_from_csv(db_name)\n\nprint(df.info())\ndf.name = db_name\n</pre> # load dataframe db_name='all_merged'  # db_name = 'dup_test' # db_name='ch2k'  df = utf.load_compact_dataframe_from_csv(db_name)  print(df.info()) df.name = db_name  <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5147 entries, 0 to 5146\nData columns (total 21 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   archiveType                    5147 non-null   object \n 1   dataSetName                    5147 non-null   object \n 2   datasetId                      5147 non-null   object \n 3   geo_meanElev                   5048 non-null   float32\n 4   geo_meanLat                    5147 non-null   float32\n 5   geo_meanLon                    5147 non-null   float32\n 6   geo_siteName                   5147 non-null   object \n 7   interpretation_direction       5147 non-null   object \n 8   interpretation_seasonality     5147 non-null   object \n 9   interpretation_variable        5147 non-null   object \n 10  interpretation_variableDetail  5147 non-null   object \n 11  originalDataURL                5147 non-null   object \n 12  originalDatabase               5147 non-null   object \n 13  paleoData_notes                5147 non-null   object \n 14  paleoData_proxy                5147 non-null   object \n 15  paleoData_sensorSpecies        5147 non-null   object \n 16  paleoData_units                5147 non-null   object \n 17  paleoData_values               5147 non-null   object \n 18  paleoData_variableName         5147 non-null   object \n 19  year                           5147 non-null   object \n 20  yearUnits                      5147 non-null   object \ndtypes: float32(3), object(18)\nmemory usage: 784.2+ KB\nNone\n</pre> <p>Now run the first part of the duplicate detection algorithm, which goes through each candidate pair and evaluates the pairs for the following criteria:</p> <ul> <li>metadata criteria:<ul> <li>archive types (<code>archiveType</code>) must be identical</li> <li>proxy types (<code>paleoData_proxy</code>) must be identical</li> </ul> </li> <li>geographical criteria:<ul> <li>elevation (<code>geo_meanElev</code>) similar, within defined tolerance (use kwarg <code>elevation_tolerance</code>, defaults to 0)</li> <li>latitude and longtitude (<code>geo_meanLat</code> and <code>geo_meanLon</code>) similar, within defined tolerance in km (use kwarg <code>dist_tolerance_km</code>, defaults to 8 km)</li> </ul> </li> <li>overlap criterion:<ul> <li>time must overlap for at least $n$ points (use kwarg <code>n_points_thresh</code> to modify, defaults to $n=10$) unless at least one of the record is shorter than <code>n_points_thresh</code></li> </ul> </li> <li>site criterion:<ul> <li>there must be some overlap in the site name (<code>geo_siteName</code>)</li> </ul> </li> <li>correlation criteria:<ul> <li>correlation between the overlapping period must be greater than defined threshold (use <code>corr_thresh</code> to modify, defaults to 0.9) or correlation of first difference must be greater than defined threshold (use <code>corr_diff_thresh</code> to modify, defaults to 0.9)</li> <li>RMSE of overlapping period must be smaller than defined threshold (use <code>rmse_thresh</code> to modify, defaults to 0.1) or RMSE of first difference must be smaller than defined threshold (use <code>rmse_diff_thresh</code> to modify, defaults to 0.1)</li> </ul> </li> <li>URL criterion:<ul> <li>URLs (<code>originalDataURL</code>) must be identical if both records originate from the same database (<code>originalDatabase</code> must be identical)</li> </ul> </li> </ul> <p>A potential duplicate candidate pair is flagged, if all of these criteria are satisfied OR the correlation between the candidates is particularly high (&gt;0.98), while there is sufficient overlap (as defined by the overlap criterion).</p> <p>The output for a database named <code>DB</code> is saved under <code>data/DB/dup_detection/dup_detection_candidates_DB.csv</code>.</p> In\u00a0[4]: Copied! <pre>## run the find duplicate algorithm\ndup.find_duplicates_optimized(df, n_points_thresh=10)\n</pre> ## run the find duplicate algorithm dup.find_duplicates_optimized(df, n_points_thresh=10)  <pre>all_merged\nStart duplicate search:\n=================================\nchecking parameters:\nproxy archive                  :  must match     \nproxy type                     :  must match     \ndistance (km)                  &lt; 8               \nelevation                      :  must match     \ntime overlap                   &gt; 10              \ncorrelation                    &gt; 0.9             \nRMSE                           &lt; 0.1             \n1st difference rmse            &lt; 0.1             \ncorrelation of 1st difference  &gt; 0.9             \n=================================\nStart duplicate search\nProgress: 0/5147\n--&gt; Found potential duplicate: 0: pages2k_0&amp;4235: iso2k_296 (n_potential_duplicates=1)\n--&gt; Found potential duplicate: 0: pages2k_0&amp;4236: iso2k_298 (n_potential_duplicates=2)\n--&gt; Found potential duplicate: 0: pages2k_0&amp;4237: iso2k_299 (n_potential_duplicates=3)\n--&gt; Found potential duplicate: 2: pages2k_6&amp;2864: fe23_northamerica_usa_az555 (n_potential_duplicates=4)\nProgress: 10/5147\n--&gt; Found potential duplicate: 14: pages2k_50&amp;1414: fe23_northamerica_canada_cana091 (n_potential_duplicates=5)\n--&gt; Found potential duplicate: 16: pages2k_62&amp;17: pages2k_63 (n_potential_duplicates=6)\nProgress: 20/5147\n--&gt; Found potential duplicate: 24: pages2k_81&amp;3973: ch2k_he08lra01_76 (n_potential_duplicates=7)\n--&gt; Found potential duplicate: 24: pages2k_81&amp;4563: iso2k_1813 (n_potential_duplicates=8)\n--&gt; Found potential duplicate: 25: pages2k_83&amp;4594: iso2k_1916 (n_potential_duplicates=9)\n--&gt; Found potential duplicate: 26: pages2k_85&amp;27: pages2k_88 (n_potential_duplicates=10)\n--&gt; Found potential duplicate: 29: pages2k_94&amp;1466: fe23_northamerica_canada_cana153 (n_potential_duplicates=11)\nProgress: 30/5147\n--&gt; Found potential duplicate: 32: pages2k_107&amp;2636: fe23_northamerica_usa_ak046 (n_potential_duplicates=12)\n--&gt; Found potential duplicate: 37: pages2k_121&amp;38: pages2k_122 (n_potential_duplicates=13)\nProgress: 40/5147\n--&gt; Found potential duplicate: 43: pages2k_132&amp;1533: fe23_northamerica_canada_cana225 (n_potential_duplicates=14)\n--&gt; Found potential duplicate: 49: pages2k_158&amp;3845: fe23_northamerica_usa_wa069 (n_potential_duplicates=15)\nProgress: 50/5147\n--&gt; Found potential duplicate: 52: pages2k_171&amp;3929: fe23_northamerica_usa_wy021 (n_potential_duplicates=16)\nProgress: 60/5147\n--&gt; Found potential duplicate: 60: pages2k_203&amp;4353: iso2k_826 (n_potential_duplicates=17)\nProgress: 70/5147\n--&gt; Found potential duplicate: 70: pages2k_225&amp;3526: fe23_northamerica_usa_nv512 (n_potential_duplicates=18)\n--&gt; Found potential duplicate: 73: pages2k_238&amp;4403: iso2k_1044 (n_potential_duplicates=19)\n--&gt; Found potential duplicate: 75: pages2k_242&amp;4155: ch2k_li06fij01_582 (n_potential_duplicates=20)\n--&gt; Found potential duplicate: 75: pages2k_242&amp;4250: iso2k_353 (n_potential_duplicates=21)\nProgress: 80/5147\n--&gt; Found potential duplicate: 81: pages2k_258&amp;4487: iso2k_1498 (n_potential_duplicates=22)\n--&gt; Found potential duplicate: 84: pages2k_263&amp;4456: iso2k_1322 (n_potential_duplicates=23)\n--&gt; Found potential duplicate: 86: pages2k_267&amp;4179: iso2k_58 (n_potential_duplicates=24)\n--&gt; Found potential duplicate: 86: pages2k_267&amp;4408: iso2k_1068 (n_potential_duplicates=25)\n--&gt; Found potential duplicate: 88: pages2k_271&amp;4133: ch2k_fe18rus01_492 (n_potential_duplicates=26)\n--&gt; Found potential duplicate: 88: pages2k_271&amp;4580: iso2k_1861 (n_potential_duplicates=27)\n--&gt; Found potential duplicate: 89: pages2k_273&amp;2496: fe23_asia_russ130w (n_potential_duplicates=28)\nProgress: 90/5147\n--&gt; Found potential duplicate: 92: pages2k_281&amp;1468: fe23_northamerica_canada_cana155 (n_potential_duplicates=29)\n--&gt; Found potential duplicate: 95: pages2k_294&amp;2611: fe23_northamerica_usa_ak021 (n_potential_duplicates=30)\n--&gt; Found potential duplicate: 98: pages2k_305&amp;100: pages2k_309 (n_potential_duplicates=31)\n--&gt; Found potential duplicate: 99: pages2k_307&amp;101: pages2k_311 (n_potential_duplicates=32)\nProgress: 100/5147\n--&gt; Found potential duplicate: 103: pages2k_315&amp;4252: iso2k_362 (n_potential_duplicates=33)\n--&gt; Found potential duplicate: 104: pages2k_317&amp;3975: ch2k_na09mal01_84 (n_potential_duplicates=34)\n--&gt; Found potential duplicate: 104: pages2k_317&amp;4549: iso2k_1754 (n_potential_duplicates=35)\n--&gt; Found potential duplicate: 106: pages2k_323&amp;1518: fe23_northamerica_canada_cana210 (n_potential_duplicates=36)\nProgress: 110/5147\nProgress: 120/5147\n--&gt; Found potential duplicate: 123: pages2k_385&amp;4063: ch2k_fe09oga01_304 (n_potential_duplicates=37)\n--&gt; Found potential duplicate: 123: pages2k_385&amp;4596: iso2k_1922 (n_potential_duplicates=38)\n--&gt; Found potential duplicate: 124: pages2k_387&amp;4064: ch2k_fe09oga01_306 (n_potential_duplicates=39)\n--&gt; Found potential duplicate: 129: pages2k_395&amp;4098: ch2k_ca07fli01_400 (n_potential_duplicates=40)\n--&gt; Found potential duplicate: 129: pages2k_395&amp;4406: iso2k_1057 (n_potential_duplicates=41)\nProgress: 130/5147\n--&gt; Found potential duplicate: 130: pages2k_397&amp;4099: ch2k_ca07fli01_402 (n_potential_duplicates=42)\n--&gt; Found potential duplicate: 135: pages2k_409&amp;4107: ch2k_qu96esv01_422 (n_potential_duplicates=43)\n--&gt; Found potential duplicate: 135: pages2k_409&amp;4213: iso2k_218 (n_potential_duplicates=44)\n--&gt; Found potential duplicate: 137: pages2k_414&amp;139: pages2k_418 (n_potential_duplicates=45)\n--&gt; Found potential duplicate: 138: pages2k_417&amp;140: pages2k_421 (n_potential_duplicates=46)\nProgress: 140/5147\n--&gt; Found potential duplicate: 146: pages2k_427&amp;152: pages2k_433 (n_potential_duplicates=47)\nProgress: 150/5147\n--&gt; Found potential duplicate: 154: pages2k_435&amp;287: pages2k_842 (n_potential_duplicates=48)\n--&gt; Found potential duplicate: 157: pages2k_444&amp;158: pages2k_445 (n_potential_duplicates=49)\n--&gt; Found potential duplicate: 157: pages2k_444&amp;159: pages2k_446 (n_potential_duplicates=50)\n--&gt; Found potential duplicate: 158: pages2k_445&amp;159: pages2k_446 (n_potential_duplicates=51)\nProgress: 160/5147\n--&gt; Found potential duplicate: 164: pages2k_462&amp;4035: ch2k_os14ucp01_236 (n_potential_duplicates=52)\n--&gt; Found potential duplicate: 164: pages2k_462&amp;4249: iso2k_350 (n_potential_duplicates=53)\n--&gt; Found potential duplicate: 167: pages2k_468&amp;1145: pages2k_3550 (n_potential_duplicates=54)\n--&gt; Found potential duplicate: 167: pages2k_468&amp;2503: fe23_asia_russ137w (n_potential_duplicates=55)\n--&gt; Found potential duplicate: 169: pages2k_472&amp;170: pages2k_474 (n_potential_duplicates=56)\n--&gt; Found potential duplicate: 169: pages2k_472&amp;172: pages2k_477 (n_potential_duplicates=57)\nProgress: 170/5147\n--&gt; Found potential duplicate: 170: pages2k_474&amp;172: pages2k_477 (n_potential_duplicates=58)\n--&gt; Found potential duplicate: 173: pages2k_478&amp;4571: iso2k_1846 (n_potential_duplicates=59)\n--&gt; Found potential duplicate: 176: pages2k_486&amp;2984: fe23_northamerica_usa_ca609 (n_potential_duplicates=60)\n--&gt; Found potential duplicate: 178: pages2k_495&amp;3950: ch2k_li06rar01_12 (n_potential_duplicates=61)\n--&gt; Found potential duplicate: 178: pages2k_495&amp;4489: iso2k_1502 (n_potential_duplicates=62)\nProgress: 180/5147\n--&gt; Found potential duplicate: 181: pages2k_500&amp;4062: ch2k_as05gua01_302 (n_potential_duplicates=63)\n--&gt; Found potential duplicate: 181: pages2k_500&amp;4502: iso2k_1559 (n_potential_duplicates=64)\nProgress: 190/5147\n--&gt; Found potential duplicate: 193: pages2k_541&amp;4258: iso2k_404 (n_potential_duplicates=65)\n--&gt; Found potential duplicate: 194: pages2k_543&amp;330: pages2k_976 (n_potential_duplicates=66)\nProgress: 200/5147\n--&gt; Found potential duplicate: 200: pages2k_565&amp;4395: iso2k_998 (n_potential_duplicates=67)\n--&gt; Found potential duplicate: 209: pages2k_583&amp;3377: fe23_northamerica_usa_mt116 (n_potential_duplicates=68)\nProgress: 210/5147\n--&gt; Found potential duplicate: 211: pages2k_592&amp;4048: ch2k_li06rar02_270 (n_potential_duplicates=69)\n--&gt; Found potential duplicate: 211: pages2k_592&amp;4488: iso2k_1500 (n_potential_duplicates=70)\n--&gt; Found potential duplicate: 217: pages2k_610&amp;4428: iso2k_1199 (n_potential_duplicates=71)\nProgress: 220/5147\n--&gt; Found potential duplicate: 224: pages2k_626&amp;3847: fe23_northamerica_usa_wa071 (n_potential_duplicates=72)\nProgress: 230/5147\nProgress: 240/5147\n--&gt; Found potential duplicate: 243: pages2k_691&amp;1385: fe23_northamerica_canada_cana062 (n_potential_duplicates=73)\nProgress: 250/5147\n--&gt; Found potential duplicate: 253: pages2k_730&amp;4256: iso2k_396 (n_potential_duplicates=74)\n--&gt; Found potential duplicate: 255: pages2k_736&amp;3932: fe23_northamerica_usa_wy024 (n_potential_duplicates=75)\nProgress: 260/5147\nProgress: 270/5147\n--&gt; Found potential duplicate: 270: pages2k_800&amp;1542: fe23_northamerica_canada_cana234 (n_potential_duplicates=76)\n--&gt; Found potential duplicate: 274: pages2k_818&amp;4278: iso2k_488 (n_potential_duplicates=77)\n--&gt; Found potential duplicate: 279: pages2k_827&amp;281: pages2k_830 (n_potential_duplicates=78)\nProgress: 280/5147\n--&gt; Found potential duplicate: 282: pages2k_831&amp;709: pages2k_2220 (n_potential_duplicates=79)\n--&gt; Found potential duplicate: 282: pages2k_831&amp;2493: fe23_asia_russ127w (n_potential_duplicates=80)\nProgress: 290/5147\n--&gt; Found potential duplicate: 293: pages2k_857&amp;3736: fe23_northamerica_usa_ut511 (n_potential_duplicates=81)\n--&gt; Found potential duplicate: 299: pages2k_881&amp;4397: iso2k_1010 (n_potential_duplicates=82)\nProgress: 300/5147\n--&gt; Found potential duplicate: 302: pages2k_893&amp;303: pages2k_895 (n_potential_duplicates=83)\n--&gt; Found potential duplicate: 302: pages2k_893&amp;305: pages2k_900 (n_potential_duplicates=84)\n--&gt; Found potential duplicate: 303: pages2k_895&amp;305: pages2k_900 (n_potential_duplicates=85)\nProgress: 310/5147\n--&gt; Found potential duplicate: 316: pages2k_940&amp;4046: ch2k_dr99abr01_264 (n_potential_duplicates=86)\n--&gt; Found potential duplicate: 316: pages2k_940&amp;4047: ch2k_dr99abr01_266 (n_potential_duplicates=87)\n--&gt; Found potential duplicate: 316: pages2k_940&amp;4188: iso2k_91 (n_potential_duplicates=88)\n--&gt; Found potential duplicate: 319: pages2k_945&amp;4191: iso2k_100 (n_potential_duplicates=89)\nProgress: 320/5147\n--&gt; Found potential duplicate: 323: pages2k_960&amp;4321: iso2k_641 (n_potential_duplicates=90)\nProgress: 330/5147\n--&gt; Found potential duplicate: 332: pages2k_982&amp;3594: fe23_northamerica_usa_or042 (n_potential_duplicates=91)\n--&gt; Found potential duplicate: 337: pages2k_1004&amp;4322: iso2k_644 (n_potential_duplicates=92)\nProgress: 340/5147\n--&gt; Found potential duplicate: 343: pages2k_1026&amp;2862: fe23_northamerica_usa_az553 (n_potential_duplicates=93)\n--&gt; Found potential duplicate: 348: pages2k_1048&amp;4432: iso2k_1212 (n_potential_duplicates=94)\nProgress: 350/5147\n--&gt; Found potential duplicate: 357: pages2k_1089&amp;3374: fe23_northamerica_usa_mt112 (n_potential_duplicates=95)\n--&gt; Found potential duplicate: 357: pages2k_1089&amp;3375: fe23_northamerica_usa_mt113 (n_potential_duplicates=96)\nProgress: 360/5147\n--&gt; Found potential duplicate: 364: pages2k_1108&amp;4407: iso2k_1060 (n_potential_duplicates=97)\n--&gt; Found potential duplicate: 367: pages2k_1116&amp;1478: fe23_northamerica_canada_cana170w (n_potential_duplicates=98)\nProgress: 370/5147\n--&gt; Found potential duplicate: 375: pages2k_1147&amp;3974: ch2k_da06maf01_78 (n_potential_duplicates=99)\n--&gt; Found potential duplicate: 375: pages2k_1147&amp;3980: ch2k_da06maf02_104 (n_potential_duplicates=100)\n--&gt; Found potential duplicate: 375: pages2k_1147&amp;4546: iso2k_1748 (n_potential_duplicates=101)\n--&gt; Found potential duplicate: 378: pages2k_1153&amp;379: pages2k_1156 (n_potential_duplicates=102)\n--&gt; Found potential duplicate: 378: pages2k_1153&amp;381: pages2k_1160 (n_potential_duplicates=103)\n--&gt; Found potential duplicate: 379: pages2k_1156&amp;381: pages2k_1160 (n_potential_duplicates=104)\nProgress: 380/5147\nProgress: 390/5147\n--&gt; Found potential duplicate: 398: pages2k_1209&amp;3102: fe23_northamerica_usa_co553 (n_potential_duplicates=105)\nProgress: 400/5147\n--&gt; Found potential duplicate: 409: pages2k_1252&amp;1419: fe23_northamerica_canada_cana096 (n_potential_duplicates=106)\nProgress: 410/5147\n--&gt; Found potential duplicate: 414: pages2k_1274&amp;4509: iso2k_1577 (n_potential_duplicates=107)\nProgress: 420/5147\n--&gt; Found potential duplicate: 420: pages2k_1293&amp;4351: iso2k_821 (n_potential_duplicates=108)\n--&gt; Found potential duplicate: 428: pages2k_1325&amp;3938: fe23_northamerica_usa_wy030 (n_potential_duplicates=109)\nProgress: 430/5147\n--&gt; Found potential duplicate: 436: pages2k_1360&amp;3953: ch2k_ur00mai01_22 (n_potential_duplicates=110)\n--&gt; Found potential duplicate: 436: pages2k_1360&amp;4189: iso2k_94 (n_potential_duplicates=111)\n--&gt; Found potential duplicate: 436: pages2k_1360&amp;4190: iso2k_98 (n_potential_duplicates=112)\n--&gt; Found potential duplicate: 437: pages2k_1362&amp;438: pages2k_1365 (n_potential_duplicates=113)\n--&gt; Found potential duplicate: 439: pages2k_1370&amp;4516: iso2k_1619 (n_potential_duplicates=114)\nProgress: 440/5147\nProgress: 450/5147\n--&gt; Found potential duplicate: 451: pages2k_1420&amp;1435: fe23_northamerica_canada_cana111 (n_potential_duplicates=115)\n--&gt; Found potential duplicate: 456: pages2k_1442&amp;457: pages2k_1444 (n_potential_duplicates=116)\nProgress: 460/5147\n--&gt; Found potential duplicate: 469: pages2k_1488&amp;517: pages2k_1628 (n_potential_duplicates=117)\n--&gt; Found potential duplicate: 469: pages2k_1488&amp;3965: ch2k_nu11pal01_52 (n_potential_duplicates=118)\n--&gt; Found potential duplicate: 469: pages2k_1488&amp;4283: iso2k_505 (n_potential_duplicates=119)\n--&gt; Found potential duplicate: 469: pages2k_1488&amp;4309: iso2k_579 (n_potential_duplicates=120)\nProgress: 470/5147\n--&gt; Found potential duplicate: 470: pages2k_1490&amp;3966: ch2k_nu11pal01_54 (n_potential_duplicates=121)\n--&gt; Found potential duplicate: 471: pages2k_1491&amp;4308: iso2k_575 (n_potential_duplicates=122)\n--&gt; Found potential duplicate: 474: pages2k_1497&amp;4588: iso2k_1885 (n_potential_duplicates=123)\n--&gt; Found potential duplicate: 477: pages2k_1515&amp;479: pages2k_1519 (n_potential_duplicates=124)\nProgress: 480/5147\n--&gt; Found potential duplicate: 480: pages2k_1520&amp;481: pages2k_1522 (n_potential_duplicates=125)\nProgress: 490/5147\n--&gt; Found potential duplicate: 490: pages2k_1547&amp;4223: iso2k_259 (n_potential_duplicates=126)\n--&gt; Found potential duplicate: 499: pages2k_1566&amp;1539: fe23_northamerica_canada_cana231 (n_potential_duplicates=127)\nProgress: 500/5147\n--&gt; Found potential duplicate: 508: pages2k_1605&amp;2981: fe23_northamerica_usa_ca606 (n_potential_duplicates=128)\nProgress: 510/5147\n--&gt; Found potential duplicate: 512: pages2k_1619&amp;514: pages2k_1623 (n_potential_duplicates=129)\n--&gt; Found potential duplicate: 517: pages2k_1628&amp;3965: ch2k_nu11pal01_52 (n_potential_duplicates=130)\n--&gt; Found potential duplicate: 517: pages2k_1628&amp;4283: iso2k_505 (n_potential_duplicates=131)\n--&gt; Found potential duplicate: 517: pages2k_1628&amp;4309: iso2k_579 (n_potential_duplicates=132)\n--&gt; Found potential duplicate: 519: pages2k_1636&amp;3857: fe23_northamerica_usa_wa081 (n_potential_duplicates=133)\nProgress: 520/5147\nProgress: 530/5147\n--&gt; Found potential duplicate: 533: pages2k_1686&amp;534: pages2k_1688 (n_potential_duplicates=134)\n--&gt; Found potential duplicate: 536: pages2k_1692&amp;2248: fe23_asia_mong012 (n_potential_duplicates=135)\n--&gt; Found potential duplicate: 538: pages2k_1703&amp;4031: ch2k_mo06ped01_226 (n_potential_duplicates=136)\n--&gt; Found potential duplicate: 538: pages2k_1703&amp;4317: iso2k_629 (n_potential_duplicates=137)\nProgress: 540/5147\n--&gt; Found potential duplicate: 543: pages2k_1712&amp;4331: iso2k_715 (n_potential_duplicates=138)\n--&gt; Found potential duplicate: 547: pages2k_1720&amp;4510: iso2k_1579 (n_potential_duplicates=139)\nProgress: 550/5147\n--&gt; Found potential duplicate: 553: pages2k_1741&amp;3880: fe23_northamerica_usa_wa104 (n_potential_duplicates=140)\n--&gt; Found potential duplicate: 555: pages2k_1750&amp;4579: iso2k_1856 (n_potential_duplicates=141)\n--&gt; Found potential duplicate: 555: pages2k_1750&amp;4795: sisal_294.0_194 (n_potential_duplicates=142)\nProgress: 560/5147\n--&gt; Found potential duplicate: 561: pages2k_1771&amp;4017: ch2k_tu01lai01_192 (n_potential_duplicates=143)\nProgress: 570/5147\n--&gt; Found potential duplicate: 572: pages2k_1804&amp;3268: fe23_northamerica_usa_me010 (n_potential_duplicates=144)\nProgress: 580/5147\n--&gt; Found potential duplicate: 585: pages2k_1859&amp;4039: ch2k_he10gua01_244 (n_potential_duplicates=145)\n--&gt; Found potential duplicate: 585: pages2k_1859&amp;4542: iso2k_1735 (n_potential_duplicates=146)\n--&gt; Found potential duplicate: 586: pages2k_1861&amp;4040: ch2k_he10gua01_246 (n_potential_duplicates=147)\nProgress: 590/5147\n--&gt; Found potential duplicate: 591: pages2k_1880&amp;2650: fe23_northamerica_usa_ak060 (n_potential_duplicates=148)\n--&gt; Found potential duplicate: 594: pages2k_1891&amp;595: pages2k_1893 (n_potential_duplicates=149)\nProgress: 600/5147\n--&gt; Found potential duplicate: 604: pages2k_1918&amp;4192: iso2k_102 (n_potential_duplicates=150)\n--&gt; Found potential duplicate: 605: pages2k_1920&amp;606: pages2k_1923 (n_potential_duplicates=151)\n--&gt; Found potential duplicate: 609: pages2k_1932&amp;610: pages2k_1934 (n_potential_duplicates=152)\nProgress: 610/5147\n--&gt; Found potential duplicate: 614: pages2k_1942&amp;3955: ch2k_zi04ifr01_26 (n_potential_duplicates=153)\n--&gt; Found potential duplicate: 614: pages2k_1942&amp;4222: iso2k_257 (n_potential_duplicates=154)\nProgress: 620/5147\n--&gt; Found potential duplicate: 624: pages2k_1972&amp;625: pages2k_1973 (n_potential_duplicates=155)\n--&gt; Found potential duplicate: 626: pages2k_1976&amp;628: pages2k_1980 (n_potential_duplicates=156)\n--&gt; Found potential duplicate: 627: pages2k_1978&amp;629: pages2k_1983 (n_potential_duplicates=157)\nProgress: 630/5147\n--&gt; Found potential duplicate: 630: pages2k_1985&amp;4449: iso2k_1294 (n_potential_duplicates=158)\n--&gt; Found potential duplicate: 632: pages2k_1989&amp;633: pages2k_1991 (n_potential_duplicates=159)\n--&gt; Found potential duplicate: 634: pages2k_1994&amp;4043: ch2k_de12anc01_258 (n_potential_duplicates=160)\n--&gt; Found potential duplicate: 639: pages2k_2013&amp;1420: fe23_northamerica_canada_cana097 (n_potential_duplicates=161)\nProgress: 640/5147\n--&gt; Found potential duplicate: 647: pages2k_2042&amp;3954: ch2k_tu95mad01_24 (n_potential_duplicates=162)\n--&gt; Found potential duplicate: 647: pages2k_2042&amp;4169: iso2k_20 (n_potential_duplicates=163)\nProgress: 650/5147\n--&gt; Found potential duplicate: 655: pages2k_2059&amp;2648: fe23_northamerica_usa_ak058 (n_potential_duplicates=164)\nProgress: 660/5147\n--&gt; Found potential duplicate: 661: pages2k_2085&amp;1344: fe23_northamerica_canada_cana002 (n_potential_duplicates=165)\n--&gt; Found potential duplicate: 663: pages2k_2094&amp;4117: ch2k_tu01dep01_450 (n_potential_duplicates=166)\n--&gt; Found potential duplicate: 663: pages2k_2094&amp;4429: iso2k_1201 (n_potential_duplicates=167)\n--&gt; Found potential duplicate: 665: pages2k_2098&amp;667: pages2k_2103 (n_potential_duplicates=168)\nProgress: 670/5147\n--&gt; Found potential duplicate: 670: pages2k_2110&amp;3103: fe23_northamerica_usa_co554 (n_potential_duplicates=169)\nProgress: 680/5147\n--&gt; Found potential duplicate: 682: pages2k_2146&amp;684: pages2k_2149 (n_potential_duplicates=170)\n--&gt; Found potential duplicate: 682: pages2k_2146&amp;685: pages2k_2150 (n_potential_duplicates=171)\n--&gt; Found potential duplicate: 684: pages2k_2149&amp;685: pages2k_2150 (n_potential_duplicates=172)\n--&gt; Found potential duplicate: 688: pages2k_2156&amp;1477: fe23_northamerica_canada_cana169w (n_potential_duplicates=173)\nProgress: 690/5147\nProgress: 700/5147\n--&gt; Found potential duplicate: 704: pages2k_2214&amp;4519: iso2k_1631 (n_potential_duplicates=174)\n--&gt; Found potential duplicate: 709: pages2k_2220&amp;2493: fe23_asia_russ127w (n_potential_duplicates=175)\nProgress: 710/5147\n--&gt; Found potential duplicate: 712: pages2k_2226&amp;2243: fe23_asia_mong007w (n_potential_duplicates=176)\nProgress: 720/5147\n--&gt; Found potential duplicate: 728: pages2k_2265&amp;2660: fe23_northamerica_usa_ak070 (n_potential_duplicates=177)\nProgress: 730/5147\n--&gt; Found potential duplicate: 736: pages2k_2287&amp;737: pages2k_2290 (n_potential_duplicates=178)\nProgress: 740/5147\n--&gt; Found potential duplicate: 742: pages2k_2300&amp;4010: ch2k_os14rip01_174 (n_potential_duplicates=179)\n--&gt; Found potential duplicate: 744: pages2k_2303&amp;2242: fe23_asia_mong006 (n_potential_duplicates=180)\n--&gt; Found potential duplicate: 747: pages2k_2309&amp;4024: ch2k_we09arr01_208 (n_potential_duplicates=181)\n--&gt; Found potential duplicate: 748: pages2k_2311&amp;4025: ch2k_we09arr01_210 (n_potential_duplicates=182)\nProgress: 750/5147\n--&gt; Found potential duplicate: 752: pages2k_2319&amp;2704: fe23_northamerica_usa_ak6 (n_potential_duplicates=183)\n--&gt; Found potential duplicate: 755: pages2k_2339&amp;757: pages2k_2344 (n_potential_duplicates=184)\nProgress: 760/5147\n--&gt; Found potential duplicate: 763: pages2k_2361&amp;3873: fe23_northamerica_usa_wa097 (n_potential_duplicates=185)\nProgress: 770/5147\n--&gt; Found potential duplicate: 774: pages2k_2402&amp;3133: fe23_northamerica_usa_co586 (n_potential_duplicates=186)\nProgress: 780/5147\n--&gt; Found potential duplicate: 781: pages2k_2430&amp;1437: fe23_northamerica_canada_cana113 (n_potential_duplicates=187)\nProgress: 790/5147\n--&gt; Found potential duplicate: 792: pages2k_2473&amp;3930: fe23_northamerica_usa_wy022 (n_potential_duplicates=188)\n--&gt; Found potential duplicate: 799: pages2k_2500&amp;800: pages2k_2502 (n_potential_duplicates=189)\nProgress: 800/5147\n--&gt; Found potential duplicate: 804: pages2k_2510&amp;4517: iso2k_1626 (n_potential_duplicates=190)\n--&gt; Found potential duplicate: 806: pages2k_2514&amp;4479: iso2k_1467 (n_potential_duplicates=191)\n--&gt; Found potential duplicate: 808: pages2k_2517&amp;4420: iso2k_1130 (n_potential_duplicates=192)\nProgress: 810/5147\n--&gt; Found potential duplicate: 813: pages2k_2534&amp;4508: iso2k_1575 (n_potential_duplicates=193)\n--&gt; Found potential duplicate: 815: pages2k_2538&amp;4581: iso2k_1862 (n_potential_duplicates=194)\nProgress: 820/5147\n--&gt; Found potential duplicate: 822: pages2k_2561&amp;1417: fe23_northamerica_canada_cana094 (n_potential_duplicates=195)\n--&gt; Found potential duplicate: 828: pages2k_2592&amp;830: pages2k_2596 (n_potential_duplicates=196)\n--&gt; Found potential duplicate: 829: pages2k_2595&amp;831: pages2k_2599 (n_potential_duplicates=197)\nProgress: 830/5147\n--&gt; Found potential duplicate: 834: pages2k_2604&amp;835: pages2k_2606 (n_potential_duplicates=198)\n--&gt; Found potential duplicate: 834: pages2k_2604&amp;4484: iso2k_1481 (n_potential_duplicates=199)\n--&gt; Found potential duplicate: 835: pages2k_2606&amp;4484: iso2k_1481 (n_potential_duplicates=200)\n--&gt; Found potential duplicate: 836: pages2k_2607&amp;837: pages2k_2609 (n_potential_duplicates=201)\n--&gt; Found potential duplicate: 836: pages2k_2607&amp;839: pages2k_2612 (n_potential_duplicates=202)\n--&gt; Found potential duplicate: 837: pages2k_2609&amp;839: pages2k_2612 (n_potential_duplicates=203)\nProgress: 840/5147\n--&gt; Found potential duplicate: 840: pages2k_2613&amp;4480: iso2k_1470 (n_potential_duplicates=204)\n--&gt; Found potential duplicate: 842: pages2k_2617&amp;4507: iso2k_1573 (n_potential_duplicates=205)\nProgress: 850/5147\n--&gt; Found potential duplicate: 850: pages2k_2634&amp;3229: fe23_northamerica_usa_id013 (n_potential_duplicates=206)\n--&gt; Found potential duplicate: 856: pages2k_2660&amp;2604: fe23_northamerica_usa_ak014 (n_potential_duplicates=207)\nProgress: 860/5147\n--&gt; Found potential duplicate: 861: pages2k_2677&amp;3931: fe23_northamerica_usa_wy023 (n_potential_duplicates=208)\n--&gt; Found potential duplicate: 867: pages2k_2703&amp;2683: fe23_northamerica_usa_ak094 (n_potential_duplicates=209)\nProgress: 870/5147\n--&gt; Found potential duplicate: 873: pages2k_2722&amp;1546: fe23_northamerica_canada_cana238 (n_potential_duplicates=210)\nProgress: 880/5147\n--&gt; Found potential duplicate: 881: pages2k_2750&amp;4534: iso2k_1708 (n_potential_duplicates=211)\n--&gt; Found potential duplicate: 882: pages2k_2752&amp;883: pages2k_2755 (n_potential_duplicates=212)\n--&gt; Found potential duplicate: 882: pages2k_2752&amp;885: pages2k_2759 (n_potential_duplicates=213)\n--&gt; Found potential duplicate: 883: pages2k_2755&amp;885: pages2k_2759 (n_potential_duplicates=214)\nProgress: 890/5147\nProgress: 900/5147\n--&gt; Found potential duplicate: 900: pages2k_2793&amp;901: pages2k_2795 (n_potential_duplicates=215)\n--&gt; Found potential duplicate: 901: pages2k_2795&amp;903: pages2k_2798 (n_potential_duplicates=216)\n--&gt; Found potential duplicate: 902: pages2k_2796&amp;903: pages2k_2798 (n_potential_duplicates=217)\nProgress: 910/5147\n--&gt; Found potential duplicate: 913: pages2k_2830&amp;2213: fe23_northamerica_mexico_mexi020 (n_potential_duplicates=218)\n--&gt; Found potential duplicate: 916: pages2k_2843&amp;3859: fe23_northamerica_usa_wa083 (n_potential_duplicates=219)\nProgress: 920/5147\nProgress: 930/5147\n--&gt; Found potential duplicate: 931: pages2k_2899&amp;932: pages2k_2901 (n_potential_duplicates=220)\n--&gt; Found potential duplicate: 933: pages2k_2904&amp;934: pages2k_2906 (n_potential_duplicates=221)\nProgress: 940/5147\n--&gt; Found potential duplicate: 940: pages2k_2922&amp;2978: fe23_northamerica_usa_ca603 (n_potential_duplicates=222)\n--&gt; Found potential duplicate: 949: pages2k_2953&amp;4307: iso2k_573 (n_potential_duplicates=223)\nProgress: 950/5147\n--&gt; Found potential duplicate: 951: pages2k_2959&amp;2233: fe23_northamerica_mexico_mexi043 (n_potential_duplicates=224)\n--&gt; Found potential duplicate: 956: pages2k_2976&amp;3224: fe23_northamerica_usa_id008 (n_potential_duplicates=225)\nProgress: 960/5147\n--&gt; Found potential duplicate: 962: pages2k_3002&amp;3595: fe23_northamerica_usa_or043 (n_potential_duplicates=226)\n--&gt; Found potential duplicate: 969: pages2k_3028&amp;970: pages2k_3030 (n_potential_duplicates=227)\n--&gt; Found potential duplicate: 969: pages2k_3028&amp;972: pages2k_3033 (n_potential_duplicates=228)\nProgress: 970/5147\n--&gt; Found potential duplicate: 970: pages2k_3030&amp;972: pages2k_3033 (n_potential_duplicates=229)\n--&gt; Found potential duplicate: 974: pages2k_3038&amp;3370: fe23_northamerica_usa_mt108 (n_potential_duplicates=230)\nProgress: 980/5147\n--&gt; Found potential duplicate: 981: pages2k_3064&amp;4327: iso2k_698 (n_potential_duplicates=231)\n--&gt; Found potential duplicate: 982: pages2k_3068&amp;4143: ch2k_zi14ifr02_522 (n_potential_duplicates=232)\n--&gt; Found potential duplicate: 982: pages2k_3068&amp;4144: ch2k_zi14ifr02_524 (n_potential_duplicates=233)\n--&gt; Found potential duplicate: 987: pages2k_3085&amp;4000: ch2k_ku00nin01_150 (n_potential_duplicates=234)\n--&gt; Found potential duplicate: 987: pages2k_3085&amp;4499: iso2k_1554 (n_potential_duplicates=235)\n--&gt; Found potential duplicate: 987: pages2k_3085&amp;4500: iso2k_1556 (n_potential_duplicates=236)\nProgress: 990/5147\n--&gt; Found potential duplicate: 994: pages2k_3107&amp;3101: fe23_northamerica_usa_co552 (n_potential_duplicates=237)\n--&gt; Found potential duplicate: 995: pages2k_3108&amp;3101: fe23_northamerica_usa_co552 (n_potential_duplicates=238)\nProgress: 1000/5147\n--&gt; Found potential duplicate: 1001: pages2k_3132&amp;3997: ch2k_qu06rab01_144 (n_potential_duplicates=239)\n--&gt; Found potential duplicate: 1001: pages2k_3132&amp;4453: iso2k_1311 (n_potential_duplicates=240)\n--&gt; Found potential duplicate: 1002: pages2k_3134&amp;3998: ch2k_qu06rab01_146 (n_potential_duplicates=241)\nProgress: 1010/5147\n--&gt; Found potential duplicate: 1014: pages2k_3170&amp;2351: fe23_australia_newz062 (n_potential_duplicates=242)\n--&gt; Found potential duplicate: 1017: pages2k_3179&amp;2647: fe23_northamerica_usa_ak057 (n_potential_duplicates=243)\n--&gt; Found potential duplicate: 1019: pages2k_3188&amp;1020: pages2k_3191 (n_potential_duplicates=244)\nProgress: 1020/5147\n--&gt; Found potential duplicate: 1021: pages2k_3196&amp;2247: fe23_asia_mong011 (n_potential_duplicates=245)\n--&gt; Found potential duplicate: 1024: pages2k_3202&amp;4539: iso2k_1727 (n_potential_duplicates=246)\nProgress: 1030/5147\n--&gt; Found potential duplicate: 1033: pages2k_3234&amp;1034: pages2k_3236 (n_potential_duplicates=247)\n--&gt; Found potential duplicate: 1033: pages2k_3234&amp;1036: pages2k_3239 (n_potential_duplicates=248)\n--&gt; Found potential duplicate: 1034: pages2k_3236&amp;1036: pages2k_3239 (n_potential_duplicates=249)\n--&gt; Found potential duplicate: 1039: pages2k_3243&amp;4166: iso2k_0 (n_potential_duplicates=250)\nProgress: 1040/5147\n--&gt; Found potential duplicate: 1046: pages2k_3263&amp;4440: iso2k_1264 (n_potential_duplicates=251)\n--&gt; Found potential duplicate: 1048: pages2k_3266&amp;4096: ch2k_go12sbv01_396 (n_potential_duplicates=252)\n--&gt; Found potential duplicate: 1048: pages2k_3266&amp;4365: iso2k_870 (n_potential_duplicates=253)\nProgress: 1050/5147\nProgress: 1060/5147\n--&gt; Found potential duplicate: 1063: pages2k_3307&amp;4244: iso2k_339 (n_potential_duplicates=254)\n--&gt; Found potential duplicate: 1065: pages2k_3313&amp;2936: fe23_northamerica_usa_ca560 (n_potential_duplicates=255)\nProgress: 1070/5147\n--&gt; Found potential duplicate: 1071: pages2k_3337&amp;1073: pages2k_3342 (n_potential_duplicates=256)\n--&gt; Found potential duplicate: 1077: pages2k_3352&amp;4128: ch2k_zi14tur01_480 (n_potential_duplicates=257)\n--&gt; Found potential duplicate: 1077: pages2k_3352&amp;4129: ch2k_zi14tur01_482 (n_potential_duplicates=258)\n--&gt; Found potential duplicate: 1077: pages2k_3352&amp;4239: iso2k_302 (n_potential_duplicates=259)\nProgress: 1080/5147\n--&gt; Found potential duplicate: 1087: pages2k_3372&amp;4087: ch2k_ki04mcv01_366 (n_potential_duplicates=260)\n--&gt; Found potential duplicate: 1087: pages2k_3372&amp;4203: iso2k_155 (n_potential_duplicates=261)\n--&gt; Found potential duplicate: 1088: pages2k_3374&amp;4088: ch2k_ki04mcv01_368 (n_potential_duplicates=262)\nProgress: 1090/5147\n--&gt; Found potential duplicate: 1099: pages2k_3404&amp;1355: fe23_northamerica_canada_cana029 (n_potential_duplicates=263)\nProgress: 1100/5147\n--&gt; Found potential duplicate: 1103: pages2k_3417&amp;1104: pages2k_3419 (n_potential_duplicates=264)\nProgress: 1110/5147\nProgress: 1120/5147\nProgress: 1130/5147\n--&gt; Found potential duplicate: 1131: pages2k_3503&amp;3848: fe23_northamerica_usa_wa072 (n_potential_duplicates=265)\n--&gt; Found potential duplicate: 1138: pages2k_3524&amp;2600: fe23_northamerica_usa_ak010 (n_potential_duplicates=266)\nProgress: 1140/5147\n--&gt; Found potential duplicate: 1145: pages2k_3550&amp;2503: fe23_asia_russ137w (n_potential_duplicates=267)\n--&gt; Found potential duplicate: 1146: pages2k_3552&amp;4511: iso2k_1581 (n_potential_duplicates=268)\n--&gt; Found potential duplicate: 1147: pages2k_3554&amp;4112: ch2k_li94sec01_436 (n_potential_duplicates=269)\n--&gt; Found potential duplicate: 1147: pages2k_3554&amp;4419: iso2k_1124 (n_potential_duplicates=270)\nProgress: 1150/5147\n--&gt; Found potential duplicate: 1152: pages2k_3571&amp;4204: iso2k_174 (n_potential_duplicates=271)\n--&gt; Found potential duplicate: 1156: pages2k_3583&amp;3180: fe23_northamerica_usa_co633 (n_potential_duplicates=272)\nProgress: 1160/5147\n--&gt; Found potential duplicate: 1161: pages2k_3599&amp;4409: iso2k_1069 (n_potential_duplicates=273)\n--&gt; Found potential duplicate: 1161: pages2k_3599&amp;4528: iso2k_1660 (n_potential_duplicates=274)\n--&gt; Found potential duplicate: 1166: pages2k_3609&amp;1376: fe23_northamerica_canada_cana053 (n_potential_duplicates=275)\nProgress: 1170/5147\n--&gt; Found potential duplicate: 1171: pages2k_3631&amp;4495: iso2k_1530 (n_potential_duplicates=276)\n--&gt; Found potential duplicate: 1175: pages2k_3642&amp;3933: fe23_northamerica_usa_wy025 (n_potential_duplicates=277)\nProgress: 1180/5147\nProgress: 1190/5147\nProgress: 1200/5147\nProgress: 1210/5147\n--&gt; Found potential duplicate: 1218: fe23_southamerica_arge016&amp;1287: fe23_southamerica_arge085 (n_potential_duplicates=278)\nProgress: 1220/5147\nProgress: 1230/5147\nProgress: 1240/5147\nProgress: 1250/5147\nProgress: 1260/5147\nProgress: 1270/5147\nProgress: 1280/5147\nProgress: 1290/5147\nProgress: 1300/5147\nProgress: 1310/5147\nProgress: 1320/5147\nProgress: 1330/5147\nProgress: 1340/5147\nProgress: 1350/5147\nProgress: 1360/5147\nProgress: 1370/5147\nProgress: 1380/5147\nProgress: 1390/5147\nProgress: 1400/5147\nProgress: 1410/5147\nProgress: 1420/5147\n--&gt; Found potential duplicate: 1425: fe23_northamerica_canada_cana100&amp;1521: fe23_northamerica_canada_cana213 (n_potential_duplicates=279)\nProgress: 1430/5147\n--&gt; Found potential duplicate: 1430: fe23_northamerica_canada_cana105&amp;1525: fe23_northamerica_canada_cana217 (n_potential_duplicates=280)\n--&gt; Found potential duplicate: 1439: fe23_northamerica_canada_cana116&amp;1476: fe23_northamerica_canada_cana168w (n_potential_duplicates=281)\nProgress: 1440/5147\nProgress: 1450/5147\nProgress: 1460/5147\nProgress: 1470/5147\n--&gt; Found potential duplicate: 1474: fe23_northamerica_canada_cana161&amp;1475: fe23_northamerica_canada_cana162 (n_potential_duplicates=282)\nProgress: 1480/5147\nProgress: 1490/5147\nProgress: 1500/5147\nProgress: 1510/5147\nProgress: 1520/5147\nProgress: 1530/5147\nProgress: 1540/5147\nProgress: 1550/5147\nProgress: 1560/5147\nProgress: 1570/5147\nProgress: 1580/5147\nProgress: 1590/5147\nProgress: 1600/5147\nProgress: 1610/5147\nProgress: 1620/5147\n--&gt; Found potential duplicate: 1622: fe23_southamerica_chil016&amp;1623: fe23_southamerica_chil017 (n_potential_duplicates=283)\nProgress: 1630/5147\nProgress: 1640/5147\nProgress: 1650/5147\nProgress: 1660/5147\nProgress: 1670/5147\nProgress: 1680/5147\nProgress: 1690/5147\nProgress: 1700/5147\nProgress: 1710/5147\nProgress: 1720/5147\nProgress: 1730/5147\nProgress: 1740/5147\nProgress: 1750/5147\nProgress: 1760/5147\nProgress: 1770/5147\nProgress: 1780/5147\nProgress: 1790/5147\nProgress: 1800/5147\nProgress: 1810/5147\nProgress: 1820/5147\nProgress: 1830/5147\nProgress: 1840/5147\nProgress: 1850/5147\nProgress: 1860/5147\nProgress: 1870/5147\nProgress: 1880/5147\nProgress: 1890/5147\nProgress: 1900/5147\nProgress: 1910/5147\nProgress: 1920/5147\nProgress: 1930/5147\nProgress: 1940/5147\nProgress: 1950/5147\nProgress: 1960/5147\nProgress: 1970/5147\nProgress: 1980/5147\nProgress: 1990/5147\nProgress: 2000/5147\nProgress: 2010/5147\nProgress: 2020/5147\nProgress: 2030/5147\n--&gt; Found potential duplicate: 2035: fe23_europe_swed019w&amp;2037: fe23_europe_swed021w (n_potential_duplicates=284)\nProgress: 2040/5147\nProgress: 2050/5147\nProgress: 2060/5147\nProgress: 2070/5147\nProgress: 2080/5147\nProgress: 2090/5147\nProgress: 2100/5147\nProgress: 2110/5147\nProgress: 2120/5147\nProgress: 2130/5147\nProgress: 2140/5147\nProgress: 2150/5147\nProgress: 2160/5147\nProgress: 2170/5147\nProgress: 2180/5147\nProgress: 2190/5147\nProgress: 2200/5147\nProgress: 2210/5147\n--&gt; Found potential duplicate: 2215: fe23_northamerica_mexico_mexi022&amp;2216: fe23_northamerica_mexico_mexi023 (n_potential_duplicates=285)\nProgress: 2220/5147\nProgress: 2230/5147\nProgress: 2240/5147\nProgress: 2250/5147\nProgress: 2260/5147\nProgress: 2270/5147\nProgress: 2280/5147\nProgress: 2290/5147\n--&gt; Found potential duplicate: 2296: fe23_australia_newz003&amp;2349: fe23_australia_newz060 (n_potential_duplicates=286)\nProgress: 2300/5147\n--&gt; Found potential duplicate: 2300: fe23_australia_newz008&amp;2381: fe23_australia_newz092 (n_potential_duplicates=287)\n--&gt; Found potential duplicate: 2304: fe23_australia_newz014&amp;2350: fe23_australia_newz061 (n_potential_duplicates=288)\n--&gt; Found potential duplicate: 2308: fe23_australia_newz018&amp;2351: fe23_australia_newz062 (n_potential_duplicates=289)\n--&gt; Found potential duplicate: 2309: fe23_australia_newz019&amp;2352: fe23_australia_newz063 (n_potential_duplicates=290)\nProgress: 2310/5147\nProgress: 2320/5147\nProgress: 2330/5147\nProgress: 2340/5147\nProgress: 2350/5147\nProgress: 2360/5147\nProgress: 2370/5147\nProgress: 2380/5147\nProgress: 2390/5147\nProgress: 2400/5147\nProgress: 2410/5147\nProgress: 2420/5147\nProgress: 2430/5147\nProgress: 2440/5147\nProgress: 2450/5147\nProgress: 2460/5147\nProgress: 2470/5147\nProgress: 2480/5147\nProgress: 2490/5147\nProgress: 2500/5147\nProgress: 2510/5147\nProgress: 2520/5147\nProgress: 2530/5147\nProgress: 2540/5147\nProgress: 2550/5147\nProgress: 2560/5147\nProgress: 2570/5147\nProgress: 2580/5147\nProgress: 2590/5147\nProgress: 2600/5147\nProgress: 2610/5147\nProgress: 2620/5147\nProgress: 2630/5147\nProgress: 2640/5147\nProgress: 2650/5147\nProgress: 2660/5147\nProgress: 2670/5147\nProgress: 2680/5147\nProgress: 2690/5147\nProgress: 2700/5147\nProgress: 2710/5147\nProgress: 2720/5147\nProgress: 2730/5147\nProgress: 2740/5147\nProgress: 2750/5147\nProgress: 2760/5147\nProgress: 2770/5147\nProgress: 2780/5147\nProgress: 2790/5147\nProgress: 2800/5147\nProgress: 2810/5147\nProgress: 2820/5147\nProgress: 2830/5147\nProgress: 2840/5147\nProgress: 2850/5147\nProgress: 2860/5147\nProgress: 2870/5147\n--&gt; Found potential duplicate: 2875: fe23_northamerica_usa_ca066&amp;3003: fe23_northamerica_usa_ca628 (n_potential_duplicates=291)\n--&gt; Found potential duplicate: 2876: fe23_northamerica_usa_ca067&amp;3003: fe23_northamerica_usa_ca628 (n_potential_duplicates=292)\nProgress: 2880/5147\nProgress: 2890/5147\n--&gt; Found potential duplicate: 2894: fe23_northamerica_usa_ca512&amp;2988: fe23_northamerica_usa_ca613 (n_potential_duplicates=293)\nProgress: 2900/5147\nProgress: 2910/5147\n--&gt; Found potential duplicate: 2911: fe23_northamerica_usa_ca535&amp;3043: fe23_northamerica_usa_ca670 (n_potential_duplicates=294)\nProgress: 2920/5147\nProgress: 2930/5147\nProgress: 2940/5147\nProgress: 2950/5147\nProgress: 2960/5147\nProgress: 2970/5147\nProgress: 2980/5147\nProgress: 2990/5147\nProgress: 3000/5147\nProgress: 3010/5147\nProgress: 3020/5147\nProgress: 3030/5147\nProgress: 3040/5147\nProgress: 3050/5147\nProgress: 3060/5147\nProgress: 3070/5147\nProgress: 3080/5147\nProgress: 3090/5147\nProgress: 3100/5147\nProgress: 3110/5147\nProgress: 3120/5147\nProgress: 3130/5147\nProgress: 3140/5147\nProgress: 3150/5147\nProgress: 3160/5147\nProgress: 3170/5147\nProgress: 3180/5147\nProgress: 3190/5147\nProgress: 3200/5147\nProgress: 3210/5147\nProgress: 3220/5147\nProgress: 3230/5147\nProgress: 3240/5147\nProgress: 3250/5147\nProgress: 3260/5147\nProgress: 3270/5147\n--&gt; Found potential duplicate: 3271: fe23_northamerica_usa_me017&amp;3272: fe23_northamerica_usa_me018 (n_potential_duplicates=295)\nProgress: 3280/5147\nProgress: 3290/5147\nProgress: 3300/5147\nProgress: 3310/5147\nProgress: 3320/5147\n--&gt; Found potential duplicate: 3326: fe23_northamerica_usa_mo&amp;3335: fe23_northamerica_usa_mo009 (n_potential_duplicates=296)\nProgress: 3330/5147\nProgress: 3340/5147\nProgress: 3350/5147\nProgress: 3360/5147\nProgress: 3370/5147\n--&gt; Found potential duplicate: 3374: fe23_northamerica_usa_mt112&amp;3375: fe23_northamerica_usa_mt113 (n_potential_duplicates=297)\nProgress: 3380/5147\nProgress: 3390/5147\nProgress: 3400/5147\nProgress: 3410/5147\n--&gt; Found potential duplicate: 3415: fe23_northamerica_usa_nj001&amp;3416: fe23_northamerica_usa_nj002 (n_potential_duplicates=298)\nProgress: 3420/5147\n--&gt; Found potential duplicate: 3429: fe23_northamerica_usa_nm024&amp;3455: fe23_northamerica_usa_nm055 (n_potential_duplicates=299)\nProgress: 3430/5147\nProgress: 3440/5147\nProgress: 3450/5147\nProgress: 3460/5147\nProgress: 3470/5147\nProgress: 3480/5147\nProgress: 3490/5147\nProgress: 3500/5147\nProgress: 3510/5147\n--&gt; Found potential duplicate: 3514: fe23_northamerica_usa_nv060&amp;3532: fe23_northamerica_usa_nv518 (n_potential_duplicates=300)\nProgress: 3520/5147\n--&gt; Found potential duplicate: 3526: fe23_northamerica_usa_nv512&amp;3535: fe23_northamerica_usa_nv521 (n_potential_duplicates=301)\n--&gt; Found potential duplicate: 3527: fe23_northamerica_usa_nv513&amp;3534: fe23_northamerica_usa_nv520 (n_potential_duplicates=302)\nProgress: 3530/5147\nProgress: 3540/5147\nProgress: 3550/5147\nProgress: 3560/5147\nProgress: 3570/5147\nProgress: 3580/5147\nProgress: 3590/5147\nProgress: 3600/5147\nProgress: 3610/5147\nProgress: 3620/5147\nProgress: 3630/5147\nProgress: 3640/5147\nProgress: 3650/5147\nProgress: 3660/5147\nProgress: 3670/5147\nProgress: 3680/5147\nProgress: 3690/5147\nProgress: 3700/5147\nProgress: 3710/5147\nProgress: 3720/5147\nProgress: 3730/5147\nProgress: 3740/5147\nProgress: 3750/5147\nProgress: 3760/5147\nProgress: 3770/5147\nProgress: 3780/5147\nProgress: 3790/5147\nProgress: 3800/5147\nProgress: 3810/5147\nProgress: 3820/5147\nProgress: 3830/5147\nProgress: 3840/5147\nProgress: 3850/5147\nProgress: 3860/5147\nProgress: 3870/5147\nProgress: 3880/5147\nProgress: 3890/5147\nProgress: 3900/5147\nProgress: 3910/5147\nProgress: 3920/5147\nProgress: 3930/5147\nProgress: 3940/5147\n--&gt; Found potential duplicate: 3946: ch2k_zi15mer01_2&amp;3947: ch2k_zi15mer01_4 (n_potential_duplicates=303)\n--&gt; Found potential duplicate: 3948: ch2k_co03pal03_6&amp;4286: iso2k_511 (n_potential_duplicates=304)\n--&gt; Found potential duplicate: 3949: ch2k_co03pal02_8&amp;4285: iso2k_509 (n_potential_duplicates=305)\nProgress: 3950/5147\n--&gt; Found potential duplicate: 3950: ch2k_li06rar01_12&amp;4489: iso2k_1502 (n_potential_duplicates=306)\n--&gt; Found potential duplicate: 3951: ch2k_co03pal07_14&amp;4291: iso2k_521 (n_potential_duplicates=307)\n--&gt; Found potential duplicate: 3953: ch2k_ur00mai01_22&amp;4189: iso2k_94 (n_potential_duplicates=308)\n--&gt; Found potential duplicate: 3953: ch2k_ur00mai01_22&amp;4190: iso2k_98 (n_potential_duplicates=309)\n--&gt; Found potential duplicate: 3954: ch2k_tu95mad01_24&amp;4169: iso2k_20 (n_potential_duplicates=310)\n--&gt; Found potential duplicate: 3955: ch2k_zi04ifr01_26&amp;4222: iso2k_257 (n_potential_duplicates=311)\n--&gt; Found potential duplicate: 3956: ch2k_re18cay01_30&amp;4382: iso2k_917 (n_potential_duplicates=312)\nProgress: 3960/5147\n--&gt; Found potential duplicate: 3960: ch2k_ku99hou01_40&amp;4345: iso2k_786 (n_potential_duplicates=313)\n--&gt; Found potential duplicate: 3960: ch2k_ku99hou01_40&amp;4346: iso2k_788 (n_potential_duplicates=314)\n--&gt; Found potential duplicate: 3965: ch2k_nu11pal01_52&amp;4283: iso2k_505 (n_potential_duplicates=315)\n--&gt; Found potential duplicate: 3965: ch2k_nu11pal01_52&amp;4309: iso2k_579 (n_potential_duplicates=316)\n--&gt; Found potential duplicate: 3968: ch2k_ca14tim01_64&amp;4272: iso2k_473 (n_potential_duplicates=317)\nProgress: 3970/5147\n--&gt; Found potential duplicate: 3973: ch2k_he08lra01_76&amp;4563: iso2k_1813 (n_potential_duplicates=318)\n--&gt; Found potential duplicate: 3974: ch2k_da06maf01_78&amp;4546: iso2k_1748 (n_potential_duplicates=319)\n--&gt; Found potential duplicate: 3975: ch2k_na09mal01_84&amp;4549: iso2k_1754 (n_potential_duplicates=320)\n--&gt; Found potential duplicate: 3976: ch2k_sw98stp01_86&amp;4176: iso2k_50 (n_potential_duplicates=321)\nProgress: 3980/5147\n--&gt; Found potential duplicate: 3980: ch2k_da06maf02_104&amp;4546: iso2k_1748 (n_potential_duplicates=322)\n--&gt; Found potential duplicate: 3983: ch2k_co03pal01_110&amp;4284: iso2k_507 (n_potential_duplicates=323)\n--&gt; Found potential duplicate: 3986: ch2k_ch98pir01_116&amp;4438: iso2k_1229 (n_potential_duplicates=324)\nProgress: 3990/5147\n--&gt; Found potential duplicate: 3991: ch2k_xi17hai01_128&amp;3994: ch2k_xi17hai01_136 (n_potential_duplicates=325)\n--&gt; Found potential duplicate: 3991: ch2k_xi17hai01_128&amp;4551: iso2k_1762 (n_potential_duplicates=326)\n--&gt; Found potential duplicate: 3992: ch2k_xi17hai01_130&amp;3993: ch2k_xi17hai01_134 (n_potential_duplicates=327)\n--&gt; Found potential duplicate: 3994: ch2k_xi17hai01_136&amp;4551: iso2k_1762 (n_potential_duplicates=328)\n--&gt; Found potential duplicate: 3995: ch2k_de14dto03_140&amp;3999: ch2k_de14dto01_148 (n_potential_duplicates=329)\n--&gt; Found potential duplicate: 3997: ch2k_qu06rab01_144&amp;4453: iso2k_1311 (n_potential_duplicates=330)\nProgress: 4000/5147\n--&gt; Found potential duplicate: 4000: ch2k_ku00nin01_150&amp;4499: iso2k_1554 (n_potential_duplicates=331)\n--&gt; Found potential duplicate: 4000: ch2k_ku00nin01_150&amp;4500: iso2k_1556 (n_potential_duplicates=332)\nProgress: 4010/5147\n--&gt; Found potential duplicate: 4014: ch2k_ev18roc01_184&amp;4015: ch2k_ev18roc01_186 (n_potential_duplicates=333)\n--&gt; Found potential duplicate: 4016: ch2k_ca13sap01_188&amp;4305: iso2k_569 (n_potential_duplicates=334)\n--&gt; Found potential duplicate: 4018: ch2k_he13mis01_194&amp;4210: iso2k_211 (n_potential_duplicates=335)\n--&gt; Found potential duplicate: 4018: ch2k_he13mis01_194&amp;4211: iso2k_213 (n_potential_duplicates=336)\nProgress: 4020/5147\n--&gt; Found potential duplicate: 4020: ch2k_zi15imp02_200&amp;4021: ch2k_zi15imp02_202 (n_potential_duplicates=337)\n--&gt; Found potential duplicate: 4022: ch2k_pf04pba01_204&amp;4532: iso2k_1701 (n_potential_duplicates=338)\n--&gt; Found potential duplicate: 4022: ch2k_pf04pba01_204&amp;4533: iso2k_1704 (n_potential_duplicates=339)\n--&gt; Found potential duplicate: 4026: ch2k_co03pal05_212&amp;4288: iso2k_515 (n_potential_duplicates=340)\nProgress: 4030/5147\n--&gt; Found potential duplicate: 4031: ch2k_mo06ped01_226&amp;4317: iso2k_629 (n_potential_duplicates=341)\n--&gt; Found potential duplicate: 4035: ch2k_os14ucp01_236&amp;4249: iso2k_350 (n_potential_duplicates=342)\n--&gt; Found potential duplicate: 4039: ch2k_he10gua01_244&amp;4542: iso2k_1735 (n_potential_duplicates=343)\nProgress: 4040/5147\n--&gt; Found potential duplicate: 4046: ch2k_dr99abr01_264&amp;4047: ch2k_dr99abr01_266 (n_potential_duplicates=344)\n--&gt; Found potential duplicate: 4046: ch2k_dr99abr01_264&amp;4188: iso2k_91 (n_potential_duplicates=345)\n--&gt; Found potential duplicate: 4047: ch2k_dr99abr01_266&amp;4188: iso2k_91 (n_potential_duplicates=346)\n--&gt; Found potential duplicate: 4048: ch2k_li06rar02_270&amp;4488: iso2k_1500 (n_potential_duplicates=347)\nProgress: 4050/5147\n--&gt; Found potential duplicate: 4052: ch2k_zi15tan01_278&amp;4053: ch2k_zi15tan01_280 (n_potential_duplicates=348)\nProgress: 4060/5147\n--&gt; Found potential duplicate: 4062: ch2k_as05gua01_302&amp;4502: iso2k_1559 (n_potential_duplicates=349)\n--&gt; Found potential duplicate: 4063: ch2k_fe09oga01_304&amp;4596: iso2k_1922 (n_potential_duplicates=350)\n--&gt; Found potential duplicate: 4067: ch2k_gu99nau01_314&amp;4328: iso2k_702 (n_potential_duplicates=351)\n--&gt; Found potential duplicate: 4067: ch2k_gu99nau01_314&amp;4329: iso2k_705 (n_potential_duplicates=352)\nProgress: 4070/5147\n--&gt; Found potential duplicate: 4070: ch2k_co03pal10_324&amp;4290: iso2k_519 (n_potential_duplicates=353)\n--&gt; Found potential duplicate: 4072: ch2k_zi15imp01_328&amp;4073: ch2k_zi15imp01_330 (n_potential_duplicates=354)\n--&gt; Found potential duplicate: 4076: ch2k_ro19yuc01_338&amp;4077: ch2k_ro19yuc01_340 (n_potential_duplicates=355)\nProgress: 4080/5147\n--&gt; Found potential duplicate: 4084: ch2k_co03pal09_358&amp;4293: iso2k_525 (n_potential_duplicates=356)\n--&gt; Found potential duplicate: 4087: ch2k_ki04mcv01_366&amp;4203: iso2k_155 (n_potential_duplicates=357)\nProgress: 4090/5147\n--&gt; Found potential duplicate: 4091: ch2k_ba04fij02_382&amp;4177: iso2k_52 (n_potential_duplicates=358)\n--&gt; Found potential duplicate: 4092: ch2k_co03pal06_386&amp;4289: iso2k_517 (n_potential_duplicates=359)\n--&gt; Found potential duplicate: 4096: ch2k_go12sbv01_396&amp;4365: iso2k_870 (n_potential_duplicates=360)\n--&gt; Found potential duplicate: 4098: ch2k_ca07fli01_400&amp;4406: iso2k_1057 (n_potential_duplicates=361)\nProgress: 4100/5147\n--&gt; Found potential duplicate: 4101: ch2k_co93tar01_408&amp;4296: iso2k_539 (n_potential_duplicates=362)\n--&gt; Found potential duplicate: 4103: ch2k_co00mal01_412&amp;4397: iso2k_1010 (n_potential_duplicates=363)\n--&gt; Found potential duplicate: 4107: ch2k_qu96esv01_422&amp;4213: iso2k_218 (n_potential_duplicates=364)\n--&gt; Found potential duplicate: 4108: ch2k_de13hai01_424&amp;4111: ch2k_de13hai01_432 (n_potential_duplicates=365)\n--&gt; Found potential duplicate: 4108: ch2k_de13hai01_424&amp;4523: iso2k_1643 (n_potential_duplicates=366)\n--&gt; Found potential duplicate: 4109: ch2k_de13hai01_426&amp;4110: ch2k_de13hai01_430 (n_potential_duplicates=367)\nProgress: 4110/5147\n--&gt; Found potential duplicate: 4111: ch2k_de13hai01_432&amp;4523: iso2k_1643 (n_potential_duplicates=368)\n--&gt; Found potential duplicate: 4112: ch2k_li94sec01_436&amp;4419: iso2k_1124 (n_potential_duplicates=369)\n--&gt; Found potential duplicate: 4113: ch2k_zi15cle01_438&amp;4114: ch2k_zi15cle01_440 (n_potential_duplicates=370)\n--&gt; Found potential duplicate: 4117: ch2k_tu01dep01_450&amp;4429: iso2k_1201 (n_potential_duplicates=371)\n--&gt; Found potential duplicate: 4118: ch2k_co03pal04_452&amp;4287: iso2k_513 (n_potential_duplicates=372)\nProgress: 4120/5147\n--&gt; Found potential duplicate: 4121: ch2k_fl18dto01_460&amp;4151: ch2k_fl18dto02_554 (n_potential_duplicates=373)\n--&gt; Found potential duplicate: 4124: ch2k_du94urv01_468&amp;4125: ch2k_du94urv01_470 (n_potential_duplicates=374)\n--&gt; Found potential duplicate: 4126: ch2k_co03pal08_472&amp;4292: iso2k_523 (n_potential_duplicates=375)\n--&gt; Found potential duplicate: 4128: ch2k_zi14tur01_480&amp;4129: ch2k_zi14tur01_482 (n_potential_duplicates=376)\n--&gt; Found potential duplicate: 4128: ch2k_zi14tur01_480&amp;4239: iso2k_302 (n_potential_duplicates=377)\n--&gt; Found potential duplicate: 4129: ch2k_zi14tur01_482&amp;4239: iso2k_302 (n_potential_duplicates=378)\nProgress: 4130/5147\n--&gt; Found potential duplicate: 4130: ch2k_li99cli01_486&amp;4506: iso2k_1571 (n_potential_duplicates=379)\n--&gt; Found potential duplicate: 4131: ch2k_zi15bun01_488&amp;4132: ch2k_zi15bun01_490 (n_potential_duplicates=380)\n--&gt; Found potential duplicate: 4133: ch2k_fe18rus01_492&amp;4580: iso2k_1861 (n_potential_duplicates=381)\n--&gt; Found potential duplicate: 4137: ch2k_wu13ton01_504&amp;4138: ch2k_wu13ton01_506 (n_potential_duplicates=382)\n--&gt; Found potential duplicate: 4139: ch2k_ki14par01_510&amp;4142: ch2k_ki14par01_518 (n_potential_duplicates=383)\nProgress: 4140/5147\n--&gt; Found potential duplicate: 4140: ch2k_ki14par01_512&amp;4141: ch2k_ki14par01_516 (n_potential_duplicates=384)\n--&gt; Found potential duplicate: 4143: ch2k_zi14ifr02_522&amp;4144: ch2k_zi14ifr02_524 (n_potential_duplicates=385)\nProgress: 4150/5147\n--&gt; Found potential duplicate: 4152: ch2k_ba04fij01_558&amp;4178: iso2k_55 (n_potential_duplicates=386)\n--&gt; Found potential duplicate: 4155: ch2k_li06fij01_582&amp;4250: iso2k_353 (n_potential_duplicates=387)\nProgress: 4160/5147\nProgress: 4170/5147\n--&gt; Found potential duplicate: 4179: iso2k_58&amp;4408: iso2k_1068 (n_potential_duplicates=388)\nProgress: 4180/5147\n--&gt; Found potential duplicate: 4189: iso2k_94&amp;4190: iso2k_98 (n_potential_duplicates=389)\nProgress: 4190/5147\n--&gt; Found potential duplicate: 4197: iso2k_120&amp;4772: sisal_253.0_171 (n_potential_duplicates=390)\nProgress: 4200/5147\n--&gt; Found potential duplicate: 4202: iso2k_140&amp;4785: sisal_278.0_184 (n_potential_duplicates=391)\nProgress: 4210/5147\n--&gt; Found potential duplicate: 4215: iso2k_236&amp;4742: sisal_205.0_141 (n_potential_duplicates=392)\nProgress: 4220/5147\nProgress: 4230/5147\n--&gt; Found potential duplicate: 4235: iso2k_296&amp;4236: iso2k_298 (n_potential_duplicates=393)\n--&gt; Found potential duplicate: 4235: iso2k_296&amp;4237: iso2k_299 (n_potential_duplicates=394)\n--&gt; Found potential duplicate: 4236: iso2k_298&amp;4237: iso2k_299 (n_potential_duplicates=395)\nProgress: 4240/5147\nProgress: 4250/5147\n--&gt; Found potential duplicate: 4255: iso2k_380&amp;4893: sisal_446.0_292 (n_potential_duplicates=396)\n--&gt; Found potential duplicate: 4257: iso2k_399&amp;4348: iso2k_806 (n_potential_duplicates=397)\n--&gt; Found potential duplicate: 4257: iso2k_399&amp;4349: iso2k_811 (n_potential_duplicates=398)\nProgress: 4260/5147\n</pre> <pre>/home/jupyter-mnevans/.conda/envs/cfr-env/lib/python3.11/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/jupyter-mnevans/.conda/envs/cfr-env/lib/python3.11/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n</pre> <pre>Progress: 4270/5147\nProgress: 4280/5147\n--&gt; Found potential duplicate: 4283: iso2k_505&amp;4309: iso2k_579 (n_potential_duplicates=399)\nProgress: 4290/5147\n--&gt; Found potential duplicate: 4295: iso2k_533&amp;4670: sisal_115.0_69 (n_potential_duplicates=400)\n--&gt; Found potential duplicate: 4298: iso2k_546&amp;4300: iso2k_549 (n_potential_duplicates=401)\n--&gt; Found potential duplicate: 4299: iso2k_547&amp;4301: iso2k_550 (n_potential_duplicates=402)\nProgress: 4300/5147\nProgress: 4310/5147\nProgress: 4320/5147\n--&gt; Found potential duplicate: 4328: iso2k_702&amp;4329: iso2k_705 (n_potential_duplicates=403)\nProgress: 4330/5147\nProgress: 4340/5147\n--&gt; Found potential duplicate: 4341: iso2k_772&amp;4342: iso2k_775 (n_potential_duplicates=404)\n--&gt; Found potential duplicate: 4345: iso2k_786&amp;4346: iso2k_788 (n_potential_duplicates=405)\n--&gt; Found potential duplicate: 4348: iso2k_806&amp;4349: iso2k_811 (n_potential_duplicates=406)\nProgress: 4350/5147\nProgress: 4360/5147\n--&gt; Found potential duplicate: 4366: iso2k_873&amp;4915: sisal_471.0_314 (n_potential_duplicates=407)\nProgress: 4370/5147\nProgress: 4380/5147\nProgress: 4390/5147\nProgress: 4400/5147\n--&gt; Found potential duplicate: 4409: iso2k_1069&amp;4528: iso2k_1660 (n_potential_duplicates=408)\nProgress: 4410/5147\n--&gt; Found potential duplicate: 4415: iso2k_1107&amp;4564: iso2k_1817 (n_potential_duplicates=409)\n--&gt; Found potential duplicate: 4415: iso2k_1107&amp;4775: sisal_271.0_174 (n_potential_duplicates=410)\nProgress: 4420/5147\n--&gt; Found potential duplicate: 4426: iso2k_1178&amp;4734: sisal_201.0_133 (n_potential_duplicates=411)\nProgress: 4430/5147\nProgress: 4440/5147\n--&gt; Found potential duplicate: 4444: iso2k_1283&amp;4445: iso2k_1286 (n_potential_duplicates=412)\n--&gt; Found potential duplicate: 4447: iso2k_1288&amp;4814: sisal_329.0_213 (n_potential_duplicates=413)\n--&gt; Found potential duplicate: 4448: iso2k_1291&amp;4816: sisal_330.0_215 (n_potential_duplicates=414)\nProgress: 4450/5147\nProgress: 4460/5147\nProgress: 4470/5147\nProgress: 4480/5147\n--&gt; Found potential duplicate: 4486: iso2k_1495&amp;4800: sisal_305.0_199 (n_potential_duplicates=415)\nProgress: 4490/5147\n--&gt; Found potential duplicate: 4490: iso2k_1504&amp;4667: sisal_113.0_66 (n_potential_duplicates=416)\n--&gt; Found potential duplicate: 4499: iso2k_1554&amp;4500: iso2k_1556 (n_potential_duplicates=417)\nProgress: 4500/5147\nProgress: 4510/5147\nProgress: 4520/5147\nProgress: 4530/5147\n--&gt; Found potential duplicate: 4532: iso2k_1701&amp;4533: iso2k_1704 (n_potential_duplicates=418)\nProgress: 4540/5147\nProgress: 4550/5147\nProgress: 4560/5147\n--&gt; Found potential duplicate: 4564: iso2k_1817&amp;4775: sisal_271.0_174 (n_potential_duplicates=419)\n--&gt; Found potential duplicate: 4565: iso2k_1820&amp;4778: sisal_272.0_177 (n_potential_duplicates=420)\n--&gt; Found potential duplicate: 4566: iso2k_1823&amp;4780: sisal_273.0_179 (n_potential_duplicates=421)\nProgress: 4570/5147\n--&gt; Found potential duplicate: 4572: iso2k_1848&amp;4578: iso2k_1855 (n_potential_duplicates=422)\n--&gt; Found potential duplicate: 4573: iso2k_1850&amp;4574: iso2k_1851 (n_potential_duplicates=423)\n--&gt; Found potential duplicate: 4579: iso2k_1856&amp;4795: sisal_294.0_194 (n_potential_duplicates=424)\nProgress: 4580/5147\nProgress: 4590/5147\nProgress: 4600/5147\nProgress: 4610/5147\n--&gt; Found potential duplicate: 4619: sisal_46.0_18&amp;4622: sisal_47.0_21 (n_potential_duplicates=425)\nProgress: 4620/5147\n--&gt; Found potential duplicate: 4620: sisal_46.0_19&amp;4623: sisal_47.0_22 (n_potential_duplicates=426)\n--&gt; Found potential duplicate: 4621: sisal_46.0_20&amp;4624: sisal_47.0_23 (n_potential_duplicates=427)\nProgress: 4630/5147\nProgress: 4640/5147\nProgress: 4650/5147\nProgress: 4660/5147\nProgress: 4670/5147\nProgress: 4680/5147\nProgress: 4690/5147\nProgress: 4700/5147\nProgress: 4710/5147\nProgress: 4720/5147\nProgress: 4730/5147\nProgress: 4740/5147\nProgress: 4750/5147\nProgress: 4760/5147\nProgress: 4770/5147\nProgress: 4780/5147\nProgress: 4790/5147\nProgress: 4800/5147\nProgress: 4810/5147\nProgress: 4820/5147\nProgress: 4830/5147\nProgress: 4840/5147\nProgress: 4850/5147\nProgress: 4860/5147\nProgress: 4870/5147\n--&gt; Found potential duplicate: 4871: sisal_430.0_270&amp;5132: sisal_896.0_531 (n_potential_duplicates=428)\n--&gt; Found potential duplicate: 4872: sisal_430.0_271&amp;5134: sisal_896.0_533 (n_potential_duplicates=429)\nProgress: 4880/5147\nProgress: 4890/5147\nProgress: 4900/5147\nProgress: 4910/5147\nProgress: 4920/5147\nProgress: 4930/5147\nProgress: 4940/5147\nProgress: 4950/5147\nProgress: 4960/5147\nProgress: 4970/5147\nProgress: 4980/5147\nProgress: 4990/5147\nProgress: 5000/5147\nProgress: 5010/5147\nProgress: 5020/5147\nProgress: 5030/5147\nProgress: 5040/5147\nProgress: 5050/5147\nProgress: 5060/5147\nProgress: 5070/5147\nProgress: 5080/5147\nProgress: 5090/5147\nProgress: 5100/5147\nProgress: 5110/5147\nProgress: 5120/5147\nProgress: 5130/5147\nProgress: 5140/5147\n============================================================\nSaved indices, IDs, distances, correlations in data/all_merged/dup_detection/\n============================================================\nDetected 429 possible duplicates in all_merged.\n============================================================\n</pre> In\u00a0[6]: Copied! <pre># dup.plot_duplicates(df, save_figures=False)\n</pre> # dup.plot_duplicates(df, save_figures=False) In\u00a0[7]: Copied! <pre>fn = utf.find(f'dup_detection_candidates_{df.name}.csv',  f'data/{df.name}/dup_detection')\n</pre> fn = utf.find(f'dup_detection_candidates_{df.name}.csv',  f'data/{df.name}/dup_detection') In\u00a0[8]: Copied! <pre>if fn != []:\n    print('----------------------------------------------------')\n    print('Sucessfully finished the duplicate detection process!'.upper())\n    print('----------------------------------------------------')\n    print('Saved the detection output file in:')\n    print()\n    print('%s.'%', '.join(fn))\n    print()\n    print('You are now able to proceed to the next notebook: dup_decision.ipynb')\nelse:\n    print('Final output file is missing.')\n    print()\n    print('Please re-run the notebook to complete duplicate detection process.')\n</pre> if fn != []:     print('----------------------------------------------------')     print('Sucessfully finished the duplicate detection process!'.upper())     print('----------------------------------------------------')     print('Saved the detection output file in:')     print()     print('%s.'%', '.join(fn))     print()     print('You are now able to proceed to the next notebook: dup_decision.ipynb') else:     print('Final output file is missing.')     print()     print('Please re-run the notebook to complete duplicate detection process.') <pre>----------------------------------------------------\nSUCESSFULLY FINISHED THE DUPLICATE DETECTION PROCESS!\n----------------------------------------------------\nSaved the detection output file in:\n\ndata/all_merged/dup_detection/dup_detection_candidates_all_merged.csv.\n\nYou are now able to proceed to the next notebook: dup_decision.ipynb\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/dup_detection/#duplicate-detection-step-1-find-the-potential-duplicates","title":"Duplicate detection - step 1: find the potential duplicates\u00b6","text":""},{"location":"notebooks/dup_detection/#set-up-working-environment","title":"Set up working environment\u00b6","text":""},{"location":"notebooks/dup_detection/#load-dataset","title":"Load dataset\u00b6","text":""},{"location":"notebooks/dup_detection/#duplicate-detection","title":"Duplicate Detection\u00b6","text":""},{"location":"notebooks/dup_detection/#find-duplicates","title":"Find duplicates\u00b6","text":""},{"location":"notebooks/dup_removal/","title":"Duplicate detection - step 3: remove true duplicates","text":"<p>This notebook runs the third part of the duplicate detection algorithm on a dataframe with the following columns:</p> <ul> <li><code>archiveType</code>       (used for duplicate detection algorithm)</li> <li><code>dataSetName</code></li> <li><code>datasetId</code></li> <li><code>geo_meanElev</code>      (used for duplicate detection algorithm)</li> <li><code>geo_meanLat</code>       (used for duplicate detection algorithm)</li> <li><code>geo_meanLon</code>       (used for duplicate detection algorithm)</li> <li><code>geo_siteName</code>      (used for duplicate detection algorithm)</li> <li><code>interpretation_direction</code></li> <li><code>interpretation_seasonality</code></li> <li><code>interpretation_variable</code></li> <li><code>interpretation_variableDetails</code></li> <li><code>originalDataURL</code></li> <li><code>originalDatabase</code></li> <li><code>paleoData_notes</code></li> <li><code>paleoData_proxy</code>   (used for duplicate detection algorithm)</li> <li><code>paleoData_units</code></li> <li><code>paleoData_values</code>  (used for duplicate detection algorithm, test for correlation, RMSE, correlation of 1st difference, RMSE of 1st difference)</li> <li><code>paleoData_variableName</code></li> <li><code>year</code>              (used for duplicate detection algorithm)</li> <li><code>yearUnits</code></li> <li>This interactive notebook (<code>dup_removal.ipynb</code>) removes the duplicates flagged in <code>dup_detection.ipynb</code>, following the decisions made in <code>dup_decision.ipynb</code>. The decisions include</li> <li>removal of redundant duplicates</li> <li>creation of composites</li> </ul> <p>Based on the operator decisions as specified in <code>data/DATABASENAME/duplicate_detection/duplicate_decisions_DATABASENAME_AUTHORINITIALS_YY-MM-DD.csv</code>.</p> <p>Ultimately a duplicate free dataframe is saved under</p> <ul> <li><code>data/DATABASENAME/DATABASENAME_dupfree.pkl</code></li> <li><code>data/DATABASENAME/DATABASENAME_dupfree_data.csv</code></li> <li><code>data/DATABASENAME/DATABASENAME_dupfree_year.csv</code></li> <li><code>data/DATABASENAME/DATABASENAME_dupfree_metadata.csv</code></li> </ul> <p>10/11/2025 by LL: tidied up with revised data organisation and prepared for documentation 02/12/2024 by LL: Modified the compositing process for metadata to fix bugs and make it more user friendly. Added some extra information to the bottom of the file (prior to the figures).</p> <p>22/10/2024 by LL: add the composite option for duplicates (create z-scores and average over shared time period) 30/09/2024 by LL:  keep all original database values for removeed duplicates with more than one original database</p> <p>Author: Lucie Luecke, created 27/9/2024</p> <p>Make sure the repo_root is added correctly, it should be: your_root_dir/dod2k This should be the working directory throughout this notebook (and all other notebooks).</p> In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n\nimport sys\nimport os\nfrom pathlib import Path\n\n# Add parent directory to path (works from any notebook in notebooks/)\n# the repo_root should be the parent directory of the notebooks folder\ncurrent_dir = Path().resolve()\n# Determine repo root\nif current_dir.name == 'dod2k': repo_root = current_dir\nelif current_dir.parent.name == 'dod2k': repo_root = current_dir.parent\nelse: raise Exception('Please review the repo root structure (see first cell).')\n\n# Update cwd and path only if needed\nif os.getcwd() != str(repo_root):\n    os.chdir(repo_root)\nif str(repo_root) not in sys.path:\n    sys.path.insert(0, str(repo_root))\n\nprint(f\"Repo root: {repo_root}\")\nif str(os.getcwd())==str(repo_root):\n    print(f\"Working directory matches repo root. \")\n</pre> %load_ext autoreload %autoreload 2  import sys import os from pathlib import Path  # Add parent directory to path (works from any notebook in notebooks/) # the repo_root should be the parent directory of the notebooks folder current_dir = Path().resolve() # Determine repo root if current_dir.name == 'dod2k': repo_root = current_dir elif current_dir.parent.name == 'dod2k': repo_root = current_dir.parent else: raise Exception('Please review the repo root structure (see first cell).')  # Update cwd and path only if needed if os.getcwd() != str(repo_root):     os.chdir(repo_root) if str(repo_root) not in sys.path:     sys.path.insert(0, str(repo_root))  print(f\"Repo root: {repo_root}\") if str(os.getcwd())==str(repo_root):     print(f\"Working directory matches repo root. \") <pre>Repo root: /home/jupyter-lluecke/dod2k\nWorking directory matches repo root. \n</pre> In\u00a0[2]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport datetime\n\nfrom dod2k_utilities import ut_functions as utf # contains utility functions\nfrom dod2k_utilities import ut_duplicate_search as dup # contains utility functions\n</pre> import pandas as pd import numpy as np import datetime  from dod2k_utilities import ut_functions as utf # contains utility functions from dod2k_utilities import ut_duplicate_search as dup # contains utility functions <p>Define the dataset which needs to be screened for duplicates. Input files for the duplicate detection mechanism need to be compact dataframes (<code>pandas</code> dataframes with standardised columns and entry formatting).</p> <p>The function <code>load_compact_dataframe_from_csv</code> loads the dataframe from a <code>csv</code> file from <code>data\\DB\\</code>, with <code>DB</code> the name of the database. The database name (<code>db_name</code>) can be</p> <ul> <li><code>pages2k</code></li> <li><code>ch2k</code></li> <li><code>iso2k</code></li> <li><code>sisal</code></li> <li><code>fe23</code></li> </ul> <p>for the individual databases, or</p> <ul> <li><code>all_merged</code></li> </ul> <p>to load the merged database of all individual databases, or can be any user defined compact dataframe.</p> In\u00a0[3]: Copied! <pre># load dataframe\ndb_name='all_merged' \n# db_name='dup_test' \ndf = utf.load_compact_dataframe_from_csv(db_name)\n\nprint(df.info())\ndf.name = db_name\n</pre> # load dataframe db_name='all_merged'  # db_name='dup_test'  df = utf.load_compact_dataframe_from_csv(db_name)  print(df.info()) df.name = db_name  <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5320 entries, 0 to 5319\nData columns (total 21 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   archiveType                    5320 non-null   object \n 1   dataSetName                    5320 non-null   object \n 2   datasetId                      5320 non-null   object \n 3   geo_meanElev                   5221 non-null   float32\n 4   geo_meanLat                    5320 non-null   float32\n 5   geo_meanLon                    5320 non-null   float32\n 6   geo_siteName                   5320 non-null   object \n 7   interpretation_direction       5320 non-null   object \n 8   interpretation_seasonality     5320 non-null   object \n 9   interpretation_variable        5320 non-null   object \n 10  interpretation_variableDetail  5320 non-null   object \n 11  originalDataURL                5320 non-null   object \n 12  originalDatabase               5320 non-null   object \n 13  paleoData_notes                5320 non-null   object \n 14  paleoData_proxy                5320 non-null   object \n 15  paleoData_sensorSpecies        5320 non-null   object \n 16  paleoData_units                5320 non-null   object \n 17  paleoData_values               5320 non-null   object \n 18  paleoData_variableName         5320 non-null   object \n 19  year                           5320 non-null   object \n 20  yearUnits                      5320 non-null   object \ndtypes: float32(3), object(18)\nmemory usage: 810.6+ KB\nNone\n</pre> <p>Set <code>datasetId</code> as dataframe index to reliably identify the duplicates:</p> In\u00a0[4]: Copied! <pre>df.set_index('datasetId', inplace = True)\ndf['datasetId']=df.index\n</pre> df.set_index('datasetId', inplace = True) df['datasetId']=df.index <p>In order to keep maximum transparency and reproduceability, put in the operator's credentials here.</p> <p>These details are used to flag the intermediate output files and provided along with the final duplicate free dataset.</p> In\u00a0[5]: Copied! <pre># initials = 'LL'\n# fullname = 'Lucie Luecke'\n# email    = 'ljluec1@st-andrews.ac.uk'\ninitials = 'MNE'\nfullname = 'Michael Evans'\nemail    = 'mnevans@umd.edu'\noperator_details = [initials, fullname, email]\n</pre> # initials = 'LL' # fullname = 'Lucie Luecke' # email    = 'ljluec1@st-andrews.ac.uk' initials = 'MNE' fullname = 'Michael Evans' email    = 'mnevans@umd.edu' operator_details = [initials, fullname, email] <p>Please specify the date of the decision process below. The decision output file is then loaded from <code>data/DBNAME/dup_detection/dup_decisions_DBNAME_INITIALS_DATE.csv</code>.</p> In\u00a0[6]: Copied! <pre># date = str(datetime.datetime.utcnow())[2:10]\n# date='25-12-11' # for Lucie's dup_test decisions\ndate='25-12-07' # for Mike's latest all_merged decisions\n# date='25-12-11' # for Lucie's latest all_merged decisions\n\nfilename      = f'data/{df.name}/dup_detection/dup_decisions_{df.name}_{initials}_{date}'\ndata, header  = dup.read_csv(filename, header=True)\ndf_decisions  = pd.read_csv(filename+'.csv', header=5)\n\nfor hh in header:\n    print(hh)\nprint(df_decisions.columns)\nprint(df.name)\n</pre> # date = str(datetime.datetime.utcnow())[2:10] # date='25-12-11' # for Lucie's dup_test decisions date='25-12-07' # for Mike's latest all_merged decisions # date='25-12-11' # for Lucie's latest all_merged decisions  filename      = f'data/{df.name}/dup_detection/dup_decisions_{df.name}_{initials}_{date}' data, header  = dup.read_csv(filename, header=True) df_decisions  = pd.read_csv(filename+'.csv', header=5)  for hh in header:     print(hh) print(df_decisions.columns) print(df.name) <pre> Decisions for duplicate candidate pairs. \n Operated by Michael Evans (MNE)\n E-Mail: mnevans@umd.edu\n Created on: 2025-12-07 14:02:05.929027 (UTC)\n 2025/12/07 MNE: updated for PAGES2k v2.2.0, Iso2k v1.1.0, CoralHydro2k v1.0.1 all from https://lipdverse.org/ . Checked: Cobb et al (2003, 2013); Dee et al (2020) composite record for Palmyra is updated (https://lipdverse.org/data/XHtmwaqI1qgs2CQxrXgK/1_0_8//CO03COPM.jsonld) in Pages2k v2.2.0 lipdverse compilation. \nIndex(['index 1', 'index 2', 'figure path', 'datasetId 1', 'datasetId 2',\n       'originalDatabase 1', 'originalDatabase 2', 'geo_siteName 1',\n       'geo_siteName 2', 'geo_meanLat 1', 'geo_meanLat 2', 'geo_meanLon 1',\n       'geo_meanLon 2', 'geo_meanElevation 1', 'geo_meanElevation 2',\n       'archiveType 1', 'archiveType 2', 'paleoData_proxy 1',\n       'paleoData_proxy 2', 'originalDataURL 1', 'originalDataURL 2', 'year 1',\n       'year 2', 'Decision 1', 'Decision 2', 'Decision type',\n       'Decision comment'],\n      dtype='object')\nall_merged\n</pre> <p>Collect the decisions associated with each candidate ID (not as pair), to identify records associated with multiple decisions.</p> In\u00a0[7]: Copied! <pre># Collect decisions for each record\ndecisions = dup.collect_record_decisions(df_decisions)\n</pre> # Collect decisions for each record decisions = dup.collect_record_decisions(df_decisions) <p>Show the decisions associated with each individual record</p> In\u00a0[8]: Copied! <pre>for rr, dd in decisions.items():\n    print(rr, dd)\n</pre> for rr, dd in decisions.items():     print(rr, dd) <pre>pages2k_0 ['REMOVE', 'REMOVE', 'REMOVE']\niso2k_296 ['KEEP', 'KEEP', 'KEEP']\niso2k_298 ['KEEP', 'REMOVE', 'KEEP']\niso2k_299 ['KEEP', 'REMOVE', 'REMOVE']\npages2k_6 ['REMOVE']\nFE23_northamerica_usa_az555 ['KEEP']\npages2k_50 ['REMOVE']\nFE23_northamerica_canada_cana091 ['KEEP']\npages2k_62 ['KEEP']\npages2k_63 ['REMOVE']\npages2k_81 ['REMOVE', 'REMOVE']\nch2k_HE08LRA01_76 ['KEEP', 'REMOVE']\niso2k_1813 ['KEEP', 'KEEP']\npages2k_83 ['REMOVE']\niso2k_1916 ['KEEP']\npages2k_85 ['KEEP']\npages2k_88 ['REMOVE']\npages2k_94 ['REMOVE']\nFE23_northamerica_canada_cana153 ['KEEP']\npages2k_107 ['REMOVE']\nFE23_northamerica_usa_ak046 ['KEEP']\npages2k_121 ['KEEP']\npages2k_122 ['REMOVE']\npages2k_132 ['REMOVE']\nFE23_northamerica_canada_cana225 ['KEEP']\npages2k_158 ['REMOVE']\nFE23_northamerica_usa_wa069 ['KEEP']\npages2k_171 ['REMOVE']\nFE23_northamerica_usa_wy021 ['KEEP']\npages2k_203 ['REMOVE']\niso2k_826 ['KEEP']\npages2k_225 ['REMOVE']\nFE23_northamerica_usa_nv512 ['KEEP', 'REMOVE']\npages2k_238 ['REMOVE']\niso2k_1044 ['KEEP']\npages2k_242 ['REMOVE', 'REMOVE']\nch2k_LI06FIJ01_582 ['KEEP', 'REMOVE']\niso2k_353 ['KEEP', 'KEEP']\npages2k_258 ['REMOVE']\niso2k_1498 ['KEEP']\npages2k_263 ['REMOVE']\niso2k_1322 ['KEEP']\npages2k_267 ['KEEP', 'REMOVE']\niso2k_58 ['REMOVE', 'REMOVE']\niso2k_1068 ['KEEP', 'KEEP']\npages2k_271 ['REMOVE', 'REMOVE']\nch2k_FE18RUS01_492 ['KEEP', 'REMOVE']\niso2k_1861 ['KEEP', 'KEEP']\npages2k_273 ['REMOVE']\nFE23_asia_russ130w ['KEEP']\npages2k_281 ['REMOVE']\nFE23_northamerica_canada_cana155 ['KEEP']\npages2k_294 ['REMOVE']\nFE23_northamerica_usa_ak021 ['KEEP']\npages2k_305 ['REMOVE']\npages2k_309 ['KEEP']\npages2k_307 ['REMOVE']\npages2k_311 ['KEEP']\npages2k_315 ['REMOVE']\niso2k_362 ['KEEP']\npages2k_317 ['REMOVE', 'REMOVE']\nch2k_NA09MAL01_84 ['KEEP', 'REMOVE']\niso2k_1754 ['KEEP', 'KEEP']\npages2k_323 ['REMOVE']\nFE23_northamerica_canada_cana210 ['KEEP']\npages2k_385 ['REMOVE', 'REMOVE']\nch2k_FE09OGA01_304 ['KEEP', 'REMOVE']\niso2k_1922 ['KEEP', 'KEEP']\npages2k_387 ['REMOVE']\nch2k_FE09OGA01_306 ['KEEP']\npages2k_395 ['REMOVE', 'REMOVE']\nch2k_CA07FLI01_400 ['KEEP', 'REMOVE']\niso2k_1057 ['KEEP', 'KEEP']\npages2k_397 ['REMOVE']\nch2k_CA07FLI01_402 ['KEEP']\npages2k_409 ['REMOVE', 'REMOVE']\nch2k_QU96ESV01_422 ['KEEP', 'REMOVE']\niso2k_218 ['KEEP', 'KEEP']\npages2k_414 ['KEEP']\npages2k_418 ['REMOVE']\npages2k_417 ['KEEP']\npages2k_421 ['REMOVE']\npages2k_427 ['KEEP']\npages2k_433 ['REMOVE']\npages2k_435 ['KEEP']\npages2k_842 ['REMOVE']\npages2k_444 ['KEEP', 'KEEP']\npages2k_445 ['REMOVE', 'KEEP']\npages2k_446 ['REMOVE', 'REMOVE']\npages2k_462 ['REMOVE', 'REMOVE']\nch2k_OS14UCP01_236 ['KEEP', 'REMOVE']\niso2k_350 ['KEEP', 'KEEP']\npages2k_468 ['KEEP', 'REMOVE']\npages2k_3550 ['REMOVE', 'REMOVE']\nFE23_asia_russ137w ['KEEP', 'KEEP']\npages2k_472 ['KEEP', 'KEEP']\npages2k_474 ['REMOVE', 'REMOVE']\npages2k_477 ['REMOVE', 'REMOVE']\npages2k_478 ['REMOVE']\niso2k_1846 ['KEEP']\npages2k_486 ['REMOVE']\nFE23_northamerica_usa_ca609 ['KEEP']\npages2k_495 ['REMOVE', 'REMOVE']\nch2k_LI06RAR01_12 ['KEEP', 'REMOVE']\niso2k_1502 ['KEEP', 'KEEP']\npages2k_500 ['REMOVE', 'REMOVE']\nch2k_AS05GUA01_302 ['KEEP', 'REMOVE']\niso2k_1559 ['KEEP', 'KEEP']\npages2k_541 ['REMOVE']\niso2k_404 ['KEEP']\npages2k_543 ['KEEP']\npages2k_976 ['REMOVE']\npages2k_565 ['REMOVE']\niso2k_998 ['KEEP']\npages2k_583 ['REMOVE']\nFE23_northamerica_usa_mt116 ['KEEP']\npages2k_592 ['REMOVE', 'REMOVE']\nch2k_LI06RAR02_270 ['KEEP', 'REMOVE']\niso2k_1500 ['KEEP', 'KEEP']\npages2k_610 ['REMOVE']\niso2k_1199 ['KEEP']\npages2k_626 ['REMOVE']\nFE23_northamerica_usa_wa071 ['KEEP']\npages2k_691 ['REMOVE']\nFE23_northamerica_canada_cana062 ['KEEP']\npages2k_730 ['REMOVE']\niso2k_396 ['KEEP']\npages2k_736 ['REMOVE']\nFE23_northamerica_usa_wy024 ['KEEP']\npages2k_800 ['REMOVE']\nFE23_northamerica_canada_cana234 ['KEEP']\npages2k_818 ['REMOVE']\niso2k_488 ['KEEP']\npages2k_827 ['KEEP']\npages2k_830 ['REMOVE']\npages2k_831 ['KEEP', 'REMOVE']\npages2k_2220 ['REMOVE', 'REMOVE']\nFE23_asia_russ127w ['KEEP', 'KEEP']\npages2k_857 ['REMOVE']\nFE23_northamerica_usa_ut511 ['KEEP']\npages2k_881 ['REMOVE']\niso2k_1010 ['KEEP', 'KEEP']\npages2k_893 ['KEEP', 'KEEP']\npages2k_895 ['REMOVE', 'KEEP']\npages2k_900 ['REMOVE', 'REMOVE']\npages2k_940 ['REMOVE', 'REMOVE', 'REMOVE']\nch2k_DR99ABR01_264 ['KEEP', 'KEEP', 'REMOVE']\nch2k_DR99ABR01_266 ['KEEP', 'REMOVE', 'REMOVE']\niso2k_91 ['KEEP', 'KEEP', 'KEEP']\npages2k_945 ['REMOVE']\niso2k_100 ['KEEP']\npages2k_960 ['REMOVE']\niso2k_641 ['KEEP']\npages2k_982 ['REMOVE']\nFE23_northamerica_usa_or042 ['KEEP']\npages2k_1004 ['REMOVE']\niso2k_644 ['KEEP']\npages2k_1026 ['REMOVE']\nFE23_northamerica_usa_az553 ['KEEP']\npages2k_1048 ['REMOVE']\niso2k_1212 ['KEEP']\npages2k_1089 ['REMOVE', 'REMOVE']\nFE23_northamerica_usa_mt112 ['KEEP', 'KEEP']\nFE23_northamerica_usa_mt113 ['KEEP', 'REMOVE']\npages2k_1108 ['REMOVE']\niso2k_1060 ['KEEP']\npages2k_1116 ['REMOVE']\nFE23_northamerica_canada_cana170w ['KEEP']\npages2k_1147 ['KEEP', 'KEEP', 'REMOVE']\nch2k_DA06MAF01_78 ['REMOVE', 'REMOVE']\nch2k_DA06MAF02_104 ['REMOVE', 'REMOVE']\niso2k_1748 ['KEEP', 'KEEP', 'KEEP']\npages2k_1153 ['KEEP', 'KEEP']\npages2k_1156 ['REMOVE', 'KEEP']\npages2k_1160 ['REMOVE', 'REMOVE']\npages2k_1209 ['REMOVE']\nFE23_northamerica_usa_co553 ['KEEP']\npages2k_1252 ['REMOVE']\nFE23_northamerica_canada_cana096 ['KEEP']\npages2k_1274 ['REMOVE']\niso2k_1577 ['KEEP']\npages2k_1293 ['REMOVE']\niso2k_821 ['KEEP']\npages2k_1325 ['REMOVE']\nFE23_northamerica_usa_wy030 ['KEEP']\npages2k_1360 ['REMOVE', 'REMOVE', 'REMOVE']\nch2k_UR00MAI01_22 ['KEEP', 'REMOVE', 'REMOVE']\niso2k_94 ['KEEP', 'KEEP', 'KEEP']\niso2k_98 ['KEEP', 'KEEP', 'REMOVE']\npages2k_1362 ['KEEP']\npages2k_1365 ['REMOVE']\npages2k_1370 ['REMOVE']\niso2k_1619 ['KEEP']\npages2k_1420 ['REMOVE']\nFE23_northamerica_canada_cana111 ['KEEP']\npages2k_1442 ['KEEP']\npages2k_1444 ['REMOVE']\npages2k_1488 ['REMOVE', 'REMOVE', 'REMOVE', 'REMOVE']\npages2k_1628 ['KEEP', 'KEEP', 'REMOVE', 'KEEP']\nch2k_NU11PAL01_52 ['KEEP', 'REMOVE', 'REMOVE', 'REMOVE']\niso2k_505 ['KEEP', 'KEEP', 'KEEP', 'KEEP']\niso2k_579 ['KEEP', 'REMOVE', 'KEEP', 'REMOVE']\npages2k_1490 ['REMOVE']\nch2k_NU11PAL01_54 ['KEEP']\npages2k_1491 ['REMOVE']\niso2k_575 ['KEEP']\npages2k_1497 ['REMOVE']\niso2k_1885 ['KEEP']\npages2k_1515 ['KEEP']\npages2k_1519 ['REMOVE']\npages2k_1520 ['REMOVE']\npages2k_1522 ['KEEP']\npages2k_1547 ['REMOVE']\niso2k_259 ['KEEP']\npages2k_1566 ['REMOVE']\nFE23_northamerica_canada_cana231 ['KEEP']\npages2k_1605 ['REMOVE']\nFE23_northamerica_usa_ca606 ['KEEP']\npages2k_1619 ['REMOVE']\npages2k_1623 ['KEEP']\npages2k_1636 ['REMOVE']\nFE23_northamerica_usa_wa081 ['KEEP']\npages2k_1686 ['COMPOSITE']\npages2k_1688 ['COMPOSITE']\npages2k_1692 ['REMOVE']\nFE23_asia_mong012 ['KEEP']\npages2k_1703 ['REMOVE', 'REMOVE']\nch2k_MO06PED01_226 ['KEEP', 'REMOVE']\niso2k_629 ['KEEP', 'KEEP']\npages2k_1712 ['REMOVE']\niso2k_715 ['KEEP']\npages2k_1720 ['REMOVE']\niso2k_1579 ['KEEP']\npages2k_1741 ['REMOVE']\nFE23_northamerica_usa_wa104 ['KEEP']\npages2k_1750 ['REMOVE', 'REMOVE']\niso2k_1856 ['KEEP', 'KEEP']\nsisal_294.0_194 ['KEEP', 'REMOVE']\npages2k_1771 ['REMOVE']\nch2k_TU01LAI01_192 ['KEEP']\npages2k_1804 ['REMOVE']\nFE23_northamerica_usa_me010 ['KEEP']\npages2k_1859 ['REMOVE', 'REMOVE']\nch2k_HE10GUA01_244 ['KEEP', 'REMOVE']\niso2k_1735 ['KEEP', 'KEEP']\npages2k_1861 ['REMOVE']\nch2k_HE10GUA01_246 ['KEEP']\npages2k_1880 ['REMOVE']\nFE23_northamerica_usa_ak060 ['KEEP']\npages2k_1891 ['REMOVE']\npages2k_1893 ['KEEP']\npages2k_1918 ['REMOVE']\niso2k_102 ['KEEP']\npages2k_1920 ['KEEP']\npages2k_1923 ['REMOVE']\npages2k_1932 ['REMOVE']\npages2k_1934 ['KEEP']\npages2k_1942 ['REMOVE', 'REMOVE']\nch2k_ZI04IFR01_26 ['KEEP', 'REMOVE']\niso2k_257 ['KEEP', 'KEEP']\npages2k_1972 ['KEEP']\npages2k_1973 ['REMOVE']\npages2k_1976 ['REMOVE']\npages2k_1980 ['KEEP']\npages2k_1978 ['REMOVE']\npages2k_1983 ['KEEP']\npages2k_1985 ['REMOVE']\niso2k_1294 ['KEEP']\npages2k_1989 ['KEEP']\npages2k_1991 ['REMOVE']\npages2k_1994 ['REMOVE']\nch2k_DE12ANC01_258 ['KEEP']\npages2k_2013 ['REMOVE']\nFE23_northamerica_canada_cana097 ['KEEP']\npages2k_2042 ['REMOVE', 'REMOVE']\nch2k_TU95MAD01_24 ['KEEP', 'REMOVE']\niso2k_20 ['KEEP', 'KEEP']\npages2k_2059 ['REMOVE']\nFE23_northamerica_usa_ak058 ['KEEP']\npages2k_2085 ['REMOVE']\nFE23_northamerica_canada_cana002 ['KEEP']\npages2k_2094 ['REMOVE', 'REMOVE']\nch2k_TU01DEP01_450 ['KEEP', 'REMOVE']\niso2k_1201 ['KEEP', 'KEEP']\npages2k_2098 ['REMOVE']\npages2k_2103 ['KEEP']\npages2k_2110 ['REMOVE']\nFE23_northamerica_usa_co554 ['KEEP']\npages2k_2146 ['REMOVE', 'KEEP']\npages2k_2149 ['KEEP', 'KEEP']\npages2k_2150 ['REMOVE', 'REMOVE']\npages2k_2156 ['REMOVE']\nFE23_northamerica_canada_cana169w ['KEEP']\npages2k_2214 ['REMOVE']\niso2k_1631 ['KEEP']\npages2k_2226 ['REMOVE']\nFE23_asia_mong007w ['KEEP']\npages2k_2265 ['REMOVE']\nFE23_northamerica_usa_ak070 ['KEEP']\npages2k_2287 ['KEEP']\npages2k_2290 ['REMOVE']\npages2k_2300 ['REMOVE']\nch2k_OS14RIP01_174 ['KEEP']\npages2k_2303 ['REMOVE']\nFE23_asia_mong006 ['KEEP']\npages2k_2309 ['REMOVE']\nch2k_WE09ARR01_208 ['KEEP']\npages2k_2311 ['REMOVE']\nch2k_WE09ARR01_210 ['KEEP']\npages2k_2319 ['REMOVE']\nFE23_northamerica_usa_ak6 ['KEEP']\npages2k_2339 ['KEEP']\npages2k_2344 ['REMOVE']\npages2k_2361 ['REMOVE']\nFE23_northamerica_usa_wa097 ['KEEP']\npages2k_2402 ['REMOVE']\nFE23_northamerica_usa_co586 ['KEEP']\npages2k_2430 ['REMOVE']\nFE23_northamerica_canada_cana113 ['KEEP']\npages2k_2473 ['REMOVE']\nFE23_northamerica_usa_wy022 ['KEEP']\npages2k_2500 ['KEEP']\npages2k_2502 ['REMOVE']\npages2k_2510 ['REMOVE']\niso2k_1626 ['KEEP']\npages2k_2514 ['REMOVE']\niso2k_1467 ['KEEP']\npages2k_2517 ['REMOVE']\niso2k_1130 ['KEEP']\npages2k_2534 ['REMOVE']\niso2k_1575 ['KEEP']\npages2k_2538 ['REMOVE']\niso2k_1862 ['KEEP']\npages2k_2561 ['REMOVE']\nFE23_northamerica_canada_cana094 ['KEEP']\npages2k_2592 ['REMOVE']\npages2k_2596 ['KEEP']\npages2k_2595 ['REMOVE']\npages2k_2599 ['KEEP']\npages2k_2604 ['KEEP', 'REMOVE']\npages2k_2606 ['REMOVE', 'REMOVE']\niso2k_1481 ['KEEP', 'KEEP']\npages2k_2607 ['KEEP', 'KEEP']\npages2k_2609 ['REMOVE', 'KEEP']\npages2k_2612 ['REMOVE', 'REMOVE']\npages2k_2613 ['REMOVE']\niso2k_1470 ['KEEP']\npages2k_2617 ['REMOVE']\niso2k_1573 ['KEEP']\npages2k_2634 ['REMOVE']\nFE23_northamerica_usa_id013 ['KEEP']\npages2k_2660 ['REMOVE']\nFE23_northamerica_usa_ak014 ['KEEP']\npages2k_2677 ['REMOVE']\nFE23_northamerica_usa_wy023 ['KEEP']\npages2k_2703 ['REMOVE']\nFE23_northamerica_usa_ak094 ['KEEP']\npages2k_2722 ['REMOVE']\nFE23_northamerica_canada_cana238 ['KEEP']\npages2k_2750 ['REMOVE']\niso2k_1708 ['KEEP']\npages2k_2752 ['KEEP', 'KEEP']\npages2k_2755 ['REMOVE', 'KEEP']\npages2k_2759 ['REMOVE', 'REMOVE']\npages2k_2793 ['KEEP']\npages2k_2795 ['REMOVE', 'KEEP']\npages2k_2798 ['REMOVE', 'REMOVE']\npages2k_2796 ['KEEP']\npages2k_2830 ['REMOVE']\nFE23_northamerica_mexico_mexi020 ['KEEP']\npages2k_2843 ['REMOVE']\nFE23_northamerica_usa_wa083 ['KEEP']\npages2k_2899 ['REMOVE']\npages2k_2901 ['KEEP']\npages2k_2904 ['KEEP']\npages2k_2906 ['REMOVE']\npages2k_2922 ['REMOVE']\nFE23_northamerica_usa_ca603 ['KEEP']\npages2k_2953 ['REMOVE']\niso2k_573 ['KEEP']\npages2k_2959 ['REMOVE']\nFE23_northamerica_mexico_mexi043 ['KEEP']\npages2k_2976 ['REMOVE']\nFE23_northamerica_usa_id008 ['KEEP']\npages2k_3002 ['REMOVE']\nFE23_northamerica_usa_or043 ['KEEP']\npages2k_3028 ['KEEP', 'KEEP']\npages2k_3030 ['REMOVE', 'KEEP']\npages2k_3033 ['REMOVE', 'REMOVE']\npages2k_3038 ['REMOVE']\nFE23_northamerica_usa_mt108 ['KEEP']\npages2k_3064 ['REMOVE']\niso2k_698 ['KEEP']\npages2k_3068 ['REMOVE', 'REMOVE']\nch2k_ZI14IFR02_522 ['KEEP', 'KEEP']\nch2k_ZI14IFR02_524 ['KEEP', 'REMOVE']\npages2k_3085 ['REMOVE', 'REMOVE', 'REMOVE']\nch2k_KU00NIN01_150 ['KEEP', 'REMOVE', 'REMOVE']\niso2k_1554 ['KEEP', 'KEEP', 'REMOVE']\niso2k_1556 ['KEEP', 'KEEP', 'KEEP']\npages2k_3107 ['REMOVE']\nFE23_northamerica_usa_co552 ['KEEP', 'KEEP']\npages2k_3108 ['REMOVE']\npages2k_3132 ['REMOVE', 'REMOVE']\nch2k_QU06RAB01_144 ['KEEP', 'REMOVE']\niso2k_1311 ['KEEP', 'KEEP']\npages2k_3134 ['REMOVE']\nch2k_QU06RAB01_146 ['KEEP']\npages2k_3170 ['REMOVE']\nFE23_australia_newz062 ['KEEP', 'KEEP']\npages2k_3179 ['REMOVE']\nFE23_northamerica_usa_ak057 ['KEEP']\npages2k_3188 ['KEEP']\npages2k_3191 ['REMOVE']\npages2k_3196 ['REMOVE']\nFE23_asia_mong011 ['KEEP']\npages2k_3202 ['REMOVE']\niso2k_1727 ['KEEP']\npages2k_3234 ['KEEP', 'KEEP']\npages2k_3236 ['REMOVE', 'KEEP']\npages2k_3239 ['REMOVE', 'REMOVE']\npages2k_3243 ['REMOVE']\niso2k_0 ['KEEP']\npages2k_3263 ['REMOVE']\niso2k_1264 ['KEEP']\npages2k_3266 ['REMOVE', 'REMOVE']\nch2k_GO12SBV01_396 ['KEEP', 'REMOVE']\niso2k_870 ['KEEP', 'KEEP']\npages2k_3307 ['REMOVE']\niso2k_339 ['KEEP']\npages2k_3313 ['REMOVE']\nFE23_northamerica_usa_ca560 ['KEEP']\npages2k_3337 ['KEEP']\npages2k_3342 ['REMOVE']\npages2k_3352 ['REMOVE', 'REMOVE', 'REMOVE']\nch2k_ZI14TUR01_480 ['KEEP', 'KEEP', 'REMOVE']\nch2k_ZI14TUR01_482 ['KEEP', 'REMOVE', 'REMOVE']\niso2k_302 ['KEEP', 'KEEP', 'KEEP']\npages2k_3372 ['REMOVE', 'REMOVE']\nch2k_KI04MCV01_366 ['KEEP', 'REMOVE']\niso2k_155 ['KEEP', 'KEEP']\npages2k_3374 ['REMOVE']\nch2k_KI04MCV01_368 ['KEEP']\npages2k_3404 ['REMOVE']\nFE23_northamerica_canada_cana029 ['KEEP']\npages2k_3417 ['KEEP']\npages2k_3419 ['REMOVE']\npages2k_3503 ['REMOVE']\nFE23_northamerica_usa_wa072 ['KEEP']\npages2k_3524 ['REMOVE']\nFE23_northamerica_usa_ak010 ['KEEP']\npages2k_3552 ['REMOVE']\niso2k_1581 ['KEEP']\npages2k_3554 ['REMOVE', 'REMOVE']\nch2k_LI94SEC01_436 ['KEEP', 'REMOVE']\niso2k_1124 ['KEEP', 'KEEP']\npages2k_3571 ['REMOVE']\niso2k_174 ['KEEP']\npages2k_3583 ['REMOVE']\nFE23_northamerica_usa_co633 ['KEEP']\npages2k_3599 ['REMOVE', 'KEEP']\niso2k_1069 ['KEEP', 'KEEP']\niso2k_1660 ['KEEP', 'REMOVE']\npages2k_3609 ['REMOVE']\nFE23_northamerica_canada_cana053 ['KEEP']\npages2k_3631 ['REMOVE']\niso2k_1530 ['KEEP']\npages2k_3642 ['REMOVE']\nFE23_northamerica_usa_wy025 ['KEEP']\nFE23_southamerica_arge016 ['KEEP']\nFE23_southamerica_arge085 ['REMOVE']\nFE23_northamerica_canada_cana100 ['REMOVE']\nFE23_northamerica_canada_cana213 ['KEEP']\nFE23_northamerica_canada_cana105 ['REMOVE']\nFE23_northamerica_canada_cana217 ['KEEP']\nFE23_northamerica_canada_cana116 ['KEEP']\nFE23_northamerica_canada_cana168w ['REMOVE']\nFE23_northamerica_canada_cana161 ['KEEP']\nFE23_northamerica_canada_cana162 ['REMOVE']\nFE23_southamerica_chil016 ['KEEP']\nFE23_southamerica_chil017 ['REMOVE']\nFE23_europe_swed019w ['COMPOSITE']\nFE23_europe_swed021w ['COMPOSITE']\nFE23_northamerica_mexico_mexi022 ['REMOVE']\nFE23_northamerica_mexico_mexi023 ['KEEP']\nFE23_australia_newz003 ['REMOVE']\nFE23_australia_newz060 ['KEEP']\nFE23_australia_newz008 ['REMOVE']\nFE23_australia_newz092 ['KEEP']\nFE23_australia_newz014 ['REMOVE']\nFE23_australia_newz061 ['KEEP']\nFE23_australia_newz018 ['REMOVE']\nFE23_australia_newz019 ['REMOVE']\nFE23_australia_newz063 ['KEEP']\nFE23_northamerica_usa_ca066 ['REMOVE']\nFE23_northamerica_usa_ca628 ['KEEP', 'KEEP']\nFE23_northamerica_usa_ca067 ['REMOVE']\nFE23_northamerica_usa_ca512 ['REMOVE']\nFE23_northamerica_usa_ca613 ['KEEP']\nFE23_northamerica_usa_ca535 ['REMOVE']\nFE23_northamerica_usa_ca670 ['KEEP']\nFE23_northamerica_usa_me017 ['REMOVE']\nFE23_northamerica_usa_me018 ['KEEP']\nFE23_northamerica_usa_mo ['KEEP']\nFE23_northamerica_usa_mo009 ['REMOVE']\nFE23_northamerica_usa_nj001 ['KEEP']\nFE23_northamerica_usa_nj002 ['REMOVE']\nFE23_northamerica_usa_nm024 ['KEEP']\nFE23_northamerica_usa_nm055 ['REMOVE']\nFE23_northamerica_usa_nv060 ['REMOVE']\nFE23_northamerica_usa_nv518 ['KEEP']\nFE23_northamerica_usa_nv521 ['KEEP']\nFE23_northamerica_usa_nv513 ['REMOVE']\nFE23_northamerica_usa_nv520 ['KEEP']\nch2k_ZI15MER01_2 ['KEEP']\nch2k_ZI15MER01_4 ['REMOVE']\nch2k_CO03PAL03_6 ['REMOVE']\niso2k_511 ['KEEP']\nch2k_CO03PAL02_8 ['REMOVE']\niso2k_509 ['KEEP']\nch2k_CO03PAL07_14 ['REMOVE']\niso2k_521 ['KEEP']\nch2k_RE18CAY01_30 ['REMOVE']\niso2k_917 ['KEEP']\nch2k_KU99HOU01_40 ['REMOVE', 'REMOVE']\niso2k_786 ['KEEP', 'REMOVE']\niso2k_788 ['KEEP', 'KEEP']\nch2k_CA14TIM01_64 ['REMOVE']\niso2k_473 ['KEEP']\nch2k_SW98STP01_86 ['REMOVE']\niso2k_50 ['KEEP']\nch2k_CO03PAL01_110 ['REMOVE']\niso2k_507 ['KEEP']\nch2k_CH98PIR01_116 ['REMOVE']\niso2k_1229 ['KEEP']\nch2k_XI17HAI01_128 ['KEEP', 'REMOVE']\nch2k_XI17HAI01_136 ['REMOVE', 'REMOVE']\niso2k_1762 ['KEEP', 'KEEP']\nch2k_XI17HAI01_130 ['KEEP']\nch2k_XI17HAI01_134 ['REMOVE']\nch2k_DE14DTO03_140 ['REMOVE']\nch2k_DE14DTO01_148 ['REMOVE']\nch2k_EV18ROC01_184 ['KEEP']\nch2k_EV18ROC01_186 ['REMOVE']\nch2k_CA13SAP01_188 ['REMOVE']\niso2k_569 ['KEEP']\nch2k_HE13MIS01_194 ['REMOVE', 'REMOVE']\niso2k_211 ['KEEP']\niso2k_213 ['REMOVE']\nch2k_ZI15IMP02_200 ['KEEP']\nch2k_ZI15IMP02_202 ['REMOVE']\nch2k_PF04PBA01_204 ['REMOVE', 'REMOVE']\niso2k_1701 ['KEEP', 'KEEP']\niso2k_1704 ['KEEP', 'REMOVE']\nch2k_CO03PAL05_212 ['REMOVE']\niso2k_515 ['KEEP']\nch2k_ZI15TAN01_278 ['KEEP']\nch2k_ZI15TAN01_280 ['REMOVE']\nch2k_GU99NAU01_314 ['KEEP', 'REMOVE']\niso2k_702 ['REMOVE', 'REMOVE']\niso2k_705 ['KEEP', 'KEEP']\nch2k_CO03PAL10_324 ['REMOVE']\niso2k_519 ['KEEP']\nch2k_ZI15IMP01_328 ['KEEP']\nch2k_ZI15IMP01_330 ['REMOVE']\nch2k_RO19YUC01_338 ['KEEP']\nch2k_RO19YUC01_340 ['REMOVE']\nch2k_CO03PAL09_358 ['REMOVE']\niso2k_525 ['KEEP']\nch2k_BA04FIJ02_382 ['REMOVE']\niso2k_52 ['KEEP']\nch2k_CO03PAL06_386 ['REMOVE']\niso2k_517 ['KEEP']\nch2k_CO93TAR01_408 ['REMOVE']\niso2k_539 ['KEEP']\nch2k_CO00MAL01_412 ['REMOVE']\nch2k_DE13HAI01_424 ['KEEP', 'REMOVE']\nch2k_DE13HAI01_432 ['REMOVE', 'REMOVE']\niso2k_1643 ['KEEP', 'KEEP']\nch2k_DE13HAI01_426 ['KEEP']\nch2k_DE13HAI01_430 ['REMOVE']\nch2k_ZI15CLE01_438 ['KEEP']\nch2k_ZI15CLE01_440 ['REMOVE']\nch2k_CO03PAL04_452 ['REMOVE']\niso2k_513 ['KEEP']\nch2k_FL18DTO01_460 ['REMOVE']\nch2k_FL18DTO02_554 ['REMOVE']\nch2k_DU94URV01_468 ['KEEP']\nch2k_DU94URV01_470 ['REMOVE']\nch2k_CO03PAL08_472 ['REMOVE']\niso2k_523 ['KEEP']\nch2k_LI99CLI01_486 ['REMOVE']\niso2k_1571 ['KEEP']\nch2k_ZI15BUN01_488 ['KEEP']\nch2k_ZI15BUN01_490 ['REMOVE']\nch2k_WU13TON01_504 ['KEEP']\nch2k_WU13TON01_506 ['REMOVE']\nch2k_KI14PAR01_510 ['KEEP']\nch2k_KI14PAR01_518 ['REMOVE']\nch2k_KI14PAR01_512 ['KEEP']\nch2k_KI14PAR01_516 ['REMOVE']\nch2k_BA04FIJ01_558 ['REMOVE']\niso2k_55 ['KEEP']\niso2k_120 ['KEEP']\nsisal_253.0_171 ['REMOVE']\niso2k_140 ['KEEP']\nsisal_278.0_184 ['REMOVE']\niso2k_236 ['KEEP']\nsisal_205.0_141 ['REMOVE']\niso2k_380 ['KEEP']\nsisal_446.0_292 ['REMOVE']\niso2k_399 ['KEEP', 'KEEP']\niso2k_806 ['REMOVE', 'KEEP']\niso2k_811 ['REMOVE', 'REMOVE']\niso2k_533 ['REMOVE']\nsisal_115.0_69 ['KEEP']\niso2k_546 ['KEEP']\niso2k_549 ['REMOVE']\niso2k_547 ['KEEP']\niso2k_550 ['REMOVE']\niso2k_772 ['KEEP']\niso2k_775 ['REMOVE']\niso2k_873 ['KEEP']\nsisal_471.0_314 ['REMOVE']\niso2k_1107 ['KEEP', 'KEEP']\niso2k_1817 ['REMOVE', 'KEEP']\nsisal_271.0_174 ['REMOVE', 'REMOVE']\niso2k_1178 ['KEEP']\nsisal_201.0_133 ['REMOVE']\niso2k_1283 ['KEEP']\niso2k_1286 ['REMOVE']\niso2k_1288 ['KEEP']\nsisal_329.0_213 ['REMOVE']\niso2k_1291 ['KEEP']\nsisal_330.0_215 ['REMOVE']\niso2k_1495 ['KEEP']\nsisal_305.0_199 ['REMOVE']\niso2k_1504 ['REMOVE']\nsisal_113.0_66 ['KEEP']\niso2k_1820 ['KEEP']\nsisal_272.0_177 ['REMOVE']\niso2k_1823 ['KEEP']\nsisal_273.0_179 ['REMOVE']\niso2k_1848 ['KEEP']\niso2k_1855 ['REMOVE']\niso2k_1850 ['KEEP']\niso2k_1851 ['REMOVE']\nsisal_46.0_18 ['KEEP']\nsisal_47.0_21 ['REMOVE']\nsisal_46.0_19 ['KEEP']\nsisal_47.0_22 ['REMOVE']\nsisal_46.0_20 ['KEEP']\nsisal_47.0_23 ['REMOVE']\nsisal_430.0_270 ['KEEP']\nsisal_896.0_531 ['REMOVE']\nsisal_430.0_271 ['KEEP']\nsisal_896.0_533 ['REMOVE']\n</pre> <p>Save all the duplicate details in one dictionary, which will be used in the duplicate free dataframe (final output) <code>df_dupfree</code> to provide details on the duplicate detection process (<code>duplicateDetails</code>).</p> In\u00a0[10]: Copied! <pre># Collect duplicate details for each record\ndup_details = dup.collect_dup_details(df_decisions, header)\n</pre> # Collect duplicate details for each record dup_details = dup.collect_dup_details(df_decisions, header) <p>Note that any one record can appear more than once and have multiple decisions associated with it (e.g. 'REMOVE', 'KEEP' or 'COMPOSITE').</p> <p>In order to remove the duplicates we need to implement the following steps:</p> <ol> <li>Records to be REMOVED. Remove all records from the dataframe which are associated with the decision 'REMOVE' and save in <code>df_cleaned</code></li> <li>Records to be COMPOSITED. Create compounds of the records and save in <code>df_composite</code></li> <li>Now check for records which have both 'REMOVE' and 'COMPOSITE' associated. These are potentially remaining duplicates. Here, the operator is once again asked to make decisions and run a 'mini' version of the duplicate workflow.</li> </ol> <p>First simply remove all the records to which the decision 'REMOVE' and/or 'COMPOSITE' applies to and store in <code>df_cleaned</code>, while all 'REMOVE' type records are stored in <code>df_duplica</code> (for later inspection).</p> In\u00a0[11]: Copied! <pre># load the records TO BE REMOVED OR COMPOSITED\nremove_IDs  = list(df_decisions['datasetId 1'][np.isin(df_decisions['Decision 1'],['REMOVE', 'COMPOSITE'])])\nremove_IDs += list(df_decisions['datasetId 2'][np.isin(df_decisions['Decision 2'],['REMOVE', 'COMPOSITE'])])\nremove_IDs  = np.unique(remove_IDs)\n\ndf_duplica =  df.loc[remove_IDs, 'datasetId'] # df containing only records which were removed\ndf_cleaned =  df.drop(remove_IDs) # df freed from 'REMOVE' type duplicates\n\nprint(f'Removed {len(df_duplica)} REMOVE or COMPOSITE type records.')\nprint(f'REMOVE type duplicate free dataset contains {len(df_cleaned)} records.')\nprint('Removed the following IDs:', remove_IDs)\n\n\nprint(df.name)\n</pre> # load the records TO BE REMOVED OR COMPOSITED remove_IDs  = list(df_decisions['datasetId 1'][np.isin(df_decisions['Decision 1'],['REMOVE', 'COMPOSITE'])]) remove_IDs += list(df_decisions['datasetId 2'][np.isin(df_decisions['Decision 2'],['REMOVE', 'COMPOSITE'])]) remove_IDs  = np.unique(remove_IDs)  df_duplica =  df.loc[remove_IDs, 'datasetId'] # df containing only records which were removed df_cleaned =  df.drop(remove_IDs) # df freed from 'REMOVE' type duplicates  print(f'Removed {len(df_duplica)} REMOVE or COMPOSITE type records.') print(f'REMOVE type duplicate free dataset contains {len(df_cleaned)} records.') print('Removed the following IDs:', remove_IDs)   print(df.name)  <pre>Removed 365 REMOVE or COMPOSITE type records.\nREMOVE type duplicate free dataset contains 4955 records.\nRemoved the following IDs: ['FE23_australia_newz003' 'FE23_australia_newz008'\n 'FE23_australia_newz014' 'FE23_australia_newz018'\n 'FE23_australia_newz019' 'FE23_europe_swed019w' 'FE23_europe_swed021w'\n 'FE23_northamerica_canada_cana100' 'FE23_northamerica_canada_cana105'\n 'FE23_northamerica_canada_cana162' 'FE23_northamerica_canada_cana168w'\n 'FE23_northamerica_mexico_mexi022' 'FE23_northamerica_usa_ca066'\n 'FE23_northamerica_usa_ca067' 'FE23_northamerica_usa_ca512'\n 'FE23_northamerica_usa_ca535' 'FE23_northamerica_usa_me017'\n 'FE23_northamerica_usa_mo009' 'FE23_northamerica_usa_mt113'\n 'FE23_northamerica_usa_nj002' 'FE23_northamerica_usa_nm055'\n 'FE23_northamerica_usa_nv060' 'FE23_northamerica_usa_nv512'\n 'FE23_northamerica_usa_nv513' 'FE23_southamerica_arge085'\n 'FE23_southamerica_chil017' 'ch2k_AS05GUA01_302' 'ch2k_BA04FIJ01_558'\n 'ch2k_BA04FIJ02_382' 'ch2k_CA07FLI01_400' 'ch2k_CA13SAP01_188'\n 'ch2k_CA14TIM01_64' 'ch2k_CH98PIR01_116' 'ch2k_CO00MAL01_412'\n 'ch2k_CO03PAL01_110' 'ch2k_CO03PAL02_8' 'ch2k_CO03PAL03_6'\n 'ch2k_CO03PAL04_452' 'ch2k_CO03PAL05_212' 'ch2k_CO03PAL06_386'\n 'ch2k_CO03PAL07_14' 'ch2k_CO03PAL08_472' 'ch2k_CO03PAL09_358'\n 'ch2k_CO03PAL10_324' 'ch2k_CO93TAR01_408' 'ch2k_DA06MAF01_78'\n 'ch2k_DA06MAF02_104' 'ch2k_DE13HAI01_424' 'ch2k_DE13HAI01_430'\n 'ch2k_DE13HAI01_432' 'ch2k_DE14DTO01_148' 'ch2k_DE14DTO03_140'\n 'ch2k_DR99ABR01_264' 'ch2k_DR99ABR01_266' 'ch2k_DU94URV01_470'\n 'ch2k_EV18ROC01_186' 'ch2k_FE09OGA01_304' 'ch2k_FE18RUS01_492'\n 'ch2k_FL18DTO01_460' 'ch2k_FL18DTO02_554' 'ch2k_GO12SBV01_396'\n 'ch2k_GU99NAU01_314' 'ch2k_HE08LRA01_76' 'ch2k_HE10GUA01_244'\n 'ch2k_HE13MIS01_194' 'ch2k_KI04MCV01_366' 'ch2k_KI14PAR01_516'\n 'ch2k_KI14PAR01_518' 'ch2k_KU00NIN01_150' 'ch2k_KU99HOU01_40'\n 'ch2k_LI06FIJ01_582' 'ch2k_LI06RAR01_12' 'ch2k_LI06RAR02_270'\n 'ch2k_LI94SEC01_436' 'ch2k_LI99CLI01_486' 'ch2k_MO06PED01_226'\n 'ch2k_NA09MAL01_84' 'ch2k_NU11PAL01_52' 'ch2k_OS14UCP01_236'\n 'ch2k_PF04PBA01_204' 'ch2k_QU06RAB01_144' 'ch2k_QU96ESV01_422'\n 'ch2k_RE18CAY01_30' 'ch2k_RO19YUC01_340' 'ch2k_SW98STP01_86'\n 'ch2k_TU01DEP01_450' 'ch2k_TU95MAD01_24' 'ch2k_UR00MAI01_22'\n 'ch2k_WU13TON01_506' 'ch2k_XI17HAI01_128' 'ch2k_XI17HAI01_134'\n 'ch2k_XI17HAI01_136' 'ch2k_ZI04IFR01_26' 'ch2k_ZI14IFR02_524'\n 'ch2k_ZI14TUR01_480' 'ch2k_ZI14TUR01_482' 'ch2k_ZI15BUN01_490'\n 'ch2k_ZI15CLE01_440' 'ch2k_ZI15IMP01_330' 'ch2k_ZI15IMP02_202'\n 'ch2k_ZI15MER01_4' 'ch2k_ZI15TAN01_280' 'iso2k_1286' 'iso2k_1504'\n 'iso2k_1554' 'iso2k_1660' 'iso2k_1704' 'iso2k_1817' 'iso2k_1851'\n 'iso2k_1855' 'iso2k_213' 'iso2k_298' 'iso2k_299' 'iso2k_533' 'iso2k_549'\n 'iso2k_550' 'iso2k_579' 'iso2k_58' 'iso2k_702' 'iso2k_775' 'iso2k_786'\n 'iso2k_806' 'iso2k_811' 'iso2k_98' 'pages2k_0' 'pages2k_1004'\n 'pages2k_1026' 'pages2k_1048' 'pages2k_107' 'pages2k_1089' 'pages2k_1108'\n 'pages2k_1116' 'pages2k_1147' 'pages2k_1156' 'pages2k_1160'\n 'pages2k_1209' 'pages2k_122' 'pages2k_1252' 'pages2k_1274' 'pages2k_1293'\n 'pages2k_132' 'pages2k_1325' 'pages2k_1360' 'pages2k_1365' 'pages2k_1370'\n 'pages2k_1420' 'pages2k_1444' 'pages2k_1488' 'pages2k_1490'\n 'pages2k_1491' 'pages2k_1497' 'pages2k_1519' 'pages2k_1520'\n 'pages2k_1547' 'pages2k_1566' 'pages2k_158' 'pages2k_1605' 'pages2k_1619'\n 'pages2k_1628' 'pages2k_1636' 'pages2k_1686' 'pages2k_1688'\n 'pages2k_1692' 'pages2k_1703' 'pages2k_171' 'pages2k_1712' 'pages2k_1720'\n 'pages2k_1741' 'pages2k_1750' 'pages2k_1771' 'pages2k_1804'\n 'pages2k_1859' 'pages2k_1861' 'pages2k_1880' 'pages2k_1891'\n 'pages2k_1918' 'pages2k_1923' 'pages2k_1932' 'pages2k_1942'\n 'pages2k_1973' 'pages2k_1976' 'pages2k_1978' 'pages2k_1985'\n 'pages2k_1991' 'pages2k_1994' 'pages2k_2013' 'pages2k_203' 'pages2k_2042'\n 'pages2k_2059' 'pages2k_2085' 'pages2k_2094' 'pages2k_2098'\n 'pages2k_2110' 'pages2k_2146' 'pages2k_2150' 'pages2k_2156'\n 'pages2k_2214' 'pages2k_2220' 'pages2k_2226' 'pages2k_225' 'pages2k_2265'\n 'pages2k_2290' 'pages2k_2300' 'pages2k_2303' 'pages2k_2309'\n 'pages2k_2311' 'pages2k_2319' 'pages2k_2344' 'pages2k_2361' 'pages2k_238'\n 'pages2k_2402' 'pages2k_242' 'pages2k_2430' 'pages2k_2473' 'pages2k_2502'\n 'pages2k_2510' 'pages2k_2514' 'pages2k_2517' 'pages2k_2534'\n 'pages2k_2538' 'pages2k_2561' 'pages2k_258' 'pages2k_2592' 'pages2k_2595'\n 'pages2k_2604' 'pages2k_2606' 'pages2k_2609' 'pages2k_2612'\n 'pages2k_2613' 'pages2k_2617' 'pages2k_263' 'pages2k_2634' 'pages2k_2660'\n 'pages2k_267' 'pages2k_2677' 'pages2k_2703' 'pages2k_271' 'pages2k_2722'\n 'pages2k_273' 'pages2k_2750' 'pages2k_2755' 'pages2k_2759' 'pages2k_2795'\n 'pages2k_2798' 'pages2k_281' 'pages2k_2830' 'pages2k_2843' 'pages2k_2899'\n 'pages2k_2906' 'pages2k_2922' 'pages2k_294' 'pages2k_2953' 'pages2k_2959'\n 'pages2k_2976' 'pages2k_3002' 'pages2k_3030' 'pages2k_3033'\n 'pages2k_3038' 'pages2k_305' 'pages2k_3064' 'pages2k_3068' 'pages2k_307'\n 'pages2k_3085' 'pages2k_3107' 'pages2k_3108' 'pages2k_3132'\n 'pages2k_3134' 'pages2k_315' 'pages2k_317' 'pages2k_3170' 'pages2k_3179'\n 'pages2k_3191' 'pages2k_3196' 'pages2k_3202' 'pages2k_323' 'pages2k_3236'\n 'pages2k_3239' 'pages2k_3243' 'pages2k_3263' 'pages2k_3266'\n 'pages2k_3307' 'pages2k_3313' 'pages2k_3342' 'pages2k_3352'\n 'pages2k_3372' 'pages2k_3374' 'pages2k_3404' 'pages2k_3419'\n 'pages2k_3503' 'pages2k_3524' 'pages2k_3550' 'pages2k_3552'\n 'pages2k_3554' 'pages2k_3571' 'pages2k_3583' 'pages2k_3599'\n 'pages2k_3609' 'pages2k_3631' 'pages2k_3642' 'pages2k_385' 'pages2k_387'\n 'pages2k_395' 'pages2k_397' 'pages2k_409' 'pages2k_418' 'pages2k_421'\n 'pages2k_433' 'pages2k_445' 'pages2k_446' 'pages2k_462' 'pages2k_468'\n 'pages2k_474' 'pages2k_477' 'pages2k_478' 'pages2k_486' 'pages2k_495'\n 'pages2k_50' 'pages2k_500' 'pages2k_541' 'pages2k_565' 'pages2k_583'\n 'pages2k_592' 'pages2k_6' 'pages2k_610' 'pages2k_626' 'pages2k_63'\n 'pages2k_691' 'pages2k_730' 'pages2k_736' 'pages2k_800' 'pages2k_81'\n 'pages2k_818' 'pages2k_83' 'pages2k_830' 'pages2k_831' 'pages2k_842'\n 'pages2k_857' 'pages2k_88' 'pages2k_881' 'pages2k_895' 'pages2k_900'\n 'pages2k_94' 'pages2k_940' 'pages2k_945' 'pages2k_960' 'pages2k_976'\n 'pages2k_982' 'sisal_201.0_133' 'sisal_205.0_141' 'sisal_253.0_171'\n 'sisal_271.0_174' 'sisal_272.0_177' 'sisal_273.0_179' 'sisal_278.0_184'\n 'sisal_294.0_194' 'sisal_305.0_199' 'sisal_329.0_213' 'sisal_330.0_215'\n 'sisal_446.0_292' 'sisal_47.0_21' 'sisal_47.0_22' 'sisal_47.0_23'\n 'sisal_471.0_314' 'sisal_896.0_531' 'sisal_896.0_533']\nall_merged\n</pre> In\u00a0[12]: Copied! <pre># add columns on decision process to df_cleaned:\ndf_cleaned['duplicateDetails']='N/A'\nfor ID in dup_details:\n    if ID in df_cleaned.index: \n        if df_cleaned.at[ID, 'duplicateDetails']=='N/A': \n            df_cleaned.at[ID, 'duplicateDetails']=dup_details[ID]\n        else: df_cleaned.at[ID, 'duplicateDetails']+=dup_details[ID]\n</pre> # add columns on decision process to df_cleaned: df_cleaned['duplicateDetails']='N/A' for ID in dup_details:     if ID in df_cleaned.index:          if df_cleaned.at[ID, 'duplicateDetails']=='N/A':              df_cleaned.at[ID, 'duplicateDetails']=dup_details[ID]         else: df_cleaned.at[ID, 'duplicateDetails']+=dup_details[ID] In\u00a0[13]: Copied! <pre># df_cleaned[df_cleaned[ 'duplicateDetails']!='N/A'].at['ch2k_DE14DTO03_140', 'duplicateDetails']\n</pre> # df_cleaned[df_cleaned[ 'duplicateDetails']!='N/A'].at['ch2k_DE14DTO03_140', 'duplicateDetails'] <p>Now identify all the records to which the decision 'COMPOSITE' applies to, create composites and store in <code>df_composite</code>.</p> In\u00a0[15]: Copied! <pre># add the column 'duplicateDetails' to df, in case it does not exist\nif 'duplicateDetails' not in df.columns: df['duplicateDetails']='N/A'\n\n# load the records to be composited\ncomp_ID_pairs = df_decisions[(df_decisions['Decision 1']=='COMPOSITE')&amp;(df_decisions['Decision 2']=='COMPOSITE')]\n\n# create new composite data and metadata from the pairs\n# loop through the composite pairs and check metadata\ndf_composite = dup.join_composites_metadata(df, comp_ID_pairs, df_decisions, header)\n</pre> # add the column 'duplicateDetails' to df, in case it does not exist if 'duplicateDetails' not in df.columns: df['duplicateDetails']='N/A'  # load the records to be composited comp_ID_pairs = df_decisions[(df_decisions['Decision 1']=='COMPOSITE')&amp;(df_decisions['Decision 2']=='COMPOSITE')]  # create new composite data and metadata from the pairs # loop through the composite pairs and check metadata df_composite = dup.join_composites_metadata(df, comp_ID_pairs, df_decisions, header) <pre>pages2k_1686 pages2k_1688\n</pre> <pre>saved figure in /home/jupyter-lluecke/dod2k/figs//all_merged/dup_detection//composite_pages2k_1686_pages2k_1688.pdf\nFE23_europe_swed019w FE23_europe_swed021w\n--------------------------------------------------------------------------------\nMetadata different for &gt;&gt;&gt;geo_siteName&lt;&lt;&lt; in: FE23_europe_swed019w (Tornetr\u00e4skr+f.,Bartoli) and FE23_europe_swed021w (Tornetr\u00e4skfos.,Bartoli). \n--------------------------------------------------------------------------------\nMetadata different for &gt;&gt;&gt;interpretation_variable&lt;&lt;&lt; in: FE23_europe_swed019w (NOT temperature NOT moisture) and FE23_europe_swed021w (N/A). \n</pre> <pre>saved figure in /home/jupyter-lluecke/dod2k/figs//all_merged/dup_detection//composite_FE23_europe_swed019w_FE23_europe_swed021w.pdf\n</pre> In\u00a0[16]: Copied! <pre>print(df_composite.info())\n</pre> print(df_composite.info()) <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2 entries, 0 to 1\nData columns (total 20 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   archiveType                    2 non-null      object \n 1   geo_meanElev                   2 non-null      float32\n 2   geo_meanLat                    2 non-null      float32\n 3   geo_meanLon                    2 non-null      float32\n 4   geo_siteName                   2 non-null      object \n 5   paleoData_proxy                2 non-null      object \n 6   yearUnits                      2 non-null      object \n 7   interpretation_variable        2 non-null      object \n 8   interpretation_direction       2 non-null      object \n 9   interpretation_seasonality     2 non-null      object \n 10  paleoData_values               2 non-null      object \n 11  year                           2 non-null      object \n 12  dataSetName                    2 non-null      object \n 13  originalDatabase               2 non-null      object \n 14  originalDataURL                2 non-null      object \n 15  paleoData_notes                2 non-null      object \n 16  interpretation_variableDetail  2 non-null      object \n 17  datasetId                      2 non-null      object \n 18  paleoData_units                2 non-null      object \n 19  duplicateDetails               2 non-null      object \ndtypes: float32(3), object(17)\nmemory usage: 428.0+ bytes\nNone\n</pre> <p>The duplicate free dataframe is obtained by joining</p> <ul> <li><code>df_cleaned</code> (duplicate free as all records with decision <code>REMOVE</code> and/or <code>COMPOSITE</code> removed) and</li> <li><code>df_composite</code> (dupicate free as duplicates are composited)</li> </ul> <p>However, there might still be duplicates between the two dataframes when a record has been associated with more than 1 duplicate candidate pair.</p> <p>The scenarios for duplicates appearing twice:</p> <ol> <li><code>REMOVE</code>/<code>KEEP</code> and <code>COMPOSITE</code>:</li> </ol> <ul> <li>duplicate pair <code>a</code> and <code>b</code> have had the decisions assigned: <code>a</code>-&gt; <code>REMOVE</code>, <code>b</code> -&gt; <code>KEEP</code></li> <li>duplicate pair <code>a</code> and <code>c</code> have had the decisions assigned: <code>a</code> -&gt; <code>COMPOSITE</code>, <code>c</code> -&gt; <code>COMPOSITE</code>.</li> </ul> <p>In this case, <code>b</code> and <code>ac</code> (the composite record of <code>a</code> and <code>c</code>) would be duplicates in the merged dataframe</p> <p>2a. <code>REMOVE</code>/<code>KEEP</code> &amp; <code>KEEP</code>/<code>REMOVE</code>:</p> <ul> <li><p>duplicate pair <code>a</code> and <code>b</code> have had the decisions assigned: <code>a</code>-&gt; <code>REMOVE</code>, <code>b</code> -&gt; <code>KEEP</code></p> </li> <li><p>duplicate pair <code>a</code> and <code>c</code> have had the decisions assigned: <code>a</code> -&gt; <code>KEEP</code>, <code>c</code> -&gt; <code>REMOVE</code>.</p> <p>In this case <code>a</code> would still be removed as <code>REMOVE</code> overrides <code>KEEP</code> in the algorithm. So only <code>b</code> will be kept and no duplicates would remain.</p> </li> </ul> <p>2b. <code>REMOVE</code>/<code>KEEP</code> &amp; <code>REMOVE</code>/<code>KEEP</code></p> <ul> <li><p>duplicate pair <code>a</code> and <code>b</code> have had the decisions assigned: <code>a</code>-&gt; <code>REMOVE</code>, <code>b</code> -&gt; <code>KEEP</code></p> </li> <li><p>duplicate pair <code>a</code> and <code>c</code> have had the decisions assigned: <code>a</code> -&gt; <code>REMOVE</code>, <code>c</code> -&gt; <code>KEEP</code>.</p> <p>In this case, <code>a</code> would be removed, but <code>b</code> and <code>c</code> will be kept and would be duplicates in the merged dataframe.</p> </li> </ul> <ol> <li><code>COMPOSITE</code> x 2</li> </ol> <ul> <li>duplicate pair <code>a</code> and <code>b</code> have had the decisions assigned: <code>a</code>-&gt; <code>COMPOSITE</code>, <code>b</code> -&gt; <code>COMPOSITE</code></li> <li>duplicate pair <code>a</code> and <code>c</code> have had the decisions assigned: <code>a</code> -&gt; <code>COMPOSITE</code>, <code>c</code> -&gt; <code>COMPOSITE</code>.</li> </ul> <p>In this case, <code>ab</code> and <code>ac</code> would be duplicates in the merged dataframe.</p> <ol> <li><code>REMOVE</code>/<code>KEEP</code> and <code>KEEP</code>/<code>KEEP</code></li> </ol> <ul> <li><p>duplicate pair <code>a</code> and <code>b</code> have had the decisions assigned: <code>a</code>-&gt; <code>REMOVE</code>, <code>b</code> -&gt; <code>KEEP</code></p> </li> <li><p>duplicate pair <code>a</code> and <code>c</code> have had the decisions assigned: <code>a</code> -&gt; <code>KEEP</code>, <code>c</code> -&gt; <code>KEEP</code>.</p> <p>In this case <code>a</code> would be removed, <code>b</code> and <code>c</code> would be kept but as <code>c</code> is not a duplicate of <code>a</code> no duplicates would remain.</p> </li> </ul> <p>Therefore, we loop through the records in the joined dataframe which have been associated with multiple duplicates.</p> <p>Merge the composites and the dataframe freed from REMOVE and COMPOSITE type records</p> In\u00a0[17]: Copied! <pre># initiate the loop\ntmp_df_dupfree = pd.concat([df_cleaned, df_composite])\ntmp_df_dupfree.index = tmp_df_dupfree['datasetId']\ntmp_decisions = decisions.copy()\n</pre> # initiate the loop tmp_df_dupfree = pd.concat([df_cleaned, df_composite]) tmp_df_dupfree.index = tmp_df_dupfree['datasetId'] tmp_decisions = decisions.copy()  <p>Create a loop which implements a mini duplicate detection on all the records which have multiple decisions associated.</p> In\u00a0[18]: Copied! <pre># Simple composite tracking for debugging only\ncomposite_log = []\n\nfor ii in range(10): \n\n    \n    tmp_df_dupfree.set_index('datasetId', inplace = True)\n    tmp_df_dupfree['datasetId']=tmp_df_dupfree.index\n    \n    print('-'*20)\n    print(f'ITERATION # {ii}')\n    \n    multiple_dups = []\n    for id in tmp_decisions.keys():\n        if len(tmp_decisions[id]) &gt; 1:\n            if id not in multiple_dups:\n                multiple_dups.append(id)\n    \n    if len(multiple_dups) &gt; 0:\n        # Check which of the multiple duplicate IDs are still in the dataframe\n        multiple_dups_new = []\n        current_ids = set(tmp_df_dupfree.index)  # Get all current IDs as a set\n        \n        for id in multiple_dups:\n            if id in current_ids:  # Simple membership check\n                multiple_dups_new.append(id)\n        \n        if len(multiple_dups_new) &gt; 0:\n            print(f'WARNING! Decisions associated with {len(multiple_dups_new)} multiple duplicates in the new dataframe.')\n            print('Please review these records below and run through a further duplicate detection workflow until no more duplicates are found.')\n        else:\n            print('No more multiple duplicates found in current dataframe.')\n            print('SUCCESS!!')\n            break\n    else:\n        print('No more multiple duplicates.')\n        print('SUCCESS!!')\n        break\n    \n    # Now we create a small dataframe which needs to be checked for duplicates.\n    df_check = tmp_df_dupfree.copy()[np.isin(tmp_df_dupfree['datasetId'], multiple_dups_new)]\n    print('Check dataframe: ')\n    df_check.name = 'tmp'\n    df_check.index = range(len(df_check))\n    print(df_check.info())\n    # We then run a brief duplicate detection algorithm on the dataframe. Note that by default the composited data has the highest value in the hierarchy.\n    pot_dup_IDs = dup.find_duplicates_optimized(df_check, n_points_thresh=10, return_data=True)\n    if len(pot_dup_IDs)==0:\n        print('SUCCESS!! NO MORE DUPLICATES DETECTED!!')\n        break\n    else:\n        yn=''\n        while yn not in ['y', 'n']:\n            yn = input('Do you want to continue with the decision process for duplicates? [y/n]')\n        if yn=='n': break\n    \n    df_check = dup.define_hierarchy(df_check)\n    dup.duplicate_decisions_multiple(df_check, operator_details=operator_details, choose_recollection=True, \n                            remove_identicals=False, backup=False, comment=False)\n    # implement the decisions\n    tmp_df_decisions  = pd.read_csv(f'data/{df_check.name}/dup_detection/dup_decisions_{df_check.name}_{initials}_{date}'+'.csv', header=5)\n    tmp_dup_details   = dup.provide_dup_details(tmp_df_decisions, header)\n\n    \n    # decisions\n    tmp_decisions = {}\n    for ind in tmp_df_decisions.index:\n        id1, id2   = tmp_df_decisions.loc[ind, ['datasetId 1', 'datasetId 2']]\n        dec1, dec2 = tmp_df_decisions.loc[ind, ['Decision 1', 'Decision 2']]\n        for id, dec in zip([id1, id2], [dec1, dec2]):\n            if id not in tmp_decisions: tmp_decisions[id] = []\n            tmp_decisions[id]+=[dec]\n    \n    df_check.set_index('datasetId', inplace = True)\n    df_check['datasetId']=df_check.index\n    \n    #drop all REMOVE or COMPOSITE types\n    tmp_remove_IDs  = list(tmp_df_decisions['datasetId 1'][np.isin(tmp_df_decisions['Decision 1'],['REMOVE', 'COMPOSITE'])])\n    tmp_remove_IDs += list(tmp_df_decisions['datasetId 2'][np.isin(tmp_df_decisions['Decision 2'],['REMOVE', 'COMPOSITE'])])\n    tmp_remove_IDs = np.unique(tmp_remove_IDs)#[id for id in np.unique(tmp_remove_IDs) if id not in tmp_remove_IDs]\n    tmp_df_cleaned = tmp_df_dupfree.drop(tmp_remove_IDs) # df freed from 'REMOVE' type duplicates\n    \n    # # composite the \n    tmp_comp_ID_pairs = tmp_df_decisions[(tmp_df_decisions['Decision 1']=='COMPOSITE')&amp;(tmp_df_decisions['Decision 2']=='COMPOSITE')]\n    \n    if len(tmp_comp_ID_pairs) &gt; 0:\n        for _, pair in tmp_comp_ID_pairs.iterrows():\n            id1, id2 = pair['datasetId 1'], pair['datasetId 2']\n            # Log what was composited\n            composite_log.append({\n                'iteration': ii,\n                'composited': [id1, id2],\n                'new_id': f\"{id1}_{id2}_composite\"  # or however you generate it\n            })\n    # # create new composite data and metadata from the pairs\n    # # loop through the composite pairs and check metadata\n    tmp_df_composite = dup.join_composites_metadata(df_check, tmp_comp_ID_pairs, tmp_df_decisions, header)\n\n    tmp_df_dupfree = pd.concat([tmp_df_cleaned, tmp_df_composite])\n    print('--'*20)\n    print('Finished iteration.')\n    \n    print('NEW DATAFRAME:')\n    print(tmp_df_dupfree.info())\n\n    print('--'*20)\n    print('--'*20)\n    if ii==19: print('STILL DUPLICATES PRESENT AFTER MULTIPLE ITERATIONS! REVISE DECISION PROCESS!!')\n\n    print('--'*20)\n\nprint(f\"Created {len(composite_log)} composites across all iterations\")\n</pre> # Simple composite tracking for debugging only composite_log = []  for ii in range(10):            tmp_df_dupfree.set_index('datasetId', inplace = True)     tmp_df_dupfree['datasetId']=tmp_df_dupfree.index          print('-'*20)     print(f'ITERATION # {ii}')          multiple_dups = []     for id in tmp_decisions.keys():         if len(tmp_decisions[id]) &gt; 1:             if id not in multiple_dups:                 multiple_dups.append(id)          if len(multiple_dups) &gt; 0:         # Check which of the multiple duplicate IDs are still in the dataframe         multiple_dups_new = []         current_ids = set(tmp_df_dupfree.index)  # Get all current IDs as a set                  for id in multiple_dups:             if id in current_ids:  # Simple membership check                 multiple_dups_new.append(id)                  if len(multiple_dups_new) &gt; 0:             print(f'WARNING! Decisions associated with {len(multiple_dups_new)} multiple duplicates in the new dataframe.')             print('Please review these records below and run through a further duplicate detection workflow until no more duplicates are found.')         else:             print('No more multiple duplicates found in current dataframe.')             print('SUCCESS!!')             break     else:         print('No more multiple duplicates.')         print('SUCCESS!!')         break          # Now we create a small dataframe which needs to be checked for duplicates.     df_check = tmp_df_dupfree.copy()[np.isin(tmp_df_dupfree['datasetId'], multiple_dups_new)]     print('Check dataframe: ')     df_check.name = 'tmp'     df_check.index = range(len(df_check))     print(df_check.info())     # We then run a brief duplicate detection algorithm on the dataframe. Note that by default the composited data has the highest value in the hierarchy.     pot_dup_IDs = dup.find_duplicates_optimized(df_check, n_points_thresh=10, return_data=True)     if len(pot_dup_IDs)==0:         print('SUCCESS!! NO MORE DUPLICATES DETECTED!!')         break     else:         yn=''         while yn not in ['y', 'n']:             yn = input('Do you want to continue with the decision process for duplicates? [y/n]')         if yn=='n': break          df_check = dup.define_hierarchy(df_check)     dup.duplicate_decisions_multiple(df_check, operator_details=operator_details, choose_recollection=True,                              remove_identicals=False, backup=False, comment=False)     # implement the decisions     tmp_df_decisions  = pd.read_csv(f'data/{df_check.name}/dup_detection/dup_decisions_{df_check.name}_{initials}_{date}'+'.csv', header=5)     tmp_dup_details   = dup.provide_dup_details(tmp_df_decisions, header)           # decisions     tmp_decisions = {}     for ind in tmp_df_decisions.index:         id1, id2   = tmp_df_decisions.loc[ind, ['datasetId 1', 'datasetId 2']]         dec1, dec2 = tmp_df_decisions.loc[ind, ['Decision 1', 'Decision 2']]         for id, dec in zip([id1, id2], [dec1, dec2]):             if id not in tmp_decisions: tmp_decisions[id] = []             tmp_decisions[id]+=[dec]          df_check.set_index('datasetId', inplace = True)     df_check['datasetId']=df_check.index          #drop all REMOVE or COMPOSITE types     tmp_remove_IDs  = list(tmp_df_decisions['datasetId 1'][np.isin(tmp_df_decisions['Decision 1'],['REMOVE', 'COMPOSITE'])])     tmp_remove_IDs += list(tmp_df_decisions['datasetId 2'][np.isin(tmp_df_decisions['Decision 2'],['REMOVE', 'COMPOSITE'])])     tmp_remove_IDs = np.unique(tmp_remove_IDs)#[id for id in np.unique(tmp_remove_IDs) if id not in tmp_remove_IDs]     tmp_df_cleaned = tmp_df_dupfree.drop(tmp_remove_IDs) # df freed from 'REMOVE' type duplicates          # # composite the      tmp_comp_ID_pairs = tmp_df_decisions[(tmp_df_decisions['Decision 1']=='COMPOSITE')&amp;(tmp_df_decisions['Decision 2']=='COMPOSITE')]          if len(tmp_comp_ID_pairs) &gt; 0:         for _, pair in tmp_comp_ID_pairs.iterrows():             id1, id2 = pair['datasetId 1'], pair['datasetId 2']             # Log what was composited             composite_log.append({                 'iteration': ii,                 'composited': [id1, id2],                 'new_id': f\"{id1}_{id2}_composite\"  # or however you generate it             })     # # create new composite data and metadata from the pairs     # # loop through the composite pairs and check metadata     tmp_df_composite = dup.join_composites_metadata(df_check, tmp_comp_ID_pairs, tmp_df_decisions, header)      tmp_df_dupfree = pd.concat([tmp_df_cleaned, tmp_df_composite])     print('--'*20)     print('Finished iteration.')          print('NEW DATAFRAME:')     print(tmp_df_dupfree.info())      print('--'*20)     print('--'*20)     if ii==19: print('STILL DUPLICATES PRESENT AFTER MULTIPLE ITERATIONS! REVISE DECISION PROCESS!!')      print('--'*20)  print(f\"Created {len(composite_log)} composites across all iterations\")   <pre>--------------------\nITERATION # 0\nWARNING! Decisions associated with 55 multiple duplicates in the new dataframe.\nPlease review these records below and run through a further duplicate detection workflow until no more duplicates are found.\nCheck dataframe: \n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 55 entries, 0 to 54\nData columns (total 22 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   archiveType                    55 non-null     object \n 1   dataSetName                    55 non-null     object \n 2   geo_meanElev                   54 non-null     float32\n 3   geo_meanLat                    55 non-null     float32\n 4   geo_meanLon                    55 non-null     float32\n 5   geo_siteName                   55 non-null     object \n 6   interpretation_direction       55 non-null     object \n 7   interpretation_seasonality     55 non-null     object \n 8   interpretation_variable        55 non-null     object \n 9   interpretation_variableDetail  55 non-null     object \n 10  originalDataURL                55 non-null     object \n 11  originalDatabase               55 non-null     object \n 12  paleoData_notes                55 non-null     object \n 13  paleoData_proxy                55 non-null     object \n 14  paleoData_sensorSpecies        55 non-null     object \n 15  paleoData_units                55 non-null     object \n 16  paleoData_values               55 non-null     object \n 17  paleoData_variableName         55 non-null     object \n 18  year                           55 non-null     object \n 19  yearUnits                      55 non-null     object \n 20  duplicateDetails               55 non-null     object \n 21  datasetId                      55 non-null     object \ndtypes: float32(3), object(19)\nmemory usage: 8.9+ KB\nNone\ntmp\nStart duplicate search:\n=================================\nchecking parameters:\nproxy archive                  :  must match     \nproxy type                     :  must match     \ndistance (km)                  &lt; 8               \nelevation                      :  must match     \ntime overlap                   &gt; 10              \ncorrelation                    &gt; 0.9             \nRMSE                           &lt; 0.1             \n1st difference rmse            &lt; 0.1             \ncorrelation of 1st difference  &gt; 0.9             \n=================================\nStart duplicate search\nProgress: 0/55\nProgress: 10/55\nProgress: 20/55\nProgress: 30/55\nProgress: 40/55\nProgress: 50/55\n============================================================\nSaved indices, IDs, distances, correlations in data/tmp/dup_detection/\n============================================================\nDetected 0 possible duplicates in tmp.\n============================================================\n============================================================\nIndices: \nIDs: \n============================================================\nSUCCESS!! NO MORE DUPLICATES DETECTED!!\nCreated 0 composites across all iterations\n</pre> <p>Do another duplicate search on the whole dataframe to make sure there are no duplicates present anymore.</p> In\u00a0[19]: Copied! <pre>tmp_df_dupfree.set_index('datasetId', inplace = True)\ntmp_df_dupfree['datasetId']=tmp_df_dupfree.index\n\n# Now we create a  dataframe which needs to be checked for duplicates.\ndf_check = tmp_df_dupfree.copy()\ndf_check.name = 'tmp'\ndf_check.index = range(len(df_check))\n# We then run a brief duplicate detection algorithm on the dataframe. Note that by default the composited data has the highest value in the hierarchy.\npot_dup_IDs = dup.find_duplicates_optimized(df_check, n_points_thresh=10, return_data=True)\nif len(pot_dup_IDs)==0:\n    print('SUCCESS!! NO MORE DUPLICATES DETECTED!!')\nelse:\n    df_check = dup.define_hierarchy(df_check)\n    dup.duplicate_decisions_multiple(df_check, operator_details=operator_details, choose_recollection=True, \n                            remove_identicals=False, backup=False)\n    # implement the decisions\n    tmp_df_decisions  = pd.read_csv(f'data/{df_check.name}/dup_detection/dup_decisions_{df_check.name}_{initials}_{date}'+'.csv', header=5)\n    tmp_dup_details   = dup.provide_dup_details(tmp_df_decisions, header)\n    \n    \n    # decisions\n    tmp_decisions = {}\n    for ind in tmp_df_decisions.index:\n        id1, id2   = tmp_df_decisions.loc[ind, ['datasetId 1', 'datasetId 2']]\n        dec1, dec2 = tmp_df_decisions.loc[ind, ['Decision 1', 'Decision 2']]\n        for id, dec in zip([id1, id2], [dec1, dec2]):\n            if id not in tmp_decisions: tmp_decisions[id] = []\n            tmp_decisions[id]+=[dec]\n    \n    df_check.set_index('datasetId', inplace = True)\n    df_check['datasetId']=df_check.index\n    \n    #drop all REMOVE or COMPOSITE types\n    tmp_remove_IDs  = list(tmp_df_decisions['datasetId 1'][np.isin(tmp_df_decisions['Decision 1'],['REMOVE', 'COMPOSITE'])])\n    tmp_remove_IDs += list(tmp_df_decisions['datasetId 2'][np.isin(tmp_df_decisions['Decision 2'],['REMOVE', 'COMPOSITE'])])\n    tmp_remove_IDs = np.unique(tmp_remove_IDs)#[id for id in np.unique(tmp_remove_IDs) if id not in tmp_remove_IDs]\n    tmp_df_cleaned = tmp_df_dupfree.drop(tmp_remove_IDs) # df freed from 'REMOVE' type duplicates\n    \n    # # composite the \n    tmp_comp_ID_pairs = tmp_df_decisions[(tmp_df_decisions['Decision 1']=='COMPOSITE')&amp;(tmp_df_decisions['Decision 2']=='COMPOSITE')]\n    \n    # # create new composite data and metadata from the pairs\n    # # loop through the composite pairs and check metadata\n    tmp_df_composite = dup.join_composites_metadata(df_check, tmp_comp_ID_pairs, tmp_df_decisions, header)\n    \n    tmp_df_dupfree = pd.concat([tmp_df_cleaned, tmp_df_composite])\n    \n    print('Finished last round of duplicate removal.')\n    print('Potentially run through this cell again to check for remaining duplicates.')\n      \n</pre> tmp_df_dupfree.set_index('datasetId', inplace = True) tmp_df_dupfree['datasetId']=tmp_df_dupfree.index  # Now we create a  dataframe which needs to be checked for duplicates. df_check = tmp_df_dupfree.copy() df_check.name = 'tmp' df_check.index = range(len(df_check)) # We then run a brief duplicate detection algorithm on the dataframe. Note that by default the composited data has the highest value in the hierarchy. pot_dup_IDs = dup.find_duplicates_optimized(df_check, n_points_thresh=10, return_data=True) if len(pot_dup_IDs)==0:     print('SUCCESS!! NO MORE DUPLICATES DETECTED!!') else:     df_check = dup.define_hierarchy(df_check)     dup.duplicate_decisions_multiple(df_check, operator_details=operator_details, choose_recollection=True,                              remove_identicals=False, backup=False)     # implement the decisions     tmp_df_decisions  = pd.read_csv(f'data/{df_check.name}/dup_detection/dup_decisions_{df_check.name}_{initials}_{date}'+'.csv', header=5)     tmp_dup_details   = dup.provide_dup_details(tmp_df_decisions, header)               # decisions     tmp_decisions = {}     for ind in tmp_df_decisions.index:         id1, id2   = tmp_df_decisions.loc[ind, ['datasetId 1', 'datasetId 2']]         dec1, dec2 = tmp_df_decisions.loc[ind, ['Decision 1', 'Decision 2']]         for id, dec in zip([id1, id2], [dec1, dec2]):             if id not in tmp_decisions: tmp_decisions[id] = []             tmp_decisions[id]+=[dec]          df_check.set_index('datasetId', inplace = True)     df_check['datasetId']=df_check.index          #drop all REMOVE or COMPOSITE types     tmp_remove_IDs  = list(tmp_df_decisions['datasetId 1'][np.isin(tmp_df_decisions['Decision 1'],['REMOVE', 'COMPOSITE'])])     tmp_remove_IDs += list(tmp_df_decisions['datasetId 2'][np.isin(tmp_df_decisions['Decision 2'],['REMOVE', 'COMPOSITE'])])     tmp_remove_IDs = np.unique(tmp_remove_IDs)#[id for id in np.unique(tmp_remove_IDs) if id not in tmp_remove_IDs]     tmp_df_cleaned = tmp_df_dupfree.drop(tmp_remove_IDs) # df freed from 'REMOVE' type duplicates          # # composite the      tmp_comp_ID_pairs = tmp_df_decisions[(tmp_df_decisions['Decision 1']=='COMPOSITE')&amp;(tmp_df_decisions['Decision 2']=='COMPOSITE')]          # # create new composite data and metadata from the pairs     # # loop through the composite pairs and check metadata     tmp_df_composite = dup.join_composites_metadata(df_check, tmp_comp_ID_pairs, tmp_df_decisions, header)          tmp_df_dupfree = pd.concat([tmp_df_cleaned, tmp_df_composite])          print('Finished last round of duplicate removal.')     print('Potentially run through this cell again to check for remaining duplicates.')        <pre>tmp\nStart duplicate search:\n=================================\nchecking parameters:\nproxy archive                  :  must match     \nproxy type                     :  must match     \ndistance (km)                  &lt; 8               \nelevation                      :  must match     \ntime overlap                   &gt; 10              \ncorrelation                    &gt; 0.9             \nRMSE                           &lt; 0.1             \n1st difference rmse            &lt; 0.1             \ncorrelation of 1st difference  &gt; 0.9             \n=================================\nStart duplicate search\nProgress: 0/4957\nProgress: 10/4957\nProgress: 20/4957\nProgress: 30/4957\nProgress: 40/4957\nProgress: 50/4957\nProgress: 60/4957\nProgress: 70/4957\nProgress: 80/4957\nProgress: 90/4957\nProgress: 100/4957\nProgress: 110/4957\nProgress: 120/4957\nProgress: 130/4957\nProgress: 140/4957\nProgress: 150/4957\nProgress: 160/4957\nProgress: 170/4957\nProgress: 180/4957\nProgress: 190/4957\nProgress: 200/4957\nProgress: 210/4957\nProgress: 220/4957\nProgress: 230/4957\nProgress: 240/4957\nProgress: 250/4957\nProgress: 260/4957\nProgress: 270/4957\nProgress: 280/4957\nProgress: 290/4957\nProgress: 300/4957\nProgress: 310/4957\nProgress: 320/4957\nProgress: 330/4957\nProgress: 340/4957\nProgress: 350/4957\nProgress: 360/4957\nProgress: 370/4957\nProgress: 380/4957\nProgress: 390/4957\nProgress: 400/4957\nProgress: 410/4957\nProgress: 420/4957\nProgress: 430/4957\nProgress: 440/4957\nProgress: 450/4957\nProgress: 460/4957\nProgress: 470/4957\nProgress: 480/4957\nProgress: 490/4957\nProgress: 500/4957\nProgress: 510/4957\nProgress: 520/4957\nProgress: 530/4957\nProgress: 540/4957\nProgress: 550/4957\nProgress: 560/4957\nProgress: 570/4957\nProgress: 580/4957\nProgress: 590/4957\nProgress: 600/4957\nProgress: 610/4957\nProgress: 620/4957\nProgress: 630/4957\nProgress: 640/4957\nProgress: 650/4957\nProgress: 660/4957\nProgress: 670/4957\nProgress: 680/4957\nProgress: 690/4957\nProgress: 700/4957\nProgress: 710/4957\nProgress: 720/4957\nProgress: 730/4957\nProgress: 740/4957\nProgress: 750/4957\nProgress: 760/4957\nProgress: 770/4957\nProgress: 780/4957\nProgress: 790/4957\nProgress: 800/4957\nProgress: 810/4957\nProgress: 820/4957\nProgress: 830/4957\nProgress: 840/4957\nProgress: 850/4957\nProgress: 860/4957\nProgress: 870/4957\nProgress: 880/4957\nProgress: 890/4957\nProgress: 900/4957\nProgress: 910/4957\nProgress: 920/4957\nProgress: 930/4957\nProgress: 940/4957\nProgress: 950/4957\nProgress: 960/4957\nProgress: 970/4957\nProgress: 980/4957\nProgress: 990/4957\nProgress: 1000/4957\nProgress: 1010/4957\nProgress: 1020/4957\nProgress: 1030/4957\nProgress: 1040/4957\nProgress: 1050/4957\nProgress: 1060/4957\nProgress: 1070/4957\nProgress: 1080/4957\nProgress: 1090/4957\nProgress: 1100/4957\nProgress: 1110/4957\nProgress: 1120/4957\nProgress: 1130/4957\nProgress: 1140/4957\nProgress: 1150/4957\nProgress: 1160/4957\nProgress: 1170/4957\nProgress: 1180/4957\nProgress: 1190/4957\nProgress: 1200/4957\nProgress: 1210/4957\nProgress: 1220/4957\nProgress: 1230/4957\nProgress: 1240/4957\nProgress: 1250/4957\nProgress: 1260/4957\nProgress: 1270/4957\nProgress: 1280/4957\nProgress: 1290/4957\nProgress: 1300/4957\nProgress: 1310/4957\nProgress: 1320/4957\nProgress: 1330/4957\nProgress: 1340/4957\nProgress: 1350/4957\nProgress: 1360/4957\nProgress: 1370/4957\nProgress: 1380/4957\nProgress: 1390/4957\nProgress: 1400/4957\nProgress: 1410/4957\nProgress: 1420/4957\nProgress: 1430/4957\nProgress: 1440/4957\nProgress: 1450/4957\nProgress: 1460/4957\nProgress: 1470/4957\nProgress: 1480/4957\nProgress: 1490/4957\nProgress: 1500/4957\nProgress: 1510/4957\nProgress: 1520/4957\nProgress: 1530/4957\nProgress: 1540/4957\nProgress: 1550/4957\nProgress: 1560/4957\nProgress: 1570/4957\nProgress: 1580/4957\nProgress: 1590/4957\nProgress: 1600/4957\nProgress: 1610/4957\nProgress: 1620/4957\nProgress: 1630/4957\nProgress: 1640/4957\nProgress: 1650/4957\nProgress: 1660/4957\nProgress: 1670/4957\nProgress: 1680/4957\nProgress: 1690/4957\nProgress: 1700/4957\nProgress: 1710/4957\nProgress: 1720/4957\nProgress: 1730/4957\nProgress: 1740/4957\nProgress: 1750/4957\nProgress: 1760/4957\nProgress: 1770/4957\nProgress: 1780/4957\nProgress: 1790/4957\nProgress: 1800/4957\nProgress: 1810/4957\nProgress: 1820/4957\nProgress: 1830/4957\nProgress: 1840/4957\nProgress: 1850/4957\nProgress: 1860/4957\nProgress: 1870/4957\nProgress: 1880/4957\nProgress: 1890/4957\nProgress: 1900/4957\nProgress: 1910/4957\nProgress: 1920/4957\nProgress: 1930/4957\nProgress: 1940/4957\nProgress: 1950/4957\nProgress: 1960/4957\nProgress: 1970/4957\nProgress: 1980/4957\nProgress: 1990/4957\nProgress: 2000/4957\nProgress: 2010/4957\nProgress: 2020/4957\nProgress: 2030/4957\nProgress: 2040/4957\nProgress: 2050/4957\nProgress: 2060/4957\nProgress: 2070/4957\nProgress: 2080/4957\nProgress: 2090/4957\nProgress: 2100/4957\nProgress: 2110/4957\nProgress: 2120/4957\nProgress: 2130/4957\nProgress: 2140/4957\nProgress: 2150/4957\nProgress: 2160/4957\nProgress: 2170/4957\nProgress: 2180/4957\nProgress: 2190/4957\nProgress: 2200/4957\nProgress: 2210/4957\nProgress: 2220/4957\nProgress: 2230/4957\nProgress: 2240/4957\nProgress: 2250/4957\nProgress: 2260/4957\nProgress: 2270/4957\nProgress: 2280/4957\nProgress: 2290/4957\nProgress: 2300/4957\nProgress: 2310/4957\nProgress: 2320/4957\nProgress: 2330/4957\nProgress: 2340/4957\nProgress: 2350/4957\nProgress: 2360/4957\nProgress: 2370/4957\nProgress: 2380/4957\nProgress: 2390/4957\nProgress: 2400/4957\nProgress: 2410/4957\nProgress: 2420/4957\nProgress: 2430/4957\nProgress: 2440/4957\nProgress: 2450/4957\nProgress: 2460/4957\nProgress: 2470/4957\nProgress: 2480/4957\nProgress: 2490/4957\nProgress: 2500/4957\nProgress: 2510/4957\nProgress: 2520/4957\nProgress: 2530/4957\nProgress: 2540/4957\nProgress: 2550/4957\nProgress: 2560/4957\nProgress: 2570/4957\nProgress: 2580/4957\nProgress: 2590/4957\nProgress: 2600/4957\nProgress: 2610/4957\nProgress: 2620/4957\nProgress: 2630/4957\nProgress: 2640/4957\nProgress: 2650/4957\nProgress: 2660/4957\nProgress: 2670/4957\nProgress: 2680/4957\nProgress: 2690/4957\nProgress: 2700/4957\nProgress: 2710/4957\nProgress: 2720/4957\nProgress: 2730/4957\nProgress: 2740/4957\nProgress: 2750/4957\nProgress: 2760/4957\nProgress: 2770/4957\nProgress: 2780/4957\nProgress: 2790/4957\nProgress: 2800/4957\nProgress: 2810/4957\nProgress: 2820/4957\nProgress: 2830/4957\nProgress: 2840/4957\nProgress: 2850/4957\nProgress: 2860/4957\nProgress: 2870/4957\nProgress: 2880/4957\nProgress: 2890/4957\nProgress: 2900/4957\nProgress: 2910/4957\nProgress: 2920/4957\nProgress: 2930/4957\nProgress: 2940/4957\nProgress: 2950/4957\nProgress: 2960/4957\nProgress: 2970/4957\nProgress: 2980/4957\nProgress: 2990/4957\nProgress: 3000/4957\nProgress: 3010/4957\nProgress: 3020/4957\nProgress: 3030/4957\nProgress: 3040/4957\nProgress: 3050/4957\nProgress: 3060/4957\nProgress: 3070/4957\nProgress: 3080/4957\nProgress: 3090/4957\nProgress: 3100/4957\nProgress: 3110/4957\nProgress: 3120/4957\nProgress: 3130/4957\nProgress: 3140/4957\nProgress: 3150/4957\nProgress: 3160/4957\nProgress: 3170/4957\nProgress: 3180/4957\nProgress: 3190/4957\nProgress: 3200/4957\nProgress: 3210/4957\nProgress: 3220/4957\nProgress: 3230/4957\nProgress: 3240/4957\nProgress: 3250/4957\nProgress: 3260/4957\nProgress: 3270/4957\nProgress: 3280/4957\nProgress: 3290/4957\nProgress: 3300/4957\nProgress: 3310/4957\nProgress: 3320/4957\nProgress: 3330/4957\nProgress: 3340/4957\nProgress: 3350/4957\nProgress: 3360/4957\nProgress: 3370/4957\nProgress: 3380/4957\nProgress: 3390/4957\nProgress: 3400/4957\nProgress: 3410/4957\nProgress: 3420/4957\nProgress: 3430/4957\nProgress: 3440/4957\nProgress: 3450/4957\nProgress: 3460/4957\nProgress: 3470/4957\nProgress: 3480/4957\nProgress: 3490/4957\nProgress: 3500/4957\nProgress: 3510/4957\nProgress: 3520/4957\nProgress: 3530/4957\nProgress: 3540/4957\nProgress: 3550/4957\nProgress: 3560/4957\nProgress: 3570/4957\nProgress: 3580/4957\nProgress: 3590/4957\nProgress: 3600/4957\nProgress: 3610/4957\nProgress: 3620/4957\nProgress: 3630/4957\nProgress: 3640/4957\nProgress: 3650/4957\nProgress: 3660/4957\nProgress: 3670/4957\nProgress: 3680/4957\nProgress: 3690/4957\nProgress: 3700/4957\nProgress: 3710/4957\nProgress: 3720/4957\nProgress: 3730/4957\nProgress: 3740/4957\nProgress: 3750/4957\nProgress: 3760/4957\nProgress: 3770/4957\nProgress: 3780/4957\nProgress: 3790/4957\nProgress: 3800/4957\nProgress: 3810/4957\nProgress: 3820/4957\nProgress: 3830/4957\nProgress: 3840/4957\nProgress: 3850/4957\nProgress: 3860/4957\nProgress: 3870/4957\nProgress: 3880/4957\nProgress: 3890/4957\nProgress: 3900/4957\nProgress: 3910/4957\nProgress: 3920/4957\nProgress: 3930/4957\nProgress: 3940/4957\nProgress: 3950/4957\nProgress: 3960/4957\nProgress: 3970/4957\nProgress: 3980/4957\nProgress: 3990/4957\nProgress: 4000/4957\nProgress: 4010/4957\nProgress: 4020/4957\nProgress: 4030/4957\nProgress: 4040/4957\nProgress: 4050/4957\nProgress: 4060/4957\nProgress: 4070/4957\nProgress: 4080/4957\nProgress: 4090/4957\nProgress: 4100/4957\n</pre> <pre>/home/jupyter-mnevans/.conda/envs/cfr-env/lib/python3.11/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[:, None]\n/home/jupyter-mnevans/.conda/envs/cfr-env/lib/python3.11/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n  c /= stddev[None, :]\n</pre> <pre>Progress: 4110/4957\nProgress: 4120/4957\nProgress: 4130/4957\nProgress: 4140/4957\nProgress: 4150/4957\nProgress: 4160/4957\nProgress: 4170/4957\nProgress: 4180/4957\nProgress: 4190/4957\nProgress: 4200/4957\nProgress: 4210/4957\nProgress: 4220/4957\nProgress: 4230/4957\nProgress: 4240/4957\nProgress: 4250/4957\nProgress: 4260/4957\nProgress: 4270/4957\nProgress: 4280/4957\nProgress: 4290/4957\nProgress: 4300/4957\nProgress: 4310/4957\nProgress: 4320/4957\nProgress: 4330/4957\nProgress: 4340/4957\nProgress: 4350/4957\nProgress: 4360/4957\nProgress: 4370/4957\nProgress: 4380/4957\nProgress: 4390/4957\nProgress: 4400/4957\nProgress: 4410/4957\nProgress: 4420/4957\nProgress: 4430/4957\nProgress: 4440/4957\nProgress: 4450/4957\nProgress: 4460/4957\nProgress: 4470/4957\nProgress: 4480/4957\nProgress: 4490/4957\nProgress: 4500/4957\nProgress: 4510/4957\nProgress: 4520/4957\nProgress: 4530/4957\nProgress: 4540/4957\nProgress: 4550/4957\nProgress: 4560/4957\nProgress: 4570/4957\nProgress: 4580/4957\nProgress: 4590/4957\nProgress: 4600/4957\nProgress: 4610/4957\nProgress: 4620/4957\nProgress: 4630/4957\nProgress: 4640/4957\nProgress: 4650/4957\nProgress: 4660/4957\nProgress: 4670/4957\nProgress: 4680/4957\nProgress: 4690/4957\nProgress: 4700/4957\nProgress: 4710/4957\nProgress: 4720/4957\nProgress: 4730/4957\nProgress: 4740/4957\nProgress: 4750/4957\nProgress: 4760/4957\nProgress: 4770/4957\nProgress: 4780/4957\nProgress: 4790/4957\nProgress: 4800/4957\nProgress: 4810/4957\nProgress: 4820/4957\nProgress: 4830/4957\nProgress: 4840/4957\nProgress: 4850/4957\nProgress: 4860/4957\nProgress: 4870/4957\nProgress: 4880/4957\nProgress: 4890/4957\nProgress: 4900/4957\nProgress: 4910/4957\nProgress: 4920/4957\nProgress: 4930/4957\nProgress: 4940/4957\nProgress: 4950/4957\n============================================================\nSaved indices, IDs, distances, correlations in data/tmp/dup_detection/\n============================================================\nDetected 0 possible duplicates in tmp.\n============================================================\n============================================================\nIndices: \nIDs: \n============================================================\nSUCCESS!! NO MORE DUPLICATES DETECTED!!\n</pre> In\u00a0[20]: Copied! <pre>df_dupfree = tmp_df_dupfree\nprint(df_dupfree.info())\n</pre> df_dupfree = tmp_df_dupfree print(df_dupfree.info()) <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 4957 entries, pages2k_5 to dod2k_composite_z_FE23_europe_swed019w_FE23_europe_swed021w\nData columns (total 22 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   archiveType                    4957 non-null   object \n 1   dataSetName                    4957 non-null   object \n 2   geo_meanElev                   4875 non-null   float32\n 3   geo_meanLat                    4957 non-null   float32\n 4   geo_meanLon                    4957 non-null   float32\n 5   geo_siteName                   4957 non-null   object \n 6   interpretation_direction       4957 non-null   object \n 7   interpretation_seasonality     4957 non-null   object \n 8   interpretation_variable        4957 non-null   object \n 9   interpretation_variableDetail  4957 non-null   object \n 10  originalDataURL                4957 non-null   object \n 11  originalDatabase               4957 non-null   object \n 12  paleoData_notes                4957 non-null   object \n 13  paleoData_proxy                4957 non-null   object \n 14  paleoData_sensorSpecies        4955 non-null   object \n 15  paleoData_units                4957 non-null   object \n 16  paleoData_values               4957 non-null   object \n 17  paleoData_variableName         4955 non-null   object \n 18  year                           4957 non-null   object \n 19  yearUnits                      4957 non-null   object \n 20  duplicateDetails               4957 non-null   object \n 21  datasetId                      4957 non-null   object \ndtypes: float32(3), object(19)\nmemory usage: 832.6+ KB\nNone\n</pre> <p>Sort the columns and assign a name to the dataframe which is used for saving purposes (determines directory and filename). Make sure that <code>date</code> and operator initials <code>initials</code> are used in the name.</p> In\u00a0[21]: Copied! <pre>df_dupfree = df_dupfree[sorted(df_dupfree.columns)]\n\nif df.name=='all_merged':\n    yn = input('Would you like to save as dod2k_v2.0?')\nif yn=='y':\n    df_dupfree.name = 'dod2k_v2.0'\nelse:\n    df_dupfree.name =f'{df.name}_{initials}_{date}_dupfree'\nos.makedirs(f'data/{df_dupfree.name}/', exist_ok=True)\n</pre> df_dupfree = df_dupfree[sorted(df_dupfree.columns)]  if df.name=='all_merged':     yn = input('Would you like to save as dod2k_v2.0?') if yn=='y':     df_dupfree.name = 'dod2k_v2.0' else:     df_dupfree.name =f'{df.name}_{initials}_{date}_dupfree' os.makedirs(f'data/{df_dupfree.name}/', exist_ok=True)  In\u00a0[22]: Copied! <pre>df_dupfree.info()\nprint(df_dupfree.name)\n</pre> df_dupfree.info() print(df_dupfree.name) <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 4957 entries, pages2k_5 to dod2k_composite_z_FE23_europe_swed019w_FE23_europe_swed021w\nData columns (total 22 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   archiveType                    4957 non-null   object \n 1   dataSetName                    4957 non-null   object \n 2   datasetId                      4957 non-null   object \n 3   duplicateDetails               4957 non-null   object \n 4   geo_meanElev                   4875 non-null   float32\n 5   geo_meanLat                    4957 non-null   float32\n 6   geo_meanLon                    4957 non-null   float32\n 7   geo_siteName                   4957 non-null   object \n 8   interpretation_direction       4957 non-null   object \n 9   interpretation_seasonality     4957 non-null   object \n 10  interpretation_variable        4957 non-null   object \n 11  interpretation_variableDetail  4957 non-null   object \n 12  originalDataURL                4957 non-null   object \n 13  originalDatabase               4957 non-null   object \n 14  paleoData_notes                4957 non-null   object \n 15  paleoData_proxy                4957 non-null   object \n 16  paleoData_sensorSpecies        4955 non-null   object \n 17  paleoData_units                4957 non-null   object \n 18  paleoData_values               4957 non-null   object \n 19  paleoData_variableName         4955 non-null   object \n 20  year                           4957 non-null   object \n 21  yearUnits                      4957 non-null   object \ndtypes: float32(3), object(19)\nmemory usage: 832.6+ KB\ndod2k_v2.0\n</pre> In\u00a0[23]: Copied! <pre># save concatenate dataframe as db_merged\ndf_dupfree.to_pickle(f'data/{df_dupfree.name}/{df_dupfree.name}_compact.pkl')\n</pre> # save concatenate dataframe as db_merged df_dupfree.to_pickle(f'data/{df_dupfree.name}/{df_dupfree.name}_compact.pkl') In\u00a0[24]: Copied! <pre># save to a list of csv files (metadata, data, year)\nutf.write_compact_dataframe_to_csv(df_dupfree)\n</pre> # save to a list of csv files (metadata, data, year) utf.write_compact_dataframe_to_csv(df_dupfree) <pre>METADATA: datasetId, archiveType, dataSetName, duplicateDetails, geo_meanElev, geo_meanLat, geo_meanLon, geo_siteName, interpretation_direction, interpretation_seasonality, interpretation_variable, interpretation_variableDetail, originalDataURL, originalDatabase, paleoData_notes, paleoData_proxy, paleoData_sensorSpecies, paleoData_units, paleoData_variableName, yearUnits\nSaved to /home/jupyter-lluecke/dod2k/data/dod2k_v2.0/dod2k_v2.0_compact_%s.csv\n</pre> In\u00a0[25]: Copied! <pre># load dataframe\nprint(utf.load_compact_dataframe_from_csv(df_dupfree.name).info())\n</pre> # load dataframe print(utf.load_compact_dataframe_from_csv(df_dupfree.name).info()) <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4957 entries, 0 to 4956\nData columns (total 22 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   archiveType                    4957 non-null   object \n 1   dataSetName                    4957 non-null   object \n 2   datasetId                      4957 non-null   object \n 3   duplicateDetails               4957 non-null   object \n 4   geo_meanElev                   4875 non-null   float32\n 5   geo_meanLat                    4957 non-null   float32\n 6   geo_meanLon                    4957 non-null   float32\n 7   geo_siteName                   4957 non-null   object \n 8   interpretation_direction       4957 non-null   object \n 9   interpretation_seasonality     4957 non-null   object \n 10  interpretation_variable        4957 non-null   object \n 11  interpretation_variableDetail  4957 non-null   object \n 12  originalDataURL                4957 non-null   object \n 13  originalDatabase               4957 non-null   object \n 14  paleoData_notes                4957 non-null   object \n 15  paleoData_proxy                4957 non-null   object \n 16  paleoData_sensorSpecies        4957 non-null   object \n 17  paleoData_units                4957 non-null   object \n 18  paleoData_values               4957 non-null   object \n 19  paleoData_variableName         4957 non-null   object \n 20  year                           4957 non-null   object \n 21  yearUnits                      4957 non-null   object \ndtypes: float32(3), object(19)\nmemory usage: 794.0+ KB\nNone\n</pre> In\u00a0[26]: Copied! <pre># write header with operator information as README txt file\nfile = open(f'data/{df_dupfree.name}/{df_dupfree.name}_dupfree_README.txt', 'w')\nfor line in header:\n    file.write(line+'\\n')\nfile.close()\n</pre> # write header with operator information as README txt file file = open(f'data/{df_dupfree.name}/{df_dupfree.name}_dupfree_README.txt', 'w') for line in header:     file.write(line+'\\n') file.close() In\u00a0[27]: Copied! <pre>fn = utf.find(df_dupfree.name, f'data/{df_dupfree.name}')\nprint(fn)\nif fn != []:\n    print('----------------------------------------------------')\n    print('Sucessfully finished the duplicate finalising process!'.upper())\n    print('----------------------------------------------------')\n    print('Saved the final output files in:')\n    print()\n    for ff in fn:\n        print('   '+os.getcwd()+'/%s.'%ff)\n    print()\n    print('The duplicate detection process is now finished and the duplicate free database is ready to use!')\nelse:\n    print('!!!!!!!!!!!!WARNING!!!!!!!!!!!')\n    print(f'Final output file is missing at data/{df_dupfree.name}.')\n    print()\n    print('Please re-run the notebook to complete duplicate finalising process.')\n</pre> fn = utf.find(df_dupfree.name, f'data/{df_dupfree.name}') print(fn) if fn != []:     print('----------------------------------------------------')     print('Sucessfully finished the duplicate finalising process!'.upper())     print('----------------------------------------------------')     print('Saved the final output files in:')     print()     for ff in fn:         print('   '+os.getcwd()+'/%s.'%ff)     print()     print('The duplicate detection process is now finished and the duplicate free database is ready to use!') else:     print('!!!!!!!!!!!!WARNING!!!!!!!!!!!')     print(f'Final output file is missing at data/{df_dupfree.name}.')     print()     print('Please re-run the notebook to complete duplicate finalising process.') <pre>['data/dod2k_v2.0/dod2k_v2.0_compact.pkl', 'data/dod2k_v2.0/dod2k_v2.0_compact_metadata.csv', 'data/dod2k_v2.0/dod2k_v2.0_compact_year.csv', 'data/dod2k_v2.0/dod2k_v2.0_compact_paleoData_values.csv', 'data/dod2k_v2.0/dod2k_v2.0_README.txt', 'data/dod2k_v2.0/dod2k_v2.0_dupfree_README.txt']\n----------------------------------------------------\nSUCESSFULLY FINISHED THE DUPLICATE FINALISING PROCESS!\n----------------------------------------------------\nSaved the final output files in:\n\n   /home/jupyter-lluecke/dod2k/data/dod2k_v2.0/dod2k_v2.0_compact.pkl.\n   /home/jupyter-lluecke/dod2k/data/dod2k_v2.0/dod2k_v2.0_compact_metadata.csv.\n   /home/jupyter-lluecke/dod2k/data/dod2k_v2.0/dod2k_v2.0_compact_year.csv.\n   /home/jupyter-lluecke/dod2k/data/dod2k_v2.0/dod2k_v2.0_compact_paleoData_values.csv.\n   /home/jupyter-lluecke/dod2k/data/dod2k_v2.0/dod2k_v2.0_README.txt.\n   /home/jupyter-lluecke/dod2k/data/dod2k_v2.0/dod2k_v2.0_dupfree_README.txt.\n\nThe duplicate detection process is now finished and the duplicate free database is ready to use!\n</pre> <p>Import plotting libraries</p> In\u00a0[28]: Copied! <pre>import matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec as GS\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\n\nfrom dod2k_utilities import ut_plot as uplt # contains plotting functions\n</pre> import matplotlib.pyplot as plt from matplotlib.gridspec import GridSpec as GS import cartopy.crs as ccrs import cartopy.feature as cfeature  from dod2k_utilities import ut_plot as uplt # contains plotting functions In\u00a0[29]: Copied! <pre>#%% print some info about the data\ndb_types = df_cleaned['originalDatabase'].unique()\n\ncol = uplt.get_colours(range(len(db_types)), 'tab10', 0, len(db_types))\n#col = ['tab:blue','tab:green', 'tab:grey', 'tab:pink', 'tab:orange']\ncounts      = []\nticks       = []\ncolours     = []\nfor ii, db in enumerate(db_types):\n    cc = df_cleaned['originalDatabase'][(df_cleaned['originalDatabase']==db)].count()\n    counts  += [cc]\n    ticks   += [db.split('(Ocn_103')[0]]\n    colours += [col[ii]]\n</pre> #%% print some info about the data db_types = df_cleaned['originalDatabase'].unique()  col = uplt.get_colours(range(len(db_types)), 'tab10', 0, len(db_types)) #col = ['tab:blue','tab:green', 'tab:grey', 'tab:pink', 'tab:orange'] counts      = [] ticks       = [] colours     = [] for ii, db in enumerate(db_types):     cc = df_cleaned['originalDatabase'][(df_cleaned['originalDatabase']==db)].count()     counts  += [cc]     ticks   += [db.split('(Ocn_103')[0]]     colours += [col[ii]] In\u00a0[30]: Copied! <pre># plot a bar chart of the number of proxy types included in the dataset\nfig = plt.figure(figsize=(8,4), dpi=200)\nax  = plt.gca()\nplt.bar(range(len(ticks)), counts, color=colours)\nplt.xlabel('database')\nplt.ylabel('count')\nax.set_xticks(range(len(ticks)), ticks, rotation=45, ha='right')\n#ax.set_xticklabels(proxy_types, rotation=45, ha='right')\nplt.title('original database')\nplt.show()\nfig.tight_layout()\nutf.figsave(fig, 'SF_removed_recs_barchart_databases', add='%s/'%df_dupfree.name)\n</pre> # plot a bar chart of the number of proxy types included in the dataset fig = plt.figure(figsize=(8,4), dpi=200) ax  = plt.gca() plt.bar(range(len(ticks)), counts, color=colours) plt.xlabel('database') plt.ylabel('count') ax.set_xticks(range(len(ticks)), ticks, rotation=45, ha='right') #ax.set_xticklabels(proxy_types, rotation=45, ha='right') plt.title('original database') plt.show() fig.tight_layout() utf.figsave(fig, 'SF_removed_recs_barchart_databases', add='%s/'%df_dupfree.name) <pre>saved figure in /figs/dod2k_v2.0//SF_removed_recs_barchart_databases.pdf\n</pre> In\u00a0[31]: Copied! <pre>#%% print some info about the data\nproxy_types   = df_cleaned['paleoData_proxy'].unique()\narchive_types = df_cleaned['archiveType'].unique()\nprint(proxy_types)\nprint(archive_types)\ncol = uplt.get_colours(range(0,len(archive_types)), 'Accent', -1, len(archive_types))\ncounts      = []\nticks       = []\ncolours     = []\nfor ii, at in enumerate(archive_types):\n    proxy_types   = df_cleaned['paleoData_proxy'][df_cleaned['archiveType']==at].unique()\n    for pt in proxy_types:\n        cc = df_cleaned['paleoData_proxy'][(df_cleaned['paleoData_proxy']==pt)&amp;(df_cleaned['archiveType']==at)].count()\n        # print('%25s'%pt+': '+str(cc))\n        counts  += [cc]\n        ticks   += [at+': '+pt]\n        colours += [col[ii]]\n        \n</pre> #%% print some info about the data proxy_types   = df_cleaned['paleoData_proxy'].unique() archive_types = df_cleaned['archiveType'].unique() print(proxy_types) print(archive_types) col = uplt.get_colours(range(0,len(archive_types)), 'Accent', -1, len(archive_types)) counts      = [] ticks       = [] colours     = [] for ii, at in enumerate(archive_types):     proxy_types   = df_cleaned['paleoData_proxy'][df_cleaned['archiveType']==at].unique()     for pt in proxy_types:         cc = df_cleaned['paleoData_proxy'][(df_cleaned['paleoData_proxy']==pt)&amp;(df_cleaned['archiveType']==at)].count()         # print('%25s'%pt+': '+str(cc))         counts  += [cc]         ticks   += [at+': '+pt]         colours += [col[ii]]          <pre>['ring width' 'residual chronology' 'ARSTAN' 'maximum latewood density'\n 'reflectance' 'd18O' 'd13C' 'Sr/Ca' 'Mg/Ca' 'temperature' 'historical'\n 'varve thickness' 'ice melt' 'alkenone' 'chironomid' 'Uk37' 'borehole'\n 'pollen' 'dinocyst' 'count' 'concentration' 'chrysophyte assemblage'\n 'calcification rate' 'foraminifera' 'dust' 'chloride' 'sulfate' 'nitrate'\n 'thickness' 'dD' 'TEX86' 'effective precipitation' 'diatom' 'multiproxy'\n 'humidification index' 'accumulation rate' 'sodium' 'growth rate']\n['Wood' 'Coral' 'LakeSediment' 'MarineSediment' 'Documents' 'GlacierIce'\n 'Borehole' 'Sclerosponge' 'Speleothem' 'Other' 'GroundIce' 'MolluskShell']\n</pre> In\u00a0[32]: Copied! <pre># plot a bar chart of the number of proxy types included in the dataset\nfig = plt.figure(figsize=(12, 6), dpi=150)\nax  = plt.gca()\nplt.bar(range(len(ticks)), counts, color=colours)\n\nplt.xlabel('proxy type')\nplt.ylabel('count')\nax.set_xticks(range(len(ticks)), ticks, rotation=45, ha='right')\n#ax.set_xticklabels(proxy_types, rotation=45, ha='right')\nax.set_yscale('log')\nplt.title('removed proxy types')\nplt.show()\nfig.tight_layout()\nutf.figsave(fig, 'SF_removed_recs_barchart_proxytypes', add='%s/'%df_dupfree.name)\n</pre> # plot a bar chart of the number of proxy types included in the dataset fig = plt.figure(figsize=(12, 6), dpi=150) ax  = plt.gca() plt.bar(range(len(ticks)), counts, color=colours)  plt.xlabel('proxy type') plt.ylabel('count') ax.set_xticks(range(len(ticks)), ticks, rotation=45, ha='right') #ax.set_xticklabels(proxy_types, rotation=45, ha='right') ax.set_yscale('log') plt.title('removed proxy types') plt.show() fig.tight_layout() utf.figsave(fig, 'SF_removed_recs_barchart_proxytypes', add='%s/'%df_dupfree.name)  <pre>saved figure in /figs/dod2k_v2.0//SF_removed_recs_barchart_proxytypes.pdf\n</pre> In\u00a0[\u00a0]: Copied! <pre>#%% plot the spatial distribution of the removeed records\nproxy_lats = df_cleaned['geo_meanLat'].values\nproxy_lons = df_cleaned['geo_meanLon'].values\n\n# plots the map\nfig = plt.figure(figsize=(10, 5), dpi=200)\ngrid = GS(1, 3)\n\nax = plt.subplot(grid[:, -2:], projection=ccrs.Robinson()) # create axis with Robinson projection of globe\nax.stock_img()\nax.add_feature(cfeature.LAND) # adds land features\nax.coastlines() # adds coastline features\n\n\nmt = 'ov^&lt;&gt;pP*XDd'*10 # generates string of marker types\narchive_marker = {aa: mm for aa, mm in zip(archive_types, mt)} # attributes marker type to each archive type\narchive_colour = {aa: cc for aa, cc in zip(archive_types, col)}\n\n# loop through the data to generate a scatter plot of each data record:\n# 1st loop: go through archive types individually (determines marker type)\n# 2nd loop: through paleo proxy types attributed to the specific archive, which is colour coded\nfor jj, at in enumerate(archive_types):\n    arch_mask = df_cleaned['archiveType']==at\n    arch_proxy_types = np.unique(df_cleaned['paleoData_proxy'][arch_mask])\n    for ii, pt in enumerate(arch_proxy_types):\n        pt_mask = df_cleaned['paleoData_proxy']==pt\n        at_mask = df_cleaned['archiveType']==at\n        plt.scatter(proxy_lons[pt_mask&amp;at_mask], proxy_lats[pt_mask&amp;at_mask], \n                    transform=ccrs.PlateCarree(), zorder=999,\n                    marker=mt[ii], color=archive_colour[at], \n                    label=at+': '+pt+' ($n=%d$)'% df_cleaned['paleoData_proxy'][(df_cleaned['paleoData_proxy']==pt)&amp;(df_cleaned['archiveType']==at)].count(), \n                    lw=.5, ec='k')\n\nplt.title('removed proxy types')\nplt.legend(bbox_to_anchor=(0.03,1.1), ncol=2, fontsize=9, framealpha=0)\ngrid.tight_layout(fig)\nutf.figsave(fig, 'SF_removed_spatial', add='%s/'%df_dupfree.name)\n</pre> #%% plot the spatial distribution of the removeed records proxy_lats = df_cleaned['geo_meanLat'].values proxy_lons = df_cleaned['geo_meanLon'].values  # plots the map fig = plt.figure(figsize=(10, 5), dpi=200) grid = GS(1, 3)  ax = plt.subplot(grid[:, -2:], projection=ccrs.Robinson()) # create axis with Robinson projection of globe ax.stock_img() ax.add_feature(cfeature.LAND) # adds land features ax.coastlines() # adds coastline features   mt = 'ov^&lt;&gt;pP*XDd'*10 # generates string of marker types archive_marker = {aa: mm for aa, mm in zip(archive_types, mt)} # attributes marker type to each archive type archive_colour = {aa: cc for aa, cc in zip(archive_types, col)}  # loop through the data to generate a scatter plot of each data record: # 1st loop: go through archive types individually (determines marker type) # 2nd loop: through paleo proxy types attributed to the specific archive, which is colour coded for jj, at in enumerate(archive_types):     arch_mask = df_cleaned['archiveType']==at     arch_proxy_types = np.unique(df_cleaned['paleoData_proxy'][arch_mask])     for ii, pt in enumerate(arch_proxy_types):         pt_mask = df_cleaned['paleoData_proxy']==pt         at_mask = df_cleaned['archiveType']==at         plt.scatter(proxy_lons[pt_mask&amp;at_mask], proxy_lats[pt_mask&amp;at_mask],                      transform=ccrs.PlateCarree(), zorder=999,                     marker=mt[ii], color=archive_colour[at],                      label=at+': '+pt+' ($n=%d$)'% df_cleaned['paleoData_proxy'][(df_cleaned['paleoData_proxy']==pt)&amp;(df_cleaned['archiveType']==at)].count(),                      lw=.5, ec='k')  plt.title('removed proxy types') plt.legend(bbox_to_anchor=(0.03,1.1), ncol=2, fontsize=9, framealpha=0) grid.tight_layout(fig) utf.figsave(fig, 'SF_removed_spatial', add='%s/'%df_dupfree.name)  <pre>/tmp/ipykernel_2411350/3409790064.py:36: UserWarning: Tight layout not applied. tight_layout cannot make axes width small enough to accommodate all axes decorations\n  grid.tight_layout(fig)\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/dup_removal/#duplicate-detection-step-3-remove-true-duplicates","title":"Duplicate detection - step 3: remove true duplicates\u00b6","text":""},{"location":"notebooks/dup_removal/#intialisation","title":"Intialisation\u00b6","text":""},{"location":"notebooks/dup_removal/#set-up-working-environment","title":"Set up working environment\u00b6","text":""},{"location":"notebooks/dup_removal/#load-dataset","title":"Load dataset\u00b6","text":""},{"location":"notebooks/dup_removal/#input-operators-credentials","title":"Input operator's credentials\u00b6","text":""},{"location":"notebooks/dup_removal/#apply-duplicate-decisions-to-dataframe","title":"Apply duplicate decisions to dataframe\u00b6","text":""},{"location":"notebooks/dup_removal/#load-duplicate-decisions-from-csv","title":"Load duplicate decisions from csv\u00b6","text":""},{"location":"notebooks/dup_removal/#1-records-to-be-removed","title":"1. Records to be REMOVED\u00b6","text":""},{"location":"notebooks/dup_removal/#2-records-to-be-composited","title":"2. Records to be COMPOSITED\u00b6","text":""},{"location":"notebooks/dup_removal/#3-check-for-overlap-between-duplicates","title":"3. Check for overlap between duplicates\u00b6","text":""},{"location":"notebooks/dup_removal/#4-create-duplicate-free-dataframe","title":"4. Create duplicate free dataframe\u00b6","text":""},{"location":"notebooks/dup_removal/#save-duplicate-free-dataframe","title":"Save duplicate free dataframe\u00b6","text":""},{"location":"notebooks/dup_removal/#save-pickle","title":"save pickle\u00b6","text":""},{"location":"notebooks/dup_removal/#save-csv","title":"save csv\u00b6","text":""},{"location":"notebooks/dup_removal/#summary-and-summary-plots-of-datasets","title":"Summary and summary plots of datasets\u00b6","text":""},{"location":"notebooks/dup_testdatabase/","title":"Duplicate test database","text":"In\u00a0[\u00a0]: Copied! In\u00a0[1]: Copied! <pre># create a duplicate database from ch2k (smallest db)\n</pre> # create a duplicate database from ch2k (smallest db) In\u00a0[2]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n\nimport sys\nimport os\nfrom pathlib import Path\n\n# Add parent directory to path (works from any notebook in notebooks/)\n# the repo_root should be the parent directory of the notebooks folder\ncurrent_dir = Path().resolve()\n# Determine repo root\nif current_dir.name == 'dod2k': repo_root = current_dir\nelif current_dir.parent.name == 'dod2k': repo_root = current_dir.parent\nelse: raise Exception('Please review the repo root structure (see first cell).')\n\n# Update cwd and path only if needed\nif os.getcwd() != str(repo_root):\n    os.chdir(repo_root)\nif str(repo_root) not in sys.path:\n    sys.path.insert(0, str(repo_root))\n\nprint(f\"Repo root: {repo_root}\")\nif str(os.getcwd())==str(repo_root):\n    print(f\"Working directory matches repo root. \")\n</pre> %load_ext autoreload %autoreload 2  import sys import os from pathlib import Path  # Add parent directory to path (works from any notebook in notebooks/) # the repo_root should be the parent directory of the notebooks folder current_dir = Path().resolve() # Determine repo root if current_dir.name == 'dod2k': repo_root = current_dir elif current_dir.parent.name == 'dod2k': repo_root = current_dir.parent else: raise Exception('Please review the repo root structure (see first cell).')  # Update cwd and path only if needed if os.getcwd() != str(repo_root):     os.chdir(repo_root) if str(repo_root) not in sys.path:     sys.path.insert(0, str(repo_root))  print(f\"Repo root: {repo_root}\") if str(os.getcwd())==str(repo_root):     print(f\"Working directory matches repo root. \") <pre>Repo root: /home/jupyter-lluecke/dod2k_v2.0/dod2k\nWorking directory matches repo root. \n</pre> In\u00a0[3]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport datetime\n\nfrom dod2k_utilities import ut_functions as utf # contains utility functions\nfrom dod2k_utilities import ut_duplicate_search as dup # contains utility functions\n</pre> import pandas as pd import numpy as np import datetime  from dod2k_utilities import ut_functions as utf # contains utility functions from dod2k_utilities import ut_duplicate_search as dup # contains utility functions <p>Define the dataset which needs to be screened for duplicates. Input files for the duplicate detection mechanism need to be compact dataframes (<code>pandas</code> dataframes with standardised columns and entry formatting).</p> <p>The function <code>load_compact_dataframe_from_csv</code> loads the dataframe from a <code>csv</code> file from <code>data\\DB\\</code>, with <code>DB</code> the name of the database. The database name (<code>db_name</code>) can be</p> <ul> <li><code>pages2k</code></li> <li><code>ch2k</code></li> <li><code>iso2k</code></li> <li><code>sisal</code></li> <li><code>fe23</code></li> </ul> <p>for the individual databases, or</p> <ul> <li><code>all_merged</code></li> </ul> <p>to load the merged database of all individual databases, or can be any user defined compact dataframe.</p> In\u00a0[4]: Copied! <pre># load dataframe\ndb_name='all_merged' \ndf = utf.load_compact_dataframe_from_csv(db_name)\n\nprint(df.info())\ndf.name = db_name\n</pre> # load dataframe db_name='all_merged'  df = utf.load_compact_dataframe_from_csv(db_name)  print(df.info()) df.name = db_name  <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5320 entries, 0 to 5319\nData columns (total 21 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   archiveType                    5320 non-null   object \n 1   dataSetName                    5320 non-null   object \n 2   datasetId                      5320 non-null   object \n 3   geo_meanElev                   5221 non-null   float32\n 4   geo_meanLat                    5320 non-null   float32\n 5   geo_meanLon                    5320 non-null   float32\n 6   geo_siteName                   5320 non-null   object \n 7   interpretation_direction       5320 non-null   object \n 8   interpretation_seasonality     5320 non-null   object \n 9   interpretation_variable        5320 non-null   object \n 10  interpretation_variableDetail  5320 non-null   object \n 11  originalDataURL                5320 non-null   object \n 12  originalDatabase               5320 non-null   object \n 13  paleoData_notes                5320 non-null   object \n 14  paleoData_proxy                5320 non-null   object \n 15  paleoData_sensorSpecies        5320 non-null   object \n 16  paleoData_units                5320 non-null   object \n 17  paleoData_values               5320 non-null   object \n 18  paleoData_variableName         5320 non-null   object \n 19  year                           5320 non-null   object \n 20  yearUnits                      5320 non-null   object \ndtypes: float32(3), object(18)\nmemory usage: 810.6+ KB\nNone\n</pre> In\u00a0[5]: Copied! <pre>m = 100\nn=1\n\n# Create synthetic duplicates\ndupdf = pd.concat([df[:m]]*n)\n\n# IMPORTANT: Reset index BEFORE the loop\ndupdf = dupdf.reset_index(drop=True)  # This creates a clean 0-99 index\n\n# Now create unique datasetIds\ndupdf['datasetId'] = [f'{i}' for i in range(len(dupdf))]\n\n# Or if you want to keep track of which are duplicates:\nfor i in range(len(dupdf)):\n    original_idx = i % m \n    dupdf.loc[i, 'datasetId'] = f\"record_{original_idx}_copy_{i//m}\"\n\n# Set index at the end if needed\ndupdf.index = range(len(dupdf))#dupdf['datasetId']\n</pre> m = 100 n=1  # Create synthetic duplicates dupdf = pd.concat([df[:m]]*n)  # IMPORTANT: Reset index BEFORE the loop dupdf = dupdf.reset_index(drop=True)  # This creates a clean 0-99 index  # Now create unique datasetIds dupdf['datasetId'] = [f'{i}' for i in range(len(dupdf))]  # Or if you want to keep track of which are duplicates: for i in range(len(dupdf)):     original_idx = i % m      dupdf.loc[i, 'datasetId'] = f\"record_{original_idx}_copy_{i//m}\"  # Set index at the end if needed dupdf.index = range(len(dupdf))#dupdf['datasetId'] In\u00a0[6]: Copied! <pre>dupdf.index\n</pre> dupdf.index Out[6]: <pre>RangeIndex(start=0, stop=100, step=1)</pre> In\u00a0[7]: Copied! <pre>dupdf.datasetId\n</pre> dupdf.datasetId Out[7]: <pre>0      record_0_copy_0\n1      record_1_copy_0\n2      record_2_copy_0\n3      record_3_copy_0\n4      record_4_copy_0\n            ...       \n95    record_95_copy_0\n96    record_96_copy_0\n97    record_97_copy_0\n98    record_98_copy_0\n99    record_99_copy_0\nName: datasetId, Length: 100, dtype: object</pre> In\u00a0[8]: Copied! <pre>print(\"=== BEFORE SAVE ===\")\nprint(\"Type of paleoData_values[0]:\", type(dupdf['paleoData_values'].iloc[0]))\nprint(\"Dtype of paleoData_values[0]:\", dupdf['paleoData_values'].iloc[0].dtype)\nprint(\"Shape:\", dupdf['paleoData_values'].iloc[0].shape)\nprint(\"First 5 values:\", dupdf['paleoData_values'].iloc[0][:5])\nprint(\"String repr:\", repr(str(dupdf['paleoData_values'].iloc[0][:5])))\n</pre> print(\"=== BEFORE SAVE ===\") print(\"Type of paleoData_values[0]:\", type(dupdf['paleoData_values'].iloc[0])) print(\"Dtype of paleoData_values[0]:\", dupdf['paleoData_values'].iloc[0].dtype) print(\"Shape:\", dupdf['paleoData_values'].iloc[0].shape) print(\"First 5 values:\", dupdf['paleoData_values'].iloc[0][:5]) print(\"String repr:\", repr(str(dupdf['paleoData_values'].iloc[0][:5])))  <pre>=== BEFORE SAVE ===\nType of paleoData_values[0]: &lt;class 'numpy.ndarray'&gt;\nDtype of paleoData_values[0]: float32\nShape: (1220,)\nFirst 5 values: [-33.32873 -35.6732  -33.1574  -34.2854  -34.4031 ]\nString repr: '[-33.32873 -35.6732  -33.1574  -34.2854  -34.4031 ]'\n</pre> In\u00a0[9]: Copied! <pre>dupdf = dupdf[sorted(dupdf.columns)]\ndupdf.name='dup_test'\nos.makedirs(f'data/{dupdf.name}/', exist_ok=True)\n</pre> dupdf = dupdf[sorted(dupdf.columns)] dupdf.name='dup_test' os.makedirs(f'data/{dupdf.name}/', exist_ok=True) In\u00a0[10]: Copied! <pre>dupdf.info()\nprint(dupdf.name)\n</pre> dupdf.info() print(dupdf.name) <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 100 entries, 0 to 99\nData columns (total 21 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   archiveType                    100 non-null    object \n 1   dataSetName                    100 non-null    object \n 2   datasetId                      100 non-null    object \n 3   geo_meanElev                   100 non-null    float32\n 4   geo_meanLat                    100 non-null    float32\n 5   geo_meanLon                    100 non-null    float32\n 6   geo_siteName                   100 non-null    object \n 7   interpretation_direction       100 non-null    object \n 8   interpretation_seasonality     100 non-null    object \n 9   interpretation_variable        100 non-null    object \n 10  interpretation_variableDetail  100 non-null    object \n 11  originalDataURL                100 non-null    object \n 12  originalDatabase               100 non-null    object \n 13  paleoData_notes                100 non-null    object \n 14  paleoData_proxy                100 non-null    object \n 15  paleoData_sensorSpecies        100 non-null    object \n 16  paleoData_units                100 non-null    object \n 17  paleoData_values               100 non-null    object \n 18  paleoData_variableName         100 non-null    object \n 19  year                           100 non-null    object \n 20  yearUnits                      100 non-null    object \ndtypes: float32(3), object(18)\nmemory usage: 15.4+ KB\ndup_test\n</pre> In\u00a0[11]: Copied! <pre>dupdf.info()\n</pre> dupdf.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 100 entries, 0 to 99\nData columns (total 21 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   archiveType                    100 non-null    object \n 1   dataSetName                    100 non-null    object \n 2   datasetId                      100 non-null    object \n 3   geo_meanElev                   100 non-null    float32\n 4   geo_meanLat                    100 non-null    float32\n 5   geo_meanLon                    100 non-null    float32\n 6   geo_siteName                   100 non-null    object \n 7   interpretation_direction       100 non-null    object \n 8   interpretation_seasonality     100 non-null    object \n 9   interpretation_variable        100 non-null    object \n 10  interpretation_variableDetail  100 non-null    object \n 11  originalDataURL                100 non-null    object \n 12  originalDatabase               100 non-null    object \n 13  paleoData_notes                100 non-null    object \n 14  paleoData_proxy                100 non-null    object \n 15  paleoData_sensorSpecies        100 non-null    object \n 16  paleoData_units                100 non-null    object \n 17  paleoData_values               100 non-null    object \n 18  paleoData_variableName         100 non-null    object \n 19  year                           100 non-null    object \n 20  yearUnits                      100 non-null    object \ndtypes: float32(3), object(18)\nmemory usage: 15.4+ KB\n</pre> In\u00a0[12]: Copied! <pre># save concatenate dataframe as db_merged\ndupdf.to_pickle(f'data/{dupdf.name}/{dupdf.name}_compact.pkl')\n</pre> # save concatenate dataframe as db_merged dupdf.to_pickle(f'data/{dupdf.name}/{dupdf.name}_compact.pkl') In\u00a0[13]: Copied! <pre># save to a list of csv files (metadata, data, year)\nutf.write_compact_dataframe_to_csv(dupdf)\n</pre> # save to a list of csv files (metadata, data, year) utf.write_compact_dataframe_to_csv(dupdf) <pre>METADATA: datasetId, archiveType, dataSetName, geo_meanElev, geo_meanLat, geo_meanLon, geo_siteName, interpretation_direction, interpretation_seasonality, interpretation_variable, interpretation_variableDetail, originalDataURL, originalDatabase, paleoData_notes, paleoData_proxy, paleoData_sensorSpecies, paleoData_units, paleoData_variableName, yearUnits\nSaved to /home/jupyter-lluecke/dod2k_v2.0/dod2k/data/dup_test/dup_test_compact_%s.csv\n</pre> In\u00a0[14]: Copied! <pre># load dataframe\nprint(utf.load_compact_dataframe_from_csv(dupdf.name).info())\n</pre> # load dataframe print(utf.load_compact_dataframe_from_csv(dupdf.name).info()) <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 100 entries, 0 to 99\nData columns (total 21 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   archiveType                    100 non-null    object \n 1   dataSetName                    100 non-null    object \n 2   datasetId                      100 non-null    object \n 3   geo_meanElev                   100 non-null    float32\n 4   geo_meanLat                    100 non-null    float32\n 5   geo_meanLon                    100 non-null    float32\n 6   geo_siteName                   100 non-null    object \n 7   interpretation_direction       100 non-null    object \n 8   interpretation_seasonality     100 non-null    object \n 9   interpretation_variable        100 non-null    object \n 10  interpretation_variableDetail  100 non-null    object \n 11  originalDataURL                100 non-null    object \n 12  originalDatabase               100 non-null    object \n 13  paleoData_notes                100 non-null    object \n 14  paleoData_proxy                100 non-null    object \n 15  paleoData_sensorSpecies        100 non-null    object \n 16  paleoData_units                100 non-null    object \n 17  paleoData_values               100 non-null    object \n 18  paleoData_variableName         100 non-null    object \n 19  year                           100 non-null    object \n 20  yearUnits                      100 non-null    object \ndtypes: float32(3), object(18)\nmemory usage: 15.4+ KB\nNone\n</pre> In\u00a0[15]: Copied! <pre>dupdf\n</pre> dupdf Out[15]: archiveType dataSetName datasetId geo_meanElev geo_meanLat geo_meanLon geo_siteName interpretation_direction interpretation_seasonality interpretation_variable ... originalDataURL originalDatabase paleoData_notes paleoData_proxy paleoData_sensorSpecies paleoData_units paleoData_values paleoData_variableName year yearUnits 0 GlacierIce Ant-WDC05A.Steig.2013 record_0_copy_0 1806.000000 -79.459999 -112.089996 WDC05A positive Annual temperature ... https://www1.ncdc.noaa.gov/pub/data/paleo/page... PAGES 2k v2.2.0 ; climateInterpretation_seasonality changed - ... d18O nan permil [-33.32873, -35.6732, -33.1574, -34.2854, -34.... d18O [2005.0, 2004.0, 2003.0, 2002.0, 2001.0, 2000.... CE 1 Wood NAm-MtLemon.Briffa.2002 record_1_copy_0 2700.000000 32.500000 -110.800003 Mt. Lemon None None None ... https://www1.ncdc.noaa.gov/pub/data/paleo/page... PAGES 2k v2.2.0 nan ring width PSME cm [2.76, 2.91, 1.88, 2.51, 2.5, 1.79, 0.915, 0.6... ring width [1568.0, 1569.0, 1570.0, 1571.0, 1572.0, 1573.... CE 2 Wood NAm-MtLemon.Briffa.2002 record_2_copy_0 2700.000000 32.500000 -110.800003 Mt. Lemon None None None ... https://www1.ncdc.noaa.gov/pub/data/paleo/page... PAGES 2k v2.2.0 nan ring width PSME nan [1.141, 1.198, 0.881, 1.091, 1.097, 0.873, 0.6... ring width [1568.0, 1569.0, 1570.0, 1571.0, 1572.0, 1573.... CE 3 Wood NAm-MtLemon.Briffa.2002 record_3_copy_0 2700.000000 32.500000 -110.800003 Mt. Lemon None None None ... https://www1.ncdc.noaa.gov/pub/data/paleo/page... PAGES 2k v2.2.0 nan residual chronology PSME nan [1.116, 1.152, 0.768, 1.151, 1.075, 0.811, 0.7... residual chronology [1568.0, 1569.0, 1570.0, 1571.0, 1572.0, 1573.... CE 4 Wood NAm-MtLemon.Briffa.2002 record_4_copy_0 2700.000000 32.500000 -110.800003 Mt. Lemon None None None ... https://www1.ncdc.noaa.gov/pub/data/paleo/page... PAGES 2k v2.2.0 nan ARSTAN PSME nan [1.143, 1.223, 0.876, 1.1, 1.126, 0.874, 0.679... ARSTAN [1568.0, 1569.0, 1570.0, 1571.0, 1572.0, 1573.... CE ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 95 Coral Ocn-Rarotonga_d18O2R.Linsley.2006 record_95_copy_0 -18.299999 -21.237801 -159.827805 Rarotonga None None None ... https://www1.ncdc.noaa.gov/pub/data/paleo/page... PAGES 2k v2.2.0 nan d13C lutea permil [-2.82, -2.91, -3.01, -3.27, -3.12, -2.84, -2.... d13C [1996.91, 1996.78, 1996.66, 1996.53, 1996.41, ... CE 96 Wood Asi-KYRG014.Solomina.2013 record_96_copy_0 69.000000 42.419998 78.970001 KYRG014 positive Summer temperature ... https://www1.ncdc.noaa.gov/pub/data/paleo/page... PAGES 2k v2.2.0 nan ring width nan nan [0.885, 1.266, 0.865, 0.979, 1.262, 1.032, 1.2... ring width [1551.0, 1552.0, 1553.0, 1554.0, 1555.0, 1556.... CE 97 Coral Ocn-Lombok.Charles.2003 record_97_copy_0 -3.000000 -8.247300 115.575699 Lombok negative subannual temperature ... https://www1.ncdc.noaa.gov/pub/data/paleo/page... PAGES 2k v2.2.0 ; climateInterpretation_seasonality changed - ... d18O NA permil [-5.284, -5.114, -5.333, -5.365, -5.237, -5.36... d18O [1990.0, 1989.92, 1989.83, 1989.75, 1989.67, 1... CE 98 LakeSediment Arc-HudsonLake.Clegg.2011 record_98_copy_0 657.000000 61.900002 -145.660004 Hudson Lake positive Jul temperature ... https://www1.ncdc.noaa.gov/pub/data/paleo/page... PAGES 2k v2.2.0 nan chironomid nan degC [12.4427, 11.8305, 11.9809, 12.1493, 12.684, 1... temperature [1996.8, 1982.85, 1963.95, 1952.0, 1934.4, 190... CE 99 Coral Ocn-SavusavuBayFiji.Bagnato.2005 record_99_copy_0 -2.000000 -16.820000 179.229996 Savusavu Bay, Fiji negative Annual temperature ... https://www1.ncdc.noaa.gov/pub/data/paleo/page... PAGES 2k v2.2.0 ; climateInterpretation_seasonality changed - ... d18O heliopora permil [-0.046, -0.134, 0.03, 0.363, 0.224, -0.14, 0.... d18O [2001.0, 2000.0, 1999.0, 1998.0, 1997.0, 1996.... CE <p>100 rows \u00d7 21 columns</p> In\u00a0[16]: Copied! <pre>dupdf_reloaded = utf.load_compact_dataframe_from_csv('dup_test')\n\nprint(\"\\n=== AFTER RELOAD ===\")\nprint(\"Type of paleoData_values[0]:\", type(dupdf_reloaded['paleoData_values'].iloc[0]))\nprint(\"Dtype of paleoData_values[0]:\", dupdf_reloaded['paleoData_values'].iloc[0].dtype)\nprint(\"Shape:\", dupdf_reloaded['paleoData_values'].iloc[0].shape)\nprint(\"First 5 values:\", dupdf_reloaded['paleoData_values'].iloc[0][:5])\nprint(\"String repr:\", repr(str(dupdf_reloaded['paleoData_values'].iloc[0][:5])))\n\n# Direct comparison\nprint(\"\\n=== COMPARISON ===\")\nprint(\"Arrays equal?:\", np.array_equal(dupdf['paleoData_values'].iloc[0], \n                                       dupdf_reloaded['paleoData_values'].iloc[0]))\nprint(\"Arrays allclose?:\", np.allclose(dupdf['paleoData_values'].iloc[0], \n                                       dupdf_reloaded['paleoData_values'].iloc[0], \n                                       equal_nan=True))\n</pre> dupdf_reloaded = utf.load_compact_dataframe_from_csv('dup_test')  print(\"\\n=== AFTER RELOAD ===\") print(\"Type of paleoData_values[0]:\", type(dupdf_reloaded['paleoData_values'].iloc[0])) print(\"Dtype of paleoData_values[0]:\", dupdf_reloaded['paleoData_values'].iloc[0].dtype) print(\"Shape:\", dupdf_reloaded['paleoData_values'].iloc[0].shape) print(\"First 5 values:\", dupdf_reloaded['paleoData_values'].iloc[0][:5]) print(\"String repr:\", repr(str(dupdf_reloaded['paleoData_values'].iloc[0][:5])))  # Direct comparison print(\"\\n=== COMPARISON ===\") print(\"Arrays equal?:\", np.array_equal(dupdf['paleoData_values'].iloc[0],                                         dupdf_reloaded['paleoData_values'].iloc[0])) print(\"Arrays allclose?:\", np.allclose(dupdf['paleoData_values'].iloc[0],                                         dupdf_reloaded['paleoData_values'].iloc[0],                                         equal_nan=True)) <pre>\n=== AFTER RELOAD ===\nType of paleoData_values[0]: &lt;class 'numpy.ndarray'&gt;\nDtype of paleoData_values[0]: float32\nShape: (1220,)\nFirst 5 values: [-33.32873 -35.6732  -33.1574  -34.2854  -34.4031 ]\nString repr: '[-33.32873 -35.6732  -33.1574  -34.2854  -34.4031 ]'\n\n=== COMPARISON ===\nArrays equal?: True\nArrays allclose?: True\n</pre> In\u00a0[17]: Copied! <pre>os.getcwd()\n</pre> os.getcwd() Out[17]: <pre>'/home/jupyter-lluecke/dod2k_v2.0/dod2k'</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/dup_testdatabase/#load-dataset","title":"Load dataset\u00b6","text":""},{"location":"notebooks/dup_testdatabase/#save-duplicate-free-dataframe","title":"Save duplicate free dataframe\u00b6","text":""},{"location":"notebooks/dup_testdatabase/#save-pickle","title":"save pickle\u00b6","text":""},{"location":"notebooks/dup_testdatabase/#save-csv","title":"save csv\u00b6","text":""},{"location":"notebooks/load_ch2k/","title":"Load CoralHydro 2k","text":"<p>load data from Coral Hydro 2k v1.0.1 (https://essd.copernicus.org/articles/15/2081/2023/)</p> <p>Dataset downloaded from LiPDverse: https://lipdverse.org/CoralHydro2k/current_version/</p> <p>Created by Kevin Fan and Lucie Luecke (LL). Base on Feng Zhu and Julien Emile-Geay's template (lipd2df notebook)</p> <p>Update 24/10/25 by LL: streamline and tidy up for publication and documentation Update 21/11/24 by LL : added option to save as csv Update 30/10/24 by LL (v4): added check for empty paleoData_values row Update 29/10/2024 by LL (v4): modified datasetId to create unique identifier for each record.</p> <p>Here we extract a dataframe with the following columns:</p> <ul> <li><code>archiveType</code></li> <li><code>dataSetName</code></li> <li><code>datasetId</code></li> <li><code>geo_meanElev</code></li> <li><code>geo_meanLat</code></li> <li><code>geo_meanLon</code></li> <li><code>geo_siteName</code></li> <li><code>interpretation_direction</code> (new in v2.0)</li> <li><code>interpretation_variable</code></li> <li><code>interpretation_variableDetail</code></li> <li><code>interpretation_seasonality</code> (new in v2.0)</li> <li><code>originalDataURL</code></li> <li><code>originalDatabase</code></li> <li><code>paleoData_notes</code></li> <li><code>paleoData_proxy</code></li> <li><code>paleoData_sensorSpecies</code></li> <li><code>paleoData_units</code></li> <li><code>paleoData_values</code></li> <li><code>paleoData_variableName</code></li> <li><code>year</code></li> <li><code>yearUnits</code></li> </ul> <p>We save a standardised compact dataframe for concatenation to DoD2k</p> <p>Make sure the repo_root is added correctly, it should be: your_root_dir/dod2k This should be the working directory throughout this notebook (and all other notebooks).</p> In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n\nimport sys\nimport os\nfrom pathlib import Path\n\n# Add parent directory to path (works from any notebook in notebooks/)\n# the repo_root should be the parent directory of the notebooks folder\ninit_dir = Path().resolve()\n# Determine repo root\nif init_dir.name == 'dod2k': repo_root = init_dir\nelif init_dir.parent.name == 'dod2k': repo_root = init_dir.parent\nelse: raise Exception('Please review the repo root structure (see first cell).')\n\n# Update cwd and path only if needed\nif os.getcwd() != str(repo_root):\n    os.chdir(repo_root)\nif str(repo_root) not in sys.path:\n    sys.path.insert(0, str(repo_root))\n\nprint(f\"Repo root: {repo_root}\")\nif str(os.getcwd())==str(repo_root):\n    print(f\"Working directory matches repo root. \")\n</pre> %load_ext autoreload %autoreload 2  import sys import os from pathlib import Path  # Add parent directory to path (works from any notebook in notebooks/) # the repo_root should be the parent directory of the notebooks folder init_dir = Path().resolve() # Determine repo root if init_dir.name == 'dod2k': repo_root = init_dir elif init_dir.parent.name == 'dod2k': repo_root = init_dir.parent else: raise Exception('Please review the repo root structure (see first cell).')  # Update cwd and path only if needed if os.getcwd() != str(repo_root):     os.chdir(repo_root) if str(repo_root) not in sys.path:     sys.path.insert(0, str(repo_root))  print(f\"Repo root: {repo_root}\") if str(os.getcwd())==str(repo_root):     print(f\"Working directory matches repo root. \") <pre>Repo root: /home/jupyter-lluecke/dod2k_v2.0/dod2k\nWorking directory matches repo root. \n</pre> In\u00a0[2]: Copied! <pre># Import packages\nimport lipd\nimport pandas as pd\nimport numpy as np\n\nfrom dod2k_utilities import ut_functions as utf # contains utility functions\nfrom dod2k_utilities import ut_plot as uplt # contains plotting functions\n</pre> # Import packages import lipd import pandas as pd import numpy as np  from dod2k_utilities import ut_functions as utf # contains utility functions from dod2k_utilities import ut_plot as uplt # contains plotting functions <p>In order to get the source data, run the cell below. This will download a series of LiPD files into the directory <code>lipdfiles</code></p> <p>Alternatively skip the cell and directly use the files as provided in this directory (see cell below next).</p> In\u00a0[3]: Copied! <pre># # Download the file (use -O to specify output filename)\n# !wget -O data/ch2k/CoralHydro2k1_0_1.zip https://lipdverse.org/CoralHydro2k/current_version/CoralHydro2k1_0_1.zip\n\n# # Unzip to the correct destination\n# !unzip data/ch2k/CoralHydro2k1_0_1.zip -d data/ch2k/ch2k_101\n</pre> # # Download the file (use -O to specify output filename) # !wget -O data/ch2k/CoralHydro2k1_0_1.zip https://lipdverse.org/CoralHydro2k/current_version/CoralHydro2k1_0_1.zip  # # Unzip to the correct destination # !unzip data/ch2k/CoralHydro2k1_0_1.zip -d data/ch2k/ch2k_101 In\u00a0[4]: Copied! <pre># load LiPD files from the given directory\n\nD = lipd.readLipd(str(repo_root)+'/data/ch2k/ch2k_101/');\nTS = lipd.extractTs(D);\nlen(TS)\nos.chdir(repo_root)\n</pre> # load LiPD files from the given directory  D = lipd.readLipd(str(repo_root)+'/data/ch2k/ch2k_101/'); TS = lipd.extractTs(D); len(TS) os.chdir(repo_root) <pre>Disclaimer: LiPD files may be updated and modified to adhere to standards\n\nFound: 179 LiPD file(s)\nreading: CH03BUN01.lpd\nreading: ZI15MER01.lpd\nreading: CO03PAL03.lpd\nreading: CO03PAL02.lpd\nreading: CA13PEL01.lpd\nreading: LI06RAR01.lpd\nreading: CO03PAL07.lpd\nreading: FL18DTO03.lpd\nreading: SM06LKF02.lpd\nreading: UR00MAI01.lpd\nreading: TU95MAD01.lpd\nreading: ZI04IFR01.lpd\nreading: RE18CAY01.lpd\nreading: KU99HOU01.lpd\nreading: OS13NLP01.lpd\nreading: EV98KIR01.lpd\nreading: LI00RAR01.lpd\nreading: NU11PAL01.lpd\nreading: MA08DTO01.lpd\nreading: AB20MEN03.lpd\nreading: CA14TIM01.lpd\nreading: KA17RYU01.lpd\nreading: MC11KIR01.lpd\nreading: AB20MEN09.lpd\nreading: HE08LRA01.lpd\nreading: DA06MAF01.lpd\nreading: SM06LKF01.lpd\nreading: NA09MAL01.lpd\nreading: SW98STP01.lpd\nreading: MU18GSI01.lpd\nreading: ZI14HOU01.lpd\nreading: FL17DTO02.lpd\nreading: DA06MAF02.lpd\nreading: SA19PAL02.lpd\nreading: CO03PAL01.lpd\nreading: ZI16ROD01.lpd\nreading: OS13NGP01.lpd\nreading: CH98PIR01.lpd\nreading: RE19GBR02.lpd\nreading: MU18RED04.lpd\nreading: GR13MAD01.lpd\nreading: XI17HAI01.lpd\nreading: DE14DTO03.lpd\nreading: KL97DAH01.lpd\nreading: QU06RAB01.lpd\nreading: DE14DTO01.lpd\nreading: KU00NIN01.lpd\nreading: TU01SIA01.lpd\nreading: RE19GBR01.lpd\nreading: GR13MAD02.lpd\nreading: AB20MEN07.lpd\nreading: BR19RED01.lpd\nreading: NU09FAN01.lpd\nreading: MU18RED01.lpd\nreading: OS14RIP01.lpd\nreading: DE14DTO02.lpd\nreading: LI04FIJ01.lpd\nreading: EV18ROC01.lpd\nreading: CA13SAP01.lpd\nreading: JI18GAL02.lpd\nreading: TU01LAI01.lpd\nreading: HE13MIS01.lpd\nreading: GU99NAU02.lpd\nreading: ZI15IMP02.lpd\nreading: PF04PBA01.lpd\nreading: SA20FAN02.lpd\nreading: WE09ARR01.lpd\nreading: CO03PAL05.lpd\nreading: XU15BVI01.lpd\nreading: HE18COC02.lpd\nreading: MU18NPI01.lpd\nreading: MO06PED01.lpd\nreading: KR20SAR01.lpd\nreading: SA18GBR01.lpd\nreading: OS14UCP01.lpd\nreading: AB20MEN08.lpd\nreading: HE13MIS02.lpd\nreading: HE10GUA01.lpd\nreading: BO14HTI01.lpd\nreading: DE12ANC01.lpd\nreading: WA17BAN01.lpd\nreading: DR99ABR01.lpd\nreading: RO19MAR01.lpd\nreading: LI06RAR02.lpd\nreading: MU18RED03.lpd\nreading: SW99LIG01.lpd\nreading: SA16CLA01.lpd\nreading: ZI15TAN01.lpd\nreading: RE19GBR03.lpd\nreading: DR00KSB01.lpd\nreading: BO14HTI02.lpd\nreading: MU17DOA01.lpd\nreading: TA18TAS01.lpd\nreading: XU15BVI03.lpd\nreading: DU94URV02.lpd\nreading: AS05GUA01.lpd\nreading: FE09OGA01.lpd\nreading: GU99NAU01.lpd\nreading: SA20FAN01.lpd\nreading: CA13DIA01.lpd\nreading: AL16PUR02.lpd\nreading: CO03PAL10.lpd\nreading: RE19GBR05.lpd\nreading: ZI15IMP01.lpd\nreading: KR20SAR02.lpd\nreading: RO19YUC01.lpd\nreading: ST13MAL01.lpd\nreading: DR00NBB01.lpd\nreading: PF19LAR01.lpd\nreading: AL16YUC01.lpd\nreading: CO03PAL09.lpd\nreading: ZI16ROD02.lpd\nreading: AB20MEN05.lpd\nreading: SH92PUN01.lpd\nreading: KI04MCV01.lpd\nreading: AL16PUR01.lpd\nreading: CH18YOA02.lpd\nreading: DE14DTO04.lpd\nreading: AB20MEN04.lpd\nreading: DE16RED01.lpd\nreading: BA04FIJ02.lpd\nreading: CO03PAL06.lpd\nreading: JI18GAL01.lpd\nreading: CH18YOA01.lpd\nreading: RE19GBR04.lpd\nreading: DO18DAV01.lpd\nreading: GO12SBV01.lpd\nreading: CA07FLI01.lpd\nreading: SW99LIG02.lpd\nreading: MC04PNG01.lpd\nreading: CO93TAR01.lpd\nreading: RO19PAR01.lpd\nreading: CO00MAL01.lpd\nreading: MO20WOA01.lpd\nreading: AB20MEN01.lpd\nreading: QU96ESV01.lpd\nreading: DE13HAI01.lpd\nreading: LI94SEC01.lpd\nreading: ZI15CLE01.lpd\nreading: MU18RED02.lpd\nreading: ZI08MAY01.lpd\nreading: TU01DEP01.lpd\nreading: CO03PAL04.lpd\nreading: RA19PAI01.lpd\nreading: AB15BHB01.lpd\nreading: FL18DTO01.lpd\nreading: MO20KOI01.lpd\nreading: DU94URV01.lpd\nreading: CO03PAL08.lpd\nreading: WU14CLI01.lpd\nreading: ZI14TUR01.lpd\nreading: AB20MEN02.lpd\nreading: LI99CLI01.lpd\nreading: ZI15BUN01.lpd\nreading: FE18RUS01.lpd\nreading: WU13TON01.lpd\nreading: KI14PAR01.lpd\nreading: ZI14IFR02.lpd\nreading: XU15BVI02.lpd\nreading: KI08PAR01.lpd\nreading: AB20MEN06.lpd\nreading: AB08MEN01.lpd\nreading: NU09KIR01.lpd\nreading: RI10PBL01.lpd\nreading: CA14BUT01.lpd\nreading: FL18DTO02.lpd\nreading: FL18DTO04.lpd\nreading: BA04FIJ01.lpd\nreading: HE02GBR01.lpd\nreading: GO08BER01.lpd\nreading: CA13TUR01.lpd\nreading: LI06FIJ01.lpd\nreading: HE18COC01.lpd\nreading: FL17DTO01.lpd\nreading: BO99MOO01.lpd\nreading: CH03LOM01.lpd\nreading: SA19PAL01.lpd\nreading: CH97BVB01.lpd\nreading: RA20TAI01.lpd\nFinished read: 179 records\nextracting paleoData...\nextracting: CH03BUN01\nextracting: ZI15MER01\nextracting: CO03PAL03\nextracting: CO03PAL02\nextracting: CA13PEL01\nextracting: LI06RAR01\nextracting: CO03PAL07\nextracting: FL18DTO03\nextracting: SM06LKF02\nextracting: UR00MAI01\nextracting: TU95MAD01\nextracting: ZI04IFR01\nextracting: RE18CAY01\nextracting: KU99HOU01\nextracting: OS13NLP01\nextracting: EV98KIR01\nextracting: LI00RAR01\nextracting: NU11PAL01\nextracting: MA08DTO01\nextracting: AB20MEN03\nextracting: CA14TIM01\nextracting: KA17RYU01\nextracting: MC11KIR01\nextracting: AB20MEN09\nextracting: HE08LRA01\nextracting: DA06MAF01\nextracting: SM06LKF01\nextracting: NA09MAL01\nextracting: SW98STP01\nextracting: MU18GSI01\nextracting: ZI14HOU01\nextracting: FL17DTO02\nextracting: DA06MAF02\nextracting: SA19PAL02\nextracting: CO03PAL01\nextracting: ZI16ROD01\nextracting: OS13NGP01\nextracting: CH98PIR01\nextracting: RE19GBR02\nextracting: MU18RED04\nextracting: GR13MAD01\nextracting: XI17HAI01\nextracting: DE14DTO03\nextracting: KL97DAH01\nextracting: QU06RAB01\nextracting: DE14DTO01\nextracting: KU00NIN01\nextracting: TU01SIA01\nextracting: RE19GBR01\nextracting: GR13MAD02\nextracting: AB20MEN07\nextracting: BR19RED01\nextracting: NU09FAN01\nextracting: MU18RED01\nextracting: OS14RIP01\nextracting: DE14DTO02\nextracting: LI04FIJ01\nextracting: EV18ROC01\nextracting: CA13SAP01\nextracting: JI18GAL02\nextracting: TU01LAI01\nextracting: HE13MIS01\nextracting: GU99NAU02\nextracting: ZI15IMP02\nextracting: PF04PBA01\nextracting: SA20FAN02\nextracting: WE09ARR01\nextracting: CO03PAL05\nextracting: XU15BVI01\nextracting: HE18COC02\nextracting: MU18NPI01\nextracting: MO06PED01\nextracting: KR20SAR01\nextracting: SA18GBR01\nextracting: OS14UCP01\nextracting: AB20MEN08\nextracting: HE13MIS02\nextracting: HE10GUA01\nextracting: BO14HTI01\nextracting: DE12ANC01\nextracting: WA17BAN01\nextracting: DR99ABR01\nextracting: RO19MAR01\nextracting: LI06RAR02\nextracting: MU18RED03\nextracting: SW99LIG01\nextracting: SA16CLA01\nextracting: ZI15TAN01\nextracting: RE19GBR03\nextracting: DR00KSB01\nextracting: BO14HTI02\nextracting: MU17DOA01\nextracting: TA18TAS01\nextracting: XU15BVI03\nextracting: DU94URV02\nextracting: AS05GUA01\nextracting: FE09OGA01\nextracting: GU99NAU01\nextracting: SA20FAN01\nextracting: CA13DIA01\nextracting: AL16PUR02\nextracting: CO03PAL10\nextracting: RE19GBR05\nextracting: ZI15IMP01\nextracting: KR20SAR02\nextracting: RO19YUC01\nextracting: ST13MAL01\nextracting: DR00NBB01\nextracting: PF19LAR01\nextracting: AL16YUC01\nextracting: CO03PAL09\nextracting: ZI16ROD02\nextracting: AB20MEN05\nextracting: SH92PUN01\nextracting: KI04MCV01\nextracting: AL16PUR01\nextracting: CH18YOA02\nextracting: DE14DTO04\nextracting: AB20MEN04\nextracting: DE16RED01\nextracting: BA04FIJ02\nextracting: CO03PAL06\nextracting: JI18GAL01\nextracting: CH18YOA01\nextracting: RE19GBR04\nextracting: DO18DAV01\nextracting: GO12SBV01\nextracting: CA07FLI01\nextracting: SW99LIG02\nextracting: MC04PNG01\nextracting: CO93TAR01\nextracting: RO19PAR01\nextracting: CO00MAL01\nextracting: MO20WOA01\nextracting: AB20MEN01\nextracting: QU96ESV01\nextracting: DE13HAI01\nextracting: LI94SEC01\nextracting: ZI15CLE01\nextracting: MU18RED02\nextracting: ZI08MAY01\nextracting: TU01DEP01\nextracting: CO03PAL04\nextracting: RA19PAI01\nextracting: AB15BHB01\nextracting: FL18DTO01\nextracting: MO20KOI01\nextracting: DU94URV01\nextracting: CO03PAL08\nextracting: WU14CLI01\nextracting: ZI14TUR01\nextracting: AB20MEN02\nextracting: LI99CLI01\nextracting: ZI15BUN01\nextracting: FE18RUS01\nextracting: WU13TON01\nextracting: KI14PAR01\nextracting: ZI14IFR02\nextracting: XU15BVI02\nextracting: KI08PAR01\nextracting: AB20MEN06\nextracting: AB08MEN01\nextracting: NU09KIR01\nextracting: RI10PBL01\nextracting: CA14BUT01\nextracting: FL18DTO02\nextracting: FL18DTO04\nextracting: BA04FIJ01\nextracting: HE02GBR01\nextracting: GO08BER01\nextracting: CA13TUR01\nextracting: LI06FIJ01\nextracting: HE18COC01\nextracting: FL17DTO01\nextracting: BO99MOO01\nextracting: CH03LOM01\nextracting: SA19PAL01\nextracting: CH97BVB01\nextracting: RA20TAI01\nCreated time series: 608 entries\n</pre> In\u00a0[5]: Copied! <pre># for ii in range(len(TS)):\n#     if np.any(['climate' in key for key in TS[ii].keys()]):\n#         print(TS[ii].keys())\n</pre> # for ii in range(len(TS)): #     if np.any(['climate' in key for key in TS[ii].keys()]): #         print(TS[ii].keys()) <p>Create empty dataframe with set of columns for compact dataframe, and populate with the LiPD data</p> In\u00a0[6]: Copied! <pre>col_str=['archiveType', 'dataSetName', 'datasetId', 'geo_meanElev', 'geo_meanLat', 'geo_meanLon', 'geo_siteName', \n         'originalDataUrl', 'paleoData_notes', 'paleoData_variableName',\n         'paleoData_archiveSpecies','paleoData_units', 'paleoData_values', 'year']\n\ndf_tmp = pd.DataFrame(index=range(len(TS)), columns=col_str)\n</pre> col_str=['archiveType', 'dataSetName', 'datasetId', 'geo_meanElev', 'geo_meanLat', 'geo_meanLon', 'geo_siteName',           'originalDataUrl', 'paleoData_notes', 'paleoData_variableName',          'paleoData_archiveSpecies','paleoData_units', 'paleoData_values', 'year']  df_tmp = pd.DataFrame(index=range(len(TS)), columns=col_str) <p>Start by populating paleoData_variableName (paleoData_proxy in dod2k standard terms)</p> In\u00a0[7]: Copied! <pre># loop over the timeseries and pick those for global temperature analysis\ni = 0                                                                                                                \nfor ts in TS: #for every time series\n    # need to filter these variables in the list\n    if ts['paleoData_variableName'] not in ['year', 'd18OUncertainty', 'SrCaUncertainty']: #filter out all ts with thee three as the var name\n        for name in col_str:     #for each of the 12 main keys, shove the wanted data into the df                                                                                    \n            try:\n                df_tmp.loc[i, name] = ts[name]                                                                       \n            except:\n                df_tmp.loc[i, name] = np.nan\n    i += 1 \n        \n# drop the rows with all NaNs (those not for global temperature analysis)\ndf = df_tmp.dropna(how='all')\n</pre> # loop over the timeseries and pick those for global temperature analysis i = 0                                                                                                                 for ts in TS: #for every time series     # need to filter these variables in the list     if ts['paleoData_variableName'] not in ['year', 'd18OUncertainty', 'SrCaUncertainty']: #filter out all ts with thee three as the var name         for name in col_str:     #for each of the 12 main keys, shove the wanted data into the df                                                                                                 try:                 df_tmp.loc[i, name] = ts[name]                                                                                    except:                 df_tmp.loc[i, name] = np.nan     i += 1           # drop the rows with all NaNs (those not for global temperature analysis) df = df_tmp.dropna(how='all') <ul> <li>Now check that paleoData_variableName has been correctly populated and does not contain NaNs:</li> </ul> In\u00a0[8]: Copied! <pre># double check the variable names we have\nset(df['paleoData_variableName'])\n</pre> # double check the variable names we have set(df['paleoData_variableName']) Out[8]: <pre>{'SrCa', 'SrCa_annual', 'd18O', 'd18O_annual', 'd18O_sw', 'd18O_sw_annual'}</pre> <ul> <li>Add more metadata to the dataframe, including originalDatabase, yearUnits, interpretation_variable (these are added manually and not from the LiPD files)</li> </ul> In\u00a0[9]: Copied! <pre># KF: adding original dataset name and yearUnits\ndf.insert(7, 'originalDatabase', ['CoralHydro2k v1.0.1']*len(df))\ndf.insert(len(df.columns), 'yearUnits', ['CE'] * len(df))\ndf.insert(1, 'interpretation_variable', ['N/A']*len(df))\ndf.insert(1, 'interpretation_variableDetail', ['N/A']*len(df))\ndf.insert(1, 'interpretation_seasonality', ['N/A']*len(df))\ndf.insert(1, 'interpretation_direction', ['N/A']*len(df))\ndf.insert(1, 'paleoData_proxy', df['paleoData_variableName'])\n</pre> # KF: adding original dataset name and yearUnits df.insert(7, 'originalDatabase', ['CoralHydro2k v1.0.1']*len(df)) df.insert(len(df.columns), 'yearUnits', ['CE'] * len(df)) df.insert(1, 'interpretation_variable', ['N/A']*len(df)) df.insert(1, 'interpretation_variableDetail', ['N/A']*len(df)) df.insert(1, 'interpretation_seasonality', ['N/A']*len(df)) df.insert(1, 'interpretation_direction', ['N/A']*len(df)) df.insert(1, 'paleoData_proxy', df['paleoData_variableName']) <ul> <li>Rename columns to fit naming conventions</li> </ul> In\u00a0[10]: Copied! <pre>df = df.rename(columns={'originalDataUrl': 'originalDataURL', 'paleoData_archiveSpecies': 'paleoData_sensorSpecies'})\n</pre> df = df.rename(columns={'originalDataUrl': 'originalDataURL', 'paleoData_archiveSpecies': 'paleoData_sensorSpecies'}) <p>Assign interpretation_variable based on paleoData_proxy type:</p> <ul> <li>d18O are temperature and moisture sensitive</li> <li>d18O_sw are misture sensitive (sw=seawater)</li> <li>SrCa are temperature sensitive</li> </ul> In\u00a0[11]: Copied! <pre># d18O is temperature and moisture\ndf.loc[np.isin(df['paleoData_proxy'], ['d18O', 'd18O_annual']), 'interpretation_variable']='temperature+moisture' \ndf.loc[np.isin(df['paleoData_proxy'], ['d18O', 'd18O_annual']), 'interpretation_variableDetail']='temperature+moisture - manually assigned by DoD2k authors for paleoData_proxy = d18O' \n# d18O_sw is moisture\ndf.loc[np.isin(df['paleoData_proxy'], ['d18O_sw', 'd18O_sw_annual']), 'interpretation_variable']='moisture' \ndf.loc[np.isin(df['paleoData_proxy'], ['d18O_sw', 'd18O_sw_annual']), 'interpretation_variableDetail']='moisture - manually assigned by DoD2k authors for paleoData_proxy = d18O_sw' \n# SrCa is temperature\ndf.loc[np.isin(df['paleoData_proxy'], ['SrCa', 'SrCa_annual']), 'interpretation_variable']='temperature' \ndf.loc[np.isin(df['paleoData_proxy'], ['SrCa', 'SrCa_annual']), 'interpretation_variableDetail']='temperature - manually assigned by DoD2k authors for paleoData_proxy = Sr/Ca' \n</pre> # d18O is temperature and moisture df.loc[np.isin(df['paleoData_proxy'], ['d18O', 'd18O_annual']), 'interpretation_variable']='temperature+moisture'  df.loc[np.isin(df['paleoData_proxy'], ['d18O', 'd18O_annual']), 'interpretation_variableDetail']='temperature+moisture - manually assigned by DoD2k authors for paleoData_proxy = d18O'  # d18O_sw is moisture df.loc[np.isin(df['paleoData_proxy'], ['d18O_sw', 'd18O_sw_annual']), 'interpretation_variable']='moisture'  df.loc[np.isin(df['paleoData_proxy'], ['d18O_sw', 'd18O_sw_annual']), 'interpretation_variableDetail']='moisture - manually assigned by DoD2k authors for paleoData_proxy = d18O_sw'  # SrCa is temperature df.loc[np.isin(df['paleoData_proxy'], ['SrCa', 'SrCa_annual']), 'interpretation_variable']='temperature'  df.loc[np.isin(df['paleoData_proxy'], ['SrCa', 'SrCa_annual']), 'interpretation_variableDetail']='temperature - manually assigned by DoD2k authors for paleoData_proxy = Sr/Ca'   <p>Now filter for certain records:</p> <ul> <li>exclude sw data</li> <li>drop <code>_annual</code> tag</li> <li>rename <code>SrCa</code> to <code>Sr/Ca</code> to match the standard terminology</li> <li>rename <code>coral</code> to <code>Coral</code> to match the standard terminology</li> <li>drop rows with no data</li> </ul> In\u00a0[12]: Copied! <pre>import re\n# KF: Extract and exclude sw values\ndf_sw = df[df['paleoData_proxy'].isin(['d18O_sw', 'd18O_sw_annual'])]\ndf = df[df['paleoData_proxy'].isin(['d18O_sw', 'd18O_sw_annual']) == False]\n\n# KF: Turn annual measurements into regular\ndf_annual = df[df['paleoData_proxy'].isin(['SrCa_annual', 'd18O_annual'])]\nfor key in ['paleoData_proxy', 'paleoData_variableName']:\n    df [key] = df[key].apply(lambda x: re.match(r'(.*)_annual', x).group(1) if re.match(r'(.*)_annual', x) else x)\n\n    # KF: Replace SrCa with Sr/Ca for concat consistency\n    df[key] = df[key].apply(lambda x: 'Sr/Ca' if re.match('SrCa', x) else x)\n\n# KF: Temp cleaning rows with NAN in year\nlength = len(df['year'])\ndf = df[df['year'].notna()]\ndf = df[df['year'].map(lambda x: len(x) &gt; 0)]\ndf = df[df['paleoData_values'].map(lambda x: not any(pd.isnull(x)))]\nprint('Number of rows discarded: ', (length - len(df['year'])))\n\n\ndf['archiveType'] = df['archiveType'].replace({'coral': 'Coral'})\n\n# # KF: Make datasetIds unique\n# df['datasetId'] = df['datasetId'] + np.array(df.index, dtype = str)\n</pre> import re # KF: Extract and exclude sw values df_sw = df[df['paleoData_proxy'].isin(['d18O_sw', 'd18O_sw_annual'])] df = df[df['paleoData_proxy'].isin(['d18O_sw', 'd18O_sw_annual']) == False]  # KF: Turn annual measurements into regular df_annual = df[df['paleoData_proxy'].isin(['SrCa_annual', 'd18O_annual'])] for key in ['paleoData_proxy', 'paleoData_variableName']:     df [key] = df[key].apply(lambda x: re.match(r'(.*)_annual', x).group(1) if re.match(r'(.*)_annual', x) else x)      # KF: Replace SrCa with Sr/Ca for concat consistency     df[key] = df[key].apply(lambda x: 'Sr/Ca' if re.match('SrCa', x) else x)  # KF: Temp cleaning rows with NAN in year length = len(df['year']) df = df[df['year'].notna()] df = df[df['year'].map(lambda x: len(x) &gt; 0)] df = df[df['paleoData_values'].map(lambda x: not any(pd.isnull(x)))] print('Number of rows discarded: ', (length - len(df['year'])))   df['archiveType'] = df['archiveType'].replace({'coral': 'Coral'})  # # KF: Make datasetIds unique # df['datasetId'] = df['datasetId'] + np.array(df.index, dtype = str) <pre>Number of rows discarded:  0\n</pre> In\u00a0[13]: Copied! <pre># KF: Type-checking\n\ndf = df.astype({'archiveType': str, 'dataSetName': str, 'datasetId': str, 'geo_meanElev': np.float32, 'geo_meanLat': np.float32, 'geo_meanLon': np.float32, 'geo_siteName': str, \n                    'originalDatabase': str, 'originalDataURL': str, 'paleoData_notes': str, 'paleoData_proxy': str, 'paleoData_units': str, 'yearUnits': str})\ndf['year']             = df['year'].map(lambda x: np.array(x, dtype = np.float32))\ndf['paleoData_values'] = df['paleoData_values'].map(lambda x: np.array(x, dtype = np.float32))\n</pre> # KF: Type-checking  df = df.astype({'archiveType': str, 'dataSetName': str, 'datasetId': str, 'geo_meanElev': np.float32, 'geo_meanLat': np.float32, 'geo_meanLon': np.float32, 'geo_siteName': str,                      'originalDatabase': str, 'originalDataURL': str, 'paleoData_notes': str, 'paleoData_proxy': str, 'paleoData_units': str, 'yearUnits': str}) df['year']             = df['year'].map(lambda x: np.array(x, dtype = np.float32)) df['paleoData_values'] = df['paleoData_values'].map(lambda x: np.array(x, dtype = np.float32)) <p>Include Common Era data only</p> In\u00a0[14]: Copied! <pre>for ii in df.index:\n    year = np.array(df.at[ii, 'year'], dtype=float)\n    vals = np.array(df.at[ii, 'paleoData_values'], dtype=float)\n    df.at[ii, 'year']             = year[year&gt;=1]\n    df.at[ii, 'paleoData_values'] = vals[year&gt;=1]\n</pre> for ii in df.index:     year = np.array(df.at[ii, 'year'], dtype=float)     vals = np.array(df.at[ii, 'paleoData_values'], dtype=float)     df.at[ii, 'year']             = year[year&gt;=1]     df.at[ii, 'paleoData_values'] = vals[year&gt;=1] <p>Note that the datasetId is not unique for each record and thus we added an additional array of strings to make the datasetId unique.</p> In\u00a0[15]: Copied! <pre>#  check that the datasetId is unique \nprint(len(df.datasetId.unique()))\n# make datasetId unique by simply adding index number\ndf.datasetId=df.apply(lambda x: x.datasetId.replace('ch2k','ch2k_')+'_'+str(x.name), axis=1)\n# check uniqueness - problem solved.\nprint(len(df.datasetId.unique()))\n</pre> #  check that the datasetId is unique  print(len(df.datasetId.unique())) # make datasetId unique by simply adding index number df.datasetId=df.apply(lambda x: x.datasetId.replace('ch2k','ch2k_')+'_'+str(x.name), axis=1) # check uniqueness - problem solved. print(len(df.datasetId.unique())) <pre>179\n272\n</pre> <p>mask out nans and set fill value, then later drop</p> In\u00a0[16]: Copied! <pre>drop_inds = []\nfor ii in df.index:\n    try:\n        year = np.array(df.at[ii, 'year'], dtype=float)\n        vals = np.array(df.at[ii, 'paleoData_values'], dtype=float)\n        df.at[ii, 'year']             = year[year&gt;=1]\n        df.at[ii, 'paleoData_values'] = vals[year&gt;=1]\n    except:\n        # print\n        df.at[ii, 'paleoData_values'] = np.array([utf.convert_to_float(y) for y in df.at[ii, 'paleoData_values']], dtype=float)\n        df.at[ii, 'year'] = np.array([utf.convert_to_float(y) for y in df.at[ii, 'year']], dtype=float)\n        \n        print(f'Converted values in paleoData_values and/or year for {ii}.')\n        # drop_inds.append(ii)\n# df_compact = df_compact.drop(drop_inds)\n\n# drop all missing values and exclude all-missing-values-rows\n\nfor ii in df.index:\n    dd   = np.array(df.at[ii, 'paleoData_values'])\n    mask = dd==-9999.99\n    df.at[ii, 'paleoData_values']=dd[~mask]\n    df.at[ii, 'year']=np.array(df.at[ii, 'year'])[~mask]\n    \ndrop_inds = []\nfor ii, row in enumerate(df.paleoData_values):\n    try:\n        if len(row)==0:\n            print(ii, 'empty row for paleodata_values')\n        elif len(df.iloc[ii]['year'])==0:\n            print(ii, 'empty row for year')\n        elif np.std(row)==0: \n            print(ii, 'std=0')\n        elif np.sum(np.diff(row)**2)==0: \n            print(ii, 'diff=0')\n        elif np.isnan(np.std(row)):\n            print(ii, 'std nan')\n        else:\n            continue\n        if df.index[ii] not in drop_inds: \n            drop_inds += [df.index[ii]]\n    except:\n        drop_inds+=[df.index[ii]]\n    \nprint(drop_inds)\ndf = df.drop(index=drop_inds)\n</pre> drop_inds = [] for ii in df.index:     try:         year = np.array(df.at[ii, 'year'], dtype=float)         vals = np.array(df.at[ii, 'paleoData_values'], dtype=float)         df.at[ii, 'year']             = year[year&gt;=1]         df.at[ii, 'paleoData_values'] = vals[year&gt;=1]     except:         # print         df.at[ii, 'paleoData_values'] = np.array([utf.convert_to_float(y) for y in df.at[ii, 'paleoData_values']], dtype=float)         df.at[ii, 'year'] = np.array([utf.convert_to_float(y) for y in df.at[ii, 'year']], dtype=float)                  print(f'Converted values in paleoData_values and/or year for {ii}.')         # drop_inds.append(ii) # df_compact = df_compact.drop(drop_inds)  # drop all missing values and exclude all-missing-values-rows  for ii in df.index:     dd   = np.array(df.at[ii, 'paleoData_values'])     mask = dd==-9999.99     df.at[ii, 'paleoData_values']=dd[~mask]     df.at[ii, 'year']=np.array(df.at[ii, 'year'])[~mask]      drop_inds = [] for ii, row in enumerate(df.paleoData_values):     try:         if len(row)==0:             print(ii, 'empty row for paleodata_values')         elif len(df.iloc[ii]['year'])==0:             print(ii, 'empty row for year')         elif np.std(row)==0:              print(ii, 'std=0')         elif np.sum(np.diff(row)**2)==0:              print(ii, 'diff=0')         elif np.isnan(np.std(row)):             print(ii, 'std nan')         else:             continue         if df.index[ii] not in drop_inds:              drop_inds += [df.index[ii]]     except:         drop_inds+=[df.index[ii]]      print(drop_inds) df = df.drop(index=drop_inds) <pre>5 std nan\n9 std nan\n10 std nan\n14 std nan\n26 std nan\n28 std nan\n36 std nan\n37 std nan\n42 std nan\n43 std nan\n44 std nan\n45 std nan\n46 std nan\n58 std nan\n76 std nan\n87 std nan\n91 std nan\n103 std nan\n116 std nan\n117 std nan\n123 std nan\n138 std nan\n139 std nan\n147 std nan\n166 std nan\n169 std nan\n171 std nan\n172 std nan\n175 std nan\n177 std nan\n186 std nan\n202 std nan\n206 std nan\n215 std nan\n219 std nan\n227 std nan\n230 std nan\n238 std nan\n239 std nan\n240 std nan\n241 std nan\n242 std nan\n243 std nan\n250 std nan\n252 std nan\n253 std nan\n254 std nan\n255 std nan\n256 std nan\n259 std nan\n270 std nan\n[10, 18, 20, 28, 58, 62, 80, 82, 92, 94, 96, 98, 100, 124, 164, 190, 198, 224, 252, 254, 268, 298, 300, 318, 364, 370, 376, 378, 384, 388, 406, 444, 454, 474, 484, 502, 508, 528, 530, 532, 534, 536, 538, 556, 560, 562, 564, 568, 570, 580, 604]\n</pre> <p>Now show the final compact dataframe</p> In\u00a0[17]: Copied! <pre>df = df[sorted(df.columns)]\ndf.reset_index(drop= True, inplace= True)\nprint(df.info())\n</pre> df = df[sorted(df.columns)] df.reset_index(drop= True, inplace= True) print(df.info()) <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 221 entries, 0 to 220\nData columns (total 21 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   archiveType                    221 non-null    object \n 1   dataSetName                    221 non-null    object \n 2   datasetId                      221 non-null    object \n 3   geo_meanElev                   186 non-null    float32\n 4   geo_meanLat                    221 non-null    float32\n 5   geo_meanLon                    221 non-null    float32\n 6   geo_siteName                   221 non-null    object \n 7   interpretation_direction       221 non-null    object \n 8   interpretation_seasonality     221 non-null    object \n 9   interpretation_variable        221 non-null    object \n 10  interpretation_variableDetail  221 non-null    object \n 11  originalDataURL                221 non-null    object \n 12  originalDatabase               221 non-null    object \n 13  paleoData_notes                221 non-null    object \n 14  paleoData_proxy                221 non-null    object \n 15  paleoData_sensorSpecies        221 non-null    object \n 16  paleoData_units                221 non-null    object \n 17  paleoData_values               221 non-null    object \n 18  paleoData_variableName         221 non-null    object \n 19  year                           221 non-null    object \n 20  yearUnits                      221 non-null    object \ndtypes: float32(3), object(18)\nmemory usage: 33.8+ KB\nNone\n</pre> In\u00a0[18]: Copied! <pre># save to a pickle file\ndf_compact = df[sorted(df.columns)]\ndf_compact.to_pickle('data/ch2k/ch2k_compact.pkl')\n</pre> # save to a pickle file df_compact = df[sorted(df.columns)] df_compact.to_pickle('data/ch2k/ch2k_compact.pkl') In\u00a0[19]: Copied! <pre># save to a list of csv files (metadata, data, year)\ndf_compact.name='ch2k'\nutf.write_compact_dataframe_to_csv(df_compact)\n</pre> # save to a list of csv files (metadata, data, year) df_compact.name='ch2k' utf.write_compact_dataframe_to_csv(df_compact) <pre>METADATA: datasetId, archiveType, dataSetName, geo_meanElev, geo_meanLat, geo_meanLon, geo_siteName, interpretation_direction, interpretation_seasonality, interpretation_variable, interpretation_variableDetail, originalDataURL, originalDatabase, paleoData_notes, paleoData_proxy, paleoData_sensorSpecies, paleoData_units, paleoData_variableName, yearUnits\nSaved to /home/jupyter-lluecke/dod2k_v2.0/dod2k/data/ch2k/ch2k_compact_%s.csv\n</pre> In\u00a0[20]: Copied! <pre># load dataframe\ndf = utf.load_compact_dataframe_from_csv('ch2k')\n</pre> # load dataframe df = utf.load_compact_dataframe_from_csv('ch2k') <p>Show spatial distribution of records, show archive and proxy types</p> In\u00a0[21]: Copied! <pre># count archive types\narchive_count = {}\nfor ii, at in enumerate(set(df['archiveType'])):\n    archive_count[at] = df.loc[df['archiveType']==at, 'archiveType'].count()\n\nsort = np.argsort([cc for cc in archive_count.values()])\narchives_sorted = np.array([cc for cc in archive_count.keys()])[sort][::-1]\n\n# Specify colour for each archive (smaller archives get grouped into the same colour)\narchive_colour, major_archives, other_archives = uplt.get_archive_colours(archives_sorted, archive_count)\n\nfig = uplt.plot_geo_archive_proxy(df, archive_colour)\nutf.save_fig(fig, f'geo_{df.name}', dir=df.name)\n</pre> # count archive types archive_count = {} for ii, at in enumerate(set(df['archiveType'])):     archive_count[at] = df.loc[df['archiveType']==at, 'archiveType'].count()  sort = np.argsort([cc for cc in archive_count.values()]) archives_sorted = np.array([cc for cc in archive_count.keys()])[sort][::-1]  # Specify colour for each archive (smaller archives get grouped into the same colour) archive_colour, major_archives, other_archives = uplt.get_archive_colours(archives_sorted, archive_count)  fig = uplt.plot_geo_archive_proxy(df, archive_colour) utf.save_fig(fig, f'geo_{df.name}', dir=df.name) <pre>0 Coral 221\nsaved figure in /home/jupyter-lluecke/dod2k_v2.0/dod2k/figs/ch2k/geo_ch2k.pdf\n</pre> <p>Now plot the coverage over the Common Era</p> In\u00a0[22]: Copied! <pre>fig = uplt.plot_coverage(df, archives_sorted, major_archives, other_archives, archive_colour)\nutf.save_fig(fig, f'time_{df.name}', dir=df.name)\n</pre> fig = uplt.plot_coverage(df, archives_sorted, major_archives, other_archives, archive_colour) utf.save_fig(fig, f'time_{df.name}', dir=df.name) <pre>saved figure in /home/jupyter-lluecke/dod2k_v2.0/dod2k/figs/ch2k/time_ch2k.pdf\n</pre> In\u00a0[23]: Copied! <pre># # check index\nprint(df.index)\n</pre> # # check index print(df.index) <pre>RangeIndex(start=0, stop=221, step=1)\n</pre> In\u00a0[24]: Copied! <pre># # check dataSetName\nkey = 'dataSetName'\nprint('%s: '%key)\nprint(df[key].values)\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # # check dataSetName key = 'dataSetName' print('%s: '%key) print(df[key].values) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>dataSetName: \n['CH03BUN01' 'ZI15MER01' 'ZI15MER01' 'CO03PAL03' 'CO03PAL02' 'LI06RAR01'\n 'CO03PAL07' 'FL18DTO03' 'UR00MAI01' 'TU95MAD01' 'ZI04IFR01' 'RE18CAY01'\n 'RE18CAY01' 'RE18CAY01' 'RE18CAY01' 'KU99HOU01' 'OS13NLP01' 'EV98KIR01'\n 'LI00RAR01' 'LI00RAR01' 'NU11PAL01' 'NU11PAL01' 'MA08DTO01' 'CA14TIM01'\n 'CA14TIM01' 'KA17RYU01' 'MC11KIR01' 'AB20MEN09' 'HE08LRA01' 'DA06MAF01'\n 'NA09MAL01' 'SW98STP01' 'MU18GSI01' 'MU18GSI01' 'FL17DTO02' 'DA06MAF02'\n 'SA19PAL02' 'SA19PAL02' 'CO03PAL01' 'ZI16ROD01' 'OS13NGP01' 'CH98PIR01'\n 'RE19GBR02' 'RE19GBR02' 'MU18RED04' 'GR13MAD01' 'XI17HAI01' 'XI17HAI01'\n 'XI17HAI01' 'XI17HAI01' 'DE14DTO03' 'KL97DAH01' 'QU06RAB01' 'QU06RAB01'\n 'DE14DTO01' 'KU00NIN01' 'TU01SIA01' 'RE19GBR01' 'RE19GBR01' 'GR13MAD02'\n 'AB20MEN07' 'BR19RED01' 'NU09FAN01' 'NU09FAN01' 'MU18RED01' 'OS14RIP01'\n 'DE14DTO02' 'LI04FIJ01' 'LI04FIJ01' 'EV18ROC01' 'EV18ROC01' 'CA13SAP01'\n 'TU01LAI01' 'HE13MIS01' 'HE13MIS01' 'ZI15IMP02' 'ZI15IMP02' 'PF04PBA01'\n 'SA20FAN02' 'WE09ARR01' 'WE09ARR01' 'CO03PAL05' 'XU15BVI01' 'HE18COC02'\n 'HE18COC02' 'MU18NPI01' 'MO06PED01' 'KR20SAR01' 'KR20SAR01' 'SA18GBR01'\n 'OS14UCP01' 'AB20MEN08' 'HE13MIS02' 'HE13MIS02' 'HE10GUA01' 'HE10GUA01'\n 'HE10GUA01' 'HE10GUA01' 'DE12ANC01' 'WA17BAN01' 'WA17BAN01' 'DR99ABR01'\n 'DR99ABR01' 'LI06RAR02' 'MU18RED03' 'SW99LIG01' 'SA16CLA01' 'ZI15TAN01'\n 'ZI15TAN01' 'RE19GBR03' 'RE19GBR03' 'DR00KSB01' 'BO14HTI02' 'BO14HTI02'\n 'MU17DOA01' 'TA18TAS01' 'XU15BVI03' 'AS05GUA01' 'FE09OGA01' 'FE09OGA01'\n 'FE09OGA01' 'FE09OGA01' 'GU99NAU01' 'SA20FAN01' 'AL16PUR02' 'CO03PAL10'\n 'RE19GBR05' 'ZI15IMP01' 'ZI15IMP01' 'KR20SAR02' 'KR20SAR02' 'RO19YUC01'\n 'RO19YUC01' 'ST13MAL01' 'ST13MAL01' 'DR00NBB01' 'PF19LAR01' 'PF19LAR01'\n 'AL16YUC01' 'CO03PAL09' 'ZI16ROD02' 'AB20MEN05' 'KI04MCV01' 'KI04MCV01'\n 'CH18YOA02' 'DE16RED01' 'BA04FIJ02' 'CO03PAL06' 'CH18YOA01' 'RE19GBR04'\n 'DO18DAV01' 'GO12SBV01' 'GO12SBV01' 'CA07FLI01' 'CA07FLI01' 'SW99LIG02'\n 'CO93TAR01' 'RO19PAR01' 'CO00MAL01' 'MO20WOA01' 'MO20WOA01' 'AB20MEN01'\n 'QU96ESV01' 'DE13HAI01' 'DE13HAI01' 'DE13HAI01' 'DE13HAI01' 'LI94SEC01'\n 'ZI15CLE01' 'ZI15CLE01' 'MU18RED02' 'ZI08MAY01' 'TU01DEP01' 'CO03PAL04'\n 'RA19PAI01' 'AB15BHB01' 'FL18DTO01' 'MO20KOI01' 'MO20KOI01' 'DU94URV01'\n 'DU94URV01' 'CO03PAL08' 'WU14CLI01' 'ZI14TUR01' 'ZI14TUR01' 'LI99CLI01'\n 'ZI15BUN01' 'ZI15BUN01' 'FE18RUS01' 'FE18RUS01' 'FE18RUS01' 'FE18RUS01'\n 'WU13TON01' 'WU13TON01' 'KI14PAR01' 'KI14PAR01' 'KI14PAR01' 'KI14PAR01'\n 'ZI14IFR02' 'ZI14IFR02' 'XU15BVI02' 'NU09KIR01' 'NU09KIR01' 'RI10PBL01'\n 'CA14BUT01' 'CA14BUT01' 'FL18DTO02' 'BA04FIJ01' 'GO08BER01' 'GO08BER01'\n 'LI06FIJ01' 'HE18COC01' 'HE18COC01' 'FL17DTO01' 'FL17DTO01' 'BO99MOO01'\n 'CH03LOM01' 'SA19PAL01' 'SA19PAL01' 'CH97BVB01' 'RA20TAI01']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 155/221\n</pre> In\u00a0[25]: Copied! <pre># # check datasetId\n\nprint(len(df.datasetId.unique()))\nprint(len(df))\nkey = 'datasetId'\nprint('%s (starts with): '%key)\nprint(df[key].values)\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint('datasetId starts with: ', np.unique([str(dd.split('_')[0]) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # # check datasetId  print(len(df.datasetId.unique())) print(len(df)) key = 'datasetId' print('%s (starts with): '%key) print(df[key].values) print(np.unique([str(type(dd)) for dd in df[key]])) print('datasetId starts with: ', np.unique([str(dd.split('_')[0]) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>221\n221\ndatasetId (starts with): \n['ch2k_CH03BUN01_0' 'ch2k_ZI15MER01_2' 'ch2k_ZI15MER01_4'\n 'ch2k_CO03PAL03_6' 'ch2k_CO03PAL02_8' 'ch2k_LI06RAR01_12'\n 'ch2k_CO03PAL07_14' 'ch2k_FL18DTO03_16' 'ch2k_UR00MAI01_22'\n 'ch2k_TU95MAD01_24' 'ch2k_ZI04IFR01_26' 'ch2k_RE18CAY01_30'\n 'ch2k_RE18CAY01_32' 'ch2k_RE18CAY01_34' 'ch2k_RE18CAY01_36'\n 'ch2k_KU99HOU01_40' 'ch2k_OS13NLP01_42' 'ch2k_EV98KIR01_44'\n 'ch2k_LI00RAR01_46' 'ch2k_LI00RAR01_48' 'ch2k_NU11PAL01_52'\n 'ch2k_NU11PAL01_54' 'ch2k_MA08DTO01_60' 'ch2k_CA14TIM01_64'\n 'ch2k_CA14TIM01_66' 'ch2k_KA17RYU01_70' 'ch2k_MC11KIR01_72'\n 'ch2k_AB20MEN09_74' 'ch2k_HE08LRA01_76' 'ch2k_DA06MAF01_78'\n 'ch2k_NA09MAL01_84' 'ch2k_SW98STP01_86' 'ch2k_MU18GSI01_88'\n 'ch2k_MU18GSI01_90' 'ch2k_FL17DTO02_102' 'ch2k_DA06MAF02_104'\n 'ch2k_SA19PAL02_106' 'ch2k_SA19PAL02_108' 'ch2k_CO03PAL01_110'\n 'ch2k_ZI16ROD01_112' 'ch2k_OS13NGP01_114' 'ch2k_CH98PIR01_116'\n 'ch2k_RE19GBR02_118' 'ch2k_RE19GBR02_120' 'ch2k_MU18RED04_122'\n 'ch2k_GR13MAD01_126' 'ch2k_XI17HAI01_128' 'ch2k_XI17HAI01_130'\n 'ch2k_XI17HAI01_134' 'ch2k_XI17HAI01_136' 'ch2k_DE14DTO03_140'\n 'ch2k_KL97DAH01_142' 'ch2k_QU06RAB01_144' 'ch2k_QU06RAB01_146'\n 'ch2k_DE14DTO01_148' 'ch2k_KU00NIN01_150' 'ch2k_TU01SIA01_152'\n 'ch2k_RE19GBR01_154' 'ch2k_RE19GBR01_156' 'ch2k_GR13MAD02_158'\n 'ch2k_AB20MEN07_160' 'ch2k_BR19RED01_162' 'ch2k_NU09FAN01_166'\n 'ch2k_NU09FAN01_168' 'ch2k_MU18RED01_172' 'ch2k_OS14RIP01_174'\n 'ch2k_DE14DTO02_176' 'ch2k_LI04FIJ01_178' 'ch2k_LI04FIJ01_180'\n 'ch2k_EV18ROC01_184' 'ch2k_EV18ROC01_186' 'ch2k_CA13SAP01_188'\n 'ch2k_TU01LAI01_192' 'ch2k_HE13MIS01_194' 'ch2k_HE13MIS01_196'\n 'ch2k_ZI15IMP02_200' 'ch2k_ZI15IMP02_202' 'ch2k_PF04PBA01_204'\n 'ch2k_SA20FAN02_206' 'ch2k_WE09ARR01_208' 'ch2k_WE09ARR01_210'\n 'ch2k_CO03PAL05_212' 'ch2k_XU15BVI01_214' 'ch2k_HE18COC02_216'\n 'ch2k_HE18COC02_218' 'ch2k_MU18NPI01_222' 'ch2k_MO06PED01_226'\n 'ch2k_KR20SAR01_228' 'ch2k_KR20SAR01_230' 'ch2k_SA18GBR01_234'\n 'ch2k_OS14UCP01_236' 'ch2k_AB20MEN08_238' 'ch2k_HE13MIS02_240'\n 'ch2k_HE13MIS02_242' 'ch2k_HE10GUA01_244' 'ch2k_HE10GUA01_246'\n 'ch2k_HE10GUA01_248' 'ch2k_HE10GUA01_250' 'ch2k_DE12ANC01_258'\n 'ch2k_WA17BAN01_260' 'ch2k_WA17BAN01_262' 'ch2k_DR99ABR01_264'\n 'ch2k_DR99ABR01_266' 'ch2k_LI06RAR02_270' 'ch2k_MU18RED03_272'\n 'ch2k_SW99LIG01_274' 'ch2k_SA16CLA01_276' 'ch2k_ZI15TAN01_278'\n 'ch2k_ZI15TAN01_280' 'ch2k_RE19GBR03_282' 'ch2k_RE19GBR03_284'\n 'ch2k_DR00KSB01_286' 'ch2k_BO14HTI02_288' 'ch2k_BO14HTI02_290'\n 'ch2k_MU17DOA01_292' 'ch2k_TA18TAS01_294' 'ch2k_XU15BVI03_296'\n 'ch2k_AS05GUA01_302' 'ch2k_FE09OGA01_304' 'ch2k_FE09OGA01_306'\n 'ch2k_FE09OGA01_308' 'ch2k_FE09OGA01_310' 'ch2k_GU99NAU01_314'\n 'ch2k_SA20FAN01_316' 'ch2k_AL16PUR02_320' 'ch2k_CO03PAL10_324'\n 'ch2k_RE19GBR05_326' 'ch2k_ZI15IMP01_328' 'ch2k_ZI15IMP01_330'\n 'ch2k_KR20SAR02_332' 'ch2k_KR20SAR02_334' 'ch2k_RO19YUC01_338'\n 'ch2k_RO19YUC01_340' 'ch2k_ST13MAL01_344' 'ch2k_ST13MAL01_346'\n 'ch2k_DR00NBB01_348' 'ch2k_PF19LAR01_350' 'ch2k_PF19LAR01_352'\n 'ch2k_AL16YUC01_354' 'ch2k_CO03PAL09_358' 'ch2k_ZI16ROD02_360'\n 'ch2k_AB20MEN05_362' 'ch2k_KI04MCV01_366' 'ch2k_KI04MCV01_368'\n 'ch2k_CH18YOA02_374' 'ch2k_DE16RED01_380' 'ch2k_BA04FIJ02_382'\n 'ch2k_CO03PAL06_386' 'ch2k_CH18YOA01_390' 'ch2k_RE19GBR04_392'\n 'ch2k_DO18DAV01_394' 'ch2k_GO12SBV01_396' 'ch2k_GO12SBV01_398'\n 'ch2k_CA07FLI01_400' 'ch2k_CA07FLI01_402' 'ch2k_SW99LIG02_404'\n 'ch2k_CO93TAR01_408' 'ch2k_RO19PAR01_410' 'ch2k_CO00MAL01_412'\n 'ch2k_MO20WOA01_414' 'ch2k_MO20WOA01_416' 'ch2k_AB20MEN01_420'\n 'ch2k_QU96ESV01_422' 'ch2k_DE13HAI01_424' 'ch2k_DE13HAI01_426'\n 'ch2k_DE13HAI01_430' 'ch2k_DE13HAI01_432' 'ch2k_LI94SEC01_436'\n 'ch2k_ZI15CLE01_438' 'ch2k_ZI15CLE01_440' 'ch2k_MU18RED02_442'\n 'ch2k_ZI08MAY01_446' 'ch2k_TU01DEP01_450' 'ch2k_CO03PAL04_452'\n 'ch2k_RA19PAI01_456' 'ch2k_AB15BHB01_458' 'ch2k_FL18DTO01_460'\n 'ch2k_MO20KOI01_462' 'ch2k_MO20KOI01_464' 'ch2k_DU94URV01_468'\n 'ch2k_DU94URV01_470' 'ch2k_CO03PAL08_472' 'ch2k_WU14CLI01_476'\n 'ch2k_ZI14TUR01_480' 'ch2k_ZI14TUR01_482' 'ch2k_LI99CLI01_486'\n 'ch2k_ZI15BUN01_488' 'ch2k_ZI15BUN01_490' 'ch2k_FE18RUS01_492'\n 'ch2k_FE18RUS01_494' 'ch2k_FE18RUS01_496' 'ch2k_FE18RUS01_498'\n 'ch2k_WU13TON01_504' 'ch2k_WU13TON01_506' 'ch2k_KI14PAR01_510'\n 'ch2k_KI14PAR01_512' 'ch2k_KI14PAR01_516' 'ch2k_KI14PAR01_518'\n 'ch2k_ZI14IFR02_522' 'ch2k_ZI14IFR02_524' 'ch2k_XU15BVI02_526'\n 'ch2k_NU09KIR01_540' 'ch2k_NU09KIR01_542' 'ch2k_RI10PBL01_546'\n 'ch2k_CA14BUT01_548' 'ch2k_CA14BUT01_550' 'ch2k_FL18DTO02_554'\n 'ch2k_BA04FIJ01_558' 'ch2k_GO08BER01_572' 'ch2k_GO08BER01_574'\n 'ch2k_LI06FIJ01_582' 'ch2k_HE18COC01_584' 'ch2k_HE18COC01_586'\n 'ch2k_FL17DTO01_590' 'ch2k_FL17DTO01_592' 'ch2k_BO99MOO01_594'\n 'ch2k_CH03LOM01_596' 'ch2k_SA19PAL01_598' 'ch2k_SA19PAL01_600'\n 'ch2k_CH97BVB01_602' 'ch2k_RA20TAI01_606']\n[\"&lt;class 'str'&gt;\"]\ndatasetId starts with:  ['ch2k']\nNo. of unique values: 221/221\n</pre> In\u00a0[26]: Copied! <pre># originalDataURL\nkey = 'originalDataURL'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([kk for kk in df[key] if 'this' in kk]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n# 'this study' should point to the correct URL (PAGES2k)\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # originalDataURL key = 'originalDataURL' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([kk for kk in df[key] if 'this' in kk])) print(np.unique([str(type(dd)) for dd in df[key]])) # 'this study' should point to the correct URL (PAGES2k) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>originalDataURL: \n['https://doi.org/10.1594/PANGAEA.874078'\n 'https://doi.pangaea.de/10.1594/PANGAEA.743953'\n 'https://doi.pangaea.de/10.1594/PANGAEA.830601'\n 'https://doi.pangaea.de/10.1594/PANGAEA.88199'\n 'https://doi.pangaea.de/10.1594/PANGAEA.88200'\n 'https://doi.pangaea.de/10.1594/PANGAEA.887712'\n 'https://doi.pangaea.de/10.1594/PANGAEA.891094'\n 'https://www.ncdc.noaa.gov/paleo/study/1003972'\n 'https://www.ncdc.noaa.gov/paleo/study/1003973'\n 'https://www.ncdc.noaa.gov/paleo/study/10373'\n 'https://www.ncdc.noaa.gov/paleo/study/10425'\n 'https://www.ncdc.noaa.gov/paleo/study/10808'\n 'https://www.ncdc.noaa.gov/paleo/study/11935'\n 'https://www.ncdc.noaa.gov/paleo/study/12278'\n 'https://www.ncdc.noaa.gov/paleo/study/12891'\n 'https://www.ncdc.noaa.gov/paleo/study/12893'\n 'https://www.ncdc.noaa.gov/paleo/study/12994'\n 'https://www.ncdc.noaa.gov/paleo/study/13035'\n 'https://www.ncdc.noaa.gov/paleo/study/13439'\n 'https://www.ncdc.noaa.gov/paleo/study/15238'\n 'https://www.ncdc.noaa.gov/paleo/study/15794'\n 'https://www.ncdc.noaa.gov/paleo/study/16217'\n 'https://www.ncdc.noaa.gov/paleo/study/16338'\n 'https://www.ncdc.noaa.gov/paleo/study/16339'\n 'https://www.ncdc.noaa.gov/paleo/study/16438'\n 'https://www.ncdc.noaa.gov/paleo/study/17035'\n 'https://www.ncdc.noaa.gov/paleo/study/17289'\n 'https://www.ncdc.noaa.gov/paleo/study/17378'\n 'https://www.ncdc.noaa.gov/paleo/study/1839'\n 'https://www.ncdc.noaa.gov/paleo/study/1842'\n 'https://www.ncdc.noaa.gov/paleo/study/1844'\n 'https://www.ncdc.noaa.gov/paleo/study/1845'\n 'https://www.ncdc.noaa.gov/paleo/study/1846'\n 'https://www.ncdc.noaa.gov/paleo/study/1847'\n 'https://www.ncdc.noaa.gov/paleo/study/1850'\n 'https://www.ncdc.noaa.gov/paleo/study/1853'\n 'https://www.ncdc.noaa.gov/paleo/study/1855'\n 'https://www.ncdc.noaa.gov/paleo/study/1856'\n 'https://www.ncdc.noaa.gov/paleo/study/1857'\n 'https://www.ncdc.noaa.gov/paleo/study/1859'\n 'https://www.ncdc.noaa.gov/paleo/study/1866'\n 'https://www.ncdc.noaa.gov/paleo/study/1867'\n 'https://www.ncdc.noaa.gov/paleo/study/1875'\n 'https://www.ncdc.noaa.gov/paleo/study/1876'\n 'https://www.ncdc.noaa.gov/paleo/study/1881'\n 'https://www.ncdc.noaa.gov/paleo/study/18895'\n 'https://www.ncdc.noaa.gov/paleo/study/1891'\n 'https://www.ncdc.noaa.gov/paleo/study/1897'\n 'https://www.ncdc.noaa.gov/paleo/study/1901'\n 'https://www.ncdc.noaa.gov/paleo/study/1903'\n 'https://www.ncdc.noaa.gov/paleo/study/1911'\n 'https://www.ncdc.noaa.gov/paleo/study/1913'\n 'https://www.ncdc.noaa.gov/paleo/study/1914'\n 'https://www.ncdc.noaa.gov/paleo/study/1915'\n 'https://www.ncdc.noaa.gov/paleo/study/19179'\n 'https://www.ncdc.noaa.gov/paleo/study/19239'\n 'https://www.ncdc.noaa.gov/paleo/study/1925'\n 'https://www.ncdc.noaa.gov/paleo/study/21011'\n 'https://www.ncdc.noaa.gov/paleo/study/21310'\n 'https://www.ncdc.noaa.gov/paleo/study/21710'\n 'https://www.ncdc.noaa.gov/paleo/study/22056'\n 'https://www.ncdc.noaa.gov/paleo/study/22252'\n 'https://www.ncdc.noaa.gov/paleo/study/22991'\n 'https://www.ncdc.noaa.gov/paleo/study/23390'\n 'https://www.ncdc.noaa.gov/paleo/study/23850'\n 'https://www.ncdc.noaa.gov/paleo/study/24477'\n 'https://www.ncdc.noaa.gov/paleo/study/24630'\n 'https://www.ncdc.noaa.gov/paleo/study/25270'\n 'https://www.ncdc.noaa.gov/paleo/study/25290'\n 'https://www.ncdc.noaa.gov/paleo/study/26531'\n 'https://www.ncdc.noaa.gov/paleo/study/27271'\n 'https://www.ncdc.noaa.gov/paleo/study/27450'\n 'https://www.ncdc.noaa.gov/paleo/study/28130'\n 'https://www.ncdc.noaa.gov/paleo/study/28451'\n 'https://www.ncdc.noaa.gov/paleo/study/29312'\n 'https://www.ncdc.noaa.gov/paleo/study/29412'\n 'https://www.ncdc.noaa.gov/paleo/study/30493'\n 'https://www.ncdc.noaa.gov/paleo/study/31552'\n 'https://www.ncdc.noaa.gov/paleo/study/33732'\n 'https://www.ncdc.noaa.gov/paleo/study/34372'\n 'https://www.ncdc.noaa.gov/paleo/study/34373'\n 'https://www.ncdc.noaa.gov/paleo/study/34392'\n 'https://www.ncdc.noaa.gov/paleo/study/34393'\n 'https://www.ncdc.noaa.gov/paleo/study/34394'\n 'https://www.ncdc.noaa.gov/paleo/study/34412'\n 'https://www.ncdc.noaa.gov/paleo/study/34413'\n 'https://www.ncdc.noaa.gov/paleo/study/34452'\n 'https://www.ncdc.noaa.gov/paleo/study/34472'\n 'https://www.ncdc.noaa.gov/paleo/study/34512'\n 'https://www.ncdc.noaa.gov/paleo/study/34552'\n 'https://www.ncdc.noaa.gov/paleo/study/34553'\n 'https://www.ncdc.noaa.gov/paleo/study/34612'\n 'https://www.ncdc.noaa.gov/paleo/study/34692'\n 'https://www.ncdc.noaa.gov/paleo/study/34953'\n 'https://www.ncdc.noaa.gov/paleo/study/6087'\n 'https://www.ncdc.noaa.gov/paleo/study/6089'\n 'https://www.ncdc.noaa.gov/paleo/study/6116'\n 'https://www.ncdc.noaa.gov/paleo/study/6184'\n 'https://www.ncdc.noaa.gov/paleo/study/8424'\n 'https://www.ncdc.noaa.gov/paleo/study/8609'\n 'https://www.ncdc.noaa.gov/paleo/study/9639']\n[]\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 101/221\n</pre> In\u00a0[27]: Copied! <pre># # originalDataSet\nkey = 'originalDatabase'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n# Note: the last two records have missing URLs\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # # originalDataSet key = 'originalDatabase' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) # Note: the last two records have missing URLs print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>originalDatabase: \n['CoralHydro2k v1.0.1']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 1/221\n</pre> In\u00a0[28]: Copied! <pre># check Elevation\nkey = 'geo_meanElev'\nprint('%s: '%key)\nprint(df[key])\nprint(np.unique(['%d'%kk for kk in df[key] if np.isfinite(kk)]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # check Elevation key = 'geo_meanElev' print('%s: '%key) print(df[key]) print(np.unique(['%d'%kk for kk in df[key] if np.isfinite(kk)])) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>geo_meanElev: \n0      -3.0\n1     -17.0\n2     -17.0\n3       NaN\n4       NaN\n       ... \n216    -5.0\n217   -10.0\n218   -10.0\n219    -7.0\n220    -6.0\nName: geo_meanElev, Length: 221, dtype: float32\n['-1' '-10' '-11' '-12' '-14' '-16' '-17' '-18' '-2' '-25' '-3' '-4' '-5'\n '-6' '-7' '-8' '-9' '0']\n[\"&lt;class 'float'&gt;\"]\nNo. of unique values: 44/221\n</pre> In\u00a0[29]: Copied! <pre># # Latitude\nkey = 'geo_meanLat'\nprint('%s: '%key)\nprint(np.unique(['%d'%kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # # Latitude key = 'geo_meanLat' print('%s: '%key) print(np.unique(['%d'%kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>geo_meanLat: \n['-10' '-11' '-12' '-13' '-14' '-15' '-16' '-17' '-18' '-19' '-21' '-22'\n '-23' '-28' '-3' '-4' '-5' '-6' '-8' '0' '1' '10' '11' '12' '13' '15'\n '16' '17' '18' '19' '2' '20' '21' '22' '23' '24' '25' '27' '28' '3' '32'\n '4' '5' '7']\n[\"&lt;class 'float'&gt;\"]\nNo. of unique values: 128/221\n</pre> In\u00a0[30]: Copied! <pre># # Longitude \nkey = 'geo_meanLon'\nprint('%s: '%key)\nprint(np.unique(['%d'%kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # # Longitude  key = 'geo_meanLon' print('%s: '%key) print(np.unique(['%d'%kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>geo_meanLon: \n['-109' '-114' '-149' '-157' '-159' '-162' '-169' '-174' '-22' '-33' '-61'\n '-64' '-66' '-67' '-80' '-82' '-86' '-88' '-91' '100' '105' '109' '110'\n '111' '113' '114' '115' '117' '118' '119' '120' '122' '123' '124' '130'\n '134' '142' '143' '144' '145' '146' '147' '148' '150' '151' '152' '153'\n '163' '166' '167' '172' '173' '179' '34' '36' '37' '38' '39' '40' '43'\n '45' '49' '55' '58' '63' '7' '70' '71' '72' '92' '96']\n[\"&lt;class 'float'&gt;\"]\nNo. of unique values: 130/221\n</pre> In\u00a0[31]: Copied! <pre># Site Name \nkey = 'geo_siteName'\nprint('%s: '%key)\nprint(df[key].values)\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # Site Name  key = 'geo_siteName' print('%s: '%key) print(df[key].values) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>geo_siteName: \n['Bunaken Island, Indonesia' 'Rowley Shoals, Australia'\n 'Rowley Shoals, Australia'\n 'Palmyra Island, United States Minor Outlying Islands'\n 'Palmyra Island, United States Minor Outlying Islands'\n 'Rarotonga, Cook Islands'\n 'Palmyra Island, United States Minor Outlying Islands'\n 'Dry Tortugas, Florida, USA' 'Maiana, Republic of Kiribati'\n 'Madang Lagoon, Papua New Guinea' 'Ifaty Reef, Madagascar'\n 'Little Cayman, Cayman Islands' 'Little Cayman, Cayman Islands'\n 'Little Cayman, Cayman Islands' 'Little Cayman, Cayman Islands'\n 'Houtman Abrolhos Islands, Australia' 'Ngeralang, Palau'\n 'Kiritimati (Christmas) Island, Republic of Kiribati'\n 'Rarotonga, Cook Islands' 'Rarotonga, Cook Islands'\n 'Palmyra Island, United States Minor Outlying Islands'\n 'Palmyra Island, United States Minor Outlying Islands'\n 'Dry Tortugas, Florida, USA' 'Timor, Indonesia' 'Timor, Indonesia'\n 'Kikai Island, Japan'\n 'Kiritimati (Christmas) Island, Republic of Kiribati'\n 'Mentawai Islands, Indonesia'\n 'Cayo Sal, Los Roques Archipelago, Venezuela'\n 'Fungu Mrima Reef, Tanzania' 'Malindi Marine Park, Kenya'\n 'Ponta Banana, Principe Island' 'Gili Selang, Bali, Indonesia'\n 'Gili Selang, Bali, Indonesia' 'Dry Tortugas, Florida, USA'\n 'Fungu Mrima Reef, Tanzania'\n 'Palmyra Island, United States Minor Outlying Islands'\n 'Palmyra Island, United States Minor Outlying Islands'\n 'Palmyra Island, United States Minor Outlying Islands'\n 'Rodrigues, Republic of Mauritius' 'Ngaragabel, Palau'\n 'Pirotan Island, Gujarat, India' 'Portland Roads, Australia'\n 'Portland Roads, Australia' 'Coral Gardens, Red Sea'\n 'Nosy Boraha, Madagascar (formerly Ile Sainte-Marie)'\n 'Fengjiawan, Wenchang, China' 'Fengjiawan, Wenchang, China'\n 'Fengjiawan, Wenchang, China' 'Fengjiawan, Wenchang, China'\n 'Dry Tortugas, Florida, USA' 'Dur-Ghella Island, Eritrea'\n 'Rabaul, East New Britain, Papua New Guinea'\n 'Rabaul, East New Britain, Papua New Guinea' 'Dry Tortugas, Florida, USA'\n 'Ningaloo Reef, Australia' 'Sialum, Huon Peninsula, Papua New Guinea'\n 'Eel Reef, Australia' 'Eel Reef, Australia'\n 'Nosy Boraha, Madagascar (formerly Ile Sainte-Marie)'\n 'Mentawai Islands, Indonesia' 'Canyon, Red Sea'\n 'Tabuaeran (Fanning Island), Republic of Kiribati'\n 'Tabuaeran (Fanning Island), Republic of Kiribati' 'Semicolon, Red Sea'\n 'Rock Islands, Palau' 'Dry Tortugas, Florida, USA' 'Vanua Levu, Fiji'\n 'Vanua Levu, Fiji' 'Rocas Atoll, Rio Grande do Norte, Brazil'\n 'Rocas Atoll, Rio Grande do Norte, Brazil' 'Sapodilla Cayes, Belize'\n 'Laing Island, Papua New Guinea' 'Misima Island, Papua New Guinea'\n 'Misima Island, Papua New Guinea' 'Rowley Shoals, Australia'\n 'Rowley Shoals, Australia' 'Peros Banhos Atoll, Chagos Archipelago'\n 'Tabuaeran (Fanning Island), Republic of Kiribati'\n 'Arlington Reef, Australia' 'Arlington Reef, Australia'\n 'Palmyra Island, United States Minor Outlying Islands'\n 'Anegada, British Virgin Islands' 'Cocos (Keeling) Islands, Australia'\n 'Cocos (Keeling) Islands, Australia' 'Nusa Penida, Indonesia'\n 'Pedra de Lume, Sal Island' 'Sarawak, Malaysia' 'Sarawak, Malaysia'\n 'Great Keppel Island, Australia' 'Ulong Channel, Palau'\n 'Mentawai Islands, Indonesia' 'Misima Island, Papua New Guinea'\n 'Misima Island, Papua New Guinea' 'Isle de Gosier, Guadeloupe'\n 'Isle de Gosier, Guadeloupe' 'Isle de Gosier, Guadeloupe'\n 'Isle de Gosier, Guadeloupe' 'Amedee Island, New Caledonia'\n 'Bandar Khayran, Oman' 'Bandar Khayran, Oman' 'Abraham Reef, Australia'\n 'Abraham Reef, Australia' 'Rarotonga, Cook Islands' 'Abu Galawa, Red Sea'\n 'Lignumvitae Basin, Florida, USA' 'Clarion Island, Mexico'\n 'Ningaloo Reef, Australia' 'Ningaloo Reef, Australia'\n 'Reef 13-050, Australia' 'Reef 13-050, Australia'\n 'Kitchen Shoals, Bermuda' 'Hon Tre Island, Vietnam'\n 'Hon Tre Island, Vietnam' 'Doangdoangan Besar, Indonesia'\n \"Ta'u, American Samoa\" 'Anegada, British Virgin Islands'\n 'Double Reef, Guam' 'Ogasawara Islands, Japan' 'Ogasawara Islands, Japan'\n 'Ogasawara Islands, Japan' 'Ogasawara Islands, Japan'\n 'Nauru Island, Republic of Nauru'\n 'Tabuaeran (Fanning Island), Republic of Kiribati'\n 'Pinacles Reef, Puerto Rico'\n 'Palmyra Island, United States Minor Outlying Islands'\n 'Clerke Reef, Australia' 'Rowley Shoals, Australia'\n 'Rowley Shoals, Australia' 'Sarawak, Malaysia' 'Sarawak, Malaysia'\n 'Puerto Morelos, Mexico' 'Puerto Morelos, Mexico'\n 'Rasdhoo Atoll, Maldives' 'Rasdhoo Atoll, Maldives'\n 'Northeast Breakers, Bermuda' 'St. Gilles Reef, La Reunion'\n 'St. Gilles Reef, La Reunion' 'Puerto Morelos, Mexico'\n 'Palmyra Island, United States Minor Outlying Islands'\n 'Rodrigues, Republic of Mauritius' 'Mentawai Islands, Indonesia'\n 'Espiritu Santo Island, Vanuatu' 'Espiritu Santo Island, Vanuatu'\n 'Lingyang Reef, Yongle Atoll' 'Red Sea' 'Savusavu Bay, Vanua Levu, Fiji'\n 'Palmyra Island, United States Minor Outlying Islands'\n 'Lingyang Reef, Yongle Atoll' 'Nomad Reef, Australia'\n 'Davies Reef, Australia' 'Sabine Bank, Vanuatu' 'Sabine Bank, Vanuatu'\n 'Flinders Reef, Australia' 'Flinders Reef, Australia'\n 'Lignumvitae Basin, Florida Bay' 'Tarawa Atoll, Republic of Kiribati'\n 'Parguera, Puerto Rico' 'Malindi Marine Park, Kenya'\n 'Wolei Atoll, Fed. States of Micronesia'\n 'Wolei Atoll, Fed. States of Micronesia' 'Mentawai Islands, Indonesia'\n 'Espiritu Santo Island, Vanuatu' 'Longwan, Qionghai, China'\n 'Longwan, Qionghai, China' 'Longwan, Qionghai, China'\n 'Longwan, Qionghai, China' 'Secas Island, Panama'\n 'Rowley Shoals, Australia' 'Rowley Shoals, Australia'\n 'Popponesset, Red Sea' 'Mayotte' 'Madang Lagoon, Papua New Guinea'\n 'Palmyra Island, United States Minor Outlying Islands'\n 'Palaui Island, Philippines' 'Batu Hitam Beach, Indonesia'\n 'Dry Tortugas, Florida, USA' 'Kosrae Island, Fed. States of Micronesia'\n 'Kosrae Island, Fed. States of Micronesia'\n 'Urvina Bay, Isabela Island, Ecuador'\n 'Urvina Bay, Isabela Island, Ecuador'\n 'Palmyra Island, United States Minor Outlying Islands'\n 'Clipperton Island' 'Tulear Reef, Madagascar' 'Tulear Reef, Madagascar'\n 'Clipperton Island' 'Ningaloo Reef, Australia' 'Ningaloo Reef, Australia'\n 'Ras Umm Sidd, Egypt' 'Ras Umm Sidd, Egypt' 'Ras Umm Sidd, Egypt'\n 'Ras Umm Sidd, Egypt' \"Ha'afera, Tonga\" \"Ha'afera, Tonga\"\n 'La Parguera, Puerto Rico' 'La Parguera, Puerto Rico'\n 'La Parguera, Puerto Rico' 'La Parguera, Puerto Rico'\n 'Ifaty Reef, Madagascar' 'Ifaty Reef, Madagascar'\n 'Anegada, British Virgin Islands'\n 'Kiritimati (Christmas) Island, Republic of Kiribati'\n 'Kiritimati (Christmas) Island, Republic of Kiribati'\n 'Port Blair, Andaman Islands, India'\n 'Butaritari Atoll, Republic of Kiribati'\n 'Butaritari Atoll, Republic of Kiribati' 'Dry Tortugas, Florida, USA'\n 'Savusavu Bay, Vanua Levu, Fiji' 'Bermuda' 'Bermuda'\n 'Savusavu Bay, Vanua Levu, Fiji' 'Cocos (Keeling) Islands, Australia'\n 'Cocos (Keeling) Islands, Australia' 'Dry Tortugas, Florida, USA'\n 'Dry Tortugas, Florida, USA' 'Moorea, French Polynesia'\n 'Padang Bai, Bali, Indonesia'\n 'Palmyra Island, United States Minor Outlying Islands'\n 'Palmyra Island, United States Minor Outlying Islands'\n 'Mahe Island, Republic of the Seychelles' 'Houbihu, Taiwan']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 103/221\n</pre> In\u00a0[32]: Copied! <pre># archiveType\nkey = 'archiveType'\nprint('%s: '%key)\nprint(np.unique(df[key]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # archiveType key = 'archiveType' print('%s: '%key) print(np.unique(df[key])) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>archiveType: \n['Coral']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 1/221\n</pre> In\u00a0[33]: Copied! <pre># paleoData_proxy\nkey = 'paleoData_proxy'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # paleoData_proxy key = 'paleoData_proxy' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>paleoData_proxy: \n['Sr/Ca' 'd18O']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 2/221\n</pre> In\u00a0[34]: Copied! <pre># climate_interpretation\nkey = 'paleoData_sensorSpecies'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # climate_interpretation key = 'paleoData_sensorSpecies' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')  <pre>paleoData_sensorSpecies: \n['Diploastrea heliopora' 'Diploria labyrinthiformis' 'Favia speciosa'\n 'Orbicella faveolata' 'Pavona clavus' 'Platygyra lamellina'\n 'Porites australiensis' 'Porites lobata' 'Porites lutea' 'Porites solida'\n 'Porites sp.' 'Pseudodiploria strigosa' 'Siderastrea radians'\n 'Siderastrea siderea' 'Siderastrea sp.' 'Siderastrea stellata'\n 'Solenastrea bournoni']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 17/221\n</pre> In\u00a0[35]: Copied! <pre># # paleoData_notes\nkey = 'paleoData_notes'\nprint('%s: '%key)\nprint(df[key].values)\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # # paleoData_notes key = 'paleoData_notes' print('%s: '%key) print(df[key].values) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>paleoData_notes: \n['This paper did not calibrate the d18O proxy or reconstruct temperature. It instead analyzed variability through time by directly using the d18O proxy.'\n 'Sr/Ca-SST recconstructed with composite plus scale method to ERSSTv3b, no regression applied'\n 'Sr/Ca-SST recconstructed with composite plus scale method to ERSSTv3b, no regression applied'\n 'nan' 'nan'\n 'Individual coral records that are part of the Rarotonga composite' 'nan'\n 'nan' 'nan' 'monthly correlations with SST not reported'\n 'Other calibration slopes are available in Zinke et al. 2004; 1920-1995 samples monthly; 1919-1658 sampled bimonthly'\n 'nan' 'nan' 'nan' 'nan'\n '1953-1993 and 1961-1993 calibration periods, first with 0.13 slope, latter -0.17 slope'\n 'nan' 'nan'\n 'Sr/Ca-SST calibrations listed were found in Linsley et al. 2004. The calibration from Linsley et al. 2000 is as follows: slope = -0.082; intercept = 11.568; rsq = 0.75'\n 'Sr/Ca-SST calibrations listed were found in Linsley et al. 2004. The calibration from Linsley et al. 2000 is as follows: slope = -0.082; intercept = 11.568; rsq = 0.75'\n 'nan' 'nan' 'nan' 'nan' 'nan'\n 'Core data is a composite of overlapping individual pieces and any replicate analyses. Data is seasonal min-max (Feb - Aug)'\n 'nan' 'Fossil Coral' 'nan'\n 'One core, this record: younger part of core MAF00-01 at monthly resolution (1896-1998);  annual correlation based on 1897-1997 data'\n 'monthly correlations not reported, only assesed with IOD; used Cole et al. (2000) slope of -0.24'\n 'nan'\n 'Sr/Ca-SST calibrations not published because of weak relationship for both GS and NP cores'\n 'Sr/Ca-SST calibrations not published because of weak relationship for both GS and NP cores'\n 'nan'\n 'One core, this record: older part of core MAF00-01 at bimonthly resolution (1622-1722)'\n 'nan' 'nan' 'nan'\n 'Totor Sr/Ca core top performed well in many sections, others less good, especially core top since 1988 to 2006. Cabri was much better. There is a table with likely best section in Totor in paper'\n 'Multiple linear regression analyses using monthly (non-detrended) anomalies for coral and instrumental SST and SSS data for the period 1970 to 2008. For NGB core, regression equation is: d18O_anom = 0.15 (0.03)*SST + 0.36 (0.07)*SSS.n'\n 'slope is not calculated, reported slope from Weber and Woodhead is used of -0.24'\n 'nan' 'nan'\n 'This record is a combination of high-resolution data from Murty et al. 2018 and annual data from Bryan et al. 2019. Studies are independent but were performed on the same coral core (CG). Calibration information, SST range, and analytical error provided are for high-resolution data from Murty et al. 2018.'\n 'nan' 'nan' 'nan' 'nan' 'nan'\n 'Same colony as 08PS-A2, but different core and core top age' 'nan' 'nan'\n 'nan' 'Same colony as 08PS-A1, but different core and core top age'\n 'used Gagan 1994 d18O slope' 'monthly correlations not reported' 'nan'\n 'nan'\n 'STM4 Sr/Ca core top was too cold vs STM2, would be careful. STM2 is really good, compares well to Rodrigures core Cabri'\n 'Fossil Coral' 'nan'\n 'Two cores were spliced at 1984 to avoid secondary aragonite'\n 'Two cores were spliced at 1984 to avoid secondary aragonite' 'nan'\n 'Multiple linear regression analyses using monthly (non-detrended) anomalies for coral and instrumental SST and SSS data for the period 1970 to 2008. For RI core, regression equation is: d18O_anom = 0.03 (0.03)*SST + 0.36 (0.05)*SSS.nn                                        n                                n                        n                n'\n 'nan' 'nan' 'nan' 'nan' 'nan' 'nan' 'monthly correlations not reported'\n 'Composite of two fossil cores. Data was sub-monthly and was linearly resampled to monthly.'\n 'Composite of two fossil cores. Data was sub-monthly and was linearly resampled to monthly.'\n 'Sr/Ca-SST recconstructed with composite plus scale method to ERSSTv3b, no regression applied'\n 'Sr/Ca-SST recconstructed with composite plus scale method to ERSSTv3b, no regression applied'\n 'd18O driven by rainfall; little SST correlation'\n 'Publication notes that core F4 has higher variance than contemporary cores and instrumental SST products and that this is likely due to lagoonal mixing.'\n 'coral was primariliy analysed for Boron and paper does not discuss much about Sr/Ca and d18O even though they were analysed'\n 'coral was primariliy analysed for Boron and paper does not discuss much about Sr/Ca and d18O even though they were analysed'\n 'nan' 'nan'\n 'Sr/Ca regression slope error is estimated to be (+/-) 0.21 degrees C; Used the Zinke (2015) method - normalised and scaled to the s.d. SST box of the period 1961-1990; equation based on composite of 2 cores'\n 'Sr/Ca regression slope error is estimated to be (+/-) 0.21 degrees C; Used the Zinke (2015) method - normalised and scaled to the s.d. SST box of the period 1961-1990; equation based on composite of 2 cores'\n 'NP1 and NP2 cores spliced together to get full NP record due to bioerosion in the NP1 core. Sr/Ca-SST calibrations not published because of weak relationship for both GS and NP cores.'\n 'nan' 'nan' 'nan'\n 'Study focuses on Ba/Ca and Y/Ca. Sr/Ca is primarily used for chronology'\n 'Multiple linear regression analyses using monthly (not detrended) anomalies for coral and instrumental SST and SSS data for the period 1970 to 2008. For UC core, regression equation is: d18O_anom = 0.09 (0.03)*SST + 0.33 (0.05)*SSS'\n 'Fossil Coral'\n 'Raw data was sub-monthly and was linearly resampled to monthly.'\n 'Raw data was sub-monthly and was linearly resampled to monthly.'\n 'This calibration data is taken from the top 40 years of the core from Hetzinger et al 2006'\n 'This calibration data is taken from the top 40 years of the core from Hetzinger et al 2006'\n 'This calibration data is taken from the top 40 years of the core from Hetzinger et al 2006'\n 'This calibration data is taken from the top 40 years of the core from Hetzinger et al 2006'\n 'Sr/Ca are average values of three colonies and replicate paths' 'nan'\n 'nan'\n 'This is a refinement of the record available previously (Druffel and Griffin, JGR 1993) which showed biennial d18O for the period 1635-1957.'\n 'This is a refinement of the record available previously (Druffel and Griffin, JGR 1993) which showed biennial d18O for the period 1635-1957.'\n 'Individual coral records that are part of the Rarotonga composite'\n 'Mean SST ranges given in paper for northern reef 7.7C; for southern reefs 5.8C'\n 'nan' 'nan'\n 'Sr/Ca-SST recconstructed with composite plus scale method to ERSSTv3b, no regression applied'\n 'Sr/Ca-SST recconstructed with composite plus scale method to ERSSTv3b, no regression applied'\n 'nan' 'nan'\n 'regressions in paper use air temps instead of SST, relevant growth information about coral found in PhD thesis (http://nbn-resolving.de/urn:nbn:de:gbv:46-ep000102521)'\n 'A composite of cores TN (CoralHydro2k ID BO14HTI01) and BB was used from 2010-1977 for the published reconstructions in Bolton et al. 2014 and Goodkin et al. 2021.'\n 'A composite of cores TN (CoralHydro2k ID BO14HTI01) and BB was used from 2010-1977 for the published reconstructions in Bolton et al. 2014 and Goodkin et al. 2021.'\n 'Calibrations to SST data were performed on a shorter, higher-resolution set of samples. See Murty et al. 2017 for more information.'\n 'The calibration equation incorporated both SST and salinity, so the d18O-SST slope is not included here to avoid misrepresentation.'\n 'nan' 'nan'\n 'Annual regression slopes (-0.213 / C, -0.140 mmol/mol / C) imply an apparent amplification of inferred SST variations on interannual and longer timescales.'\n 'Annual regression slopes (-0.213 / C, -0.140 mmol/mol / C) imply an apparent amplification of inferred SST variations on interannual and longer timescales.'\n 'Annual regression slopes (-0.213 / C, -0.140 mmol/mol / C) imply an apparent amplification of inferred SST variations on interannual and longer timescales.'\n 'Annual regression slopes (-0.213 / C, -0.140 mmol/mol / C) imply an apparent amplification of inferred SST variations on interannual and longer timescales.'\n 'Oxygen isotope data has NOT been corrected for the acid fractionation difference (acid-alpha) between standards (calcite) and coral samples (aragonite). Prior to 1895/1896 the data exhibits a kinetic overprint.'\n 'nan' 'Study focuses on use of Sr/U rather than Sr/Ca' 'nan' 'nan'\n 'Sr/Ca-SST recconstructed with composite plus scale method to ERSSTv3b, no regression applied'\n 'Sr/Ca-SST recconstructed with composite plus scale method to ERSSTv3b, no regression applied'\n 'nan' 'nan' 'focused more on Sr/U calibrations'\n 'focused more on Sr/U calibrations' 'nan' 'nan'\n 'regressions in paper use air temps instead of SST, relevant growth information about coral found in PhD thesis (http://nbn-resolving.de/urn:nbn:de:gbv:46-ep000102521)'\n 'See Pfeiffer et al. 2004 for more monthly/bimonthly d18O calibrations and Pfeiffer et al. 2019 for annual d18O and Sr/Ca calibrations.'\n 'See Pfeiffer et al. 2004 for more monthly/bimonthly d18O calibrations and Pfeiffer et al. 2019 for annual d18O and Sr/Ca calibrations.'\n 'Study focuses on use of Sr/U rather than Sr/Ca' 'nan' 'nan'\n 'Fossil Coral' 'regression information based on Kilbourne MS Thesis'\n 'regression information based on Kilbourne MS Thesis'\n 'Microatoll; coral rubble samples; data reported in Supplements of paper'\n 'Study focuses on use of Sr/U rather than Sr/Ca; multiple locations Atlantic and Pacific; Sr/Ca uncertainty 1 deg C; Sr-U uncertainty 0.5 deg C; Sr/Ca-SST slope not indicated'\n 'nan' 'nan'\n 'Microatoll; coral rubble samples; data reported in Supplements of paper'\n 'nan'\n 'Study uses multiple cores from multiple locations in the Great Barrier Reef; spans 15-18S latitude; multiple Sr/Ca regression equations in the paper. Supplemnet has all data.'\n 'd18O is a composite of cores 06SB-A1 and 07SB-A2'\n 'd18O is a composite of cores 06SB-A1 and 07SB-A2' 'nan' 'nan' 'nan'\n 'monthly correlations not reported' 'nan'\n 'Extra information supplied that is not included in publication such as higher precision slope value. Noted as exposed to open ocean; seasonally influenced by river discharge'\n 'uncertainty on Sr/Ca intercept is 0.0018'\n 'uncertainty on Sr/Ca intercept is 0.0018'\n 'Note: Length of coral record increased for Abram et al., 2020 publication relative to Abram et al., 2015'\n 'nan' 'nan' 'nan' 'nan' 'nan' 'nan'\n 'Sr/Ca-SST recconstructed with composite plus scale method to ERSSTv3b, no regression applied'\n 'Sr/Ca-SST recconstructed with composite plus scale method to ERSSTv3b, no regression applied'\n 'Mean SST ranges given in paper for northern reef 7.7C; for southern reefs 5.8C'\n 'in situ d18o; for annual (sr/Ca, slope- -0.0583, intercept - 10.378)'\n 'monthly correlations not reimported' 'nan'\n 'Sr/Ca calibration equation originally found in Ramos et al. 2017; Monthly Sr/Ca data available from 1880-2012; Monthly and seasonal (DJFM vs JJAS, data input on Jan and July) d18O data available from 1894-2012 and 1880-1893, respectively.'\n 'monthly correlations not reported, instead used IOD season' 'nan' 'nan'\n 'nan'\n 'Core was collected in a subhorizontal, not vertical, orientation from coral colony; indistinct growth banding in top 50 years of core'\n 'Core was collected in a subhorizontal, not vertical, orientation from coral colony; indistinct growth banding in top 50 years of core'\n 'nan'\n 'Composite of cores C2B (13.1m depth), C4B (8.2m), C6A (11.3m), and CF1B (found on beach); reconstructed d18Osw data are anomalies'\n 'Published slopes based on composite coral results. Used -0.20.02 permil per 1 deg C regressions'\n 'Published slopes based on composite coral results. Used -0.20.02 permil per 1 deg C regressions'\n 'authors describe density banding as poor; some fish grazing scars'\n 'Sr/Ca-SST recconstructed with composite plus scale method to ERSST3b, no regression applied'\n 'Sr/Ca-SST recconstructed with composite plus scale method to ERSST3b, no regression applied'\n 'Annual regression slopes (-0.29 / C, -0.115 mmol/mol / C) imply an apparent amplification of inferred SST variations on interannual and longer timescales.'\n 'Annual regression slopes (-0.29 / C, -0.115 mmol/mol / C) imply an apparent amplification of inferred SST variations on interannual and longer timescales.'\n 'Annual regression slopes (-0.29 / C, -0.115 mmol/mol / C) imply an apparent amplification of inferred SST variations on interannual and longer timescales.'\n 'Annual regression slopes (-0.29 / C, -0.115 mmol/mol / C) imply an apparent amplification of inferred SST variations on interannual and longer timescales.'\n 'Microatoll with core taken from top and side and spliced together; As with all coral timeseries, exact months are not known, so annual averages sometimes represent more or less than 12 months.'\n 'Microatoll with core taken from top and side and spliced together; As with all coral timeseries, exact months are not known, so annual averages sometimes represent more or less than 12 months.'\n 'This is the bottom part of a core that included an unconformity. Base of the coral is u-series dated and age model is from bands counted up from there.'\n 'This is the bottom part of a core that included an unconformity. Base of the coral is u-series dated and age model is from bands counted up from there.'\n 'This is the bottom part of a core that included an unconformity. Base of the coral is u-series dated and age model is from bands counted up from there.'\n 'This is the bottom part of a core that included an unconformity. Base of the coral is u-series dated and age model is from bands counted up from there.'\n 'Published slopes based on composite coral results. Used -0.20.02 permil per 1 deg C regressions'\n 'Published slopes based on composite coral results. Used -0.20.02 permil per 1 deg C regressions'\n 'nan' 'nan' 'nan'\n 'slope and y-intercept information only available for mean annual calibrations'\n 'used published values (i.e., not locally derived) for Sr/Ca-SST and d18O-SST slopes'\n 'used published values (i.e., not locally derived) for Sr/Ca-SST and d18O-SST slopes'\n 'nan' 'nan' 'See Goodkin et al 2005 for interannual Sr/Ca calibration'\n 'See Goodkin et al 2005 for interannual Sr/Ca calibration'\n 'mm-scale drilling but available data is at annual resolution'\n 'Sr/Ca regression slope error is estimated to be (+/-) 0.21 degrees C; Used the Zinke (2015) method - normalised and scaled to the s.d. SST box of the period 1961-1990; equation based on composite of 2 cores'\n 'Sr/Ca regression slope error is estimated to be (+/-) 0.21 degrees C; Used the Zinke (2015) method - normalised and scaled to the s.d. SST box of the period 1961-1990; equation based on composite of 2 cores'\n 'nan' 'nan'\n '7 calibration equations are available in Boiseau et al. 1998 (this database contains Equation (1) information)'\n 'This paper did not calibrate the d18O proxy or reconstruct temperature. It instead analyzed variability through time by directly using the d18O proxy.'\n 'nan' 'nan' 'nan'\n 'Monthly Sr/Ca data available from 1788-2013; Monthly and seasonal (DJFM vs JJAS, data input on Jan and July) d18O data available from 1906-2013 and 1788-1905 respectively.']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 73/221\n</pre> In\u00a0[36]: Copied! <pre># paleoData_variableName\nkey = 'paleoData_variableName'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n</pre> # paleoData_variableName key = 'paleoData_variableName' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) <pre>paleoData_variableName: \n['Sr/Ca' 'd18O']\n[\"&lt;class 'str'&gt;\"]\n</pre> In\u00a0[37]: Copied! <pre># climate_interpretation\nkey = 'interpretation_direction'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # climate_interpretation key = 'interpretation_direction' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>interpretation_direction: \n['N/A']\nNo. of unique values: 1/221\n</pre> In\u00a0[38]: Copied! <pre># climate_interpretation\nkey = 'interpretation_seasonality'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # climate_interpretation key = 'interpretation_seasonality' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>interpretation_seasonality: \n['N/A']\nNo. of unique values: 1/221\n</pre> In\u00a0[39]: Copied! <pre># climate_interpretation\nkey = 'interpretation_variable'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # climate_interpretation key = 'interpretation_variable' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>interpretation_variable: \n['temperature' 'temperature+moisture']\nNo. of unique values: 2/221\n</pre> In\u00a0[40]: Copied! <pre># climate_interpretation\nkey = 'interpretation_variableDetail'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # climate_interpretation key = 'interpretation_variableDetail' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>interpretation_variableDetail: \n['temperature - manually assigned by DoD2k authors for paleoData_proxy = Sr/Ca'\n 'temperature+moisture - manually assigned by DoD2k authors for paleoData_proxy = d18O']\nNo. of unique values: 2/221\n</pre> In\u00a0[41]: Copied! <pre># # paleoData_values\nkey = 'paleoData_values'\n\nprint('%s: '%key)\nfor ii, vv in enumerate(df[key][:20]):\n    try: \n        print('%-30s: %s -- %s'%(df['dataSetName'].iloc[ii][:30], str(np.nanmin(vv)), str(np.nanmax(vv))))\n        print(type(vv))\n    except: print(df['dataSetName'].iloc[ii], 'NaNs detected.')\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n</pre> # # paleoData_values key = 'paleoData_values'  print('%s: '%key) for ii, vv in enumerate(df[key][:20]):     try:          print('%-30s: %s -- %s'%(df['dataSetName'].iloc[ii][:30], str(np.nanmin(vv)), str(np.nanmax(vv))))         print(type(vv))     except: print(df['dataSetName'].iloc[ii], 'NaNs detected.') print(np.unique([str(type(dd)) for dd in df[key]])) <pre>paleoData_values: \nCH03BUN01                     : -5.758 -- -4.6518\n&lt;class 'numpy.ndarray'&gt;\nZI15MER01                     : 8.80159 -- 9.006902\n&lt;class 'numpy.ndarray'&gt;\nZI15MER01                     : 8.80159 -- 9.006902\n&lt;class 'numpy.ndarray'&gt;\nCO03PAL03                     : -5.38 -- -4.11\n&lt;class 'numpy.ndarray'&gt;\nCO03PAL02                     : -5.295 -- -4.338\n&lt;class 'numpy.ndarray'&gt;\nLI06RAR01                     : -5.13 -- -3.82\n&lt;class 'numpy.ndarray'&gt;\nCO03PAL07                     : -5.51 -- -4.44\n&lt;class 'numpy.ndarray'&gt;\nFL18DTO03                     : 8.891 -- 9.476\n&lt;class 'numpy.ndarray'&gt;\nUR00MAI01                     : -5.304433 -- -3.752342\n&lt;class 'numpy.ndarray'&gt;\nTU95MAD01                     : -5.895 -- -4.578\n&lt;class 'numpy.ndarray'&gt;\nZI04IFR01                     : -5.43 -- -3.41\n&lt;class 'numpy.ndarray'&gt;\nRE18CAY01                     : -4.812 -- -3.629\n&lt;class 'numpy.ndarray'&gt;\nRE18CAY01                     : 8.807 -- 9.1\n&lt;class 'numpy.ndarray'&gt;\nRE18CAY01                     : 8.863 -- 9.043\n&lt;class 'numpy.ndarray'&gt;\nRE18CAY01                     : -4.577 -- -3.915\n&lt;class 'numpy.ndarray'&gt;\nKU99HOU01                     : -4.7 -- -3.04\n&lt;class 'numpy.ndarray'&gt;\nOS13NLP01                     : -6.1125712 -- -5.112277\n&lt;class 'numpy.ndarray'&gt;\nEV98KIR01                     : -5.233 -- -3.748\n&lt;class 'numpy.ndarray'&gt;\nLI00RAR01                     : -4.9993 -- -3.5122\n&lt;class 'numpy.ndarray'&gt;\nLI00RAR01                     : 9.1651 -- 9.75\n&lt;class 'numpy.ndarray'&gt;\n[\"&lt;class 'numpy.ndarray'&gt;\"]\n</pre> In\u00a0[42]: Copied! <pre># paleoData_units\nkey = 'paleoData_units'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # paleoData_units key = 'paleoData_units' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>paleoData_units: \n['mmol/mol' 'permil']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 2/221\n</pre> In\u00a0[43]: Copied! <pre># # year\nkey = 'year'\nprint('%s: '%key)\nfor ii, vv in enumerate(df[key][:20]):\n    try: print('%-30s: %s -- %s'%(df['dataSetName'].iloc[ii][:30], str(np.nanmin(vv)), str(np.nanmax(vv))))\n    except: print('NaNs detected.', vv)\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n</pre> # # year key = 'year' print('%s: '%key) for ii, vv in enumerate(df[key][:20]):     try: print('%-30s: %s -- %s'%(df['dataSetName'].iloc[ii][:30], str(np.nanmin(vv)), str(np.nanmax(vv))))     except: print('NaNs detected.', vv) print(np.unique([str(type(dd)) for dd in df[key]])) <pre>year: \nCH03BUN01                     : 1860.0 -- 1990.58\nZI15MER01                     : 1891.0 -- 2009.0\nZI15MER01                     : 1891.0 -- 2009.0\nCO03PAL03                     : 1317.17 -- 1406.49\nCO03PAL02                     : 1149.08 -- 1220.205\nLI06RAR01                     : 1906.88 -- 1999.75\nCO03PAL07                     : 1635.02 -- 1666.48\nFL18DTO03                     : 1997.646 -- 2012.208\nUR00MAI01                     : 1840.0 -- 1994.5\nTU95MAD01                     : 1922.542 -- 1991.292\nZI04IFR01                     : 1659.625 -- 1995.625\nRE18CAY01                     : 1887.04 -- 2012.54\nRE18CAY01                     : 1887.04 -- 2012.54\nRE18CAY01                     : 1887.0 -- 2011.0\nRE18CAY01                     : 1887.0 -- 2011.0\nKU99HOU01                     : 1794.71 -- 1994.38\nOS13NLP01                     : 1990.17 -- 2008.17\nEV98KIR01                     : 1938.292 -- 1993.625\nLI00RAR01                     : 1726.753 -- 1996.8641\nLI00RAR01                     : 1726.753 -- 1996.8641\n[\"&lt;class 'numpy.ndarray'&gt;\"]\n</pre> In\u00a0[44]: Copied! <pre># yearUnits\nkey = 'yearUnits'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # yearUnits key = 'yearUnits' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>yearUnits: \n['CE']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 1/221\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/load_ch2k/#load-coralhydro-2k","title":"Load CoralHydro 2k\u00b6","text":""},{"location":"notebooks/load_ch2k/#set-up-working-environment","title":"Set up working environment\u00b6","text":""},{"location":"notebooks/load_ch2k/#load-source-data","title":"Load source data\u00b6","text":""},{"location":"notebooks/load_ch2k/#create-compact-dataframe","title":"Create compact dataframe\u00b6","text":""},{"location":"notebooks/load_ch2k/#populate-dataframe","title":"populate dataframe\u00b6","text":""},{"location":"notebooks/load_ch2k/#rename-entries-according-to-standard-terminology","title":"Rename entries according to standard terminology\u00b6","text":""},{"location":"notebooks/load_ch2k/#drop-missing-entries-and-standardize-missing-data-format","title":"Drop missing entries and standardize missing data format\u00b6","text":""},{"location":"notebooks/load_ch2k/#save-compact-dataframe","title":"save compact dataframe\u00b6","text":""},{"location":"notebooks/load_ch2k/#save-pickle","title":"save pickle\u00b6","text":""},{"location":"notebooks/load_ch2k/#save-csv","title":"save csv\u00b6","text":""},{"location":"notebooks/load_ch2k/#visualise-dataframe","title":"Visualise dataframe\u00b6","text":""},{"location":"notebooks/load_ch2k/#display-dataframe","title":"Display dataframe\u00b6","text":""},{"location":"notebooks/load_ch2k/#display-identification-metadata-datasetname-datasetid-originaldataurl-originaldatabase","title":"Display identification metadata: dataSetName, datasetId, originalDataURL, originalDatabase\u00b6","text":""},{"location":"notebooks/load_ch2k/#index","title":"index\u00b6","text":""},{"location":"notebooks/load_ch2k/#datasetname-associated-with-each-record-may-not-be-unique","title":"dataSetName (associated with each record, may not be unique)\u00b6","text":""},{"location":"notebooks/load_ch2k/#datasetid-unique-identifier-as-given-by-original-authors-includes-original-database-token","title":"datasetId (unique identifier, as given by original authors, includes original database token)\u00b6","text":""},{"location":"notebooks/load_ch2k/#originaldataurl-urldoi-of-original-published-record-where-available","title":"originalDataURL (URL/DOI of original published record where available)\u00b6","text":""},{"location":"notebooks/load_ch2k/#originaldatabase-original-database-used-as-input-for-dataframe","title":"originalDatabase (original database used as input for dataframe)\u00b6","text":""},{"location":"notebooks/load_ch2k/#geographical-metadata-elevation-latitude-longitude-site-name","title":"geographical metadata: elevation, latitude, longitude, site name\u00b6","text":""},{"location":"notebooks/load_ch2k/#geo_meanelev-mean-elevation-in-m","title":"geo_meanElev (mean elevation in m)\u00b6","text":""},{"location":"notebooks/load_ch2k/#geo_meanlat-mean-latitude-in-degrees-n","title":"geo_meanLat (mean latitude in degrees N)\u00b6","text":""},{"location":"notebooks/load_ch2k/#geo_meanlon-mean-longitude","title":"geo_meanLon (mean longitude)\u00b6","text":""},{"location":"notebooks/load_ch2k/#geo_sitename-name-of-collection-site","title":"geo_siteName (name of collection site)\u00b6","text":""},{"location":"notebooks/load_ch2k/#proxy-metadata-archive-type-proxy-type-interpretation","title":"proxy metadata: archive type, proxy type, interpretation\u00b6","text":""},{"location":"notebooks/load_ch2k/#archivetype-archive-type","title":"archiveType (archive type)\u00b6","text":""},{"location":"notebooks/load_ch2k/#paleodata_proxy-proxy-type","title":"paleoData_proxy (proxy type)\u00b6","text":""},{"location":"notebooks/load_ch2k/#paleodata_sensorspecies-further-information-on-proxy-type-species","title":"paleoData_sensorSpecies (further information on proxy type: species)\u00b6","text":""},{"location":"notebooks/load_ch2k/#paleodata_notes-notes","title":"paleoData_notes (notes)\u00b6","text":""},{"location":"notebooks/load_ch2k/#paleodata_variablename","title":"paleoData_variableName\u00b6","text":""},{"location":"notebooks/load_ch2k/#climate-metadata-interpretation-variable-direction-seasonality","title":"climate metadata: interpretation variable, direction, seasonality\u00b6","text":""},{"location":"notebooks/load_ch2k/#interpretation_direction","title":"interpretation_direction\u00b6","text":""},{"location":"notebooks/load_ch2k/#interpretation_seasonality","title":"interpretation_seasonality\u00b6","text":""},{"location":"notebooks/load_ch2k/#interpretation_variable","title":"interpretation_variable\u00b6","text":""},{"location":"notebooks/load_ch2k/#interpretation_variabledetail","title":"interpretation_variableDetail\u00b6","text":""},{"location":"notebooks/load_ch2k/#data","title":"data\u00b6","text":""},{"location":"notebooks/load_ch2k/#paleodata_values","title":"paleoData_values\u00b6","text":""},{"location":"notebooks/load_ch2k/#paleodata_units","title":"paleoData_units\u00b6","text":""},{"location":"notebooks/load_ch2k/#year","title":"year\u00b6","text":""},{"location":"notebooks/load_ch2k/#yearunits","title":"yearUnits\u00b6","text":""},{"location":"notebooks/load_fe23/","title":"Load FE23","text":"<p>load TRW data from FE23 (https://doi.org/10.25921/8hpf-a451)</p> <p>Dataset downloaded from NCEI: https://www.ncei.noaa.gov/access/paleo-search/study/36773</p> <p>Created 25/10/2024 by Lucie Luecke (LL)</p> <p>Updated 24/10/2025 by LL: tidied up and streamlined for documentation and publication Updated 21/11/2024 by LL: added csv saving of compact dataframe, removed redundant output.</p> <p>Here we extract a dataframe with the following columns:</p> <ul> <li><code>archiveType</code></li> <li><code>dataSetName</code></li> <li><code>datasetId</code></li> <li><code>geo_meanElev</code></li> <li><code>geo_meanLat</code></li> <li><code>geo_meanLon</code></li> <li><code>geo_siteName</code></li> <li><code>interpretation_direction</code> (new in v2.0)</li> <li><code>interpretation_variable</code></li> <li><code>interpretation_variableDetail</code></li> <li><code>interpretation_seasonality</code> (new in v2.0)</li> <li><code>originalDataURL</code></li> <li><code>originalDatabase</code></li> <li><code>paleoData_notes</code></li> <li><code>paleoData_proxy</code></li> <li><code>paleoData_sensorSpecies</code></li> <li><code>paleoData_units</code></li> <li><code>paleoData_values</code></li> <li><code>paleoData_variableName</code></li> <li><code>year</code></li> <li><code>yearUnits</code></li> </ul> <p>We save a standardised compact dataframe for concatenation to DoD2k</p> <p>Make sure the repo_root is added correctly, it should be: your_root_dir/dod2k This should be the working directory throughout this notebook (and all other notebooks).</p> In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n\nimport sys\nimport os\nfrom pathlib import Path\n\n# Add parent directory to path (works from any notebook in notebooks/)\n# the repo_root should be the parent directory of the notebooks folder\ninit_dir = Path().resolve()\nprint(init_dir)\n# Determine repo root\nif init_dir.name == 'dod2k': repo_root = init_dir\nelif init_dir.parent.name == 'dod2k': repo_root = init_dir.parent\nelif init_dir.parent.parent.name == 'dod2k': repo_root = init_dir.parent.parent\nelse: raise Exception('Please review the repo root structure (see first cell).')\n\n# Update cwd and path only if needed\nif os.getcwd() != str(repo_root):\n    os.chdir(repo_root)\nif str(repo_root) not in sys.path:\n    sys.path.insert(0, str(repo_root))\n\nprint(f\"Repo root: {repo_root}\")\nif str(os.getcwd())==str(repo_root):\n    print(f\"Working directory matches repo root. \")\n</pre> %load_ext autoreload %autoreload 2  import sys import os from pathlib import Path  # Add parent directory to path (works from any notebook in notebooks/) # the repo_root should be the parent directory of the notebooks folder init_dir = Path().resolve() print(init_dir) # Determine repo root if init_dir.name == 'dod2k': repo_root = init_dir elif init_dir.parent.name == 'dod2k': repo_root = init_dir.parent elif init_dir.parent.parent.name == 'dod2k': repo_root = init_dir.parent.parent else: raise Exception('Please review the repo root structure (see first cell).')  # Update cwd and path only if needed if os.getcwd() != str(repo_root):     os.chdir(repo_root) if str(repo_root) not in sys.path:     sys.path.insert(0, str(repo_root))  print(f\"Repo root: {repo_root}\") if str(os.getcwd())==str(repo_root):     print(f\"Working directory matches repo root. \") <pre>/home/jupyter-lluecke/dod2k_v2.0/dod2k/notebooks\nRepo root: /home/jupyter-lluecke/dod2k_v2.0/dod2k\nWorking directory matches repo root. \n</pre> In\u00a0[2]: Copied! <pre>import xarray as xr\nimport pandas as pd\nimport numpy as np\n\nfrom dod2k_utilities import ut_functions as utf # contains utility functions\nfrom dod2k_utilities import ut_plot as uplt # contains plotting functions\n</pre> import xarray as xr import pandas as pd import numpy as np  from dod2k_utilities import ut_functions as utf # contains utility functions from dod2k_utilities import ut_plot as uplt # contains plotting functions <p>Specify the data and metadata which we are looking to extract from FE23 for the standardised 'compact dataframe':</p> In\u00a0[3]: Copied! <pre>vars = ['chronos', 'lonlat', 'investigator', 'trwsSm', 'chronology', 'country', 'species', \n        'elevation', 'sitename', 'treetime']\n</pre> vars = ['chronos', 'lonlat', 'investigator', 'trwsSm', 'chronology', 'country', 'species',          'elevation', 'sitename', 'treetime'] <p>In order to get the source data, run the cell below (Warning: this is downloading a very large netCDF), which downloads the full dataset from NCEI (25GB) and extracts a slice based on the relevant metadata (~60MB).</p> <p>Alternatively skip the cell and directly use the slice as provided in this directory (see cell below next).</p> In\u00a0[4]: Copied! <pre># # download and unzip FE23 \n# !wget -O /data/fe23/franke2022-fe23.nc https://www.ncei.noaa.gov/pub/data/paleo/contributions_by_author/franke2022/franke2022-fe23.nc\n# fe23_full  = xr.open_dataset('fe23/franke2022-fe23.nc')\n\n# # save slice of FE23 with only relevant variables as netCDF (fe23_full is 25GB)\n# fe23_slice = fe23_full[vars]\n# fe23_slice.to_netcdf('data/fe23/franke2022-fe23_slice.nc')\n</pre> # # download and unzip FE23  # !wget -O /data/fe23/franke2022-fe23.nc https://www.ncei.noaa.gov/pub/data/paleo/contributions_by_author/franke2022/franke2022-fe23.nc # fe23_full  = xr.open_dataset('fe23/franke2022-fe23.nc')  # # save slice of FE23 with only relevant variables as netCDF (fe23_full is 25GB) # fe23_slice = fe23_full[vars] # fe23_slice.to_netcdf('data/fe23/franke2022-fe23_slice.nc') In\u00a0[5]: Copied! <pre>fe23_slice = xr.open_dataset('data/fe23/franke2022-fe23_slice.nc')\n</pre> fe23_slice = xr.open_dataset('data/fe23/franke2022-fe23_slice.nc') In\u00a0[6]: Copied! <pre>print(fe23_slice)\n</pre> print(fe23_slice) <pre>&lt;xarray.Dataset&gt; Size: 58MB\nDimensions:       (ttime: 1159, nseries: 278, nregion: 22, lonlat: 2,\n                   nchars_cinv: 42, nchars_chr: 32, nchars_ctry: 22,\n                   nchars_csp: 6, nchars_cn: 51)\nCoordinates:\n    lonlat        (nseries, nregion, lonlat) float64 98kB ...\nDimensions without coordinates: ttime, nseries, nregion, nchars_cinv,\n                                nchars_chr, nchars_ctry, nchars_csp, nchars_cn\nData variables:\n    chronos       (ttime, nseries, nregion) float64 57MB ...\n    investigator  (nchars_cinv, nseries, nregion) |S1 257kB ...\n    trwsSm        (nseries, nregion) float64 49kB ...\n    chronology    (nchars_chr, nseries, nregion) |S1 196kB ...\n    country       (nchars_ctry, nseries, nregion) |S1 135kB ...\n    species       (nchars_csp, nseries, nregion) |S1 37kB ...\n    elevation     (nseries, nregion) float64 49kB ...\n    sitename      (nchars_cn, nseries, nregion) |S1 312kB ...\n    treetime      (ttime) float64 9kB ...\nAttributes:\n    reference:      Franke, J; Evans, MN; Schurer, AP; Hegerl, GC, 2022, Clim...\n    doi:            https://doi.org/10.25921/8hpf-a451\n    creation_time:  27-Oct-2024 11:45:29\n</pre> In\u00a0[7]: Copied! <pre>df_fe23 = {}\n\nfor var in vars:\n    print(var)\n    df_fe23[var] = []\n    for ii in fe23_slice.nregion:        # loop through the regions\n        fe23_slice[var] = np.squeeze(fe23_slice[var])\n        # print(fe23_full[var].shape)\n        for jj in fe23_slice.nseries:        # loop through the records in any one region\n            if var in ['chronos']:  data = fe23_slice[var][:, jj, ii].data\n            elif var in ['trwsSm', 'elevation']: data = float(fe23_slice[var][jj, ii].data)\n            elif var in ['lonlat', 'trwsSm']:    data = fe23_slice[var][jj, ii, :].data\n            elif var in ['investigator', 'chronology', 'country', 'species', 'sitename']:\n                data = b''.join([ss for ss in fe23_slice[var][:, jj, ii].data]).decode(\"latin-1\").replace(' ','')\n    \n            if ~np.all(np.isnan(fe23_slice['chronos'][:, jj, ii].data)):\n                df_fe23[var].append(data)\n# len(all_trees)\n</pre> df_fe23 = {}  for var in vars:     print(var)     df_fe23[var] = []     for ii in fe23_slice.nregion:        # loop through the regions         fe23_slice[var] = np.squeeze(fe23_slice[var])         # print(fe23_full[var].shape)         for jj in fe23_slice.nseries:        # loop through the records in any one region             if var in ['chronos']:  data = fe23_slice[var][:, jj, ii].data             elif var in ['trwsSm', 'elevation']: data = float(fe23_slice[var][jj, ii].data)             elif var in ['lonlat', 'trwsSm']:    data = fe23_slice[var][jj, ii, :].data             elif var in ['investigator', 'chronology', 'country', 'species', 'sitename']:                 data = b''.join([ss for ss in fe23_slice[var][:, jj, ii].data]).decode(\"latin-1\").replace(' ','')                  if ~np.all(np.isnan(fe23_slice['chronos'][:, jj, ii].data)):                 df_fe23[var].append(data) # len(all_trees)  <pre>chronos\nlonlat\ninvestigator\ntrwsSm\nchronology\ncountry\nspecies\nelevation\nsitename\ntreetime\n</pre> <p>Create empty dataframe and populate with the data from the netCDF</p> In\u00a0[8]: Copied! <pre>df_compact = pd.DataFrame(columns=['archiveType', 'interpretation_variable', 'dataSetName', 'datasetId', \n                                   'geo_meanElev', 'geo_meanLat', 'geo_meanLon', 'geo_siteName', \n                                   'originalDatabase', 'originalDataURL', 'paleoData_notes', 'paleoData_proxy', \n                                   'paleoData_units', 'paleoData_values', 'year', 'yearUnits'])\n</pre> df_compact = pd.DataFrame(columns=['archiveType', 'interpretation_variable', 'dataSetName', 'datasetId',                                     'geo_meanElev', 'geo_meanLat', 'geo_meanLon', 'geo_siteName',                                     'originalDatabase', 'originalDataURL', 'paleoData_notes', 'paleoData_proxy',                                     'paleoData_units', 'paleoData_values', 'year', 'yearUnits']) In\u00a0[9]: Copied! <pre>df_compact['paleoData_values'] = df_fe23['chronos']\ndf_compact['year']             = [fe23_slice.treetime.data for ii in range(len(df_compact))]\n</pre> df_compact['paleoData_values'] = df_fe23['chronos'] df_compact['year']             = [fe23_slice.treetime.data for ii in range(len(df_compact))] <p>The netCDF has a homogeneous time coordinate, but may have missing values. We use only not-nan data:</p> In\u00a0[10]: Copied! <pre>for ii in df_compact.index:\n    dd=utf.convert_to_nparray(df_compact.at[ii, 'paleoData_values'])\n    df_compact.at[ii, 'paleoData_values']=dd.data[~dd.mask]\n    df_compact.at[ii, 'year']=np.array(df_compact.at[ii, 'year'])[~dd.mask]\n</pre> for ii in df_compact.index:     dd=utf.convert_to_nparray(df_compact.at[ii, 'paleoData_values'])     df_compact.at[ii, 'paleoData_values']=dd.data[~dd.mask]     df_compact.at[ii, 'year']=np.array(df_compact.at[ii, 'year'])[~dd.mask] In\u00a0[11]: Copied! <pre>df_compact[['geo_meanLon', 'geo_meanLat']] = df_fe23['lonlat']\ndf_compact['geo_meanElev']                 = df_fe23['elevation']\n</pre> df_compact[['geo_meanLon', 'geo_meanLat']] = df_fe23['lonlat'] df_compact['geo_meanElev']                 = df_fe23['elevation'] In\u00a0[12]: Copied! <pre>df_compact['datasetId']   = df_fe23['chronology']\ndf_compact['datasetId']   = df_compact['datasetId'].apply(lambda x: x.replace('.rwl',''))\ndf_compact['dataSetName'] = df_compact['datasetId']\ndf_compact['datasetId']   = df_compact['datasetId'].apply(lambda x: 'FE23_'+x)\n</pre> df_compact['datasetId']   = df_fe23['chronology'] df_compact['datasetId']   = df_compact['datasetId'].apply(lambda x: x.replace('.rwl','')) df_compact['dataSetName'] = df_compact['datasetId'] df_compact['datasetId']   = df_compact['datasetId'].apply(lambda x: 'FE23_'+x) <p>Keep populating the metadata columns from the netCDF metadata.</p> <p>The original data URL can be reconstructed from NCEI using the dataSetName.</p> In\u00a0[13]: Copied! <pre>url = 'https://www.ncei.noaa.gov/pub/data/paleo/treering/measurements/'\ndf_compact['geo_siteName']            = df_fe23['sitename']\ndf_compact['paleoData_sensorSpecies'] = df_fe23['species']\ndf_compact['paleoData_notes']         = df_fe23['investigator']\ndf_compact['paleoData_notes']         = df_compact['paleoData_notes'].apply(lambda x: 'Investigator: '+x)\ndf_compact['originalDataURL']         = df_compact['dataSetName'].apply(lambda x: url+x.replace('_','/')+'-noaa.rwl')\n</pre> url = 'https://www.ncei.noaa.gov/pub/data/paleo/treering/measurements/' df_compact['geo_siteName']            = df_fe23['sitename'] df_compact['paleoData_sensorSpecies'] = df_fe23['species'] df_compact['paleoData_notes']         = df_fe23['investigator'] df_compact['paleoData_notes']         = df_compact['paleoData_notes'].apply(lambda x: 'Investigator: '+x) df_compact['originalDataURL']         = df_compact['dataSetName'].apply(lambda x: url+x.replace('_','/')+'-noaa.rwl') In\u00a0[14]: Copied! <pre>df_compact['archiveType']      = 'Wood' # fills called 'paleoData_variableName' \ndf_compact['paleoData_proxy']  = 'ring width' # fills column called 'paleoData_variableName' \ndf_compact['paleoData_units']  = 'standardized_anomalies' # fills column called 'paleoData_units' \ndf_compact['originalDatabase'] = 'FE23 (Breitenmoser et al. (2014))' # fills column 'originalDatabase' \ndf_compact['yearUnits']        = 'CE'  # fills column 'yearUnits'\ndf_compact['paleoData_variableName'] = 'ring width'  # fills column 'yearUnits'\n</pre> df_compact['archiveType']      = 'Wood' # fills called 'paleoData_variableName'  df_compact['paleoData_proxy']  = 'ring width' # fills column called 'paleoData_variableName'  df_compact['paleoData_units']  = 'standardized_anomalies' # fills column called 'paleoData_units'  df_compact['originalDatabase'] = 'FE23 (Breitenmoser et al. (2014))' # fills column 'originalDatabase'  df_compact['yearUnits']        = 'CE'  # fills column 'yearUnits' df_compact['paleoData_variableName'] = 'ring width'  # fills column 'yearUnits' <p>The climate interpretation variable in the netCDF is given as an integer (1: temperature sensitive, 2: moisture sensitive, 3: temperature and moisture sensitive, 4: not temperature and not moisture sensitive)</p> In\u00a0[15]: Copied! <pre>TM = {1.:'temperature', 2.:'moisture', 3.:'temperature+moisture', 4.: 'NOT temperature NOT moisture', 0:'nan'}\ndf_compact['interpretation_variable'] = df_fe23['trwsSm']\ndf_compact['interpretation_variable'] = df_compact['interpretation_variable'].apply(lambda x: TM[x] if ~np.isnan(x) else 'N/A')\ndf_compact['interpretation_variableDetail'] = 'N/A'\ndf_compact['interpretation_seasonality'] = 'N/A'\ndf_compact['interpretation_direction'] = 'N/A'\n</pre> TM = {1.:'temperature', 2.:'moisture', 3.:'temperature+moisture', 4.: 'NOT temperature NOT moisture', 0:'nan'} df_compact['interpretation_variable'] = df_fe23['trwsSm'] df_compact['interpretation_variable'] = df_compact['interpretation_variable'].apply(lambda x: TM[x] if ~np.isnan(x) else 'N/A') df_compact['interpretation_variableDetail'] = 'N/A' df_compact['interpretation_seasonality'] = 'N/A' df_compact['interpretation_direction'] = 'N/A' <p>Drop rows with no data, all zero rows, all nan rows, all constant rows</p> In\u00a0[16]: Copied! <pre>drop_inds = []\nfor ii in range(df_compact.shape[0]):\n    if len(df_compact.iloc[ii]['year'])==0:\n        print('empty', ii, df_compact.iloc[ii]['year'], df_compact.iloc[ii]['originalDatabase'])\n        print(df_compact.iloc[ii]['paleoData_values'])\n        drop_inds += [df_compact.index[ii]]\n        \nfor ii, row in enumerate(df_compact.paleoData_values):\n    if np.std(row)==0: \n        print(ii, 'std=0')\n    elif np.sum(np.diff(row)**2)==0: \n        print(ii, 'diff=0')\n    elif np.isnan(np.std(row)):\n        print(ii, 'std nan')\n    else:\n        continue\n    if df.index[ii] not in drop_inds: \n        drop_inds += [df_compact.index[ii]]\n    \nprint(drop_inds)\ndf_compact = df_compact.drop(index=drop_inds)\n</pre> drop_inds = [] for ii in range(df_compact.shape[0]):     if len(df_compact.iloc[ii]['year'])==0:         print('empty', ii, df_compact.iloc[ii]['year'], df_compact.iloc[ii]['originalDatabase'])         print(df_compact.iloc[ii]['paleoData_values'])         drop_inds += [df_compact.index[ii]]          for ii, row in enumerate(df_compact.paleoData_values):     if np.std(row)==0:          print(ii, 'std=0')     elif np.sum(np.diff(row)**2)==0:          print(ii, 'diff=0')     elif np.isnan(np.std(row)):         print(ii, 'std nan')     else:         continue     if df.index[ii] not in drop_inds:          drop_inds += [df_compact.index[ii]]      print(drop_inds) df_compact = df_compact.drop(index=drop_inds) <pre>[]\n</pre> <p>Check that the datasetId is unique and that each record has an ID</p> In\u00a0[17]: Copied! <pre>#  check that the datasetId is unique \nassert len(df_compact.datasetId.unique())==len(df_compact)\n</pre> #  check that the datasetId is unique  assert len(df_compact.datasetId.unique())==len(df_compact) In\u00a0[18]: Copied! <pre># save to a pickle file (security: is it better to save to csv?)\ndf_compact = df_compact[sorted(df_compact.columns)]\ndf_compact.to_pickle('data/fe23/fe23_compact.pkl')\n</pre> # save to a pickle file (security: is it better to save to csv?) df_compact = df_compact[sorted(df_compact.columns)] df_compact.to_pickle('data/fe23/fe23_compact.pkl') In\u00a0[19]: Copied! <pre># save to a list of csv files (metadata, data, year)\ndf_compact.name='fe23'\nutf.write_compact_dataframe_to_csv(df_compact)\n</pre> # save to a list of csv files (metadata, data, year) df_compact.name='fe23' utf.write_compact_dataframe_to_csv(df_compact) <pre>METADATA: datasetId, archiveType, dataSetName, geo_meanElev, geo_meanLat, geo_meanLon, geo_siteName, interpretation_direction, interpretation_seasonality, interpretation_variable, interpretation_variableDetail, originalDataURL, originalDatabase, paleoData_notes, paleoData_proxy, paleoData_sensorSpecies, paleoData_units, paleoData_variableName, yearUnits\nSaved to /home/jupyter-lluecke/dod2k_v2.0/dod2k/data/fe23/fe23_compact_%s.csv\n</pre> In\u00a0[20]: Copied! <pre># load dataframe to check that it loads correctly\ndf = utf.load_compact_dataframe_from_csv('fe23')\n</pre> # load dataframe to check that it loads correctly df = utf.load_compact_dataframe_from_csv('fe23') <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2754 entries, 0 to 2753\nData columns (total 21 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   archiveType                    2754 non-null   object \n 1   dataSetName                    2754 non-null   object \n 2   datasetId                      2754 non-null   object \n 3   geo_meanElev                   2710 non-null   float32\n 4   geo_meanLat                    2754 non-null   float32\n 5   geo_meanLon                    2754 non-null   float32\n 6   geo_siteName                   2754 non-null   object \n 7   interpretation_direction       2754 non-null   object \n 8   interpretation_seasonality     2754 non-null   object \n 9   interpretation_variable        2754 non-null   object \n 10  interpretation_variableDetail  2754 non-null   object \n 11  originalDataURL                2754 non-null   object \n 12  originalDatabase               2754 non-null   object \n 13  paleoData_notes                2754 non-null   object \n 14  paleoData_proxy                2754 non-null   object \n 15  paleoData_sensorSpecies        2754 non-null   object \n 16  paleoData_units                2754 non-null   object \n 17  paleoData_values               2754 non-null   object \n 18  paleoData_variableName         2754 non-null   object \n 19  year                           2754 non-null   object \n 20  yearUnits                      2754 non-null   object \ndtypes: float32(3), object(18)\nmemory usage: 419.7+ KB\nNone\n</pre> <p>Show spatial distribution of records, show archive and proxy types</p> In\u00a0[21]: Copied! <pre># count archive types\narchive_count = {}\nfor ii, at in enumerate(set(df['archiveType'])):\n    archive_count[at] = df.loc[df['archiveType']==at, 'archiveType'].count()\n\nsort = np.argsort([cc for cc in archive_count.values()])\narchives_sorted = np.array([cc for cc in archive_count.keys()])[sort][::-1]\n\n# Specify colour for each archive (smaller archives get grouped into the same colour)\narchive_colour, major_archives, other_archives = uplt.get_archive_colours(archives_sorted, archive_count)\n\nfig = uplt.plot_geo_archive_proxy(df, archive_colour)\nutf.save_fig(fig, f'geo_{df.name}', dir=df.name)\n</pre> # count archive types archive_count = {} for ii, at in enumerate(set(df['archiveType'])):     archive_count[at] = df.loc[df['archiveType']==at, 'archiveType'].count()  sort = np.argsort([cc for cc in archive_count.values()]) archives_sorted = np.array([cc for cc in archive_count.keys()])[sort][::-1]  # Specify colour for each archive (smaller archives get grouped into the same colour) archive_colour, major_archives, other_archives = uplt.get_archive_colours(archives_sorted, archive_count)  fig = uplt.plot_geo_archive_proxy(df, archive_colour) utf.save_fig(fig, f'geo_{df.name}', dir=df.name) <pre>0 Wood 2754\nsaved figure in /home/jupyter-lluecke/dod2k_v2.0/dod2k/figs/fe23/geo_fe23.pdf\n</pre> <p>Now plot the coverage over the Common Era</p> In\u00a0[22]: Copied! <pre>fig = uplt.plot_coverage(df, archives_sorted, major_archives, other_archives, archive_colour)\nutf.save_fig(fig, f'time_{df.name}', dir=df.name)\n</pre> fig = uplt.plot_coverage(df, archives_sorted, major_archives, other_archives, archive_colour) utf.save_fig(fig, f'time_{df.name}', dir=df.name) <pre>saved figure in /home/jupyter-lluecke/dod2k_v2.0/dod2k/figs/fe23/time_fe23.pdf\n</pre> In\u00a0[23]: Copied! <pre># # check index\nprint(df.index)\n</pre> # # check index print(df.index) <pre>RangeIndex(start=0, stop=2754, step=1)\n</pre> In\u00a0[24]: Copied! <pre># # check dataSetName\nkey = 'dataSetName'\nprint('%s: '%key)\nprint(df[key].values)\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n</pre> # # check dataSetName key = 'dataSetName' print('%s: '%key) print(df[key].values) print(np.unique([str(type(dd)) for dd in df[key]])) <pre>dataSetName: \n['africa_keny001' 'africa_keny002' 'africa_morc001' ...\n 'northamerica_usa_wy034' 'northamerica_usa_wy035'\n 'northamerica_usa_wy036']\n[\"&lt;class 'str'&gt;\"]\n</pre> In\u00a0[25]: Copied! <pre># # check datasetId\n\nprint(len(df.datasetId.unique()))\nprint(len(df))\nkey = 'datasetId'\nprint('%s (starts with): '%key)\nprint(df[key].values)\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint('datasetId starts with: ', np.unique([str(dd.split('_')[0]) for dd in df[key]]))\n</pre> # # check datasetId  print(len(df.datasetId.unique())) print(len(df)) key = 'datasetId' print('%s (starts with): '%key) print(df[key].values) print(np.unique([str(type(dd)) for dd in df[key]])) print('datasetId starts with: ', np.unique([str(dd.split('_')[0]) for dd in df[key]])) <pre>2754\n2754\ndatasetId (starts with): \n['FE23_africa_keny001' 'FE23_africa_keny002' 'FE23_africa_morc001' ...\n 'FE23_northamerica_usa_wy034' 'FE23_northamerica_usa_wy035'\n 'FE23_northamerica_usa_wy036']\n[\"&lt;class 'str'&gt;\"]\ndatasetId starts with:  ['FE23']\n</pre> In\u00a0[26]: Copied! <pre># originalDataURL\nkey = 'originalDataURL'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([kk for kk in df[key] if 'this' in kk]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n# 'this study' should point to the correct URL (PAGES2k)\n</pre> # originalDataURL key = 'originalDataURL' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([kk for kk in df[key] if 'this' in kk])) print(np.unique([str(type(dd)) for dd in df[key]])) # 'this study' should point to the correct URL (PAGES2k) <pre>originalDataURL: \n['https://www.ncei.noaa.gov/pub/data/paleo/treering/measurements/africa/keny001-noaa.rwl'\n 'https://www.ncei.noaa.gov/pub/data/paleo/treering/measurements/africa/keny002-noaa.rwl'\n 'https://www.ncei.noaa.gov/pub/data/paleo/treering/measurements/africa/morc001-noaa.rwl'\n ...\n 'https://www.ncei.noaa.gov/pub/data/paleo/treering/measurements/southamerica/chil016-noaa.rwl'\n 'https://www.ncei.noaa.gov/pub/data/paleo/treering/measurements/southamerica/chil017-noaa.rwl'\n 'https://www.ncei.noaa.gov/pub/data/paleo/treering/measurements/southamerica/chil018-noaa.rwl']\n[]\n[\"&lt;class 'str'&gt;\"]\n</pre> In\u00a0[27]: Copied! <pre># # originalDataSet\nkey = 'originalDatabase'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n# Note: the last two records have missing URLs\n</pre> # # originalDataSet key = 'originalDatabase' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) # Note: the last two records have missing URLs <pre>originalDatabase: \n['FE23 (Breitenmoser et al. (2014))']\n[\"&lt;class 'str'&gt;\"]\n</pre> In\u00a0[28]: Copied! <pre># check Elevation\nkey = 'geo_meanElev'\nprint('%s: '%key)\nprint(df[key])\nprint(np.unique(['%d'%kk for kk in df[key] if np.isfinite(kk)]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n</pre> # check Elevation key = 'geo_meanElev' print('%s: '%key) print(df[key]) print(np.unique(['%d'%kk for kk in df[key] if np.isfinite(kk)])) print(np.unique([str(type(dd)) for dd in df[key]])) <pre>geo_meanElev: \n0       2010.0\n1       2010.0\n2       2200.0\n3       1700.0\n4       2200.0\n         ...  \n2749    2500.0\n2750    2542.0\n2751    1319.0\n2752    2400.0\n2753    2378.0\nName: geo_meanElev, Length: 2754, dtype: float32\n['0' '1' '10' '100' '1000' '1002' '1005' '1006' '101' '1010' '1020' '1030'\n '1036' '1040' '1047' '105' '1050' '1051' '1052' '1055' '1060' '1065'\n '1067' '107' '1070' '1071' '1075' '108' '1080' '1085' '109' '1090' '1095'\n '1097' '110' '1100' '111' '1110' '1120' '1128' '1130' '1132' '1140'\n '1146' '115' '1150' '1155' '1156' '1158' '116' '1160' '1167' '1169'\n '1170' '1175' '1180' '1194' '12' '120' '1200' '1201' '1206' '1208' '1219'\n '1220' '1224' '1225' '1230' '1231' '1234' '1235' '1237' '1240' '1250'\n '1253' '126' '1260' '1270' '1275' '1280' '1285' '13' '130' '1300' '1302'\n '131' '1310' '1311' '1315' '1317' '1319' '1320' '1325' '1330' '1340'\n '135' '1350' '1354' '136' '1360' '1366' '1367' '1370' '1372' '1375'\n '1377' '138' '1380' '1385' '1390' '1391' '1392' '1395' '14' '140' '1400'\n '1402' '1405' '1410' '1415' '1417' '1418' '1420' '1425' '143' '1432'\n '1433' '1436' '1440' '1448' '145' '1450' '1460' '1463' '1464' '1465'\n '1468' '1469' '1470' '1474' '1475' '1480' '149' '1490' '1493' '1494'\n '1495' '15' '150' '1500' '1510' '152' '1520' '1524' '1525' '153' '1530'\n '1531' '1540' '1545' '1550' '1555' '1560' '1565' '1570' '1580' '1585'\n '1586' '1595' '1596' '1598' '16' '160' '1600' '1601' '1620' '1625' '1630'\n '1633' '164' '1640' '1644' '1645' '165' '1650' '1656' '1658' '1660'\n '1670' '1675' '1676' '168' '1680' '1682' '1690' '1694' '17' '170' '1700'\n '1701' '1706' '1707' '1710' '1720' '1722' '1723' '1725' '1731' '1735'\n '1737' '1740' '175' '1750' '1755' '1760' '1767' '1768' '1770' '1772'\n '1775' '1780' '1785' '1790' '1793' '1798' '18' '180' '1800' '1803' '1804'\n '1811' '1817' '182' '1820' '1825' '1828' '1829' '183' '1830' '1840'\n '1841' '1848' '185' '1850' '1852' '1853' '1859' '1860' '1862' '1870'\n '1875' '188' '1889' '189' '1890' '19' '190' '1900' '1905' '191' '1910'\n '192' '1920' '1921' '1922' '1925' '1938' '194' '1940' '1942' '1945' '195'\n '1950' '1951' '1958' '1960' '1965' '1966' '1969' '197' '1970' '1975'\n '1980' '1981' '199' '1996' '2' '20' '200' '2000' '2002' '2004' '201'\n '2010' '2011' '2012' '2013' '2020' '2024' '2027' '2030' '2042' '205'\n '2050' '2057' '2060' '2065' '207' '2070' '2072' '2073' '2075' '208'\n '2080' '2084' '2085' '209' '2090' '2097' '2098' '210' '2100' '2103' '211'\n '2115' '2118' '2121' '213' '2130' '2133' '2134' '214' '2140' '2142' '215'\n '2150' '2160' '2164' '2165' '217' '2170' '2179' '218' '2180' '2185'\n '2187' '2194' '2195' '2196' '220' '2200' '2210' '2215' '2225' '2229'\n '223' '2242' '225' '2250' '2255' '2256' '2265' '2268' '2270' '2271'\n '2272' '228' '2280' '2284' '2286' '2289' '229' '2290' '230' '2300' '2301'\n '2310' '2316' '232' '2320' '2323' '233' '2332' '2333' '2346' '2347' '235'\n '2350' '2362' '2370' '2375' '2377' '2378' '2380' '2385' '239' '2392'\n '2393' '2394' '24' '240' '2400' '2407' '2408' '2417' '2420' '2423' '243'\n '2438' '244' '2441' '245' '246' '2460' '2465' '2469' '2475' '2484' '2498'\n '2499' '25' '250' '2500' '251' '2514' '2515' '2530' '2535' '2542' '2550'\n '2560' '257' '258' '2580' '259' '2590' '2591' '2592' '26' '260' '2600'\n '2605' '2615' '262' '2621' '2626' '2630' '2636' '2637' '2641' '2645'\n '2650' '2651' '2652' '2658' '267' '2670' '2682' '2688' '2690' '2696'\n '2697' '27' '270' '2700' '2713' '2727' '2730' '2731' '274' '2740' '2741'\n '2743' '2745' '2746' '275' '2750' '2755' '2760' '2774' '2790' '280'\n '2800' '2804' '2805' '2816' '282' '2820' '2828' '2835' '285' '2850'\n '2865' '2877' '2880' '2890' '2894' '2895' '2896' '290' '2900' '291'\n '2925' '2926' '2930' '2940' '295' '2950' '2956' '2960' '297' '2970'\n '2987' '2990' '3' '30' '300' '3000' '3017' '3020' '3025' '3033' '3048'\n '305' '3050' '3065' '307' '308' '3095' '310' '3100' '3110' '3113' '3115'\n '3120' '3125' '314' '3140' '315' '3150' '3154' '317' '3170' '3190' '320'\n '3200' '3208' '321' '3218' '3220' '3221' '3230' '3235' '325' '3250'\n '3261' '3276' '329' '3290' '3291' '330' '3300' '3320' '3330' '335' '3352'\n '3353' '3370' '3378' '339' '340' '3400' '3413' '3415' '342' '3420' '3425'\n '345' '3450' '3470' '3475' '3480' '35' '350' '3500' '3505' '3519' '3535'\n '3536' '354' '355' '3570' '360' '3600' '362' '3630' '366' '3660' '3688'\n '370' '3700' '3719' '3720' '3740' '375' '376' '378' '38' '380' '3800'\n '381' '384' '385' '387' '390' '392' '395' '396' '40' '400' '401' '402'\n '405' '408' '410' '411' '413' '420' '421' '424' '425' '426' '427' '43'\n '430' '438' '44' '440' '442' '443' '445' '45' '450' '455' '457' '459'\n '46' '460' '465' '468' '469' '47' '470' '475' '480' '482' '490' '493'\n '494' '5' '50' '500' '501' '503' '510' '512' '518' '520' '523' '525' '53'\n '530' '535' '540' '55' '550' '555' '558' '56' '560' '564' '570' '575'\n '576' '579' '580' '582' '590' '597' '6' '60' '600' '607' '61' '610' '611'\n '612' '620' '625' '63' '630' '631' '64' '640' '645' '646' '65' '650'\n '658' '660' '67' '670' '672' '675' '68' '680' '690' '7' '70' '700' '701'\n '705' '710' '715' '716' '720' '725' '730' '731' '738' '74' '740' '745'\n '747' '75' '750' '755' '76' '762' '765' '77' '770' '775' '78' '780' '785'\n '790' '792' '798' '8' '80' '800' '803' '805' '808' '810' '820' '822'\n '823' '825' '830' '838' '840' '85' '850' '853' '854' '860' '867' '87'\n '870' '872' '875' '880' '884' '89' '890' '9' '90' '900' '910' '914' '915'\n '918' '920' '923' '925' '929' '930' '940' '945' '95' '950' '952' '960'\n '967' '970' '975' '976' '980' '988' '99' '990' '991' '994' '995']\n[\"&lt;class 'float'&gt;\"]\n</pre> In\u00a0[29]: Copied! <pre># # Latitude\nkey = 'geo_meanLat'\nprint('%s: '%key)\nprint(np.unique(['%d'%kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n</pre> # # Latitude key = 'geo_meanLat' print('%s: '%key) print(np.unique(['%d'%kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) <pre>geo_meanLat: \n['-18' '-22' '-23' '-24' '-25' '-26' '-27' '-31' '-32' '-33' '-34' '-35'\n '-36' '-37' '-38' '-39' '-40' '-41' '-42' '-43' '-44' '-45' '-46' '-50'\n '-53' '-54' '-7' '0' '16' '17' '19' '20' '21' '23' '24' '25' '26' '27'\n '28' '29' '30' '31' '32' '33' '34' '35' '36' '37' '38' '39' '40' '41'\n '42' '43' '44' '45' '46' '47' '48' '49' '50' '51' '52' '53' '54' '55'\n '56' '57' '58' '59' '60' '61' '62' '63' '64' '65' '66' '67' '68' '69'\n '70' '71' '72']\n[\"&lt;class 'float'&gt;\"]\n</pre> In\u00a0[30]: Copied! <pre># # Longitude \nkey = 'geo_meanLon'\nprint('%s: '%key)\nprint(np.unique(['%d'%kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n</pre> # # Longitude  key = 'geo_meanLon' print('%s: '%key) print(np.unique(['%d'%kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) <pre>geo_meanLon: \n['-1' '-100' '-101' '-102' '-103' '-104' '-105' '-106' '-107' '-108'\n '-109' '-110' '-111' '-112' '-113' '-114' '-115' '-116' '-117' '-118'\n '-119' '-120' '-121' '-122' '-123' '-124' '-125' '-126' '-127' '-128'\n '-129' '-130' '-133' '-134' '-135' '-136' '-137' '-138' '-139' '-140'\n '-141' '-142' '-143' '-144' '-145' '-146' '-147' '-148' '-149' '-150'\n '-151' '-152' '-153' '-154' '-159' '-162' '-163' '-2' '-3' '-4' '-5'\n '-58' '-6' '-61' '-62' '-63' '-64' '-65' '-66' '-67' '-68' '-69' '-7'\n '-70' '-71' '-72' '-73' '-74' '-75' '-76' '-77' '-78' '-79' '-8' '-80'\n '-81' '-82' '-83' '-84' '-85' '-86' '-87' '-88' '-89' '-9' '-90' '-91'\n '-92' '-93' '-94' '-95' '-96' '-97' '-98' '-99' '0' '1' '10' '100' '101'\n '103' '104' '105' '106' '107' '109' '11' '110' '111' '112' '114' '115'\n '117' '118' '119' '12' '122' '125' '127' '128' '129' '13' '130' '132'\n '133' '136' '137' '138' '14' '141' '142' '143' '145' '146' '147' '148'\n '149' '15' '150' '151' '153' '154' '155' '158' '159' '16' '160' '163'\n '165' '167' '168' '169' '17' '170' '171' '172' '173' '174' '175' '176'\n '177' '18' '19' '2' '20' '21' '22' '23' '24' '25' '26' '27' '28' '29'\n '30' '31' '32' '33' '34' '35' '36' '37' '4' '41' '42' '43' '44' '45' '5'\n '50' '51' '53' '56' '57' '58' '59' '6' '60' '64' '65' '69' '7' '71' '72'\n '74' '75' '76' '77' '78' '79' '8' '80' '81' '82' '83' '84' '85' '86' '87'\n '88' '89' '9' '90' '91' '93' '94' '95' '97' '98' '99']\n[\"&lt;class 'float'&gt;\"]\n</pre> In\u00a0[31]: Copied! <pre># Site Name \nkey = 'geo_siteName'\nprint('%s: '%key)\nprint(df[key].values)\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n</pre> # Site Name  key = 'geo_siteName' print('%s: '%key) print(df[key].values) print(np.unique([str(type(dd)) for dd in df[key]])) <pre>geo_siteName: \n['RagatiForestStationNyeriDistrict' 'RagatiForestStationNyeriDistrict'\n 'Tounfite' ... 'DevilsTowerNationalMonument' 'CookingHillside'\n 'KretecVale']\n[\"&lt;class 'str'&gt;\"]\n</pre> In\u00a0[32]: Copied! <pre># archiveType\nkey = 'archiveType'\nprint('%s: '%key)\nprint(np.unique(df[key]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n</pre> # archiveType key = 'archiveType' print('%s: '%key) print(np.unique(df[key])) print(np.unique([str(type(dd)) for dd in df[key]])) <pre>archiveType: \n['Wood']\n[\"&lt;class 'str'&gt;\"]\n</pre> In\u00a0[33]: Copied! <pre># paleoData_proxy\nkey = 'paleoData_proxy'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n</pre> # paleoData_proxy key = 'paleoData_proxy' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) <pre>paleoData_proxy: \n['ring width']\n[\"&lt;class 'str'&gt;\"]\n</pre> In\u00a0[34]: Copied! <pre># climate_interpretation\nkey = 'paleoData_sensorSpecies'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n</pre> # climate_interpretation key = 'paleoData_sensorSpecies' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]]))  <pre>paleoData_sensorSpecies: \n['ABAL' 'ABAM' 'ABBA' 'ABBO' 'ABCE' 'ABCI' 'ABCO' 'ABLA' 'ABMA' 'ABPI'\n 'ABPN' 'ABPR' 'ABSB' 'ABSP' 'ACRU' 'ACSH' 'ADHO' 'ADUS' 'AGAU' 'ARAR'\n 'ATCU' 'ATSE' 'AUCH' 'BEPU' 'CABU' 'CADE' 'CADN' 'CARO' 'CDAT' 'CDBR'\n 'CDDE' 'CDLI' 'CEAN' 'CESP' 'CHLA' 'CHNO' 'DABI' 'DACO' 'FAGR' 'FASY'\n 'FICU' 'FRNI' 'HABI' 'JGAU' 'JUEX' 'JUFO' 'JUOC' 'JUPH' 'JUPR' 'JURE'\n 'JUSC' 'JUSP' 'JUVI' 'LADE' 'LAGM' 'LALA' 'LALY' 'LAOC' 'LASI' 'LGFR'\n 'LIBI' 'LITU' 'NOBE' 'NOGU' 'NOME' 'NOPU' 'NOSO' 'PCAB' 'PCEN' 'PCGL'\n 'PCGN' 'PCMA' 'PCOB' 'PCOM' 'PCPU' 'PCRU' 'PCSH' 'PCSI' 'PCSM' 'PCSP'\n 'PHAL' 'PHAS' 'PHGL' 'PHTR' 'PIAL' 'PIAM' 'PIAR' 'PIBA' 'PIBN' 'PIBR'\n 'PICE' 'PICL' 'PICO' 'PIEC' 'PIED' 'PIFL' 'PIHA' 'PIHR' 'PIJE' 'PIKO'\n 'PILA' 'PILE' 'PILO' 'PIMO' 'PIMU' 'PIMZ' 'PINI' 'PIPA' 'PIPE' 'PIPI'\n 'PIPN' 'PIPO' 'PIPU' 'PIRE' 'PIRI' 'PIRO' 'PISF' 'PISI' 'PISP' 'PIST'\n 'PISY' 'PITA' 'PITO' 'PIUN' 'PIVI' 'PIWA' 'PLRA' 'PLUV' 'PPDE' 'PPSP'\n 'PRMA' 'PSMA' 'PSME' 'PTAN' 'QUAL' 'QUDG' 'QUFR' 'QUHA' 'QUKE' 'QULO'\n 'QULY' 'QUMA' 'QUMC' 'QUPE' 'QUPR' 'QURO' 'QURU' 'QUSP' 'QUST' 'QUVE'\n 'TABA' 'TADI' 'TAMU' 'TEGR' 'THOC' 'THPL' 'TSCA' 'TSCR' 'TSDU' 'TSHE'\n 'TSME' 'ULSP' 'VIKE' 'WICE']\n[\"&lt;class 'str'&gt;\"]\n</pre> In\u00a0[35]: Copied! <pre># # paleoData_notes\nkey = 'paleoData_notes'\nprint('%s: '%key)\nprint(df[key].values)\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n</pre> # # paleoData_notes key = 'paleoData_notes' print('%s: '%key) print(df[key].values) print(np.unique([str(type(dd)) for dd in df[key]])) <pre>paleoData_notes: \n['Investigator: Stahle' 'Investigator: Stahle' 'Investigator: Stockton'\n ... 'Investigator: Stambaugh' 'Investigator: King' 'Investigator: King']\n[\"&lt;class 'str'&gt;\"]\n</pre> In\u00a0[36]: Copied! <pre># paleoData_variableName\nkey = 'paleoData_variableName'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n</pre> # paleoData_variableName key = 'paleoData_variableName' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) <pre>paleoData_variableName: \n['ring width']\n[\"&lt;class 'str'&gt;\"]\n</pre> In\u00a0[37]: Copied! <pre># climate_interpretation\nkey = 'interpretation_direction'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # climate_interpretation key = 'interpretation_direction' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>interpretation_direction: \n['N/A']\nNo. of unique values: 1/2754\n</pre> In\u00a0[38]: Copied! <pre># climate_interpretation\nkey = 'interpretation_seasonality'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # climate_interpretation key = 'interpretation_seasonality' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>interpretation_seasonality: \n['N/A']\nNo. of unique values: 1/2754\n</pre> In\u00a0[39]: Copied! <pre># climate_interpretation\nkey = 'interpretation_variable'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # climate_interpretation key = 'interpretation_variable' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>interpretation_variable: \n['N/A' 'NOT temperature NOT moisture' 'moisture' 'temperature'\n 'temperature+moisture']\nNo. of unique values: 5/2754\n</pre> In\u00a0[40]: Copied! <pre># climate_interpretation\nkey = 'interpretation_variableDetail'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # climate_interpretation key = 'interpretation_variableDetail' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>interpretation_variableDetail: \n['N/A']\nNo. of unique values: 1/2754\n</pre> In\u00a0[41]: Copied! <pre># # paleoData_values\nkey = 'paleoData_values'\n\nprint('%s: '%key)\nfor ii, vv in enumerate(df[key][:20]):\n    try: \n        print('%-30s: %s -- %s'%(df['dataSetName'].iloc[ii][:30], str(np.nanmin(vv)), str(np.nanmax(vv))))\n        print(type(vv))\n    except: print(df['dataSetName'].iloc[ii], 'NaNs detected.')\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n</pre> # # paleoData_values key = 'paleoData_values'  print('%s: '%key) for ii, vv in enumerate(df[key][:20]):     try:          print('%-30s: %s -- %s'%(df['dataSetName'].iloc[ii][:30], str(np.nanmin(vv)), str(np.nanmax(vv))))         print(type(vv))     except: print(df['dataSetName'].iloc[ii], 'NaNs detected.') print(np.unique([str(type(dd)) for dd in df[key]])) <pre>paleoData_values: \nafrica_keny001                : 0.4 -- 1.423\n&lt;class 'numpy.ndarray'&gt;\nafrica_keny002                : 0.499 -- 1.631\n&lt;class 'numpy.ndarray'&gt;\nafrica_morc001                : -0.014 -- 2.226\n&lt;class 'numpy.ndarray'&gt;\nafrica_morc002                : 0.323 -- 1.587\n&lt;class 'numpy.ndarray'&gt;\nafrica_morc003                : 0.004 -- 1.617\n&lt;class 'numpy.ndarray'&gt;\nafrica_morc011                : 0.005 -- 2.094\n&lt;class 'numpy.ndarray'&gt;\nafrica_morc012                : 0.435 -- 1.866\n&lt;class 'numpy.ndarray'&gt;\nafrica_morc013                : 0.166 -- 1.389\n&lt;class 'numpy.ndarray'&gt;\nafrica_morc014                : -0.025 -- 2.012\n&lt;class 'numpy.ndarray'&gt;\nafrica_safr001                : 0.485 -- 2.129\n&lt;class 'numpy.ndarray'&gt;\nafrica_zimb001                : 0.15 -- 2.415\n&lt;class 'numpy.ndarray'&gt;\nafrica_zimb002                : 0.178 -- 2.044\n&lt;class 'numpy.ndarray'&gt;\nafrica_zimb003                : 0.24 -- 2.701\n&lt;class 'numpy.ndarray'&gt;\nsouthamerica_arge             : 0.161 -- 1.867\n&lt;class 'numpy.ndarray'&gt;\nsouthamerica_arge001          : 0.336 -- 2.362\n&lt;class 'numpy.ndarray'&gt;\nsouthamerica_arge002          : 0.478 -- 1.815\n&lt;class 'numpy.ndarray'&gt;\nsouthamerica_arge004          : 0.508 -- 1.714\n&lt;class 'numpy.ndarray'&gt;\nsouthamerica_arge005          : 0.313 -- 1.563\n&lt;class 'numpy.ndarray'&gt;\nsouthamerica_arge006          : 0.203 -- 1.791\n&lt;class 'numpy.ndarray'&gt;\nsouthamerica_arge007          : 0.368 -- 1.652\n&lt;class 'numpy.ndarray'&gt;\n[\"&lt;class 'numpy.ndarray'&gt;\"]\n</pre> In\u00a0[42]: Copied! <pre># paleoData_units\nkey = 'paleoData_units'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n</pre> # paleoData_units key = 'paleoData_units' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) <pre>paleoData_units: \n['standardized_anomalies']\n[\"&lt;class 'str'&gt;\"]\n</pre> In\u00a0[43]: Copied! <pre># # year\nkey = 'year'\nprint('%s: '%key)\nfor ii, vv in enumerate(df[key][:20]):\n    try: print('%-30s: %s -- %s'%(df['dataSetName'].iloc[ii][:30], str(np.nanmin(vv)), str(np.nanmax(vv))))\n    except: print('NaNs detected.', vv)\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n</pre> # # year key = 'year' print('%s: '%key) for ii, vv in enumerate(df[key][:20]):     try: print('%-30s: %s -- %s'%(df['dataSetName'].iloc[ii][:30], str(np.nanmin(vv)), str(np.nanmax(vv))))     except: print('NaNs detected.', vv) print(np.unique([str(type(dd)) for dd in df[key]])) <pre>year: \nafrica_keny001                : 1944.0 -- 1993.0\nafrica_keny002                : 1950.0 -- 1994.0\nafrica_morc001                : 1360.0 -- 1983.0\nafrica_morc002                : 1686.0 -- 1984.0\nafrica_morc003                : 1755.0 -- 1984.0\nafrica_morc011                : 1598.0 -- 1984.0\nafrica_morc012                : 1813.0 -- 1984.0\nafrica_morc013                : 1854.0 -- 1984.0\nafrica_morc014                : 1200.0 -- 1984.0\nafrica_safr001                : 1665.0 -- 1976.0\nafrica_zimb001                : 1925.0 -- 1994.0\nafrica_zimb002                : 1877.0 -- 1997.0\nafrica_zimb003                : 1880.0 -- 1996.0\nsouthamerica_arge             : 1900.0 -- 1974.0\nsouthamerica_arge001          : 1605.0 -- 1974.0\nsouthamerica_arge002          : 1800.0 -- 1974.0\nsouthamerica_arge004          : 1532.0 -- 1974.0\nsouthamerica_arge005          : 1641.0 -- 1974.0\nsouthamerica_arge006          : 1449.0 -- 1974.0\nsouthamerica_arge007          : 1579.0 -- 1974.0\n[\"&lt;class 'numpy.ndarray'&gt;\"]\n</pre> In\u00a0[44]: Copied! <pre># yearUnits\nkey = 'yearUnits'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n</pre> # yearUnits key = 'yearUnits' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) <pre>yearUnits: \n['CE']\n[\"&lt;class 'str'&gt;\"]\n</pre>"},{"location":"notebooks/load_fe23/#load-fe23","title":"Load FE23\u00b6","text":""},{"location":"notebooks/load_fe23/#set-up-working-environment","title":"Set up working environment\u00b6","text":""},{"location":"notebooks/load_fe23/#load-the-source-data","title":"load the source data\u00b6","text":""},{"location":"notebooks/load_fe23/#create-compact-dataframe","title":"create compact dataframe\u00b6","text":""},{"location":"notebooks/load_fe23/#save-compact-dataframe","title":"save compact dataframe\u00b6","text":""},{"location":"notebooks/load_fe23/#save-pickle","title":"save pickle\u00b6","text":""},{"location":"notebooks/load_fe23/#save-csv","title":"save csv\u00b6","text":""},{"location":"notebooks/load_fe23/#visualise-dataframe","title":"Visualise dataframe\u00b6","text":""},{"location":"notebooks/load_fe23/#display-dataframe","title":"Display dataframe\u00b6","text":""},{"location":"notebooks/load_fe23/#display-identification-metadata-datasetname-datasetid-originaldataurl-originaldatabase","title":"Display identification metadata: dataSetName, datasetId, originalDataURL, originalDatabase\u00b6","text":""},{"location":"notebooks/load_fe23/#index","title":"index\u00b6","text":""},{"location":"notebooks/load_fe23/#datasetname-associated-with-each-record-may-not-be-unique","title":"dataSetName (associated with each record, may not be unique)\u00b6","text":""},{"location":"notebooks/load_fe23/#datasetid-unique-identifier-as-given-by-original-authors-includes-original-database-token","title":"datasetId (unique identifier, as given by original authors, includes original database token)\u00b6","text":""},{"location":"notebooks/load_fe23/#originaldataurl-urldoi-of-original-published-record-where-available","title":"originalDataURL (URL/DOI of original published record where available)\u00b6","text":""},{"location":"notebooks/load_fe23/#originaldatabase-original-database-used-as-input-for-dataframe","title":"originalDatabase (original database used as input for dataframe)\u00b6","text":""},{"location":"notebooks/load_fe23/#geographical-metadata-elevation-latitude-longitude-site-name","title":"geographical metadata: elevation, latitude, longitude, site name\u00b6","text":""},{"location":"notebooks/load_fe23/#geo_meanelev-mean-elevation-in-m","title":"geo_meanElev (mean elevation in m)\u00b6","text":""},{"location":"notebooks/load_fe23/#geo_meanlat-mean-latitude-in-degrees-n","title":"geo_meanLat (mean latitude in degrees N)\u00b6","text":""},{"location":"notebooks/load_fe23/#geo_meanlon-mean-longitude","title":"geo_meanLon (mean longitude)\u00b6","text":""},{"location":"notebooks/load_fe23/#geo_sitename-name-of-collection-site","title":"geo_siteName (name of collection site)\u00b6","text":""},{"location":"notebooks/load_fe23/#proxy-metadata-archive-type-proxy-type-interpretation","title":"proxy metadata: archive type, proxy type, interpretation\u00b6","text":""},{"location":"notebooks/load_fe23/#archivetype-archive-type","title":"archiveType (archive type)\u00b6","text":""},{"location":"notebooks/load_fe23/#paleodata_proxy-proxy-type","title":"paleoData_proxy (proxy type)\u00b6","text":""},{"location":"notebooks/load_fe23/#paleodata_sensorspecies-further-information-on-proxy-type-species","title":"paleoData_sensorSpecies (further information on proxy type: species)\u00b6","text":""},{"location":"notebooks/load_fe23/#paleodata_notes-notes","title":"paleoData_notes (notes)\u00b6","text":""},{"location":"notebooks/load_fe23/#paleodata_variablename","title":"paleoData_variableName\u00b6","text":""},{"location":"notebooks/load_fe23/#climate-metadata-interpretation-variable-direction-seasonality","title":"climate metadata: interpretation variable, direction, seasonality\u00b6","text":""},{"location":"notebooks/load_fe23/#interpretation_direction","title":"interpretation_direction\u00b6","text":""},{"location":"notebooks/load_fe23/#interpretation_seasonality","title":"interpretation_seasonality\u00b6","text":""},{"location":"notebooks/load_fe23/#interpretation_variable","title":"interpretation_variable\u00b6","text":""},{"location":"notebooks/load_fe23/#interpretation_variabledetail","title":"interpretation_variableDetail\u00b6","text":""},{"location":"notebooks/load_fe23/#data","title":"data\u00b6","text":""},{"location":"notebooks/load_fe23/#paleodata_values","title":"paleoData_values\u00b6","text":""},{"location":"notebooks/load_fe23/#paleodata_units","title":"paleoData_units\u00b6","text":""},{"location":"notebooks/load_fe23/#year","title":"year\u00b6","text":""},{"location":"notebooks/load_fe23/#yearunits","title":"yearUnits\u00b6","text":""},{"location":"notebooks/load_iso2k/","title":"Load Iso 2k","text":"<p>load data from Iso 2k (https://doi.org/10.5194/essd-12-2261-2020)</p> <p>Data downloaded from LiPDverse: https://lipdverse.org/iso2k/current_version/, downloaded 31/10/2025 by LL</p> <p>Author: Lucie Luecke (LL)</p> <p>30/10/2025 LL: updated Iso2k to version 1.1.2</p> <p>24/10/2025 LL: tidied up and streamlined for documentation and publication</p> <p>21/11/2024 LL: added csv saving of compact dataframe, removed redundant output.</p> <p>Here we extract a dataframe with the following columns:</p> <ul> <li><code>archiveType</code></li> <li><code>dataSetName</code></li> <li><code>datasetId</code></li> <li><code>geo_meanElev</code></li> <li><code>geo_meanLat</code></li> <li><code>geo_meanLon</code></li> <li><code>geo_siteName</code></li> <li><code>interpretation_direction</code> (new in v2.0)</li> <li><code>interpretation_variable</code></li> <li><code>interpretation_variableDetail</code></li> <li><code>interpretation_seasonality</code> (new in v2.0)</li> <li><code>originalDataURL</code></li> <li><code>originalDatabase</code></li> <li><code>paleoData_notes</code></li> <li><code>paleoData_proxy</code></li> <li><code>paleoData_sensorSpecies</code></li> <li><code>paleoData_units</code></li> <li><code>paleoData_values</code></li> <li><code>paleoData_variableName</code></li> <li><code>year</code></li> <li><code>yearUnits</code></li> </ul> <p>We save a standardised compact dataframe for concatenation to DoD2k</p> <p>Make sure the repo_root is added correctly, it should be: your_root_dir/dod2k This should be the working directory throughout this notebook (and all other notebooks).</p> In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n\nimport sys\nimport os\nfrom pathlib import Path\n\n# Add parent directory to path (works from any notebook in notebooks/)\n# the repo_root should be the parent directory of the notebooks folder\ninit_dir = Path().resolve()\n# Determine repo root\nif init_dir.name == 'dod2k': repo_root = init_dir\nelif init_dir.parent.name == 'dod2k': repo_root = init_dir.parent\nelse: raise Exception('Please review the repo root structure (see first cell).')\n\n# Update cwd and path only if needed\nif os.getcwd() != str(repo_root):\n    os.chdir(repo_root)\nif str(repo_root) not in sys.path:\n    sys.path.insert(0, str(repo_root))\n\nprint(f\"Repo root: {repo_root}\")\nif str(os.getcwd())==str(repo_root):\n    print(f\"Working directory matches repo root. \")\n</pre> %load_ext autoreload %autoreload 2  import sys import os from pathlib import Path  # Add parent directory to path (works from any notebook in notebooks/) # the repo_root should be the parent directory of the notebooks folder init_dir = Path().resolve() # Determine repo root if init_dir.name == 'dod2k': repo_root = init_dir elif init_dir.parent.name == 'dod2k': repo_root = init_dir.parent else: raise Exception('Please review the repo root structure (see first cell).')  # Update cwd and path only if needed if os.getcwd() != str(repo_root):     os.chdir(repo_root) if str(repo_root) not in sys.path:     sys.path.insert(0, str(repo_root))  print(f\"Repo root: {repo_root}\") if str(os.getcwd())==str(repo_root):     print(f\"Working directory matches repo root. \") <pre>Repo root: /home/jupyter-lluecke/dod2k\nWorking directory matches repo root. \n</pre> In\u00a0[2]: Copied! <pre># Import packages\nimport lipd\nimport pandas as pd\nimport numpy as np\n\nfrom dod2k_utilities import ut_functions as utf # contains utility functions\nfrom dod2k_utilities import ut_plot as uplt # contains plotting functions\n</pre> # Import packages import lipd import pandas as pd import numpy as np  from dod2k_utilities import ut_functions as utf # contains utility functions from dod2k_utilities import ut_plot as uplt # contains plotting functions <p>In order to get the source data, run the cell below. This will download a series of LiPD files into the directory <code>lipdfiles</code></p> <p>Alternatively skip the cell and directly use the files as provided in this directory (see cell below next).</p> In\u00a0[3]: Copied! <pre># # Download the file (use -O to specify output filename)\n# !wget -O data/iso2k/iso2k1_1_2.zip https://lipdverse.org/iso2k/current_version/iso2k1_1_2.zip\n\n# # Unzip to the correct destination\n# !unzip data/iso2k/iso2k1_1_2.zip -d data/iso2k/iso2k1_1_2\n</pre> # # Download the file (use -O to specify output filename) # !wget -O data/iso2k/iso2k1_1_2.zip https://lipdverse.org/iso2k/current_version/iso2k1_1_2.zip  # # Unzip to the correct destination # !unzip data/iso2k/iso2k1_1_2.zip -d data/iso2k/iso2k1_1_2 <p>Load downloaded LiPD files and extract data and metadata from the directory</p> In\u00a0[4]: Copied! <pre># load LiPD files from the given directory\n\nD = lipd.readLipd(str(repo_root)+'/data/iso2k/iso2k1_1_2/');\nTS = lipd.extractTs(D);\nlen(TS)\nos.chdir(repo_root)\n</pre> # load LiPD files from the given directory  D = lipd.readLipd(str(repo_root)+'/data/iso2k/iso2k1_1_2/'); TS = lipd.extractTs(D); len(TS) os.chdir(repo_root) <pre>Disclaimer: LiPD files may be updated and modified to adhere to standards\n\nFound: 509 LiPD file(s)\nreading: CO14WUCL.lpd\nreading: LS96CUPU.lpd\nreading: TR18GRPM.lpd\nreading: IC00OE03.lpd\nreading: IC02GR17.lpd\nreading: IC00OEKC.lpd\nreading: IC03HORB.lpd\nreading: MS09TCIS.lpd\nreading: CO95TUNG.lpd\nreading: CO17MUMA.lpd\nreading: IC16WE30.lpd\nreading: LS10WRNA.lpd\nreading: IC10VICR.lpd\nreading: MS09DOIP.lpd\nreading: LS91BEWA.lpd\nreading: IC13ST12.lpd\nreading: CO98SWPR.lpd\nreading: CO04BAFI.lpd\nreading: SP11STBR.lpd\nreading: CO17DESC03A.lpd\nreading: LS07EAGO.lpd\nreading: IC13STSP.lpd\nreading: LS13WASU.lpd\nreading: LS16STPA.lpd\nreading: LS14LASO.lpd\nreading: IC09VIA8.lpd\nreading: CO99DRGB.lpd\nreading: CO00URMA.lpd\nreading: IC11LAB4.lpd\nreading: IC96MOSS.lpd\nreading: IC06VING.lpd\nreading: LS07LAHA.lpd\nreading: IC00STHN.lpd\nreading: IC99IS89.lpd\nreading: LS14TAKA.lpd\nreading: LS09LAEL.lpd\nreading: SP08HUCN.lpd\nreading: IC97THGL.lpd\nreading: CO09NUTB.lpd\nreading: MS10JTMA.lpd\nreading: MS96LKSS.lpd\nreading: SP10BEIN.lpd\nreading: LS15AIKA.lpd\nreading: IC13ST01.lpd\nreading: CO04KIVA.lpd\nreading: TR13JOAR.lpd\nreading: LS04JOCH.lpd\nreading: LS15DOSA.lpd\nreading: IC02STTA.lpd\nreading: LS09SCMI.lpd\nreading: MS11KDMS.lpd\nreading: IC16WE22.lpd\nreading: SP13ASBC.lpd\nreading: MS09TRFD.lpd\nreading: TR16LAAN.lpd\nreading: CO13HENG.lpd\nreading: SP12CACN.lpd\nreading: CO96QUVA.lpd\nreading: LS11TICH.lpd\nreading: LS10STSP.lpd\nreading: LS13YUSA.lpd\nreading: IC14VL1K.lpd\nreading: IC00OE16.lpd\nreading: SP09GRID.lpd\nreading: IC00STTY.lpd\nreading: IC13ST03.lpd\nreading: SP10MEMX.lpd\nreading: IC15GANK.lpd\nreading: CO97CHSY.lpd\nreading: SP07ASUS.lpd\nreading: CO04ZIMG.lpd\nreading: CO98EVXM.lpd\nreading: TR17EDSW.lpd\nreading: IC85LAD2.lpd\nreading: IC10VISD.lpd\nreading: TR11SAHU00.lpd\nreading: IC16WE20.lpd\nreading: IC02MUBI.lpd\nreading: TR13POMA.lpd\nreading: LS91HOMI.lpd\nreading: SP07FLOM.lpd\nreading: SP07DEUS.lpd\nreading: LS15LAOH.lpd\nreading: IC13STWD.lpd\nreading: CO14ZIMG.lpd\nreading: IW15MELD.lpd\nreading: MS99LWSC.lpd\nreading: IC12WACD.lpd\nreading: LS05ANJE.lpd\nreading: SP13PAVU.lpd\nreading: IC02HEWD.lpd\nreading: IC99OE10.lpd\nreading: IC04FIML.lpd\nreading: MS15VIBC.lpd\nreading: IC13ST91.lpd\nreading: IC09VICC.lpd\nreading: IC16WE17.lpd\nreading: CO17DESC04A.lpd\nreading: CO14OSPA.lpd\nreading: CO06LIFI.lpd\nreading: CO14ZIHO.lpd\nreading: IC96ISE9.lpd\nreading: TR16LAFO.lpd\nreading: IC05ISAU.lpd\nreading: LS15SHNE.lpd\nreading: LS14ATJU.lpd\nreading: IC13ST15.lpd\nreading: CO99SWFB.lpd\nreading: IC99OE07.lpd\nreading: SP06DYCN.lpd\nreading: MS09DTNA.lpd\nreading: SP05MAAT.lpd\nreading: LS16STCL.lpd\nreading: IC12RHME.lpd\nreading: LS02HATI.lpd\nreading: LS10HEQI.lpd\nreading: CO92SHPU.lpd\nreading: LS12THAY.lpd\nreading: IC00SC29.lpd\nreading: LS09RUWA.lpd\nreading: IC06VID3.lpd\nreading: IC99SCFH.lpd\nreading: TR14KOSP.lpd\nreading: LS10CLTA.lpd\nreading: LS11ANSE.lpd\nreading: IC10VISA.lpd\nreading: IC00OE20.lpd\nreading: TR12BRBO.lpd\nreading: TR10ANMO.lpd\nreading: CO17LITW01A.lpd\nreading: CO14CATI.lpd\nreading: LS09SASP.lpd\nreading: LS06JONA.lpd\nreading: TR08THCO.lpd\nreading: LS09BAUM.lpd\nreading: IC06THPR.lpd\nreading: SP12KEBZ.lpd\nreading: IC13STN7.lpd\nreading: LS06MIBO.lpd\nreading: TR06TRBO.lpd\nreading: CO03COPM.lpd\nreading: SP04YUCN.lpd\nreading: CO93COTW.lpd\nreading: CO13COTB.lpd\nreading: GI19POYU.lpd\nreading: TR11GRTP.lpd\nreading: IC06KAS1.lpd\nreading: LS03HAIG.lpd\nreading: CO94DUUR.lpd\nreading: IC99OE02.lpd\nreading: IC12SIWG.lpd\nreading: CO13CABL.lpd\nreading: IC01JO01.lpd\nreading: CO11NUPM.lpd\nreading: CO17WUBO1G.lpd\nreading: TR11BAPU.lpd\nreading: CO17WUBO1B.lpd\nreading: IC00OE23.lpd\nreading: CO13COXM.lpd\nreading: IC99OE03.lpd\nreading: IC99OE04.lpd\nreading: LS15DOCH.lpd\nreading: MS06DLGN.lpd\nreading: LS13KOLA.lpd\nreading: MS11CSCN.lpd\nreading: MS04ICNI.lpd\nreading: LS01HOCH.lpd\nreading: CO06MOPE.lpd\nreading: IC82STVE.lpd\nreading: CO04MGNG.lpd\nreading: LS17VAWA01.lpd\nreading: CO96SWFB.lpd\nreading: IC07KADF.lpd\nreading: MS11SENS.lpd\nreading: SP06DRIT.lpd\nreading: SP13AYID.lpd\nreading: MS03LKNS.lpd\nreading: IC00SC18.lpd\nreading: MS15VIBM.lpd\nreading: MS11FAPM.lpd\nreading: LS15BHAL.lpd\nreading: CO02KUBE.lpd\nreading: CO99GUMI.lpd\nreading: IC16WE19.lpd\nreading: CO17WUBO1E.lpd\nreading: TR07REVI.lpd\nreading: CO96SWBB.lpd\nreading: IC16WE28.lpd\nreading: IC95FI87.lpd\nreading: SP13MCUS.lpd\nreading: LS14FEZA.lpd\nreading: LS11MOES.lpd\nreading: TR16WEXI.lpd\nreading: TR13SIKO.lpd\nreading: LS13LUBA.lpd\nreading: LS09MIKU.lpd\nreading: TR07RECA.lpd\nreading: MS14GFSS.lpd\nreading: CO08KIPR.lpd\nreading: LS12WOQU.lpd\nreading: LS09MAAS.lpd\nreading: SP10WINZ.lpd\nreading: IC00OE22.lpd\nreading: IC08VI84.lpd\nreading: LS02ROSA.lpd\nreading: CO99KUHO.lpd\nreading: LS16THQI01.lpd\nreading: ElJunco.Zhang.2014.lpd\nreading: ClelandLake.Steinman.2016.lpd\nreading: IC99OE05.lpd\nreading: SS05ROTN.lpd\nreading: IC11STTA.lpd\nreading: CO17WUBO1D.lpd\nreading: CO03CHBU.lpd\nreading: LS11KOMA.lpd\nreading: MS11KWFS.lpd\nreading: TR12BECO.lpd\nreading: LS08LICA.lpd\nreading: IC02GR07.lpd\nreading: LS11RYKA.lpd\nreading: LS14FOTI.lpd\nreading: LS00WODE.lpd\nreading: CO04INMI.lpd\nreading: LS99LASI.lpd\nreading: IC10VIP2.lpd\nreading: CO17RAPA.lpd\nreading: IC13STN8.lpd\nreading: IC17EKN3.lpd\nreading: CO12GOVA.lpd\nreading: SP08ZHCN.lpd\nreading: IC08HODF.lpd\nreading: CO18DATO01A.lpd\nreading: TR11MAPE.lpd\nreading: MS07KPPB.lpd\nreading: IC03HOIL.lpd\nreading: TR15BABO.lpd\nreading: LS17STNU01.lpd\nreading: IC10VIP1.lpd\nreading: IC95THHU.lpd\nreading: IC00EIGG.lpd\nreading: LS96VOAM.lpd\nreading: CO18RELC01A.lpd\nreading: CO98QUNC.lpd\nreading: MS03SMAS.lpd\nreading: IC10VIMI.lpd\nreading: IC06THDD.lpd\nreading: LS14KOSA.lpd\nreading: IC14MAFH.lpd\nreading: TR13JOJS.lpd\nreading: LS13BAFL.lpd\nreading: IC08VI87.lpd\nreading: MS07LSPG.lpd\nreading: LS06CRLA.lpd\nreading: TR11SIMO.lpd\nreading: IC00OE12.lpd\nreading: IC13ST13.lpd\nreading: CO17WUBO1C.lpd\nreading: TR18XUJA00.lpd\nreading: TR08HORA.lpd\nreading: SP14APPE.lpd\nreading: LS07ROTI.lpd\nreading: IC92HOML.lpd\nreading: IC13ST04.lpd\nreading: LS03RUED.lpd\nreading: IC98FIPE.lpd\nreading: TR15YOLL.lpd\nreading: IC00OE02.lpd\nreading: LS04YUWA.lpd\nreading: CO00COKY.lpd\nreading: TR19LEOU.lpd\nreading: IC11LIMI.lpd\nreading: LS16THN301.lpd\nreading: IC00COPR.lpd\nreading: CO18HECO01B.lpd\nreading: LS12GOBA.lpd\nreading: TR11BAVO.lpd\nreading: CO07CAFR.lpd\nreading: CO08GOBE.lpd\nreading: LS14PRLO.lpd\nreading: CO05BAFI.lpd\nreading: IC13OPAN.lpd\nreading: TR11XUPH.lpd\nreading: LS11NECA.lpd\nreading: LS06DENA.lpd\nreading: TR17GRTP.lpd\nreading: TR19LEBR.lpd\nreading: MS07RCNA.lpd\nreading: LS06STFO.lpd\nreading: IC17BERI.lpd\nreading: TR18XUGA00.lpd\nreading: LS06SHBE.lpd\nreading: SP08SPUS.lpd\nreading: IC10VISE.lpd\nreading: LS97HOKA.lpd\nreading: SP07WABR.lpd\nreading: LS09MAZO.lpd\nreading: CO94LISE.lpd\nreading: LS12ANBI.lpd\nreading: CO04PFRE.lpd\nreading: LS08STCR.lpd\nreading: IC00OE15.lpd\nreading: MS13RSAS.lpd\nreading: IC09EIBE.lpd\nreading: IC06WAML.lpd\nreading: TR13SITA.lpd\nreading: CO17WUBO1F.lpd\nreading: MS14MOLS.lpd\nreading: IC08VI79.lpd\nreading: LS10HOLO.lpd\nreading: IC13ST24.lpd\nreading: SP11BOAT.lpd\nreading: MS12AMMM.lpd\nreading: IC13ST22.lpd\nreading: IC13THFE.lpd\nreading: CO01TUNG.lpd\nreading: IC99ISS2.lpd\nreading: SP13CHPE.lpd\nreading: IC99OE09.lpd\nreading: IC11BEVL.lpd\nreading: IC10NAB5.lpd\nreading: IC00OE18.lpd\nreading: IC16WENG.lpd\nreading: LS15DENA.lpd\nreading: LS13STEL.lpd\nreading: SP13LAMX.lpd\nreading: CO98CHPI.lpd\nreading: SS03HAJA.lpd\nreading: LS05HOAG.lpd\nreading: IC17KOEL.lpd\nreading: IC17EK4K.lpd\nreading: AKIII.Terwilliger.2013.lpd\nreading: IC13ST05.lpd\nreading: CO04LIFI.lpd\nreading: IC94THDP.lpd\nreading: LS09SAWA.lpd\nreading: IC02THKI.lpd\nreading: LS12ANYE.lpd\nreading: CO14CABT.lpd\nreading: SP10CACN.lpd\nreading: IC00SC16.lpd\nreading: IC16GOBP.lpd\nreading: SP09REPE.lpd\nreading: LS89LATI.lpd\nreading: SP03BMIL.lpd\nreading: CO06QUNG.lpd\nreading: IC17EKPV.lpd\nreading: LS10HESA.lpd\nreading: IC02SCGN.lpd\nreading: CO03CHBA.lpd\nreading: SL13TEAK.lpd\nreading: SP13SUSA.lpd\nreading: MS15BATP.lpd\nreading: IC99OE08.lpd\nreading: CO15ABSP.lpd\nreading: TR17SAMA.lpd\nreading: MS11ANMN.lpd\nreading: LS06POVE.lpd\nreading: LS10ANBL.lpd\nreading: MS07JRGM.lpd\nreading: LS02BEPY.lpd\nreading: LS11BIPU.lpd\nreading: MS08UAPM.lpd\nreading: IC08THGZ.lpd\nreading: LS07ANMA.lpd\nreading: MS05LKLF.lpd\nreading: LS09SHJO.lpd\nreading: MS12CCCH.lpd\nreading: TR13SAWA00.lpd\nreading: MS02FLPC.lpd\nreading: TR11MAJA.lpd\nreading: IC16WE27.lpd\nreading: SP05CRBR.lpd\nreading: IC00OE05.lpd\nreading: CO13CAHN.lpd\nreading: IC16WE23.lpd\nreading: MS06DLDS.lpd\nreading: LS16WIGH.lpd\nreading: SH18CASA.lpd\nreading: IC00SC21.lpd\nreading: MS01HDAS.lpd\nreading: MS06DLGS.lpd\nreading: CO06MOTO.lpd\nreading: LS12STRE.lpd\nreading: LS09SAJU.lpd\nreading: IC11DILO.lpd\nreading: MS05ESCR.lpd\nreading: LS15OEPO.lpd\nreading: LS10GOBL.lpd\nreading: IC99OE01.lpd\nreading: SP09FLTR.lpd\nreading: CO04LIRA.lpd\nreading: SP12NOBR.lpd\nreading: TR19LECR.lpd\nreading: MS07SWGG.lpd\nreading: TR12SAMU.lpd\nreading: CO05KUBE.lpd\nreading: IC06VIGR.lpd\nreading: MS04BLCB.lpd\nreading: IC10VIGI.lpd\nreading: MS97HLCB.lpd\nreading: CO00KUNI.lpd\nreading: TR15NACA.lpd\nreading: CO05ASGU.lpd\nreading: IC99OE06.lpd\nreading: LS09LACA.lpd\nreading: LS15SHBO.lpd\nreading: CO99LICL.lpd\nreading: IC13PALD.lpd\nreading: IC10VIGR.lpd\nreading: IC11KIPW.lpd\nreading: CO98BOFP.lpd\nreading: MS03KTCB.lpd\nreading: SS05ROES.lpd\nreading: IW17OPOY.lpd\nreading: TR13BROA.lpd\nreading: SP98KUCN.lpd\nreading: TR13JOPC.lpd\nreading: SP13KAPE.lpd\nreading: IC00OE21.lpd\nreading: LS98CUPE.lpd\nreading: IC09KANG.lpd\nreading: CO18HECO01A.lpd\nreading: IC09VIRE.lpd\nreading: IC00THDS.lpd\nreading: LS14ZHJU.lpd\nreading: IC13ST21.lpd\nreading: IC07KARB.lpd\nreading: CO13DESC01A.lpd\nreading: IC15MANE.lpd\nreading: LS12KENO.lpd\nreading: LS06TIST.lpd\nreading: IC00OE13.lpd\nreading: IC09OPAN.lpd\nreading: SP12LAMX.lpd\nreading: MS07MMNP.lpd\nreading: MS14MORR.lpd\nreading: CO08ABSU.lpd\nreading: SL13TEAG.lpd\nreading: LS14LESA.lpd\nreading: IC14EKVK.lpd\nreading: TR19LEFC.lpd\nreading: LS14YAHA.lpd\nreading: CO04PFCH.lpd\nreading: IC09KAD3.lpd\nreading: IC12MUJR.lpd\nreading: IC08VISV.lpd\nreading: IC00SC26.lpd\nreading: IC93BAGI.lpd\nreading: LS10ZHHU.lpd\nreading: IC00OE11.lpd\nreading: IC76FIDE.lpd\nreading: LS11BACH.lpd\nreading: CO10HEIG.lpd\nreading: CO00DRBE.lpd\nreading: SP13SLNA.lpd\nreading: SP10SUSE.lpd\nreading: CO06DATZ.lpd\nreading: LS01HUFA.lpd\nreading: CO09NAKY.lpd\nreading: MS03BRNS.lpd\nreading: CO17XISC01A.lpd\nreading: IC08VI77.lpd\nreading: SP10DOCN.lpd\nreading: IC10VIP3.lpd\nreading: MS11ANMS.lpd\nreading: IC97GRGI.lpd\nreading: IC08DILO.lpd\nreading: MS13KWFS.lpd\nreading: TR08VOOL.lpd\nreading: SP04DEUS.lpd\nreading: IC00OE19.lpd\nreading: IC13BOCG.lpd\nreading: LS12STLI.lpd\nreading: LS99CUVA.lpd\nreading: CO94HEAQ.lpd\nreading: IC00OE14.lpd\nreading: CO08HEVE.lpd\nreading: IC17EKN1.lpd\nreading: SP10HAUS.lpd\nreading: LS15JOPA.lpd\nreading: LS15ABSH.lpd\nreading: IC10VISG.lpd\nreading: LS17BAHA01.lpd\nreading: IC10VID3.lpd\nreading: SP12ERUS.lpd\nreading: CO00FERA.lpd\nreading: IC02GR05.lpd\nreading: SP12FOAT.lpd\nreading: TR16WEMI.lpd\nreading: IC98THSA.lpd\nreading: IC08VIRE.lpd\nreading: SP11BEMX.lpd\nreading: SP08VBPE.lpd\nreading: IC13THQU.lpd\nreading: TR15YONW.lpd\nreading: CO17DESC02A.lpd\nreading: IC10VIRE.lpd\nreading: MS07LSSP.lpd\nreading: IC09KA79.lpd\nreading: TR04EVLI.lpd\nreading: CO09FEOG.lpd\nreading: LS00SEJU.lpd\nreading: IC13ST06.lpd\nreading: MS10BLMS.lpd\nreading: MS11CSCS.lpd\nreading: SH16MEIN.lpd\nreading: CO17DESC01A.lpd\nreading: SP99DEUS.lpd\nreading: CO17WUBO1A.lpd\nFinished read: 509 records\nextracting paleoData...\nextracting: CO14WUCL\nextracting: LS96CUPU\nextracting: TR18GRPM\nextracting: IC00OE03\nextracting: IC02GR17\nextracting: IC00OEKC\nextracting: IC03HORB\nextracting: MS09TCIS\nextracting: CO95TUNG\nextracting: CO17MUMA\nextracting: IC16WE30\nextracting: LS10WRNA\nextracting: IC10VICR\nextracting: MS09DOIP\nextracting: LS91BEWA\nextracting: IC13ST12\nextracting: CO98SWPR\nextracting: CO04BAFI\nextracting: SP11STBR\nextracting: CO17DESC03A\nextracting: LS07EAGO\nextracting: IC13STSP\nextracting: LS13WASU\nextracting: LS16STPA\nextracting: LS14LASO\nextracting: IC09VIA8\nextracting: CO99DRGB\nextracting: CO00URMA\nextracting: IC11LAB4\nextracting: IC96MOSS\nextracting: IC06VING\nextracting: LS07LAHA\nextracting: IC00STHN\nextracting: IC99IS89\nextracting: LS14TAKA\nextracting: LS09LAEL\nextracting: SP08HUCN\nextracting: IC97THGL\nextracting: CO09NUTB\nextracting: MS10JTMA\nextracting: MS96LKSS\nextracting: SP10BEIN\nextracting: LS15AIKA\nextracting: IC13ST01\nextracting: CO04KIVA\nextracting: TR13JOAR\nextracting: LS04JOCH\nextracting: LS15DOSA\nextracting: IC02STTA\nextracting: LS09SCMI\nextracting: MS11KDMS\nextracting: IC16WE22\nextracting: SP13ASBC\nextracting: MS09TRFD\nextracting: TR16LAAN\nextracting: CO13HENG\nextracting: SP12CACN\nextracting: CO96QUVA\nextracting: LS11TICH\nextracting: LS10STSP\nextracting: LS13YUSA\nextracting: IC14VL1K\nextracting: IC00OE16\nextracting: SP09GRID\nextracting: IC00STTY\nextracting: IC13ST03\nextracting: SP10MEMX\nextracting: IC15GANK\nextracting: CO97CHSY\nextracting: SP07ASUS\nextracting: CO04ZIMG\nextracting: CO98EVXM\nextracting: TR17EDSW\nextracting: IC85LAD2\nextracting: IC10VISD\nextracting: TR11SAHU00\nextracting: IC16WE20\nextracting: IC02MUBI\nextracting: TR13POMA\nextracting: LS91HOMI\nextracting: SP07FLOM\nextracting: SP07DEUS\nextracting: LS15LAOH\nextracting: IC13STWD\nextracting: CO14ZIMG\nextracting: IW15MELD\nextracting: MS99LWSC\nextracting: IC12WACD\nextracting: LS05ANJE\nextracting: SP13PAVU\nextracting: IC02HEWD\nextracting: IC99OE10\nextracting: IC04FIML\nextracting: MS15VIBC\nextracting: IC13ST91\nextracting: IC09VICC\nextracting: IC16WE17\nextracting: CO17DESC04A\nextracting: CO14OSPA\nextracting: CO06LIFI\nextracting: CO14ZIHO\nextracting: IC96ISE9\nextracting: TR16LAFO\nextracting: IC05ISAU\nextracting: LS15SHNE\nextracting: LS14ATJU\nextracting: IC13ST15\nextracting: CO99SWFB\nextracting: IC99OE07\nextracting: SP06DYCN\nextracting: MS09DTNA\nextracting: SP05MAAT\nextracting: LS16STCL\nextracting: IC12RHME\nextracting: LS02HATI\nextracting: LS10HEQI\nextracting: CO92SHPU\nextracting: LS12THAY\nextracting: IC00SC29\nextracting: LS09RUWA\nextracting: IC06VID3\nextracting: IC99SCFH\nextracting: TR14KOSP\nextracting: LS10CLTA\nextracting: LS11ANSE\nextracting: IC10VISA\nextracting: IC00OE20\nextracting: TR12BRBO\nextracting: TR10ANMO\nextracting: CO17LITW01A\nextracting: CO14CATI\nextracting: LS09SASP\nextracting: LS06JONA\nextracting: TR08THCO\nextracting: LS09BAUM\nextracting: IC06THPR\nextracting: SP12KEBZ\nextracting: IC13STN7\nextracting: LS06MIBO\nextracting: TR06TRBO\nextracting: CO03COPM\nextracting: SP04YUCN\nextracting: CO93COTW\nextracting: CO13COTB\nextracting: GI19POYU\nextracting: TR11GRTP\nextracting: IC06KAS1\nextracting: LS03HAIG\nextracting: CO94DUUR\nextracting: IC99OE02\nextracting: IC12SIWG\nextracting: CO13CABL\nextracting: IC01JO01\nextracting: CO11NUPM\nextracting: CO17WUBO1G\nextracting: TR11BAPU\nextracting: CO17WUBO1B\nextracting: IC00OE23\nextracting: CO13COXM\nextracting: IC99OE03\nextracting: IC99OE04\nextracting: LS15DOCH\nextracting: MS06DLGN\nextracting: LS13KOLA\nextracting: MS11CSCN\nextracting: MS04ICNI\nextracting: LS01HOCH\nextracting: CO06MOPE\nextracting: IC82STVE\nextracting: CO04MGNG\nextracting: LS17VAWA01\nextracting: CO96SWFB\nextracting: IC07KADF\nextracting: MS11SENS\nextracting: SP06DRIT\nextracting: SP13AYID\nextracting: MS03LKNS\nextracting: IC00SC18\nextracting: MS15VIBM\nextracting: MS11FAPM\nextracting: LS15BHAL\nextracting: CO02KUBE\nextracting: CO99GUMI\nextracting: IC16WE19\nextracting: CO17WUBO1E\nextracting: TR07REVI\nextracting: CO96SWBB\nextracting: IC16WE28\nextracting: IC95FI87\nextracting: SP13MCUS\nextracting: LS14FEZA\nextracting: LS11MOES\nextracting: TR16WEXI\nextracting: TR13SIKO\nextracting: LS13LUBA\nextracting: LS09MIKU\nextracting: TR07RECA\nextracting: MS14GFSS\nextracting: CO08KIPR\nextracting: LS12WOQU\nextracting: LS09MAAS\nextracting: SP10WINZ\nextracting: IC00OE22\nextracting: IC08VI84\nextracting: LS02ROSA\nextracting: CO99KUHO\nextracting: LS16THQI01\nextracting: ElJunco.Zhang.2014\nextracting: ClelandLake.Steinman.2016\nextracting: IC99OE05\nextracting: SS05ROTN\nextracting: IC11STTA\nextracting: CO17WUBO1D\nextracting: CO03CHBU\nextracting: LS11KOMA\nextracting: MS11KWFS\nextracting: TR12BECO\nextracting: LS08LICA\nextracting: IC02GR07\nextracting: LS11RYKA\nextracting: LS14FOTI\nextracting: LS00WODE\nextracting: CO04INMI\nextracting: LS99LASI\nextracting: IC10VIP2\nextracting: CO17RAPA\nextracting: IC13STN8\nextracting: IC17EKN3\nextracting: CO12GOVA\nextracting: SP08ZHCN\nextracting: IC08HODF\nextracting: CO18DATO01A\nextracting: TR11MAPE\nextracting: MS07KPPB\nextracting: IC03HOIL\nextracting: TR15BABO\nextracting: LS17STNU01\nextracting: IC10VIP1\nextracting: IC95THHU\nextracting: IC00EIGG\nextracting: LS96VOAM\nextracting: CO18RELC01A\nextracting: CO98QUNC\nextracting: MS03SMAS\nextracting: IC10VIMI\nextracting: IC06THDD\nextracting: LS14KOSA\nextracting: IC14MAFH\nextracting: TR13JOJS\nextracting: LS13BAFL\nextracting: IC08VI87\nextracting: MS07LSPG\nextracting: LS06CRLA\nextracting: TR11SIMO\nextracting: IC00OE12\nextracting: IC13ST13\nextracting: CO17WUBO1C\nextracting: TR18XUJA00\nextracting: TR08HORA\nextracting: SP14APPE\nextracting: LS07ROTI\nextracting: IC92HOML\nextracting: IC13ST04\nextracting: LS03RUED\nextracting: IC98FIPE\nextracting: TR15YOLL\nextracting: IC00OE02\nextracting: LS04YUWA\nextracting: CO00COKY\nextracting: TR19LEOU\nextracting: IC11LIMI\nextracting: LS16THN301\nextracting: IC00COPR\nextracting: CO18HECO01B\nextracting: LS12GOBA\nextracting: TR11BAVO\nextracting: CO07CAFR\nextracting: CO08GOBE\nextracting: LS14PRLO\nextracting: CO05BAFI\nextracting: IC13OPAN\nextracting: TR11XUPH\nextracting: LS11NECA\nextracting: LS06DENA\nextracting: TR17GRTP\nextracting: TR19LEBR\nextracting: MS07RCNA\nextracting: LS06STFO\nextracting: IC17BERI\nextracting: TR18XUGA00\nextracting: LS06SHBE\nextracting: SP08SPUS\nextracting: IC10VISE\nextracting: LS97HOKA\nextracting: SP07WABR\nextracting: LS09MAZO\nextracting: CO94LISE\nextracting: LS12ANBI\nextracting: CO04PFRE\nextracting: LS08STCR\nextracting: IC00OE15\nextracting: MS13RSAS\nextracting: IC09EIBE\nextracting: IC06WAML\nextracting: TR13SITA\nextracting: CO17WUBO1F\nextracting: MS14MOLS\nextracting: IC08VI79\nextracting: LS10HOLO\nextracting: IC13ST24\nextracting: SP11BOAT\nextracting: MS12AMMM\nextracting: IC13ST22\nextracting: IC13THFE\nextracting: CO01TUNG\nextracting: IC99ISS2\nextracting: SP13CHPE\nextracting: IC99OE09\nextracting: IC11BEVL\nextracting: IC10NAB5\nextracting: IC00OE18\nextracting: IC16WENG\nextracting: LS15DENA\nextracting: LS13STEL\nextracting: SP13LAMX\nextracting: CO98CHPI\nextracting: SS03HAJA\nextracting: LS05HOAG\nextracting: IC17KOEL\nextracting: IC17EK4K\nextracting: AKIII.Terwilliger.2013\nextracting: IC13ST05\nextracting: CO04LIFI\nextracting: IC94THDP\nextracting: LS09SAWA\nextracting: IC02THKI\nextracting: LS12ANYE\nextracting: CO14CABT\nextracting: SP10CACN\nextracting: IC00SC16\nextracting: IC16GOBP\nextracting: SP09REPE\nextracting: LS89LATI\nextracting: SP03BMIL\nextracting: CO06QUNG\nextracting: IC17EKPV\nextracting: LS10HESA\nextracting: IC02SCGN\nextracting: CO03CHBA\nextracting: SL13TEAK\nextracting: SP13SUSA\nextracting: MS15BATP\nextracting: IC99OE08\nextracting: CO15ABSP\nextracting: TR17SAMA\nextracting: MS11ANMN\nextracting: LS06POVE\nextracting: LS10ANBL\nextracting: MS07JRGM\nextracting: LS02BEPY\nextracting: LS11BIPU\nextracting: MS08UAPM\nextracting: IC08THGZ\nextracting: LS07ANMA\nextracting: MS05LKLF\nextracting: LS09SHJO\nextracting: MS12CCCH\nextracting: TR13SAWA00\nextracting: MS02FLPC\nextracting: TR11MAJA\nextracting: IC16WE27\nextracting: SP05CRBR\nextracting: IC00OE05\nextracting: CO13CAHN\nextracting: IC16WE23\nextracting: MS06DLDS\nextracting: LS16WIGH\nextracting: SH18CASA\nextracting: IC00SC21\nextracting: MS01HDAS\nextracting: MS06DLGS\nextracting: CO06MOTO\nextracting: LS12STRE\nextracting: LS09SAJU\nextracting: IC11DILO\nextracting: MS05ESCR\nextracting: LS15OEPO\nextracting: LS10GOBL\nextracting: IC99OE01\nextracting: SP09FLTR\nextracting: CO04LIRA\nextracting: SP12NOBR\nextracting: TR19LECR\nextracting: MS07SWGG\nextracting: TR12SAMU\nextracting: CO05KUBE\nextracting: IC06VIGR\nextracting: MS04BLCB\nextracting: IC10VIGI\nextracting: MS97HLCB\nextracting: CO00KUNI\nextracting: TR15NACA\nextracting: CO05ASGU\nextracting: IC99OE06\nextracting: LS09LACA\nextracting: LS15SHBO\nextracting: CO99LICL\nextracting: IC13PALD\nextracting: IC10VIGR\nextracting: IC11KIPW\nextracting: CO98BOFP\nextracting: MS03KTCB\nextracting: SS05ROES\nextracting: IW17OPOY\nextracting: TR13BROA\nextracting: SP98KUCN\nextracting: TR13JOPC\nextracting: SP13KAPE\nextracting: IC00OE21\nextracting: LS98CUPE\nextracting: IC09KANG\nextracting: CO18HECO01A\nextracting: IC09VIRE\nextracting: IC00THDS\nextracting: LS14ZHJU\nextracting: IC13ST21\nextracting: IC07KARB\nextracting: CO13DESC01A\nextracting: IC15MANE\nextracting: LS12KENO\nextracting: LS06TIST\nextracting: IC00OE13\nextracting: IC09OPAN\nextracting: SP12LAMX\nextracting: MS07MMNP\nextracting: MS14MORR\nextracting: CO08ABSU\nextracting: SL13TEAG\nextracting: LS14LESA\nextracting: IC14EKVK\nextracting: TR19LEFC\nextracting: LS14YAHA\nextracting: CO04PFCH\nextracting: IC09KAD3\nextracting: IC12MUJR\nextracting: IC08VISV\nextracting: IC00SC26\nextracting: IC93BAGI\nextracting: LS10ZHHU\nextracting: IC00OE11\nextracting: IC76FIDE\nextracting: LS11BACH\nextracting: CO10HEIG\nextracting: CO00DRBE\nextracting: SP13SLNA\nextracting: SP10SUSE\nextracting: CO06DATZ\nextracting: LS01HUFA\nextracting: CO09NAKY\nextracting: MS03BRNS\nextracting: CO17XISC01A\nextracting: IC08VI77\nextracting: SP10DOCN\nextracting: IC10VIP3\nextracting: MS11ANMS\nextracting: IC97GRGI\nextracting: IC08DILO\nextracting: MS13KWFS\nextracting: TR08VOOL\nextracting: SP04DEUS\nextracting: IC00OE19\nextracting: IC13BOCG\nextracting: LS12STLI\nextracting: LS99CUVA\nextracting: CO94HEAQ\nextracting: IC00OE14\nextracting: CO08HEVE\nextracting: IC17EKN1\nextracting: SP10HAUS\nextracting: LS15JOPA\nextracting: LS15ABSH\nextracting: IC10VISG\nextracting: LS17BAHA01\nextracting: IC10VID3\nextracting: SP12ERUS\nextracting: CO00FERA\nextracting: IC02GR05\nextracting: SP12FOAT\nextracting: TR16WEMI\nextracting: IC98THSA\nextracting: IC08VIRE\nextracting: SP11BEMX\nextracting: SP08VBPE\nextracting: IC13THQU\nextracting: TR15YONW\nextracting: CO17DESC02A\nextracting: IC10VIRE\nextracting: MS07LSSP\nextracting: IC09KA79\nextracting: TR04EVLI\nextracting: CO09FEOG\nextracting: LS00SEJU\nextracting: IC13ST06\nextracting: MS10BLMS\nextracting: MS11CSCS\nextracting: SH16MEIN\nextracting: CO17DESC01A\nextracting: SP99DEUS\nextracting: CO17WUBO1A\nCreated time series: 1964 entries\n</pre> In\u00a0[5]: Copied! <pre># all_keys = []\n# for ii in range(len(TS)):\n#     keys = TS[ii].keys()\n#     for key in keys:\n#         if key not in all_keys: all_keys.append(key)\n</pre> # all_keys = [] # for ii in range(len(TS)): #     keys = TS[ii].keys() #     for key in keys: #         if key not in all_keys: all_keys.append(key) <p>Create empty dataframe with set of columns for compact dataframe, and populate with the LiPD data</p> In\u00a0[6]: Copied! <pre>col_str=['archiveType', 'dataSetName', 'datasetId', 'geo_meanElev', 'geo_meanLat', 'geo_meanLon', 'geo_siteName', \n         'originalDataUrl', 'paleoData_notes', 'paleoData_variableName','paleoData_proxy',\n         'paleoData_archiveSpecies','paleoData_units','paleoData_interpretation',\n         'paleoData_values', 'year']\n\ndf_tmp = pd.DataFrame(index=range(len(TS)), columns=col_str)\ndf_tmp.info()\n</pre> col_str=['archiveType', 'dataSetName', 'datasetId', 'geo_meanElev', 'geo_meanLat', 'geo_meanLon', 'geo_siteName',           'originalDataUrl', 'paleoData_notes', 'paleoData_variableName','paleoData_proxy',          'paleoData_archiveSpecies','paleoData_units','paleoData_interpretation',          'paleoData_values', 'year']  df_tmp = pd.DataFrame(index=range(len(TS)), columns=col_str) df_tmp.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1964 entries, 0 to 1963\nData columns (total 16 columns):\n #   Column                    Non-Null Count  Dtype \n---  ------                    --------------  ----- \n 0   archiveType               0 non-null      object\n 1   dataSetName               0 non-null      object\n 2   datasetId                 0 non-null      object\n 3   geo_meanElev              0 non-null      object\n 4   geo_meanLat               0 non-null      object\n 5   geo_meanLon               0 non-null      object\n 6   geo_siteName              0 non-null      object\n 7   originalDataUrl           0 non-null      object\n 8   paleoData_notes           0 non-null      object\n 9   paleoData_variableName    0 non-null      object\n 10  paleoData_proxy           0 non-null      object\n 11  paleoData_archiveSpecies  0 non-null      object\n 12  paleoData_units           0 non-null      object\n 13  paleoData_interpretation  0 non-null      object\n 14  paleoData_values          0 non-null      object\n 15  year                      0 non-null      object\ndtypes: object(16)\nmemory usage: 245.6+ KB\n</pre> <p>Start by populating paleoData_variableName (paleoData_proxy in dod2k standard terms)</p> In\u00a0[7]: Copied! <pre># loop over the timeseries and pick those for global temperature analysis\ni = 0                                                                                                                \nfor ts in TS: #for every time series\n    # need to filter these variables in the list\n    # if ts['paleoData_variableName'] not in ['year', 'd18OUncertainty', 'SrCaUncertainty',\n    #                                        'uncertainty', 'uncertainty1s', 'uncertaintyHigh', \n    #                                         'uncertaintyHigh1s', 'uncertaintyLow', 'uncertaintyLow1s',\n    #                                        'needsToBeChanged', 'deleteThisColumn','notes',\n    #                                        'sampleID', 'section', 'site']: \n    for name in col_str:     #for each of the 12 main keys, shove the wanted data into the df                                                                                    \n        if name in ts:\n            df_tmp.loc[i, name] = ts[name]                                                                       \n        else:\n            df_tmp.loc[i, name] = np.nan\n    i += 1 \n</pre> # loop over the timeseries and pick those for global temperature analysis i = 0                                                                                                                 for ts in TS: #for every time series     # need to filter these variables in the list     # if ts['paleoData_variableName'] not in ['year', 'd18OUncertainty', 'SrCaUncertainty',     #                                        'uncertainty', 'uncertainty1s', 'uncertaintyHigh',      #                                         'uncertaintyHigh1s', 'uncertaintyLow', 'uncertaintyLow1s',     #                                        'needsToBeChanged', 'deleteThisColumn','notes',     #                                        'sampleID', 'section', 'site']:      for name in col_str:     #for each of the 12 main keys, shove the wanted data into the df                                                                                             if name in ts:             df_tmp.loc[i, name] = ts[name]                                                                                else:             df_tmp.loc[i, name] = np.nan     i += 1  <ul> <li>Drop rows with missing data:<ul> <li>all NaNs</li> <li>NaNs for <code>year</code> or <code>paleoData_values</code> (no data records)</li> </ul> </li> <li>Populate missing <code>paleoData_proxy</code> entries with <code>paleoData_variableName</code></li> </ul> In\u00a0[8]: Copied! <pre># drop the rows with all NaNs (those not for global temperature analysis)\nprint(len(df_tmp))\ndf = df_tmp.dropna(how='all')\nprint(len(df))\nfor subset in ['year', 'paleoData_values']:\n    df = df.dropna(subset=subset, how='all')\n    print(len(df))\n    \nfor ii in df.index:\n    if not isinstance(df.at[ii, 'paleoData_proxy'], str):\n        df.at[ii, 'paleoData_proxy'] = df.at[ii, 'paleoData_variableName']\n</pre> # drop the rows with all NaNs (those not for global temperature analysis) print(len(df_tmp)) df = df_tmp.dropna(how='all') print(len(df)) for subset in ['year', 'paleoData_values']:     df = df.dropna(subset=subset, how='all')     print(len(df))      for ii in df.index:     if not isinstance(df.at[ii, 'paleoData_proxy'], str):         df.at[ii, 'paleoData_proxy'] = df.at[ii, 'paleoData_variableName'] <pre>1964\n1964\n1662\n1662\n</pre> <ul> <li>Ensure standard terminology for <code>paleoData_proxy</code></li> </ul> In\u00a0[9]: Copied! <pre>df['paleoData_proxy'] = df['paleoData_proxy'].replace({'d2H': 'dD'})\n</pre> df['paleoData_proxy'] = df['paleoData_proxy'].replace({'d2H': 'dD'}) <p>Keep only the record types which are included in https://doi.org/10.5194/essd-12-2261-2020 (which is 'a global compilation of <code>dD</code> and <code>d18O</code> records')</p> In\u00a0[10]: Copied! <pre>df = df[(df['paleoData_proxy']=='d18O')|(df['paleoData_proxy']=='dD')]\n</pre> df = df[(df['paleoData_proxy']=='d18O')|(df['paleoData_proxy']=='dD')] <ul> <li>Add the missing columns to the dataframe, including <code>originalDatabase</code>, <code>yearUnits</code>, <code>interpretation_variable</code> (these are added manually and not from the LiPD files)</li> </ul> In\u00a0[11]: Copied! <pre># KF: adding original dataset name and yearUnits\ndf.insert(7, 'originalDatabase', ['Iso2k v1.1.2']*len(df))\ndf.insert(len(df.columns), 'yearUnits', ['CE'] * len(df))\ndf.insert(1, 'interpretation_variable', ['N/A']*len(df))\ndf.insert(1, 'interpretation_variableDetail', ['N/A']*len(df))\ndf.insert(1, 'interpretation_seasonality', ['N/A']*len(df))\ndf.insert(1, 'interpretation_direction', ['N/A']*len(df))\n</pre> # KF: adding original dataset name and yearUnits df.insert(7, 'originalDatabase', ['Iso2k v1.1.2']*len(df)) df.insert(len(df.columns), 'yearUnits', ['CE'] * len(df)) df.insert(1, 'interpretation_variable', ['N/A']*len(df)) df.insert(1, 'interpretation_variableDetail', ['N/A']*len(df)) df.insert(1, 'interpretation_seasonality', ['N/A']*len(df)) df.insert(1, 'interpretation_direction', ['N/A']*len(df)) <ul> <li>Rename columns to fit naming conventions</li> </ul> In\u00a0[12]: Copied! <pre>df = df.rename(columns={'originalDataUrl': 'originalDataURL',\n                        'paleoData_archiveSpecies': 'paleoData_sensorSpecies'})\n</pre> df = df.rename(columns={'originalDataUrl': 'originalDataURL',                         'paleoData_archiveSpecies': 'paleoData_sensorSpecies'}) In\u00a0[13]: Copied! <pre>df.info()\n</pre> df.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 620 entries, 0 to 1962\nData columns (total 22 columns):\n #   Column                         Non-Null Count  Dtype \n---  ------                         --------------  ----- \n 0   archiveType                    620 non-null    object\n 1   interpretation_direction       620 non-null    object\n 2   interpretation_seasonality     620 non-null    object\n 3   interpretation_variableDetail  620 non-null    object\n 4   interpretation_variable        620 non-null    object\n 5   dataSetName                    620 non-null    object\n 6   datasetId                      620 non-null    object\n 7   geo_meanElev                   593 non-null    object\n 8   geo_meanLat                    620 non-null    object\n 9   geo_meanLon                    620 non-null    object\n 10  geo_siteName                   620 non-null    object\n 11  originalDatabase               620 non-null    object\n 12  originalDataURL                610 non-null    object\n 13  paleoData_notes                541 non-null    object\n 14  paleoData_variableName         620 non-null    object\n 15  paleoData_proxy                620 non-null    object\n 16  paleoData_sensorSpecies        48 non-null     object\n 17  paleoData_units                616 non-null    object\n 18  paleoData_interpretation       620 non-null    object\n 19  paleoData_values               620 non-null    object\n 20  year                           620 non-null    object\n 21  yearUnits                      620 non-null    object\ndtypes: object(22)\nmemory usage: 111.4+ KB\n</pre> In\u00a0[14]: Copied! <pre>#  check that the datasetId is unique \nprint('dataset length: ', len(df))\nprint('unique datasetIds:', len(df.datasetId.unique()))\n# make datasetId unique by simply adding index number\ndf.datasetId=df.apply(lambda x: 'iso2k_'+str(x.name), axis=1)\n# check uniqueness - problem solved.\nassert len(df)== len(df.datasetId.unique())\n</pre> #  check that the datasetId is unique  print('dataset length: ', len(df)) print('unique datasetIds:', len(df.datasetId.unique())) # make datasetId unique by simply adding index number df.datasetId=df.apply(lambda x: 'iso2k_'+str(x.name), axis=1) # check uniqueness - problem solved. assert len(df)== len(df.datasetId.unique()) <pre>dataset length:  620\nunique datasetIds: 427\n</pre> In\u00a0[15]: Copied! <pre>keys = ['interpretation_direction', 'interpretation_seasonality',\n       'interpretation_variable', 'interpretation_variableDetail']\n\nfor ii in df.index:\n    if type(df_tmp.at[ii, 'paleoData_interpretation'])!=list: continue\n    for key in keys:\n        if key.split('_')[-1] in df_tmp.at[ii, 'paleoData_interpretation'][0].keys():\n            df.at[ii, key] = df_tmp.at[ii, 'paleoData_interpretation'][0][key.split('_')[-1]]\n            \n</pre> keys = ['interpretation_direction', 'interpretation_seasonality',        'interpretation_variable', 'interpretation_variableDetail']  for ii in df.index:     if type(df_tmp.at[ii, 'paleoData_interpretation'])!=list: continue     for key in keys:         if key.split('_')[-1] in df_tmp.at[ii, 'paleoData_interpretation'][0].keys():             df.at[ii, key] = df_tmp.at[ii, 'paleoData_interpretation'][0][key.split('_')[-1]]              <p>The entries for interpretation_variable need to be standardised according to our terminology (note that the original variable is still available in interpretation_variableDetail). Original entries:</p> <p><code>['N/A' 'circulationIndex' 'circulationVariable' 'deleteMe'  'effectivePrecipitation' 'evaporation' 'hydrologicBalance'  'precipitation' 'precipitationIsotope' 'salinity' 'seasonality'  'streamflow' 'temperature']</code></p> In\u00a0[16]: Copied! <pre>interpretation_dict = {'precipitation': 'moisture', 'precipitationIsotope': 'moisture', 'effectivePrecipitation': 'moisture',\n                      'evaporation': 'moisture', 'hydrologicBalance': 'moisture', 'salinity': 'moisture', \n                       'circulationIndex': 'N/A', 'circulationVariable': 'N/A', 'deleteMe': 'N/A', 'seasonality': 'N/A',\n                      'streamflow': 'N/A'}\n\nfor key in interpretation_dict.keys():\n    mask = df['interpretation_variable'] == key\n    df.loc[mask, 'interpretation_variableDetail'] = (\n        'Original interpretation_variable: '\n        + key + ', interpretation_variableDetail: ' + df.loc[mask, 'interpretation_variableDetail']\n    )\n\ndf['interpretation_variable'] = df['interpretation_variable'].replace(\n    interpretation_dict)\n</pre> interpretation_dict = {'precipitation': 'moisture', 'precipitationIsotope': 'moisture', 'effectivePrecipitation': 'moisture',                       'evaporation': 'moisture', 'hydrologicBalance': 'moisture', 'salinity': 'moisture',                         'circulationIndex': 'N/A', 'circulationVariable': 'N/A', 'deleteMe': 'N/A', 'seasonality': 'N/A',                       'streamflow': 'N/A'}  for key in interpretation_dict.keys():     mask = df['interpretation_variable'] == key     df.loc[mask, 'interpretation_variableDetail'] = (         'Original interpretation_variable: '         + key + ', interpretation_variableDetail: ' + df.loc[mask, 'interpretation_variableDetail']     )  df['interpretation_variable'] = df['interpretation_variable'].replace(     interpretation_dict) In\u00a0[17]: Copied! <pre>df = df.drop(labels=['paleoData_interpretation'], axis=1)\n</pre> df = df.drop(labels=['paleoData_interpretation'], axis=1) In\u00a0[18]: Copied! <pre>df = df.astype({'archiveType': str, 'dataSetName': str, 'datasetId': str, 'geo_meanElev': np.float32, 'geo_meanLat': np.float32, 'geo_meanLon': np.float32, 'geo_siteName': str, \n                    'originalDatabase': str, 'originalDataURL': str, 'paleoData_notes': str, 'paleoData_proxy': str, 'paleoData_units': str, 'yearUnits': str})\n\ndrop_inds = []\nfor ii in df.index:\n    try:\n        year = np.array(df.at[ii, 'year'], dtype=float)\n        vals = np.array(df.at[ii, 'paleoData_values'], dtype=float)\n        df.at[ii, 'year']             = year[year&gt;=1]\n        df.at[ii, 'paleoData_values'] = vals[year&gt;=1]\n    except:\n        df.at[ii, 'paleoData_values'] = np.array([utf.convert_to_float(y) for y in df.at[ii, 'paleoData_values']], dtype=float)\n        df.at[ii, 'year'] = np.array([utf.convert_to_float(y) for y in df.at[ii, 'year']], dtype=float)\n    \n        print(f'Converted values in paleoData_values and/or year for {ii}.')\n</pre>  df = df.astype({'archiveType': str, 'dataSetName': str, 'datasetId': str, 'geo_meanElev': np.float32, 'geo_meanLat': np.float32, 'geo_meanLon': np.float32, 'geo_siteName': str,                      'originalDatabase': str, 'originalDataURL': str, 'paleoData_notes': str, 'paleoData_proxy': str, 'paleoData_units': str, 'yearUnits': str})  drop_inds = [] for ii in df.index:     try:         year = np.array(df.at[ii, 'year'], dtype=float)         vals = np.array(df.at[ii, 'paleoData_values'], dtype=float)         df.at[ii, 'year']             = year[year&gt;=1]         df.at[ii, 'paleoData_values'] = vals[year&gt;=1]     except:         df.at[ii, 'paleoData_values'] = np.array([utf.convert_to_float(y) for y in df.at[ii, 'paleoData_values']], dtype=float)         df.at[ii, 'year'] = np.array([utf.convert_to_float(y) for y in df.at[ii, 'year']], dtype=float)              print(f'Converted values in paleoData_values and/or year for {ii}.') <p>Include Common Era data only</p> <p>Note that the datasetId is not unique for each record and thus we added an additional array of strings to make the datasetId unique.</p> <p>mask out nans and set fill value, then later drop</p> In\u00a0[19]: Copied! <pre># drop all missing values and exclude all-missing-values-rows\n\nfor ii in df.index:\n    dd   = np.array(df.at[ii, 'paleoData_values'])\n    mask = dd==-9999.99\n    df.at[ii, 'paleoData_values']=dd[~mask]\n    df.at[ii, 'year']=np.array(df.at[ii, 'year'])[~mask]\n    \ndrop_inds = []\nfor ii, row in enumerate(df.paleoData_values):\n    try:\n        if len(row)==0:\n            print(ii, 'empty row for paleodata_values')\n        elif len(df.iloc[ii]['year'])==0:\n            print(ii, 'empty row for year')\n        elif np.std(row)==0: \n            print(ii, 'std=0')\n        elif np.sum(np.diff(row)**2)==0: \n            print(ii, 'diff=0')\n        elif np.isnan(np.std(row)):\n            print(ii, 'std nan')\n        else:\n            continue\n        if df.index[ii] not in drop_inds: \n            drop_inds += [df.index[ii]]\n    except:\n        drop_inds+=[df.index[ii]]\n    \nprint(drop_inds)\ndf = df.drop(index=drop_inds)\n</pre> # drop all missing values and exclude all-missing-values-rows  for ii in df.index:     dd   = np.array(df.at[ii, 'paleoData_values'])     mask = dd==-9999.99     df.at[ii, 'paleoData_values']=dd[~mask]     df.at[ii, 'year']=np.array(df.at[ii, 'year'])[~mask]      drop_inds = [] for ii, row in enumerate(df.paleoData_values):     try:         if len(row)==0:             print(ii, 'empty row for paleodata_values')         elif len(df.iloc[ii]['year'])==0:             print(ii, 'empty row for year')         elif np.std(row)==0:              print(ii, 'std=0')         elif np.sum(np.diff(row)**2)==0:              print(ii, 'diff=0')         elif np.isnan(np.std(row)):             print(ii, 'std nan')         else:             continue         if df.index[ii] not in drop_inds:              drop_inds += [df.index[ii]]     except:         drop_inds+=[df.index[ii]]      print(drop_inds) df = df.drop(index=drop_inds) <pre>1 std nan\n2 std nan\n4 std nan\n13 std nan\n14 std nan\n20 empty row for paleodata_values\n43 std nan\n48 std nan\n52 std nan\n58 std nan\n59 std nan\n60 std nan\n63 std nan\n65 std nan\n72 std nan\n78 std nan\n92 std nan\n94 std nan\n96 std nan\n104 std nan\n106 std nan\n108 std nan\n111 std nan\n118 std nan\n119 std nan\n120 std nan\n122 std nan\n123 std nan\n125 std nan\n126 std nan\n127 std nan\n135 std nan\n138 std nan\n148 std nan\n163 empty row for paleodata_values\n165 empty row for paleodata_values\n174 std nan\n175 std nan\n183 std nan\n186 std nan\n188 std nan\n189 std nan\n193 std nan\n194 std nan\n198 std nan\n206 empty row for paleodata_values\n207 empty row for paleodata_values\n208 empty row for paleodata_values\n209 empty row for paleodata_values\n210 empty row for paleodata_values\n211 std nan\n212 std nan\n213 std nan\n214 std nan\n215 std nan\n216 std nan\n217 std nan\n222 std nan\n223 std nan\n231 std nan\n234 std nan\n238 std nan\n246 std nan\n254 std nan\n257 std nan\n259 empty row for paleodata_values\n262 std nan\n268 std nan\n280 std nan\n282 std nan\n295 std nan\n296 std nan\n297 std nan\n299 std nan\n301 std nan\n302 std nan\n304 std nan\n308 std nan\n309 std nan\n313 std nan\n314 std nan\n319 std nan\n320 std nan\n327 std nan\n328 std nan\n329 std nan\n330 std nan\n331 std nan\n332 std nan\n333 std nan\n334 std nan\n339 std nan\n348 std nan\n351 empty row for paleodata_values\n352 std nan\n353 std nan\n354 std nan\n355 std nan\n356 std nan\n359 std nan\n361 std nan\n363 std nan\n364 std nan\n365 std nan\n370 std nan\n371 std nan\n378 std nan\n380 std nan\n381 std nan\n382 std nan\n383 std nan\n384 std nan\n385 std nan\n386 std nan\n388 std nan\n392 std nan\n393 std nan\n395 std nan\n396 std nan\n405 empty row for paleodata_values\n412 std nan\n413 std nan\n414 std nan\n415 std nan\n416 std nan\n419 std nan\n420 std nan\n422 std nan\n423 std nan\n425 std nan\n427 std nan\n429 std nan\n433 std nan\n435 std nan\n436 std nan\n439 std nan\n440 std nan\n448 std nan\n459 std nan\n466 std nan\n467 std nan\n469 std nan\n470 std nan\n475 std nan\n476 std nan\n483 std nan\n497 std nan\n498 std nan\n499 std nan\n501 std nan\n502 std nan\n513 std nan\n516 std nan\n517 std nan\n518 std nan\n519 std nan\n520 std nan\n521 std nan\n522 std nan\n523 std nan\n524 std nan\n530 std nan\n536 std nan\n543 std nan\n550 std nan\n553 diff=0\n554 empty row for paleodata_values\n557 std nan\n558 std nan\n560 std nan\n562 std nan\n568 std nan\n569 std nan\n572 std nan\n580 std nan\n581 std nan\n598 std nan\n599 empty row for paleodata_values\n610 std nan\n611 std nan\n612 std nan\n613 std nan\n619 std nan\n[np.int64(8), np.int64(10), np.int64(14), np.int64(45), np.int64(48), np.int64(63), np.int64(153), np.int64(190), np.int64(209), np.int64(231), np.int64(232), np.int64(234), np.int64(242), np.int64(246), np.int64(262), np.int64(277), np.int64(317), np.int64(322), np.int64(337), np.int64(355), np.int64(360), np.int64(371), np.int64(378), np.int64(418), np.int64(420), np.int64(423), np.int64(425), np.int64(430), np.int64(438), np.int64(441), np.int64(442), np.int64(461), np.int64(470), np.int64(494), np.int64(530), np.int64(536), np.int64(564), np.int64(565), np.int64(583), np.int64(589), np.int64(594), np.int64(596), np.int64(614), np.int64(621), np.int64(635), np.int64(660), np.int64(663), np.int64(666), np.int64(670), np.int64(673), np.int64(674), np.int64(679), np.int64(681), np.int64(688), np.int64(691), np.int64(692), np.int64(694), np.int64(711), np.int64(713), np.int64(755), np.int64(763), np.int64(776), np.int64(816), np.int64(839), np.int64(847), np.int64(851), np.int64(861), np.int64(876), np.int64(904), np.int64(907), np.int64(968), np.int64(970), np.int64(972), np.int64(978), np.int64(981), np.int64(984), np.int64(993), np.int64(1002), np.int64(1004), np.int64(1022), np.int64(1025), np.int64(1048), np.int64(1050), np.int64(1071), np.int64(1073), np.int64(1074), np.int64(1075), np.int64(1076), np.int64(1077), np.int64(1078), np.int64(1079), np.int64(1103), np.int64(1136), np.int64(1154), np.int64(1159), np.int64(1162), np.int64(1163), np.int64(1164), np.int64(1167), np.int64(1176), np.int64(1185), np.int64(1191), np.int64(1196), np.int64(1197), np.int64(1207), np.int64(1210), np.int64(1226), np.int64(1232), np.int64(1234), np.int64(1240), np.int64(1243), np.int64(1246), np.int64(1249), np.int64(1253), np.int64(1262), np.int64(1273), np.int64(1276), np.int64(1278), np.int64(1279), np.int64(1298), np.int64(1301), np.int64(1324), np.int64(1327), np.int64(1328), np.int64(1329), np.int64(1338), np.int64(1350), np.int64(1354), np.int64(1364), np.int64(1365), np.int64(1375), np.int64(1383), np.int64(1390), np.int64(1401), np.int64(1409), np.int64(1411), np.int64(1421), np.int64(1423), np.int64(1454), np.int64(1493), np.int64(1522), np.int64(1524), np.int64(1526), np.int64(1527), np.int64(1537), np.int64(1541), np.int64(1561), np.int64(1608), np.int64(1611), np.int64(1614), np.int64(1623), np.int64(1625), np.int64(1656), np.int64(1662), np.int64(1668), np.int64(1673), np.int64(1678), np.int64(1679), np.int64(1681), np.int64(1684), np.int64(1685), np.int64(1686), np.int64(1705), np.int64(1725), np.int64(1742), np.int64(1760), np.int64(1764), np.int64(1767), np.int64(1776), np.int64(1778), np.int64(1788), np.int64(1793), np.int64(1808), np.int64(1810), np.int64(1815), np.int64(1841), np.int64(1843), np.int64(1879), np.int64(1881), np.int64(1928), np.int64(1938), np.int64(1945), np.int64(1947), np.int64(1958), np.int64(1962)]\n</pre> <p>Now show the final compact dataframe</p> In\u00a0[20]: Copied! <pre>df = df[sorted(df.columns)]\ndf.reset_index(drop= True, inplace= True)\nprint(df.info())\n</pre> df = df[sorted(df.columns)] df.reset_index(drop= True, inplace= True) print(df.info()) <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 435 entries, 0 to 434\nData columns (total 21 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   archiveType                    435 non-null    object \n 1   dataSetName                    435 non-null    object \n 2   datasetId                      435 non-null    object \n 3   geo_meanElev                   415 non-null    float32\n 4   geo_meanLat                    435 non-null    float32\n 5   geo_meanLon                    435 non-null    float32\n 6   geo_siteName                   435 non-null    object \n 7   interpretation_direction       435 non-null    object \n 8   interpretation_seasonality     435 non-null    object \n 9   interpretation_variable        435 non-null    object \n 10  interpretation_variableDetail  435 non-null    object \n 11  originalDataURL                435 non-null    object \n 12  originalDatabase               435 non-null    object \n 13  paleoData_notes                435 non-null    object \n 14  paleoData_proxy                435 non-null    object \n 15  paleoData_sensorSpecies        43 non-null     object \n 16  paleoData_units                435 non-null    object \n 17  paleoData_values               435 non-null    object \n 18  paleoData_variableName         435 non-null    object \n 19  year                           435 non-null    object \n 20  yearUnits                      435 non-null    object \ndtypes: float32(3), object(18)\nmemory usage: 66.4+ KB\nNone\n</pre> In\u00a0[21]: Copied! <pre># save to a pickle file\ndf = df[sorted(df.columns)]\ndf.to_pickle('data/iso2k/iso2k_compact.pkl')\n</pre> # save to a pickle file df = df[sorted(df.columns)] df.to_pickle('data/iso2k/iso2k_compact.pkl') In\u00a0[22]: Copied! <pre># save to a list of csv files (metadata, data, year)\ndf.name= 'iso2k'\nutf.write_compact_dataframe_to_csv(df)\n</pre> # save to a list of csv files (metadata, data, year) df.name= 'iso2k' utf.write_compact_dataframe_to_csv(df) <pre>METADATA: datasetId, archiveType, dataSetName, geo_meanElev, geo_meanLat, geo_meanLon, geo_siteName, interpretation_direction, interpretation_seasonality, interpretation_variable, interpretation_variableDetail, originalDataURL, originalDatabase, paleoData_notes, paleoData_proxy, paleoData_sensorSpecies, paleoData_units, paleoData_variableName, yearUnits\nSaved to /home/jupyter-lluecke/dod2k/data/iso2k/iso2k_compact_%s.csv\n</pre> In\u00a0[23]: Copied! <pre># load dataframe\ndf = utf.load_compact_dataframe_from_csv('iso2k')\n</pre> # load dataframe df = utf.load_compact_dataframe_from_csv('iso2k') <p>Show spatial distribution of records, show archive and proxy types</p> In\u00a0[24]: Copied! <pre># count archive types\narchive_count = {}\nfor ii, at in enumerate(set(df['archiveType'])):\n    archive_count[at] = df.loc[df['archiveType']==at, 'archiveType'].count()\n\nsort = np.argsort([cc for cc in archive_count.values()])\narchives_sorted = np.array([cc for cc in archive_count.keys()])[sort][::-1]\n\n# Specify colour for each archive (smaller archives get grouped into the same colour)\narchive_colour, major_archives, other_archives = uplt.get_archive_colours(archives_sorted, archive_count)\n\nfig = uplt.plot_geo_archive_proxy(df, archive_colour)\nutf.save_fig(fig, f'geo_{df.name}', dir=df.name)\n</pre> # count archive types archive_count = {} for ii, at in enumerate(set(df['archiveType'])):     archive_count[at] = df.loc[df['archiveType']==at, 'archiveType'].count()  sort = np.argsort([cc for cc in archive_count.values()]) archives_sorted = np.array([cc for cc in archive_count.keys()])[sort][::-1]  # Specify colour for each archive (smaller archives get grouped into the same colour) archive_colour, major_archives, other_archives = uplt.get_archive_colours(archives_sorted, archive_count)  fig = uplt.plot_geo_archive_proxy(df, archive_colour) utf.save_fig(fig, f'geo_{df.name}', dir=df.name) <pre>0 GlacierIce 146\n1 Coral 115\n2 LakeSediment 67\n3 Speleothem 41\n4 Wood 33\n5 MarineSediment 24\n6 GroundIce 6\n7 Sclerosponge 2\n8 MolluskShell 1\nsaved figure in /home/jupyter-lluecke/dod2k/figs/iso2k/geo_iso2k.pdf\n</pre> <p>Now plot the coverage over the Common Era</p> In\u00a0[25]: Copied! <pre>fig = uplt.plot_coverage(df, archives_sorted, major_archives, other_archives, archive_colour)\nutf.save_fig(fig, f'time_{df.name}', dir=df.name)\n</pre> fig = uplt.plot_coverage(df, archives_sorted, major_archives, other_archives, archive_colour) utf.save_fig(fig, f'time_{df.name}', dir=df.name) <pre>saved figure in /home/jupyter-lluecke/dod2k/figs/iso2k/time_iso2k.pdf\n</pre> In\u00a0[26]: Copied! <pre># # check index\nprint(df.index)\n</pre> # # check index print(df.index) <pre>RangeIndex(start=0, stop=435, step=1)\n</pre> In\u00a0[27]: Copied! <pre># # check dataSetName\nkey = 'dataSetName'\nprint('%s: '%key)\nprint(df[key].values)\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # # check dataSetName key = 'dataSetName' print('%s: '%key) print(df[key].values) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>dataSetName: \n['CO14WUCL' 'IC00OEKC' 'MS09TCIS' 'CO95TUNG' 'CO17MUMA' 'CO17MUMA'\n 'IC16WE30' 'LS10WRNA' 'IC10VICR' 'IC10VICR' 'CO98SWPR' 'CO04BAFI'\n 'CO04BAFI' 'CO04BAFI' 'SP11STBR' 'CO17DESC03A' 'CO17DESC03A' 'IC13STSP'\n 'LS13WASU' 'LS13WASU' 'LS16STPA' 'IC09VIA8' 'CO99DRGB' 'CO00URMA'\n 'CO00URMA' 'IC11LAB4' 'IC96MOSS' 'IC06VING' 'IC00STHN' 'IC99IS89'\n 'LS09LAEL' 'SP08HUCN' 'IC97THGL' 'CO09NUTB' 'MS96LKSS' 'MS96LKSS'\n 'SP10BEIN' 'CO04KIVA' 'IC02STTA' 'MS11KDMS' 'IC16WE22' 'MS09TRFD'\n 'MS09TRFD' 'TR16LAAN' 'CO13HENG' 'CO13HENG' 'SP12CACN' 'CO96QUVA'\n 'LS11TICH' 'SP09GRID' 'SP09GRID' 'IC13ST03' 'IC15GANK' 'IC15GANK'\n 'CO97CHSY' 'SP07ASUS' 'CO04ZIMG' 'CO98EVXM' 'IC85LAD2' 'IC10VISD'\n 'IC10VISD' 'TR11SAHU00' 'IC16WE20' 'TR13POMA' 'LS91HOMI' 'SP07FLOM'\n 'SP07DEUS' 'SP07DEUS' 'LS15LAOH' 'IC13STWD' 'IC13STWD' 'IC13STWD'\n 'CO14ZIMG' 'CO14ZIMG' 'IW15MELD' 'IC12WACD' 'IC02HEWD' 'IC04FIML'\n 'IC09VICC' 'IC16WE17' 'CO17DESC04A' 'CO17DESC04A' 'CO14OSPA' 'CO14OSPA'\n 'CO06LIFI' 'IC96ISE9' 'IC05ISAU' 'CO99SWFB' 'CO99SWFB' 'SP06DYCN'\n 'SP05MAAT' 'LS16STCL' 'IC12RHME' 'LS02HATI' 'LS10HEQI' 'LS12THAY'\n 'IC00SC29' 'IC06VID3' 'IC99SCFH' 'TR14KOSP' 'TR14KOSP' 'LS11ANSE'\n 'IC10VISA' 'IC10VISA' 'TR12BRBO' 'TR10ANMO' 'CO14CATI' 'LS09SASP'\n 'LS09SASP' 'LS06JONA' 'TR08THCO' 'LS09BAUM' 'IC06THPR' 'IC06THPR'\n 'IC06THPR' 'IC13STN7' 'TR06TRBO' 'CO03COPM' 'CO03COPM' 'CO03COPM'\n 'CO03COPM' 'CO03COPM' 'CO03COPM' 'CO03COPM' 'CO03COPM' 'CO03COPM'\n 'CO03COPM' 'CO03COPM' 'CO03COPM' 'SP04YUCN' 'CO93COTW' 'CO13COTB'\n 'GI19POYU' 'GI19POYU' 'GI19POYU' 'GI19POYU' 'TR11GRTP' 'IC06KAS1'\n 'IC12SIWG' 'CO13CABL' 'CO13CABL' 'IC01JO01' 'CO11NUPM' 'CO11NUPM'\n 'CO17WUBO1G' 'TR11BAPU' 'CO17WUBO1B' 'CO13COXM' 'MS06DLGN' 'MS06DLGN'\n 'LS13KOLA' 'CO06MOPE' 'IC82STVE' 'IC82STVE' 'LS17VAWA01' 'CO96SWFB'\n 'IC07KADF' 'MS11SENS' 'SP06DRIT' 'SP13AYID' 'SP13AYID' 'CO02KUBE'\n 'CO99GUMI' 'CO99GUMI' 'IC16WE19' 'CO96SWBB' 'IC16WE28' 'IC95FI87'\n 'SP13MCUS' 'LS11MOES' 'TR16WEXI' 'TR13SIKO' 'MS14GFSS' 'MS14GFSS'\n 'LS12WOQU' 'SP10WINZ' 'SP10WINZ' 'IC08VI84' 'LS02ROSA' 'CO99KUHO'\n 'CO99KUHO' 'LS16THQI01' 'ClelandLake.Steinman.2016'\n 'ClelandLake.Steinman.2016' 'SS05ROTN' 'IC11STTA' 'CO17WUBO1D' 'CO03CHBU'\n 'LS11KOMA' 'TR12BECO' 'LS08LICA' 'LS11RYKA' 'LS14FOTI' 'LS00WODE'\n 'IC10VIP2' 'IC10VIP2' 'CO17RAPA' 'IC13STN8' 'IC17EKN3' 'CO12GOVA'\n 'SP08ZHCN' 'CO18DATO01A' 'TR11MAPE' 'MS07KPPB' 'MS07KPPB' 'IC03HOIL'\n 'TR15BABO' 'IC10VIP1' 'IC10VIP1' 'IC95THHU' 'IC95THHU' 'IC95THHU'\n 'IC95THHU' 'IC95THHU' 'IC00EIGG' 'LS96VOAM' 'CO18RELC01A' 'CO98QUNC'\n 'IC10VIMI' 'IC10VIMI' 'IC06THDD' 'LS14KOSA' 'IC14MAFH' 'IC08VI87'\n 'LS06CRLA' 'CO17WUBO1C' 'TR08HORA' 'IC92HOML' 'LS03RUED' 'IC98FIPE'\n 'TR15YOLL' 'CO00COKY' 'IC11LIMI' 'LS16THN301' 'LS16THN301' 'LS16THN301'\n 'LS16THN301' 'IC00COPR' 'TR11BAVO' 'TR11BAVO' 'CO07CAFR' 'CO08GOBE'\n 'CO05BAFI' 'IC13OPAN' 'LS06DENA' 'TR17GRTP' 'IC17BERI' 'TR18XUGA00'\n 'LS06SHBE' 'SP08SPUS' 'IC10VISE' 'IC10VISE' 'SP07WABR' 'CO94LISE'\n 'CO04PFRE' 'LS08STCR' 'IC09EIBE' 'IC06WAML' 'IC08VI79' 'LS10HOLO'\n 'SP11BOAT' 'MS12AMMM' 'IC13THFE' 'CO01TUNG' 'CO01TUNG' 'IC99ISS2'\n 'IC11BEVL' 'IC10NAB5' 'IC00OE18' 'IC16WENG' 'LS15DENA' 'LS13STEL'\n 'CO98CHPI' 'IC17EK4K' 'CO04LIFI' 'IC94THDP' 'LS09SAWA' 'IC02THKI'\n 'CO14CABT' 'CO14CABT' 'CO14CABT' 'SP10CACN' 'SP10CACN' 'IC00SC16'\n 'IC16GOBP' 'LS89LATI' 'SP03BMIL' 'CO06QUNG' 'IC17EKPV' 'IC02SCGN'\n 'CO03CHBA' 'CO15ABSP' 'TR17SAMA' 'LS10ANBL' 'LS02BEPY' 'MS05LKLF'\n 'LS09SHJO' 'MS12CCCH' 'TR13SAWA00' 'MS02FLPC' 'TR11MAJA' 'SP05CRBR'\n 'IC00OE05' 'IC16WE23' 'MS06DLDS' 'MS06DLDS' 'LS16WIGH' 'LS16WIGH'\n 'SH18CASA' 'IC00SC21' 'MS01HDAS' 'MS06DLGS' 'MS06DLGS' 'CO06MOTO'\n 'CO06MOTO' 'LS12STRE' 'LS09SAJU' 'LS09SAJU' 'IC11DILO' 'LS15OEPO'\n 'SP09FLTR' 'CO04LIRA' 'CO04LIRA' 'CO04LIRA' 'SP12NOBR' 'MS07SWGG'\n 'TR12SAMU' 'TR12SAMU' 'TR12SAMU' 'CO05KUBE' 'IC06VIGR' 'IC10VIGI'\n 'IC10VIGI' 'CO00KUNI' 'CO00KUNI' 'TR15NACA' 'CO05ASGU' 'LS09LACA'\n 'LS15SHBO' 'CO99LICL' 'CO99LICL' 'CO99LICL' 'IC13PALD' 'IC10VIGR'\n 'IC11KIPW' 'CO98BOFP' 'SS05ROES' 'IW17OPOY' 'TR13BROA' 'SP98KUCN'\n 'IC09KANG' 'IC09VIRE' 'IC00THDS' 'IC00THDS' 'LS14ZHJU' 'IC13ST21'\n 'IC07KARB' 'CO13DESC01A' 'CO13DESC01A' 'IC15MANE' 'LS06TIST' 'IC09OPAN'\n 'IC09OPAN' 'IC14EKVK' 'LS14YAHA' 'LS14YAHA' 'CO04PFCH' 'CO04PFCH'\n 'IC12MUJR' 'IC08VISV' 'IC00SC26' 'IC93BAGI' 'IC93BAGI' 'IC76FIDE'\n 'IC76FIDE' 'LS11BACH' 'CO10HEIG' 'CO00DRBE' 'CO00DRBE' 'SP10SUSE'\n 'CO06DATZ' 'LS01HUFA' 'LS01HUFA' 'CO09NAKY' 'MS03BRNS' 'CO17XISC01A'\n 'CO17XISC01A' 'IC10VIP3' 'IC10VIP3' 'IC97GRGI' 'SP04DEUS' 'IC13BOCG'\n 'IC13BOCG' 'IC13BOCG' 'IC13BOCG' 'LS12STLI' 'IC00OE14' 'CO08HEVE'\n 'SP10HAUS' 'SP10HAUS' 'SP10HAUS' 'IC10VISG' 'IC10VISG' 'LS17BAHA01'\n 'LS17BAHA01' 'IC10VID3' 'IC10VID3' 'IC10VID3' 'IC10VID3' 'IC10VID3'\n 'IC10VID3' 'IC10VID3' 'IC10VID3' 'SP12ERUS' 'CO00FERA' 'IC02GR05'\n 'SP12FOAT' 'TR16WEMI' 'IC98THSA' 'IC08VIRE' 'SP11BEMX' 'SP08VBPE'\n 'IC13THQU' 'TR15YONW' 'CO17DESC02A' 'CO17DESC02A' 'IC10VIRE' 'IC10VIRE'\n 'IC09KA79' 'TR04EVLI' 'CO09FEOG' 'CO17DESC01A' 'CO17DESC01A' 'SP99DEUS'\n 'SP99DEUS']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 327/435\n</pre> In\u00a0[28]: Copied! <pre># # check datasetId\n\nprint(len(df.datasetId.unique()))\nprint(len(df))\nkey = 'datasetId'\nprint('%s (starts with): '%key)\nprint(df[key].values)\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint('datasetId starts with: ', np.unique([str(dd.split('_')[0]) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # # check datasetId  print(len(df.datasetId.unique())) print(len(df)) key = 'datasetId' print('%s (starts with): '%key) print(df[key].values) print(np.unique([str(type(dd)) for dd in df[key]])) print('datasetId starts with: ', np.unique([str(dd.split('_')[0]) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>435\n435\ndatasetId (starts with): \n['iso2k_0' 'iso2k_12' 'iso2k_17' 'iso2k_20' 'iso2k_23' 'iso2k_25'\n 'iso2k_26' 'iso2k_31' 'iso2k_35' 'iso2k_37' 'iso2k_50' 'iso2k_52'\n 'iso2k_55' 'iso2k_58' 'iso2k_60' 'iso2k_67' 'iso2k_68' 'iso2k_72'\n 'iso2k_74' 'iso2k_77' 'iso2k_78' 'iso2k_88' 'iso2k_91' 'iso2k_94'\n 'iso2k_98' 'iso2k_100' 'iso2k_102' 'iso2k_104' 'iso2k_110' 'iso2k_112'\n 'iso2k_117' 'iso2k_120' 'iso2k_124' 'iso2k_127' 'iso2k_134' 'iso2k_137'\n 'iso2k_140' 'iso2k_155' 'iso2k_174' 'iso2k_179' 'iso2k_185' 'iso2k_203'\n 'iso2k_206' 'iso2k_207' 'iso2k_211' 'iso2k_213' 'iso2k_215' 'iso2k_218'\n 'iso2k_220' 'iso2k_236' 'iso2k_239' 'iso2k_244' 'iso2k_249' 'iso2k_250'\n 'iso2k_252' 'iso2k_254' 'iso2k_257' 'iso2k_259' 'iso2k_264' 'iso2k_267'\n 'iso2k_270' 'iso2k_272' 'iso2k_273' 'iso2k_279' 'iso2k_281' 'iso2k_284'\n 'iso2k_287' 'iso2k_290' 'iso2k_293' 'iso2k_296' 'iso2k_298' 'iso2k_299'\n 'iso2k_300' 'iso2k_302' 'iso2k_304' 'iso2k_311' 'iso2k_320' 'iso2k_324'\n 'iso2k_339' 'iso2k_341' 'iso2k_345' 'iso2k_346' 'iso2k_347' 'iso2k_350'\n 'iso2k_353' 'iso2k_358' 'iso2k_362' 'iso2k_373' 'iso2k_376' 'iso2k_380'\n 'iso2k_396' 'iso2k_399' 'iso2k_404' 'iso2k_410' 'iso2k_415' 'iso2k_424'\n 'iso2k_433' 'iso2k_443' 'iso2k_446' 'iso2k_449' 'iso2k_450' 'iso2k_454'\n 'iso2k_457' 'iso2k_460' 'iso2k_463' 'iso2k_465' 'iso2k_473' 'iso2k_474'\n 'iso2k_477' 'iso2k_478' 'iso2k_481' 'iso2k_485' 'iso2k_488' 'iso2k_490'\n 'iso2k_492' 'iso2k_498' 'iso2k_503' 'iso2k_505' 'iso2k_507' 'iso2k_509'\n 'iso2k_511' 'iso2k_513' 'iso2k_515' 'iso2k_517' 'iso2k_519' 'iso2k_521'\n 'iso2k_523' 'iso2k_525' 'iso2k_527' 'iso2k_533' 'iso2k_539' 'iso2k_541'\n 'iso2k_546' 'iso2k_547' 'iso2k_549' 'iso2k_550' 'iso2k_556' 'iso2k_558'\n 'iso2k_567' 'iso2k_569' 'iso2k_571' 'iso2k_573' 'iso2k_575' 'iso2k_579'\n 'iso2k_581' 'iso2k_585' 'iso2k_587' 'iso2k_591' 'iso2k_607' 'iso2k_610'\n 'iso2k_611' 'iso2k_629' 'iso2k_630' 'iso2k_633' 'iso2k_640' 'iso2k_641'\n 'iso2k_644' 'iso2k_646' 'iso2k_650' 'iso2k_653' 'iso2k_656' 'iso2k_698'\n 'iso2k_702' 'iso2k_705' 'iso2k_706' 'iso2k_715' 'iso2k_718' 'iso2k_723'\n 'iso2k_726' 'iso2k_742' 'iso2k_745' 'iso2k_747' 'iso2k_757' 'iso2k_762'\n 'iso2k_766' 'iso2k_772' 'iso2k_775' 'iso2k_778' 'iso2k_781' 'iso2k_786'\n 'iso2k_788' 'iso2k_799' 'iso2k_806' 'iso2k_811' 'iso2k_818' 'iso2k_821'\n 'iso2k_824' 'iso2k_826' 'iso2k_828' 'iso2k_834' 'iso2k_836' 'iso2k_841'\n 'iso2k_844' 'iso2k_848' 'iso2k_857' 'iso2k_859' 'iso2k_864' 'iso2k_866'\n 'iso2k_868' 'iso2k_870' 'iso2k_873' 'iso2k_879' 'iso2k_880' 'iso2k_883'\n 'iso2k_886' 'iso2k_889' 'iso2k_892' 'iso2k_897' 'iso2k_899' 'iso2k_900'\n 'iso2k_902' 'iso2k_903' 'iso2k_906' 'iso2k_909' 'iso2k_910' 'iso2k_913'\n 'iso2k_917' 'iso2k_921' 'iso2k_928' 'iso2k_931' 'iso2k_932' 'iso2k_935'\n 'iso2k_938' 'iso2k_954' 'iso2k_965' 'iso2k_975' 'iso2k_979' 'iso2k_990'\n 'iso2k_995' 'iso2k_998' 'iso2k_1000' 'iso2k_1010' 'iso2k_1014'\n 'iso2k_1019' 'iso2k_1033' 'iso2k_1036' 'iso2k_1039' 'iso2k_1044'\n 'iso2k_1054' 'iso2k_1056' 'iso2k_1057' 'iso2k_1060' 'iso2k_1068'\n 'iso2k_1069' 'iso2k_1083' 'iso2k_1086' 'iso2k_1099' 'iso2k_1102'\n 'iso2k_1106' 'iso2k_1107' 'iso2k_1111' 'iso2k_1114' 'iso2k_1118'\n 'iso2k_1124' 'iso2k_1130' 'iso2k_1132' 'iso2k_1148' 'iso2k_1151'\n 'iso2k_1170' 'iso2k_1173' 'iso2k_1178' 'iso2k_1190' 'iso2k_1199'\n 'iso2k_1201' 'iso2k_1203' 'iso2k_1205' 'iso2k_1212' 'iso2k_1214'\n 'iso2k_1216' 'iso2k_1218' 'iso2k_1221' 'iso2k_1223' 'iso2k_1229'\n 'iso2k_1255' 'iso2k_1264' 'iso2k_1267' 'iso2k_1269' 'iso2k_1277'\n 'iso2k_1283' 'iso2k_1286' 'iso2k_1287' 'iso2k_1288' 'iso2k_1291'\n 'iso2k_1294' 'iso2k_1296' 'iso2k_1304' 'iso2k_1307' 'iso2k_1311'\n 'iso2k_1315' 'iso2k_1320' 'iso2k_1322' 'iso2k_1340' 'iso2k_1344'\n 'iso2k_1358' 'iso2k_1366' 'iso2k_1380' 'iso2k_1384' 'iso2k_1395'\n 'iso2k_1397' 'iso2k_1398' 'iso2k_1407' 'iso2k_1416' 'iso2k_1419'\n 'iso2k_1425' 'iso2k_1430' 'iso2k_1434' 'iso2k_1439' 'iso2k_1440'\n 'iso2k_1446' 'iso2k_1449' 'iso2k_1458' 'iso2k_1462' 'iso2k_1466'\n 'iso2k_1467' 'iso2k_1470' 'iso2k_1474' 'iso2k_1476' 'iso2k_1480'\n 'iso2k_1481' 'iso2k_1488' 'iso2k_1495' 'iso2k_1498' 'iso2k_1500'\n 'iso2k_1502' 'iso2k_1504' 'iso2k_1519' 'iso2k_1525' 'iso2k_1528'\n 'iso2k_1529' 'iso2k_1530' 'iso2k_1534' 'iso2k_1542' 'iso2k_1545'\n 'iso2k_1554' 'iso2k_1556' 'iso2k_1557' 'iso2k_1559' 'iso2k_1563'\n 'iso2k_1566' 'iso2k_1569' 'iso2k_1571' 'iso2k_1573' 'iso2k_1575'\n 'iso2k_1577' 'iso2k_1579' 'iso2k_1581' 'iso2k_1587' 'iso2k_1590'\n 'iso2k_1593' 'iso2k_1595' 'iso2k_1619' 'iso2k_1626' 'iso2k_1628'\n 'iso2k_1631' 'iso2k_1633' 'iso2k_1637' 'iso2k_1639' 'iso2k_1643'\n 'iso2k_1644' 'iso2k_1645' 'iso2k_1653' 'iso2k_1658' 'iso2k_1660'\n 'iso2k_1690' 'iso2k_1695' 'iso2k_1698' 'iso2k_1701' 'iso2k_1704'\n 'iso2k_1708' 'iso2k_1710' 'iso2k_1713' 'iso2k_1716' 'iso2k_1719'\n 'iso2k_1727' 'iso2k_1729' 'iso2k_1732' 'iso2k_1735' 'iso2k_1738'\n 'iso2k_1740' 'iso2k_1745' 'iso2k_1748' 'iso2k_1750' 'iso2k_1753'\n 'iso2k_1754' 'iso2k_1756' 'iso2k_1762' 'iso2k_1763' 'iso2k_1770'\n 'iso2k_1772' 'iso2k_1779' 'iso2k_1790' 'iso2k_1795' 'iso2k_1798'\n 'iso2k_1799' 'iso2k_1800' 'iso2k_1801' 'iso2k_1811' 'iso2k_1813'\n 'iso2k_1817' 'iso2k_1820' 'iso2k_1823' 'iso2k_1832' 'iso2k_1835'\n 'iso2k_1837' 'iso2k_1839' 'iso2k_1846' 'iso2k_1848' 'iso2k_1850'\n 'iso2k_1851' 'iso2k_1852' 'iso2k_1853' 'iso2k_1854' 'iso2k_1855'\n 'iso2k_1856' 'iso2k_1861' 'iso2k_1862' 'iso2k_1864' 'iso2k_1867'\n 'iso2k_1869' 'iso2k_1872' 'iso2k_1875' 'iso2k_1883' 'iso2k_1885'\n 'iso2k_1898' 'iso2k_1901' 'iso2k_1902' 'iso2k_1903' 'iso2k_1906'\n 'iso2k_1916' 'iso2k_1918' 'iso2k_1922' 'iso2k_1950' 'iso2k_1951'\n 'iso2k_1952' 'iso2k_1955']\n[\"&lt;class 'str'&gt;\"]\ndatasetId starts with:  ['iso2k']\nNo. of unique values: 435/435\n</pre> In\u00a0[29]: Copied! <pre># originalDataURL\nkey = 'originalDataURL'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([kk for kk in df[key] if 'this' in kk]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n# 'this study' should point to the correct URL (PAGES2k)\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # originalDataURL key = 'originalDataURL' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([kk for kk in df[key] if 'this' in kk])) print(np.unique([str(type(dd)) for dd in df[key]])) # 'this study' should point to the correct URL (PAGES2k) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>originalDataURL: \n['This compilation' 'http://doi.pangaea.de/10.1594/PANGAEA.711835'\n 'http://doi.pangaea.de/10.1594/PANGAEA.735717'\n 'http://doi.pangaea.de/10.1594/PANGAEA.738188'\n 'http://doi.pangaea.de/10.1594/PANGAEA.776444'\n 'http://doi.pangaea.de/10.1594/PANGAEA.780423'\n 'http://www.iceandclimate.nbi.ku.dk/data/Vinther_etal_2010_data_02feb2010.xls'\n 'http://www1.ncdc.noaa.gov/pub/data/paleo/paleolimnology/europe/germany/ammersee_1999.txt'\n 'http://www1.ncdc.noaa.gov/pub/data/paleo/paleolimnology/europe/uk/lough-na-shade2010.txt'\n 'http://www1.ncdc.noaa.gov/pub/data/paleo/treering/isotope/asia/russia/'\n 'http://www1.ncdc.noaa.gov/pub/data/paleo/treering/isotope/southamerica/'\n 'https://agupubs.onlinelibrary.wiley.com/action/downloadSupplement?doi=10.1002%2F2016JC012458&amp;file=jgrc22050-sup-0002-2016JC012458-ds01.xlsx'\n 'https://ars-els-cdn-com.libezp.lib.lsu.edu/content/image/1-s2.0-S1367912016304138-mmc1.xlsx'\n 'https://doi.org/10.1594/PANGAEA.864869'\n 'https://doi.org/10.1594/PANGAEA.869839'\n 'https://doi.org/10.1594/PANGAEA.869840'\n 'https://doi.org/10.1594/PANGAEA.892303'\n 'https://doi.org/10.25921/6e73-as97'\n 'https://doi.pangaea.de/10.1594/PANGAEA.104862'\n 'https://doi.pangaea.de/10.1594/PANGAEA.104880'\n 'https://doi.pangaea.de/10.1594/PANGAEA.104886'\n 'https://doi.pangaea.de/10.1594/PANGAEA.104889'\n 'https://doi.pangaea.de/10.1594/PANGAEA.728240'\n 'https://doi.pangaea.de/10.1594/PANGAEA.760166'\n 'https://doi.pangaea.de/10.1594/PANGAEA.770159'\n 'https://doi.pangaea.de/10.1594/PANGAEA.786354'\n 'https://doi.pangaea.de/10.1594/PANGAEA.786356'\n 'https://doi.pangaea.de/10.1594/PANGAEA.824732'\n 'https://doi.pangaea.de/10.1594/PANGAEA.829390'\n 'https://doi.pangaea.de/10.1594/PANGAEA.842166'\n 'https://doi.pangaea.de/10.1594/PANGAEA.849161'\n 'https://doi.pangaea.de/10.1594/PANGAEA.857573'\n 'https://doi.pangaea.de/10.1594/PANGAEA.866368'\n 'https://doi.pangaea.de/10.1594/PANGAEA.871279'\n 'https://doi.pangaea.de/10.1594/PANGAEA.873563'\n 'https://doi.pangaea.de/10.1594/PANGAEA.874205'\n 'https://doi.pangaea.de/10.1594/PANGAEA.880012'\n 'https://doi.pangaea.de/10.1594/PANGAEA.880396'\n 'https://doi.pangaea.de/10.1594/PANGAEA.880528'\n 'https://www.ncdc.noaa.gov/cdo/f?p=519:1:0::::P1_STUDY_ID:13174'\n 'https://www.ncdc.noaa.gov/cdo/f?p=519:1:::::P1_STUDY_ID:5472'\n 'https://www.ncdc.noaa.gov/cdo/f?p=519:1:::::P1_STUDY_ID:8647'\n 'https://www.ncdc.noaa.gov/paleo-search/study/11173'\n 'https://www.ncdc.noaa.gov/paleo-search/study/11180'\n 'https://www.ncdc.noaa.gov/paleo-search/study/15072'\n 'https://www.ncdc.noaa.gov/paleo-search/study/17798'\n 'https://www.ncdc.noaa.gov/paleo-search/study/21171'\n 'https://www.ncdc.noaa.gov/paleo-search/study/22471'\n 'https://www.ncdc.noaa.gov/paleo-search/study/22479'\n 'https://www.ncdc.noaa.gov/paleo-search/study/22543'\n 'https://www.ncdc.noaa.gov/paleo-search/study/22589'\n 'https://www.ncdc.noaa.gov/paleo-search/study/2431'\n 'https://www.ncdc.noaa.gov/paleo-search/study/2436'\n 'https://www.ncdc.noaa.gov/paleo-search/study/2448'\n 'https://www.ncdc.noaa.gov/paleo-search/study/2466'\n 'https://www.ncdc.noaa.gov/paleo-search/study/2492'\n 'https://www.ncdc.noaa.gov/paleo-search/study/2519'\n 'https://www.ncdc.noaa.gov/paleo-search/study/5416'\n 'https://www.ncdc.noaa.gov/paleo-search/study/5425'\n 'https://www.ncdc.noaa.gov/paleo-search/study/5427'\n 'https://www.ncdc.noaa.gov/paleo-search/study/5441'\n 'https://www.ncdc.noaa.gov/paleo-search/study/5541'\n 'https://www.ncdc.noaa.gov/paleo-search/study/5966'\n 'https://www.ncdc.noaa.gov/paleo-search/study/6182'\n 'https://www.ncdc.noaa.gov/paleo-search/study/9741'\n 'https://www.ncdc.noaa.gov/paleo-search/study/9742'\n 'https://www.ncdc.noaa.gov/paleo/study/10373'\n 'https://www.ncdc.noaa.gov/paleo/study/10455'\n 'https://www.ncdc.noaa.gov/paleo/study/10670'\n 'https://www.ncdc.noaa.gov/paleo/study/10750'\n 'https://www.ncdc.noaa.gov/paleo/study/10808'\n 'https://www.ncdc.noaa.gov/paleo/study/10889'\n 'https://www.ncdc.noaa.gov/paleo/study/11131'\n 'https://www.ncdc.noaa.gov/paleo/study/11148'\n 'https://www.ncdc.noaa.gov/paleo/study/11197'\n 'https://www.ncdc.noaa.gov/paleo/study/12426'\n 'https://www.ncdc.noaa.gov/paleo/study/12841'\n 'https://www.ncdc.noaa.gov/paleo/study/12885'\n 'https://www.ncdc.noaa.gov/paleo/study/12891'\n 'https://www.ncdc.noaa.gov/paleo/study/12893'\n 'https://www.ncdc.noaa.gov/paleo/study/12916'\n 'https://www.ncdc.noaa.gov/paleo/study/12942'\n 'https://www.ncdc.noaa.gov/paleo/study/12994'\n 'https://www.ncdc.noaa.gov/paleo/study/13079'\n 'https://www.ncdc.noaa.gov/paleo/study/13114'\n 'https://www.ncdc.noaa.gov/paleo/study/13175'\n 'https://www.ncdc.noaa.gov/paleo/study/13195'\n 'https://www.ncdc.noaa.gov/paleo/study/13439'\n 'https://www.ncdc.noaa.gov/paleo/study/13540'\n 'https://www.ncdc.noaa.gov/paleo/study/13543'\n 'https://www.ncdc.noaa.gov/paleo/study/13625'\n 'https://www.ncdc.noaa.gov/paleo/study/13670'\n 'https://www.ncdc.noaa.gov/paleo/study/13672'\n 'https://www.ncdc.noaa.gov/paleo/study/13954'\n 'https://www.ncdc.noaa.gov/paleo/study/14129'\n 'https://www.ncdc.noaa.gov/paleo/study/14174'\n 'https://www.ncdc.noaa.gov/paleo/study/14510'\n 'https://www.ncdc.noaa.gov/paleo/study/14652'\n 'https://www.ncdc.noaa.gov/paleo/study/15444'\n 'https://www.ncdc.noaa.gov/paleo/study/15794'\n 'https://www.ncdc.noaa.gov/paleo/study/16139'\n 'https://www.ncdc.noaa.gov/paleo/study/16155'\n 'https://www.ncdc.noaa.gov/paleo/study/16216'\n 'https://www.ncdc.noaa.gov/paleo/study/16339'\n 'https://www.ncdc.noaa.gov/paleo/study/16357'\n 'https://www.ncdc.noaa.gov/paleo/study/16438'\n 'https://www.ncdc.noaa.gov/paleo/study/17289'\n 'https://www.ncdc.noaa.gov/paleo/study/17378'\n 'https://www.ncdc.noaa.gov/paleo/study/17380'\n 'https://www.ncdc.noaa.gov/paleo/study/17576'\n 'https://www.ncdc.noaa.gov/paleo/study/17796'\n 'https://www.ncdc.noaa.gov/paleo/study/17919'\n 'https://www.ncdc.noaa.gov/paleo/study/18315'\n 'https://www.ncdc.noaa.gov/paleo/study/18355'\n 'https://www.ncdc.noaa.gov/paleo/study/1839'\n 'https://www.ncdc.noaa.gov/paleo/study/1842'\n 'https://www.ncdc.noaa.gov/paleo/study/1843'\n 'https://www.ncdc.noaa.gov/paleo/study/1844'\n 'https://www.ncdc.noaa.gov/paleo/study/1845'\n 'https://www.ncdc.noaa.gov/paleo/study/1846'\n 'https://www.ncdc.noaa.gov/paleo/study/1847'\n 'https://www.ncdc.noaa.gov/paleo/study/1853'\n 'https://www.ncdc.noaa.gov/paleo/study/1855'\n 'https://www.ncdc.noaa.gov/paleo/study/1856'\n 'https://www.ncdc.noaa.gov/paleo/study/1857'\n 'https://www.ncdc.noaa.gov/paleo/study/1859'\n 'https://www.ncdc.noaa.gov/paleo/study/1861'\n 'https://www.ncdc.noaa.gov/paleo/study/1866'\n 'https://www.ncdc.noaa.gov/paleo/study/1867'\n 'https://www.ncdc.noaa.gov/paleo/study/1872'\n 'https://www.ncdc.noaa.gov/paleo/study/1875'\n 'https://www.ncdc.noaa.gov/paleo/study/1876'\n 'https://www.ncdc.noaa.gov/paleo/study/1881'\n 'https://www.ncdc.noaa.gov/paleo/study/1885'\n 'https://www.ncdc.noaa.gov/paleo/study/1886'\n 'https://www.ncdc.noaa.gov/paleo/study/18895'\n 'https://www.ncdc.noaa.gov/paleo/study/1890'\n 'https://www.ncdc.noaa.gov/paleo/study/1891'\n 'https://www.ncdc.noaa.gov/paleo/study/1901'\n 'https://www.ncdc.noaa.gov/paleo/study/1903'\n 'https://www.ncdc.noaa.gov/paleo/study/1911'\n 'https://www.ncdc.noaa.gov/paleo/study/1913'\n 'https://www.ncdc.noaa.gov/paleo/study/1914'\n 'https://www.ncdc.noaa.gov/paleo/study/1915'\n 'https://www.ncdc.noaa.gov/paleo/study/1916'\n 'https://www.ncdc.noaa.gov/paleo/study/19179'\n 'https://www.ncdc.noaa.gov/paleo/study/1920'\n 'https://www.ncdc.noaa.gov/paleo/study/1924'\n 'https://www.ncdc.noaa.gov/paleo/study/1925'\n 'https://www.ncdc.noaa.gov/paleo/study/19421'\n 'https://www.ncdc.noaa.gov/paleo/study/20002'\n 'https://www.ncdc.noaa.gov/paleo/study/20126'\n 'https://www.ncdc.noaa.gov/paleo/study/20350'\n 'https://www.ncdc.noaa.gov/paleo/study/20610'\n 'https://www.ncdc.noaa.gov/paleo/study/20904'\n 'https://www.ncdc.noaa.gov/paleo/study/20930'\n 'https://www.ncdc.noaa.gov/paleo/study/21250'\n 'https://www.ncdc.noaa.gov/paleo/study/22120'\n 'https://www.ncdc.noaa.gov/paleo/study/22409'\n 'https://www.ncdc.noaa.gov/paleo/study/22477'\n 'https://www.ncdc.noaa.gov/paleo/study/22502'\n 'https://www.ncdc.noaa.gov/paleo/study/22519'\n 'https://www.ncdc.noaa.gov/paleo/study/22531'\n 'https://www.ncdc.noaa.gov/paleo/study/22532'\n 'https://www.ncdc.noaa.gov/paleo/study/22541'\n 'https://www.ncdc.noaa.gov/paleo/study/22542'\n 'https://www.ncdc.noaa.gov/paleo/study/22547'\n 'https://www.ncdc.noaa.gov/paleo/study/22548'\n 'https://www.ncdc.noaa.gov/paleo/study/22549'\n 'https://www.ncdc.noaa.gov/paleo/study/22592'\n 'https://www.ncdc.noaa.gov/paleo/study/22602'\n 'https://www.ncdc.noaa.gov/paleo/study/22712'\n 'https://www.ncdc.noaa.gov/paleo/study/23076'\n 'https://www.ncdc.noaa.gov/paleo/study/23081'\n 'https://www.ncdc.noaa.gov/paleo/study/23092'\n 'https://www.ncdc.noaa.gov/paleo/study/23095'\n 'https://www.ncdc.noaa.gov/paleo/study/23231'\n 'https://www.ncdc.noaa.gov/paleo/study/23232'\n 'https://www.ncdc.noaa.gov/paleo/study/23233'\n 'https://www.ncdc.noaa.gov/paleo/study/23850'\n 'https://www.ncdc.noaa.gov/paleo/study/2424'\n 'https://www.ncdc.noaa.gov/paleo/study/2447'\n 'https://www.ncdc.noaa.gov/paleo/study/24630'\n 'https://www.ncdc.noaa.gov/paleo/study/2494'\n 'https://www.ncdc.noaa.gov/paleo/study/29432'\n 'https://www.ncdc.noaa.gov/paleo/study/5423'\n 'https://www.ncdc.noaa.gov/paleo/study/5428'\n 'https://www.ncdc.noaa.gov/paleo/study/5431'\n 'https://www.ncdc.noaa.gov/paleo/study/5433'\n 'https://www.ncdc.noaa.gov/paleo/study/5537'\n 'https://www.ncdc.noaa.gov/paleo/study/5596'\n 'https://www.ncdc.noaa.gov/paleo/study/5968'\n 'https://www.ncdc.noaa.gov/paleo/study/6087'\n 'https://www.ncdc.noaa.gov/paleo/study/6089'\n 'https://www.ncdc.noaa.gov/paleo/study/6095'\n 'https://www.ncdc.noaa.gov/paleo/study/6111'\n 'https://www.ncdc.noaa.gov/paleo/study/6115'\n 'https://www.ncdc.noaa.gov/paleo/study/6116'\n 'https://www.ncdc.noaa.gov/paleo/study/6180'\n 'https://www.ncdc.noaa.gov/paleo/study/6184'\n 'https://www.ncdc.noaa.gov/paleo/study/6242'\n 'https://www.ncdc.noaa.gov/paleo/study/8608'\n 'https://www.ncdc.noaa.gov/paleo/study/8609'\n 'https://www.ncdc.noaa.gov/paleo/study/8629'\n 'https://www.ncdc.noaa.gov/paleo/study/8632'\n 'https://www.ncdc.noaa.gov/paleo/study/8635'\n 'https://www.ncdc.noaa.gov/paleo/study/8637'\n 'https://www.ncdc.noaa.gov/paleo/study/8639'\n 'https://www.ncdc.noaa.gov/paleo/study/8640'\n 'https://www.ncdc.noaa.gov/paleo/study/8700'\n 'https://www.ncdc.noaa.gov/paleo/study/8725'\n 'https://www.ncdc.noaa.gov/paleo/study/9739'\n 'https://www.ncdc.noaa.gov/paleo/study/9745'\n 'https://www.ncdc.noaa.gov/paleo/study/9790'\n 'https://www.ncdc.noaa.gov/paleo/study/9792'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/icecore/greenland/gisp/dye2/dye2ad77.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/icecore/polar/agassiz/a87del18_1yr.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/icecore/polar/devon/d7273del_5yr.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/treering/isotope/asia/nepal/ganesh2018d18o.txt'\n 'nan' 'this compilation' 'www.ncdc.noaa.gov/paleo-search/study/27330'\n 'www.ncdc.noaa.gov/paleo/study/2474']\n['this compilation']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 223/435\n</pre> In\u00a0[30]: Copied! <pre># # originalDataSet\nkey = 'originalDatabase'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n# Note: the last two records have missing URLs\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # # originalDataSet key = 'originalDatabase' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) # Note: the last two records have missing URLs print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>originalDatabase: \n['Iso2k v1.1.2']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 1/435\n</pre> In\u00a0[31]: Copied! <pre># check Elevation\nkey = 'geo_meanElev'\nprint('%s: '%key)\nprint(df[key])\nprint(np.unique(['%d'%kk for kk in df[key] if np.isfinite(kk)]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # check Elevation key = 'geo_meanElev' print('%s: '%key) print(df[key]) print(np.unique(['%d'%kk for kk in df[key] if np.isfinite(kk)])) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>geo_meanElev: \n0       -13.1\n1      1439.0\n2       178.0\n3        -3.0\n4        -5.0\n        ...  \n430      -5.6\n431      -6.0\n432      -6.0\n433     360.0\n434     360.0\nName: geo_meanElev, Length: 435, dtype: float32\n['-1' '-10' '-1048' '-11' '-12' '-1250' '-1295' '-13' '-136' '-14' '-143'\n '-16' '-18' '-2' '-2391' '-25' '-2543' '-3' '-3975' '-4' '-5' '-531'\n '-547' '-6' '-620' '-694' '-695' '-7' '-8' '-852' '-875' '-884' '-9'\n '-968' '0' '10' '1000' '1030' '104' '1054' '1126' '113' '1140' '1156'\n '12' '1200' '124' '1244' '1250' '1260' '1300' '1354' '1363' '1370' '1439'\n '150' '1542' '160' '1600' '1626' '1640' '170' '1700' '1713' '1730' '1742'\n '178' '1800' '1806' '1835' '1850' '1885' '1900' '1957' '1975' '20' '2002'\n '2023' '2030' '2147' '2185' '2200' '2234' '2242' '230' '2315' '2316'\n '2350' '240' '2450' '2451' '2480' '250' '2531' '2543' '2598' '265' '2700'\n '2733' '2777' '2800' '2820' '2840' '2874' '2882' '2907' '2917' '293'\n '294' '2947' '2960' '3' '300' '3018' '3040' '3087' '3092' '3098' '3100'\n '3150' '3160' '3174' '3193' '3194' '320' '3200' '3203' '3233' '3238'\n '330' '331' '332' '3330' '340' '3406' '3450' '3488' '3500' '3528' '360'\n '3600' '3747' '3810' '3848' '3890' '3900' '3950' '4' '40' '400' '4062'\n '4078' '415' '4150' '4200' '4250' '44' '4418' '4450' '4500' '4512' '4718'\n '477' '48' '4800' '5' '50' '500' '509' '520' '5325' '533' '5340' '550'\n '560' '5670' '5680' '570' '5893' '59' '5950' '6' '600' '6048' '6070'\n '6200' '626' '63' '630' '6350' '64' '650' '6518' '6542' '657' '670' '68'\n '680' '693' '698' '700' '703' '7200' '733' '740' '750' '754' '780' '880'\n '9' '913' '94' '97' '976' '990']\n[\"&lt;class 'float'&gt;\"]\nNo. of unique values: 228/435\n</pre> In\u00a0[32]: Copied! <pre># # Latitude\nkey = 'geo_meanLat'\nprint('%s: '%key)\nprint(np.unique(['%d'%kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # # Latitude key = 'geo_meanLat' print('%s: '%key) print(np.unique(['%d'%kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>geo_meanLat: \n['-1' '-10' '-11' '-12' '-13' '-14' '-15' '-16' '-17' '-18' '-20' '-21'\n '-22' '-23' '-27' '-28' '-3' '-4' '-41' '-42' '-46' '-5' '-51' '-64'\n '-66' '-69' '-7' '-70' '-71' '-72' '-73' '-74' '-75' '-76' '-77' '-78'\n '-79' '-8' '-82' '-84' '-89' '-9' '0' '1' '10' '11' '12' '13' '16' '17'\n '18' '19' '2' '21' '22' '23' '24' '25' '27' '28' '29' '3' '30' '31' '32'\n '33' '34' '35' '36' '37' '38' '39' '4' '40' '41' '42' '43' '44' '45' '46'\n '47' '48' '49' '5' '50' '51' '52' '54' '55' '6' '60' '62' '63' '64' '65'\n '66' '67' '68' '69' '7' '70' '71' '72' '73' '75' '76' '77' '78' '79' '80'\n '81']\n[\"&lt;class 'float'&gt;\"]\nNo. of unique values: 293/435\n</pre> In\u00a0[33]: Copied! <pre># # Longitude \nkey = 'geo_meanLon'\nprint('%s: '%key)\nprint(np.unique(['%d'%kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # # Longitude  key = 'geo_meanLon' print('%s: '%key) print(np.unique(['%d'%kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>geo_meanLon: \n['-1' '-105' '-109' '-110' '-111' '-112' '-113' '-116' '-117' '-118'\n '-119' '-12' '-122' '-123' '-13' '-133' '-136' '-138' '-140' '-145'\n '-146' '-149' '-153' '-157' '-159' '-16' '-160' '-161' '-162' '-174'\n '-22' '-26' '-35' '-36' '-37' '-38' '-39' '-4' '-41' '-42' '-43' '-44'\n '-45' '-46' '-49' '-50' '-51' '-54' '-57' '-6' '-60' '-61' '-64' '-66'\n '-67' '-68' '-69' '-70' '-71' '-73' '-74' '-75' '-76' '-77' '-79' '-8'\n '-80' '-82' '-83' '-84' '-85' '-86' '-88' '-89' '-9' '-91' '-92' '-94'\n '-95' '-99' '0' '1' '10' '100' '102' '104' '105' '106' '108' '109' '11'\n '110' '112' '113' '115' '117' '119' '12' '120' '122' '123' '124' '126'\n '134' '14' '142' '143' '144' '145' '148' '151' '152' '153' '159' '162'\n '165' '166' '167' '169' '17' '172' '173' '179' '18' '20' '24' '29' '30'\n '31' '34' '35' '37' '39' '4' '40' '43' '5' '54' '55' '6' '60' '64' '65'\n '7' '70' '71' '74' '76' '77' '79' '8' '81' '82' '85' '86' '89' '9' '90'\n '91' '93' '94' '95' '96' '97']\n[\"&lt;class 'float'&gt;\"]\nNo. of unique values: 288/435\n</pre> In\u00a0[34]: Copied! <pre># Site Name \nkey = 'geo_siteName'\nprint('%s: '%key)\nprint(df[key].values)\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # Site Name  key = 'geo_siteName' print('%s: '%key) print(df[key].values) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>geo_siteName: \n['Clipperton Atoll' 'DML Plateau (Kottas Camp) FB9802' 'Ionian Sea'\n 'Madang, Papua New Guinea' 'Doangdoangan Besar' 'Doangdoangan Besar'\n 'B30' 'Nam Co' 'Crete' 'Crete' 'Principe, Gulf of Guinea'\n 'Savusavu Bay, Fiji' 'Savusavu Bay, Fiji' 'Savusavu Bay, Fiji'\n 'Lapa Grande Cave, Brazil' 'Hainan Island' 'Hainan Island'\n 'SPRESSO (South Pole)' 'Lake Sugan' 'Lake Sugan' 'Paradise'\n 'Agassiz A84-A87' 'Abraham Reef, Great Barrier Reef, Australia'\n 'Maiana Atoll, Republic of Kiribati' 'Maiana Atoll, Republic of Kiribati'\n 'IND 22B4 Coastal DML' 'Siple Station' 'NGRIP' 'Hercules NA\u00a9vA\u00a9'\n 'Eastern DML L89 (DML Ice divide traverse L)' 'Laguna de Salvador'\n 'Heshang Cave, China' 'Guliya' 'Fanning Island' 'Sargasso Sea'\n 'Sargasso Sea' 'Dandak Cave, India'\n 'Malo Channel, Espiritu Santo Island, Vanuatu' 'TD96 Talos Dome'\n 'Makassar strait' 'B22' 'Feni Drift' 'Feni Drift' 'Angouleme, France'\n 'Misima Island, Papua New Guinea' 'Misima Island, Papua New Guinea'\n 'Tianmen Cave, China' 'Tangoa Island, Vanuatu' 'Lake Challa'\n 'Liang Luar Cave, Indonesia' 'Liang Luar Cave, Indonesia'\n 'US-ITASE-2000-3' 'Noijin Kangsang Glacier' 'Noijin Kangsang Glacier'\n 'Beau Vallon Bay, Mahe Island, Republic of the Seychelles'\n 'pink Panter Cave, New Mexico' 'Ifaty Reef, SW Madagascar' 'Kiritimati'\n 'DYE2' 'Site D' 'Site D' 'Humla, Nepal' 'B20'\n 'Mackenzie Delta, NWT, Canada' 'Miragoane' 'Qunf Cave, Oman'\n 'Devils Icebox Cave, Missouri' 'Devils Icebox Cave, Missouri' 'Ohrid'\n 'WDC05A' 'WDC05A' 'WDC05A' 'Tulear Reef, SW Madagascar'\n 'Tulear Reef, SW Madagascar' 'Lena River Delta' 'Col du Dome'\n 'Windy Dome' 'Mt Logan (PRCol)' 'Camp Century' 'B17' 'Hainan Island'\n 'Hainan Island' 'Palau' 'Palau' 'Savusavu Bay, Fiji'\n 'DML Ritscherflya E91' 'Austfonna' 'Lignumvitae Basin, Florida Bay'\n 'Lignumvitae Basin, Florida Bay' 'Dongge Cave, China'\n 'Spannagel Cave, Austria' 'Cleland Lake' 'Mt Erebus Saddle - MES'\n 'tibetanus' 'Qinghai' 'Ayr Lake' 'B29' 'DYE-3' 'Fiescherhorn'\n 'Lake Estanyola de Gerber, Spanish Pyrenees'\n 'Lake Estanyola de Gerber, Spanish Pyrenees' 'Seven mile' 'Site A'\n 'Site A' 'Purisima, Northern Bolivia'\n 'Monteverde Cloud Forest, Costa Rica' 'Ombai Strait, Timor Island'\n 'Spooky Lake' 'Spooky Lake' 'Lake Nar'\n 'Columbia Icefield, British Columbia, Canada' 'Lago Umayo'\n 'Puruogangri ice cap' 'Puruogangri ice cap' 'Puruogangri ice cap'\n 'NUS 07-1' 'Boibar Valley, Northern Pakistan' 'Palmyra (composite)'\n 'Palmyra (composite)' 'Palmyra (composite)' 'Palmyra (composite)'\n 'Palmyra (composite)' 'Palmyra (composite)' 'Palmyra (composite)'\n 'Palmyra (composite)' 'Palmyra (composite)' 'Palmyra (composite)'\n 'Palmyra (composite)' 'Palmyra (composite)' 'Dongge Cave, China'\n 'Tarawa Atoll, Republic of Kiribati' 'Fanning Island (composite)'\n 'DHP174 site, Yukon, NW Canada' 'DHP174 site, Yukon, NW Canada'\n 'DHP174 site, Yukon, NW Canada' 'DHP174 site, Yukon, NW Canada'\n 'Reting site, China' 'DML Fimbulisen S100' 'Whitehall Glacier WGH'\n 'Belize' 'Belize' 'EDC Dome C' 'Palmyra' 'Palmyra' 'Northwestern shore'\n 'Puerto Maldonado Peru' 'Northwestern shore'\n 'Kiritimati Island, Line Islands, Kiribati' 'Great Bahama Bank north'\n 'Great Bahama Bank north' 'Lake Lading'\n 'Pedra de Lume, Cape Verde Islands' 'Vernagtferner VF79_core1'\n 'Vernagtferner VF79_core1' 'Wahoo Lake Shelf Core'\n 'Lignumvitae Basin, Florida Bay' 'Dome F 1993' 'Norwegian Sea'\n 'Buca della Renella, Italy Flowstone' 'Liang Luar Cave, Indonesia'\n 'Liang Luar Cave, Indonesia' 'North East Breakers, Bermuda' 'Nauru'\n 'Nauru' 'B19' 'Alinas Reef, Biscayne National Park, Florida' 'B28'\n 'Agassiz A87' 'Crystal Cave, California' 'Estanya' 'Xinpu, China'\n 'Russia, Koksu, Russian Altai' 'Savu Sea' 'Savu Sea' 'Quartz Lake'\n 'South Island Composite Record, New Zealand'\n 'South Island Composite Record, New Zealand' 'Agassiz A84'\n 'Lake Salpeten' 'Houtman Abrolhos Islands' 'Houtman Abrolhos Islands'\n 'Lake Qinghai' 'Cleland Lake' 'Cleland Lake'\n 'Tongue of the Ocean, Bahamas' 'TALDICE -Talos Dome' 'Northwestern shore'\n 'Bunaken Island' 'Lake Malawi' 'Almagre Mountain site, Colorado'\n 'Cattle Pond, Dongdao Island' 'Kasenda' 'Lake Titicaca' 'Dervanoi' 'PC2'\n 'PC2' 'Palaui Island' 'NUS 08-7' 'NVFL-3' 'Vanuatu'\n 'Wanxiang Cave, China' 'Fonoifua Island' 'Perambikulam, Kerala, India'\n 'Panama Basin' 'Panama Basin' 'Illimani' 'Selva Negra, Bolivia' 'PC1'\n 'PC1' 'Nevado HuascarA\u00a1n Core 2' 'Nevado HuascarA\u00a1n Core 2'\n 'Nevado HuascarA\u00a1n Core 2' 'Nevado HuascarA\u00a1n Core 2'\n 'Nevado HuascarA\u00a1n Core 2' 'Grenzgletscher' 'Ammersee' 'Little Cayman'\n 'Amedee Island, New Caledonia' 'Milcent' 'Milcent' 'Dunde Ice Cap'\n 'Sacred Lake' 'Fiescherhorn' 'Agassiz A87' 'Lake Lamongan'\n 'Northwestern shore' 'Northwestern Russian Tundra' 'Mt Logan (NWCol)'\n 'Edward' 'Penny Ice cap' 'Allt Lanlas, Ceredigion' 'Malindi Marine Park'\n 'Miaoergou' 'Lake N3' 'Lake N3' 'Lake N3' 'Lake N3' 'Plateau Remote'\n 'Volcan Granada, Argentina' 'Volcan Granada, Argentina' 'Coral Sea'\n 'Bermuda south shore' 'Savusavu Bay, Fiji' 'Akademii Nauk' 'Lake Nar'\n 'Qamdo, China' 'RICE (Rosevelt Island)' 'Ganesh' 'Berry Pond'\n 'Buckeye Creek Cave, West Virginia' 'Site G' 'Site G'\n 'Botuvera Cave, Brazil' 'Secas Island, Gulf of Chiriqui, Panama'\n 'La Reunion, Southwestern Indian Ocean' 'Crevice' 'Belukha glacier'\n 'Malan Glacier' 'Agassiz A79' 'Lough-na-Shade'\n 'Klapferloch Cave, Austria' 'Mediterranean (Minorca)' 'Ferrigno'\n 'Deplik Tabat Reef, Madang Lagoon' 'Deplik Tabat Reef, Madang Lagoon'\n 'DML Fimbulisen S20' 'VLG' 'IND 25B5 Coastal DML'\n 'DML Plateau (DML18) FB9804' 'NG-stack' 'Lake Nar' 'Lago El Grancho'\n 'Pirotan Island' '400th km' 'Savusavu Bay' 'Dyer Plateau'\n 'Washington Lake' 'Kilimanjaro SIF2' 'Butaritari Atoll, Gilbert Islands'\n 'Butaritari Atoll, Gilbert Islands' 'Butaritari Atoll, Gilbert Islands'\n 'Jiuxian Cave, China' 'Jiuxian Cave, China' 'B16' 'Bruce Plateau'\n 'Lake Tiaglamimine' 'Soreq Cave, Israel' 'Rabaul' 'PV-10'\n 'DML Georg-von-Neumayer Station B04' 'Lombok Strait, Bali, Indonesia'\n 'Taitaitanopo Island' 'Manali' 'Lake Blektjarnen' 'Pyramid'\n 'Laurentian Fan, western subpolar North Atlantic' 'Jones' 'Cape Hatteras'\n 'Wache, Bhutan' 'Chilean margin, Southern Ocean (Pacific sector)'\n 'Jagdalpur, Central India' 'Botuvera Cave, Brazil'\n 'DML Plateau (DML05) FB9807' 'B23' 'Dry Tortugas, 62MC'\n 'Dry Tortugas, 62MC' 'Lake Ghirla' 'Lake Ghirla' 'Saloum shell middens'\n 'B21' 'Arabian Sea' 'Great Bahama Bank south 118MC'\n 'Great Bahama Bank south 118MC' 'Buccoo Reef, Tobago'\n 'Buccoo Reef, Tobago' 'Renner' 'El Junco Lake' 'El Junco Lake'\n 'Lomonosovfonna' 'Laguna Potrok Aike' 'Sofular Cave, Turkey'\n 'Rarotonga, Cook Islands, South Pacific'\n 'Rarotonga, Cook Islands, South Pacific'\n 'Rarotonga, Cook Islands, South Pacific' 'Diva de Maura Cave, Brazil'\n 'Gulf of Guinea, EEA' 'Mu Cang Chai, Northern Vietnam'\n 'Mu Cang Chai, Northern Vietnam' 'Mu Cang Chai, Northern Vietnam'\n 'North East Breakers, Bermuda' 'GRIP Full' 'GISP2 Summer' 'GISP2 Summer'\n 'Ningaloo Reef, West Australia' 'Ningaloo Reef, West Australia'\n 'QuebecLabrador' 'Guam Coral' 'Laguna Castilla' 'Lake Bosumtwi'\n 'Clipperton Atoll' 'Clipperton Atoll' 'Clipperton Atoll' 'DSS Law Dome'\n 'GRIP' 'Prince-of-Wales' 'Moorea, French Polynesia' 'Exuma Sound Bahamas'\n 'Oyogos Yar' 'Nizanda, S Mexico' 'ShiHua Cave, China' 'NGRIP' 'Renland'\n 'Dasuopu' 'Dasuopu' 'El Junco Lake' 'US-ITASE-2002-1'\n 'East Rongbuk Glacier' 'Hainan Island' 'Hainan Island' 'NEEM' 'Steel'\n 'Akademii Nauk' 'Akademii Nauk'\n 'Vostok composite VRS13 ( a stack of 15 individual isotopic records from snow pits and shallow cores recovered in the vicinity of Vostok Station)'\n 'Hala Lake' 'Hala Lake' 'Peros Banhos Atoll' 'Peros Banhos Atoll'\n 'James Ross Island' 'San Valentin' 'B26' 'GISP2-B' 'GISP2-B'\n 'Devon Ice Cap' 'Devon Ice Cap' 'Challa' 'Guadeloupe'\n 'Northeast Breakers, Bermuda' 'Northeast Breakers, Bermuda'\n 'Korallgrottan, Sweden' 'Mafia Archipelago, Tanzania' 'Farewell Lake'\n 'Farewell Lake' 'Malindi' 'Nordic Seas' 'Hainan Island' 'Hainan Island'\n 'PC3' 'PC3' 'GISP2' 'Crystal Cave, Wisconsin' 'Colle Gnifetti KCI'\n 'Colle Gnifetti KCI' 'Colle Gnifetti KCI' 'Colle Gnifetti KCI' 'Lime'\n 'DML Plateau (DML14) FB9815'\n 'Cayo Sal, Los Roques Archipelago, Venezuela'\n 'Buckeye Creek Cave, West Virginia' 'Buckeye Creek Cave, West Virginia'\n 'Buckeye Creek Cave, West Virginia' 'Site E' 'Site E' 'Haklyutvatnet'\n 'Haklyutvatnet' 'DYE3' 'DYE3' 'DYE3' 'DYE3' 'DYE3' 'DYE3' 'DYE3' 'DYE3'\n 'Oregon Caves' 'Ras Umm Sidd, Red Sea' 'B32Site DML05'\n 'Spannagel Cave, Austria' 'MiMei, China' 'Sajama' 'Renland'\n 'Cueva del Diablo, Mexico' 'Cueva del Tigre Perdido, Peru'\n 'Quelccaya Ice Cap' 'National Botanic Gardens of Wales' 'Hainan Island'\n 'Hainan Island' 'Renland' 'Renland' 'Agassiz A79' 'Liberia, Costa Rica'\n 'Miyanohama' 'Hainan Island' 'Hainan Island' 'Cold Water Cave, Iowa'\n 'Cold Water Cave, Iowa']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 298/435\n</pre> In\u00a0[35]: Copied! <pre># archiveType\nkey = 'archiveType'\nprint('%s: '%key)\nprint(np.unique(df[key]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # archiveType key = 'archiveType' print('%s: '%key) print(np.unique(df[key])) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>archiveType: \n['Coral' 'GlacierIce' 'GroundIce' 'LakeSediment' 'MarineSediment'\n 'MolluskShell' 'Sclerosponge' 'Speleothem' 'Wood']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 9/435\n</pre> In\u00a0[36]: Copied! <pre># paleoData_proxy\nkey = 'paleoData_proxy'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # paleoData_proxy key = 'paleoData_proxy' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>paleoData_proxy: \n['d18O' 'dD']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 2/435\n</pre> In\u00a0[37]: Copied! <pre># climate_interpretation\nkey = 'paleoData_sensorSpecies'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # climate_interpretation key = 'paleoData_sensorSpecies' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')  <pre>paleoData_sensorSpecies: \n['Ceratoporella nicholsoni' 'Diploria labyrinthiformis'\n 'Diploria strigosa' 'Hydnophora microconos, Porites lobata'\n 'Montastraea faveolata' 'NA' 'NaN' 'P. australiensis, possibly P. lobata'\n 'Porites' 'Porites lobata' 'Porites lutea' 'Porites sp.'\n 'Siderastrea radians' 'Siderastrea siderea' 'heliopora'\n 'labyrinthiformis' 'lamellina' 'lobata' 'lutea' 'nan']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 20/435\n</pre> In\u00a0[38]: Copied! <pre># # paleoData_notes\nkey = 'paleoData_notes'\nprint('%s: '%key)\nprint(df[key].values)\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # # paleoData_notes key = 'paleoData_notes' print('%s: '%key) print(df[key].values) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>paleoData_notes: \n['; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Marine sediment (MS)'\n '; climateInterpretation_seasonality changed - was originally subannual; archiveType changed - was originally Coral (CO)'\n 'nan' 'nan'\n '; archiveType changed  was originally Ice core (IC); archiveType changed  was originally Ice core (IC)'\n '; archiveType changed - was originally Lake sediment (LS)' 'nan'\n 'summer signal' '; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Speleothem (SP); archiveType changed - was originally Speleothem (SP)'\n 'nan' 'nan' '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Lake sediment (LS)'\n '; archiveType changed - was originally Lake sediment (LS)' 'nan'\n '; archiveType changed - was originally Ice core (IC); archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Coral (CO)'\n '; climateInterpretation_seasonality changed  was originally bimonthly; archiveType changed  was originally Coral (CO); archiveType changed  was originally Coral (CO)'\n '; archiveType changed  was originally Coral (CO); archiveType changed  was originally Coral (CO)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC); archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Lake sediment (LS)'\n '; archiveType changed - was originally Speleothem (SP)'\n '; archiveType changed - was originally Ice core (IC); archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Marine sediment (MS)'\n '; archiveType changed - was originally Marine sediment (MS)'\n '; archiveType changed - was originally Speleothem (SP)'\n '; climateInterpretation_seasonality changed - was originally subannual'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Marine sediment (MS); archiveType changed - was originally Marine sediment (MS)'\n '; archiveType changed  was originally Ice core (IC); archiveType changed  was originally Ice core (IC)'\n '; archiveType changed - was originally Marine sediment (MS); archiveType changed - was originally Marine sediment (MS)'\n '; archiveType changed - was originally Marine sediment (MS); archiveType changed - was originally Marine sediment (MS)'\n '; archiveType changed  was originally Tree ring cellulose (TR)'\n '; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Speleothem (SP)'\n '; climateInterpretation_seasonality changed - was originally annual; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Lake sediment (LS)'\n '; archiveType changed - was originally Speleothem (SP); archiveType changed - was originally Speleothem (SP)'\n '; archiveType changed - was originally Speleothem (SP); archiveType changed - was originally Speleothem (SP)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC); archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC)'\n '; climateInterpretation_seasonality changed - was originally monthly; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Speleothem (SP)'\n '; climateInterpretation_seasonality changed - was originally monthly; archiveType changed - was originally Coral (CO)'\n '; climateInterpretation_seasonality changed  was originally monthly; archiveType changed  was originally Coral (CO)'\n '; archiveType changed - was originally Ice core (IC); archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC)' 'nan'\n '; archiveType changed  was originally Ice core (IC); archiveType changed  was originally Ice core (IC)'\n 'nan'\n '; archiveType changed - was originally Lake sediment (LS); archiveType changed - was originally Lake sediment (LS)'\n '; archiveType changed - was originally Speleothem (SP); archiveType changed - was originally Speleothem (SP)'\n '; archiveType changed - was originally Speleothem (SP)'\n '; archiveType changed - was originally Speleothem (SP)'\n '; archiveType changed - was originally Lake sediment (LS)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC)'\n '; climateInterpretation_seasonality changed - was originally 3 4 5 6 7 8 9 10 11 12 13 14; archiveType changed - was originally Coral (CO)'\n '; climateInterpretation_seasonality changed - was originally 3 4 5 6 7 8 9 10 11 12 13 14; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Ice Wedge (IW); archiveType changed - was originally Ice Wedge (IW)'\n '; archiveType changed - was originally Ice core (IC); archiveType changed - was originally Ice core (IC)'\n '; archiveType changed  was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC); archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC); archiveType changed - was originally Ice core (IC)'\n 'nan' 'nan'\n '; climateInterpretation_seasonality changed - was originally monthly; archiveType changed - was originally Coral (CO)'\n '; climateInterpretation_seasonality changed - was originally monthly; archiveType changed - was originally Coral (CO)'\n '; climateInterpretation_seasonality changed - was originally monthly; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Speleothem (SP)'\n '; archiveType changed - was originally Speleothem (SP)' 'nan'\n '; archiveType changed - was originally Ice core (IC)' 'nan'\n '; archiveType changed - was originally Lake sediment (LS)'\n '; archiveType changed - was originally Lake sediment (LS)'\n '; archiveType changed  was originally Ice core (IC); archiveType changed  was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC); archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC); archiveType changed - was originally Ice core (IC)'\n 'nan' 'nan' '; archiveType changed - was originally Lake sediment (LS)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC)' 'nan'\n '; archiveType changed  was originally Tree ring cellulose (TR)'\n '; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Lake sediment (LS)'\n '; archiveType changed - was originally Lake sediment (LS)'\n '; archiveType changed - was originally Lake sediment (LS)'\n '; archiveType changed  was originally Tree ring cellulose (TR)'\n '; archiveType changed - was originally Lake sediment (LS)'\n '; archiveType changed - was originally Ice core (IC); archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Tree ring cellulose (TR)'\n '; climateInterpretation_seasonality changed - was originally subannual; archiveType changed - was originally Coral (CO); archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Coral (CO); archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Coral (CO); archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Coral (CO); archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Coral (CO); archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Coral (CO); archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Coral (CO); archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Coral (CO); archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Coral (CO); archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Coral (CO); archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Coral (CO); archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Coral (CO); archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Speleothem (SP)'\n '; climateInterpretation_seasonality changed - was originally monthly; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Coral (CO)' 'nan' 'nan' 'nan'\n 'nan' 'nan' '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Ice core (IC)'\n '; climateInterpretation_seasonality changed - was originally monthly; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Coral (CO)' 'nan'\n '; archiveType changed - was originally Tree ring cellulose (TR); archiveType changed - was originally Tree ring cellulose'\n 'nan' '; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Marine sediment (MS)'\n '; archiveType changed - was originally Marine sediment (MS)'\n '; archiveType changed - was originally Lake sediment (LS)'\n '; climateInterpretation_seasonality changed - was originally monthly; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC)' 'nan'\n '; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Marine sediment (MS); archiveType changed - was originally Marine sediment (MS)'\n '; archiveType changed - was originally Speleothem (SP)'\n '; archiveType changed - was originally Speleothem (SP)'\n '; archiveType changed - was originally Speleothem (SP)'\n '; climateInterpretation_seasonality changed - was originally monthly; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Ice core (IC); archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Coral (CO)'\n '; archiveType changed  was originally Ice core (IC); archiveType changed  was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC); archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Speleothem (SP)'\n '; archiveType changed - was originally Lake sediment (LS)' 'nan'\n '; archiveType changed - was originally Tree ring cellulose (TR)'\n '; archiveType changed - was originally Marine sediment (MS); archiveType changed - was originally Marine sediment (MS)'\n '; archiveType changed - was originally Marine sediment (MS); archiveType changed - was originally Marine sediment (MS)'\n '; archiveType changed - was originally Lake sediment (LS)'\n '; archiveType changed - was originally Speleothem (SP)'\n '; archiveType changed - was originally Speleothem (SP)'\n '; archiveType changed  was originally Ice core (IC); archiveType changed  was originally Ice core (IC)'\n '; archiveType changed - was originally Lake sediment (LS); archiveType changed - was originally Lake sediment (LS)'\n '; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Coral (CO)' 'nan' 'P/E lake'\n 'nan' '; archiveType changed - was originally Sclerosponge (SS)'\n '; archiveType changed - was originally Ice core (IC)' 'nan'\n '; climateInterpretation_seasonality changed - was originally monthly; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Lake sediment (LS)' 'nan'\n '; archiveType changed - was originally Lake sediment (LS)'\n '; archiveType changed - was originally Lake sediment (LS)'\n '; archiveType changed - was originally Lake sediment (LS)' 'nan'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC)' 'nan'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC)'\n '; climateInterpretation_seasonality changed - was originally monthly; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Speleothem (SP)' 'nan'\n '; archiveType changed - was originally Tree ring cellulose (TR); archiveType changed - was originally Tree ring cellulose'\n '; archiveType changed - was originally Marine sediment (MS)'\n '; archiveType changed - was originally Marine sediment (MS)'\n '; archiveType changed - was originally Ice core (IC); archiveType changed - was originally Ice core (IC)'\n 'nan' '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC); archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Lake sediment (LS)' 'nan'\n '; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC); archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Lake sediment (LS)'\n '; archiveType changed - was originally Ice core (IC); archiveType changed - was originally Ice core (IC)'\n '; archiveType changed  was originally Ice core (IC); archiveType changed  was originally Ice core (IC)'\n '; archiveType changed - was originally Lake sediment (LS)' 'nan'\n '; archiveType changed  was originally Tree ring cellulose (TR)'\n '; archiveType changed - was originally Ice core (IC); archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Lake sediment (LS)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed  was originally Tree ring cellulose (TR)'\n '; climateInterpretation_seasonality changed - was originally -12 -11 1 2 3 4 5 6 7 8 9 10; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Ice core (IC); archiveType changed - was originally Ice core (IC)'\n 'nan' 'nan' 'nan' 'nan'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Tree ring cellulose (TR); archiveType changed - was originally Tree ring cellulose'\n '; archiveType changed - was originally Tree ring cellulose (TR); archiveType changed - was originally Tree ring cellulose'\n '; climateInterpretation_seasonality changed - was originally annual; archiveType changed - was originally Coral (CO)'\n '; climateInterpretation_seasonality changed - was originally monthly; archiveType changed - was originally Coral (CO)'\n '; climateInterpretation_seasonality changed - was originally annual; archiveType changed - was originally Coral (CO)'\n '; archiveType changed  was originally Ice core (IC)'\n '; archiveType changed - was originally Lake sediment (LS); archiveType changed - was originally Lake sediment (LS)'\n 'nan' '; archiveType changed - was originally Ice core (IC)' 'nan'\n '; archiveType changed - was originally Lake sediment (LS)'\n '; archiveType changed - was originally Speleothem (SP)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Speleothem (SP); archiveType changed - was originally Speleothem (SP)'\n '; climateInterpretation_seasonality changed - was originally subannual; archiveType changed - was originally Coral (CO)'\n '; climateInterpretation_seasonality changed - was originally monthly; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Lake sediment (LS)'\n '; archiveType changed - was originally Ice core (IC); archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC); archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC); archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Lake sediment (LS)'\n '; archiveType changed - was originally Speleothem (SP)'\n '; archiveType changed - was originally Marine sediment (MS)'\n '; archiveType changed - was originally Ice core (IC)'\n '; climateInterpretation_seasonality changed - was originally seasonal; archiveType changed - was originally Coral (CO); archiveType changed - was originally Coral (CO)'\n '; climateInterpretation_seasonality changed - was originally seasonal; archiveType changed - was originally Coral (CO); archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC); archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Lake sediment (LS)'\n '; archiveType changed - was originally Lake sediment (LS)'\n '; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Ice core (IC)'\n '; climateInterpretation_seasonality changed - was originally monthly; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Lake sediment (LS)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Speleothem (SP)'\n '; archiveType changed - was originally Speleothem (SP)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Lake sediment (LS); archiveType changed - was originally Lake sediment (LS)'\n '; archiveType changed - was originally Speleothem (SP)'\n '; climateInterpretation_seasonality changed - was originally monthly; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC)'\n '; climateInterpretation_seasonality changed - was originally monthly; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Coral (CO)' 'nan'\n '; archiveType changed - was originally Lake sediment (LS)'\n '; archiveType changed - was originally Lake sediment (LS)'\n '; archiveType changed - was originally Marine sediment (MS)'\n '; archiveType changed - was originally Lake sediment (LS); archiveType changed - was originally Lake sediment (LS)'\n '; archiveType changed - was originally Marine sediment (MS)' 'nan'\n '; archiveType changed - was originally Marine sediment (MS)'\n '; archiveType changed - was originally Tree ring cellulose (TR); archiveType changed - was originally Tree ring cellulose (TR)'\n '; archiveType changed - was originally Speleothem (SP); archiveType changed - was originally Speleothem (SP)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed  was originally Ice core (IC); archiveType changed  was originally Ice core (IC)'\n '; archiveType changed - was originally Marine sediment (MS)'\n '; archiveType changed - was originally Marine sediment (MS)' 'nan' 'nan'\n 'nan' '; archiveType changed  was originally Ice core (IC)' 'nan'\n '; archiveType changed - was originally Marine sediment (MS)'\n '; archiveType changed - was originally Marine sediment (MS)'\n '; climateInterpretation_seasonality changed - was originally annual; archiveType changed - was originally Coral (CO)'\n '; climateInterpretation_seasonality changed - was originally annual; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Lake sediment (LS)'\n '; archiveType changed  was originally Lake sediment (LS)'\n '; archiveType changed  was originally Lake sediment (LS)'\n '; archiveType changed  was originally Ice core (IC)'\n '; archiveType changed - was originally Lake sediment (LS)'\n '; archiveType changed - was originally Speleothem (SP)'\n '; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Speleothem (SP); archiveType changed - was originally Speleothem (SP)'\n '; archiveType changed - was originally Marine sediment (MS)'\n '; archiveType changed - was originally Tree ring cellulose (TR); archiveType changed - was originally Tree ring cellulose'\n '; archiveType changed - was originally Tree ring cellulose (TR); archiveType changed - was originally Tree ring cellulose'\n '; archiveType changed - was originally Tree ring cellulose (TR); archiveType changed - was originally Tree ring cellulose'\n '; climateInterpretation_seasonality changed - was originally monthly; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Ice core (IC); archiveType changed - was originally Ice core (IC)'\n '; archiveType changed  was originally Ice core (IC)'\n '; archiveType changed  was originally Ice core (IC)'\n '; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Coral (CO)'\n '; archiveType changed  was originally Tree ring cellulose (TR)'\n '; climateInterpretation_seasonality changed - was originally monthly; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Lake sediment (LS)'\n '; archiveType changed - was originally Lake sediment (LS); archiveType changed - was originally Lake sediment (LS)'\n '; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Coral (CO)'\n '; climateInterpretation_seasonality changed - was originally monthly; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC)'\n '; climateInterpretation_seasonality changed - was originally monthly; archiveType changed - was originally Coral (CO)'\n '; climateInterpretation_seasonality changed - was originally annual; archiveType changed - was originally Sclerosponge (SS)'\n '; archiveType changed - was originally Ice Wedge (IW); archiveType changed - was originally Ice Wedge (IW)'\n 'nan'\n '; archiveType changed - was originally Speleothem (SP); archiveType changed - was originally Speleothem (SP)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC); archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Lake sediment (LS)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC); archiveType changed - was originally Ice core (IC)'\n 'nan' 'nan'\n '; archiveType changed - was originally Ice core (IC); archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Lake sediment (LS); archiveType changed - was originally Lake sediment (LS)'\n 'nan' 'nan' '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Lake sediment (LS); archiveType changed - was originally Lake sediment (LS)'\n '; archiveType changed - was originally Lake sediment (LS); archiveType changed - was originally Lake sediment (LS)'\n '; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC); archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC); archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC); archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC); archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Lake sediment (LS); archiveType changed - was originally Lake sediment (LS)'\n '; climateInterpretation_seasonality changed - was originally monthly; archiveType changed - was originally Coral (CO)'\n '; climateInterpretation_seasonality changed - was originally -12 -11 -10 -9 -8 1 2 3 4 5 6 7; archiveType changed - was originally Coral (CO)'\n '; climateInterpretation_seasonality changed - was originally -12 -11 -10 -9 -8 1 2 3 4 5 6 7; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Speleothem (SP)'\n '; climateInterpretation_seasonality changed - was originally monthly, bimonthly; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Lake sediment (LS)'\n '; archiveType changed - was originally Lake sediment (LS)'\n '; climateInterpretation_seasonality changed - was originally monthly; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Marine sediment (MS)' 'nan' 'nan'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed  was originally Ice core (IC)'\n '; archiveType changed - was originally Speleothem (SP)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Lake sediment (LS)'\n '; archiveType changed - was originally Ice core (IC)'\n '; climateInterpretation_seasonality changed - was originally monthly; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Speleothem (SP)'\n '; archiveType changed - was originally Speleothem (SP)'\n '; archiveType changed - was originally Speleothem (SP)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC)' 'nan' 'nan'\n '; archiveType changed  was originally Ice core (IC)'\n '; archiveType changed  was originally Ice core (IC)'\n '; archiveType changed  was originally Ice core (IC)'\n '; archiveType changed  was originally Ice core (IC)'\n '; archiveType changed  was originally Ice core (IC)'\n '; archiveType changed  was originally Ice core (IC)'\n '; archiveType changed  was originally Ice core (IC)'\n '; archiveType changed  was originally Ice core (IC)'\n '; archiveType changed - was originally Speleothem (SP)'\n '; climateInterpretation_seasonality changed - was originally bimonthly; archiveType changed - was originally Coral (CO)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Speleothem (SP)' 'nan'\n '; archiveType changed - was originally Ice core (IC); archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC); archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Speleothem (SP)'\n '; archiveType changed - was originally Speleothem (SP)'\n '; paleoData_variableName changed - was originally d18OiceSMOW; paleoData_units changed - was originally per mil SMOW; climateInterpretation_seasonality changed - was originally austral winter; archiveType changed - was originally ice core'\n '; archiveType changed  was originally Tree ring cellulose (TR)' 'nan'\n 'nan' '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Ice core (IC)'\n '; archiveType changed - was originally Tree ring cellulose (TR)'\n '; climateInterpretation_seasonality changed - was originally bimonthly'\n 'nan' 'nan' '; archiveType changed - was originally Speleothem (SP)'\n '; archiveType changed - was originally Speleothem (SP)']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 39/435\n</pre> In\u00a0[39]: Copied! <pre># climate_interpretation\nkey = 'interpretation_direction'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # climate_interpretation key = 'interpretation_direction' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>interpretation_direction: \n['Increase' 'N/A' 'NaN' 'T_air (positive), P_amount (negative)'\n 'T_air (positive), P_amount (negative), SPEI (negative)' 'decrease'\n 'decrease/increase'\n 'depends (orbital timescale: More Indian Monsoon moisture--&gt;more enriched. Since 3ka: Indian source has been stable, so amount effect dominates: more rainfall, more intense hydrological cycle --&gt;More depleted)'\n 'increase' 'negaitive' 'negative' 'positive'\n 'positive for d18O-temperature relation, negative for d13C-precipiation amount']\nNo. of unique values: 13/435\n</pre> In\u00a0[40]: Copied! <pre># climate_interpretation\nkey = 'interpretation_seasonality'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # climate_interpretation key = 'interpretation_seasonality' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>interpretation_seasonality: \n['Annual' 'Apr-Jul' 'Aug-Jul' 'Dec-Feb' 'Dec-May' 'Growing Season' 'Jan'\n 'Jan-Apr' 'Jan-Jun' 'Jul-Sep' 'Jun-Aug' 'Jun-Sep' 'Mar' 'Mar-Nov'\n 'May-Dec' 'May-Oct' 'May-Sep' 'N/A' 'Nov-Feb' 'Nov-Jan' 'Oct-Apr'\n 'Oct-Dec' 'Oct-Sep' 'Spr-Sum' 'Summer' 'Wet Season' 'Winter' 'deleteMe'\n 'subannual']\nNo. of unique values: 29/435\n</pre> In\u00a0[41]: Copied! <pre># climate_interpretation\nkey = 'interpretation_variable'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # climate_interpretation key = 'interpretation_variable' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>interpretation_variable: \n['N/A' 'moisture' 'temperature']\nNo. of unique values: 3/435\n</pre> In\u00a0[42]: Copied! <pre># climate_interpretation\nkey = 'interpretation_variableDetail'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # climate_interpretation key = 'interpretation_variableDetail' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>interpretation_variableDetail: \n['0.58 +- 0.11ppt/degrees C'\n 'Changes in annual temperature modulated by oceanic circulation and gradients'\n 'Maximum air temperature, seasonal' 'Maximum temperature' 'N/A' 'NaN'\n 'Original interpretation_variable: circulationIndex, interpretation_variableDetail: lake water'\n 'Original interpretation_variable: circulationVariable, interpretation_variableDetail: Indian monsoon'\n 'Original interpretation_variable: circulationVariable, interpretation_variableDetail: More negative d18O values correspond to stronger amount'\n 'Original interpretation_variable: circulationVariable, interpretation_variableDetail: N/A'\n 'Original interpretation_variable: circulationVariable, interpretation_variableDetail: tropical or North Pacific moisture'\n 'Original interpretation_variable: deleteMe, interpretation_variableDetail: N/A'\n 'Original interpretation_variable: deleteMe, interpretation_variableDetail: more positive values of d13C indicate a spread of C4 prairy grasses and decline of C3 forest plants, more positive d18O indicates evaporation of soil water which is stronger in the prairy environment than in the forsest'\n 'Original interpretation_variable: effectivePrecipitation, interpretation_variableDetail: E:P lake water'\n 'Original interpretation_variable: effectivePrecipitation, interpretation_variableDetail: LakeLevel@surface'\n 'Original interpretation_variable: effectivePrecipitation, interpretation_variableDetail: N/A'\n 'Original interpretation_variable: effectivePrecipitation, interpretation_variableDetail: Seasonal'\n 'Original interpretation_variable: effectivePrecipitation, interpretation_variableDetail: air@surface'\n 'Original interpretation_variable: effectivePrecipitation, interpretation_variableDetail: lake level'\n 'Original interpretation_variable: effectivePrecipitation, interpretation_variableDetail: lake water'\n 'Original interpretation_variable: effectivePrecipitation, interpretation_variableDetail: lake, winds in eastern Patagonia'\n 'Original interpretation_variable: effectivePrecipitation, interpretation_variableDetail: soil moisture'\n 'Original interpretation_variable: evaporation, interpretation_variableDetail: Aleutian Low/westerly storm trajectories'\n 'Original interpretation_variable: evaporation, interpretation_variableDetail: Indian Monsoon Strength'\n 'Original interpretation_variable: evaporation, interpretation_variableDetail: N/A'\n 'Original interpretation_variable: hydrologicBalance, interpretation_variableDetail: groundwater'\n 'Original interpretation_variable: hydrologicBalance, interpretation_variableDetail: lake water'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: Amount of rainfall change'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: Australian-Indonesian Summer monsoon; More negative d18O values correspond to stronger amount'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: Australian-Indonesian monsoon rainfall'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: Continental Sweden'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: ENSO/PDO'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: East Asian Monsoon Strength'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: East Asian Monsoon Strength; more negative values of d18O are interpreted as indicative of increased monsoon strength'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: Indian Summer Monsoon; more negative values of d18O are interpreted as indicative of increased monsoon strength'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: Lower precipitation produces higher d13C and Sr/Ca values'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: Monsoon strength'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: More negative d18O values correspond to stronger amount'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: N/A'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: Precipitation'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: SAM'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: Seasonal'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: Seasonal, annual'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: South China Sea'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: Southern Tibet'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: The interpretation is made for an older section of the sample. Last 2k data was not the focus of the manuscript'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: Variations in NAO (related to the amount of rainfall. Season not specified)'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: air@surface'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: amount of rainfall'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: d18O changes of speleothems reflect effects of temperature on raifnall d18O, rainfall amounts affect cave hydrology and biomass density above the cave, which is recorded in d13C of speleothems'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: higher values are related to less rainfall - this can be realted to less moisture influex from the Caribbean due to a southward shift of the ITCZ in phases when high amounts of meltwater enter the cooling north Atlantic Ocean; after ~4.3 ka the connection to the north Atalatic is lost and ENSO becomes more important with warm ENSO events (El Nino) causing higher d18O'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: in the southern tropical Andes'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: more negative values of d18O are interpreted as indicative of increased summer monsoon precipitation'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: more positive d18O values are interpreted to represent wetter conditions'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: precipitation'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: relative portion of summer (SAM) vs winter rainfall'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: variations in paleoprecipitation amount on a multi-annual timescale (On longer timescales, however, the flowstone?s growth dynamics have to be considered)'\n 'Original interpretation_variable: precipitationIsotope, interpretation_variableDetail: Competing influence of polar and maritime airmasses'\n 'Original interpretation_variable: precipitationIsotope, interpretation_variableDetail: East Asian Monsoon rainfall'\n 'Original interpretation_variable: precipitationIsotope, interpretation_variableDetail: minimum temperature'\n 'Original interpretation_variable: precipitationIsotope, interpretation_variableDetail: moisture'\n 'Original interpretation_variable: precipitationIsotope, interpretation_variableDetail: of precipitation'\n 'Original interpretation_variable: precipitationIsotope, interpretation_variableDetail: precipitation'\n 'Original interpretation_variable: precipitationIsotope, interpretation_variableDetail: precipitation amount'\n 'Original interpretation_variable: precipitationIsotope, interpretation_variableDetail: rain'\n 'Original interpretation_variable: precipitationIsotope, interpretation_variableDetail: relative humidity'\n 'Original interpretation_variable: precipitationIsotope, interpretation_variableDetail: summer monsoon'\n 'Original interpretation_variable: salinity, interpretation_variableDetail: N/A'\n 'Original interpretation_variable: salinity, interpretation_variableDetail: sea surface'\n 'Original interpretation_variable: salinity, interpretation_variableDetail: surface'\n 'Original interpretation_variable: seasonality, interpretation_variableDetail: N/A'\n 'Original interpretation_variable: seasonality, interpretation_variableDetail: changes of d18O in speleothems reflect changes of the average d18O of rainfall in the region related to rainfall seasonality'\n 'Original interpretation_variable: seasonality, interpretation_variableDetail: relative amount of winter snowfall'\n 'Original interpretation_variable: streamflow, interpretation_variableDetail: N/A'\n 'Original interpretation_variable: streamflow, interpretation_variableDetail: lake water'\n 'Seasonal, annual' 'air@condensationLevel' 'air@surface' 'lake water'\n 'near sea surface' 'regional and hemispheric temperature' 'sea surface'\n 'sea@surface' 'sub surface (30m)' 'sub surface (~50 m)'\n 'subsurface (60-80m)' 'subsurface, 136 m' 'subsurface, 143 m' 'surface'\n 'variations in air temperature due to large-scale atmospheric patterns'\n 'variations in winter temperature in the Alps']\nNo. of unique values: 91/435\n</pre> In\u00a0[43]: Copied! <pre># # paleoData_values\nkey = 'paleoData_values'\n\nprint('%s: '%key)\nfor ii, vv in enumerate(df[key][:20]):\n    try: \n        print('%-30s: %s -- %s'%(df['dataSetName'].iloc[ii][:30], str(np.nanmin(vv)), str(np.nanmax(vv))))\n        print(type(vv))\n    except: print(df['dataSetName'].iloc[ii], 'NaNs detected.')\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n</pre> # # paleoData_values key = 'paleoData_values'  print('%s: '%key) for ii, vv in enumerate(df[key][:20]):     try:          print('%-30s: %s -- %s'%(df['dataSetName'].iloc[ii][:30], str(np.nanmin(vv)), str(np.nanmax(vv))))         print(type(vv))     except: print(df['dataSetName'].iloc[ii], 'NaNs detected.') print(np.unique([str(type(dd)) for dd in df[key]])) <pre>paleoData_values: \nCO14WUCL                      : -5.8101864 -- -4.7915373\n&lt;class 'numpy.ndarray'&gt;\nIC00OEKC                      : -35.4 -- -26.84\n&lt;class 'numpy.ndarray'&gt;\nMS09TCIS                      : -0.333 -- 1.063\n&lt;class 'numpy.ndarray'&gt;\nCO95TUNG                      : -5.895 -- -4.578\n&lt;class 'numpy.ndarray'&gt;\nCO17MUMA                      : -6.438 -- -4.634\n&lt;class 'numpy.ndarray'&gt;\nCO17MUMA                      : -6.37924 -- -4.69934\n&lt;class 'numpy.ndarray'&gt;\nIC16WE30                      : -38.5 -- -31.5\n&lt;class 'numpy.ndarray'&gt;\nLS10WRNA                      : -4.84 -- -3.46\n&lt;class 'numpy.ndarray'&gt;\nIC10VICR                      : -37.7 -- -30.23\n&lt;class 'numpy.ndarray'&gt;\nIC10VICR                      : -36.56 -- -24.67\n&lt;class 'numpy.ndarray'&gt;\nCO98SWPR                      : -4.25 -- -2.91\n&lt;class 'numpy.ndarray'&gt;\nCO04BAFI                      : -5.45 -- -4.48\n&lt;class 'numpy.ndarray'&gt;\nCO04BAFI                      : -5.39 -- -4.13\n&lt;class 'numpy.ndarray'&gt;\nCO04BAFI                      : -0.26 -- 0.36\n&lt;class 'numpy.ndarray'&gt;\nSP11STBR                      : -7.48 -- -4.89\n&lt;class 'numpy.ndarray'&gt;\nCO17DESC03A                   : -5.34 -- -4.7\n&lt;class 'numpy.ndarray'&gt;\nCO17DESC03A                   : -0.03 -- 0.53\n&lt;class 'numpy.ndarray'&gt;\nIC13STSP                      : -422.0458 -- -370.413\n&lt;class 'numpy.ndarray'&gt;\nLS13WASU                      : -171.0 -- -140.0\n&lt;class 'numpy.ndarray'&gt;\nLS13WASU                      : -180.0 -- -142.0\n&lt;class 'numpy.ndarray'&gt;\n[\"&lt;class 'numpy.ndarray'&gt;\"]\n</pre> In\u00a0[44]: Copied! <pre># paleoData_units\nkey = 'paleoData_units'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # paleoData_units key = 'paleoData_units' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>paleoData_units: \n['nan' 'permil' 'unitless' 'yr AD' 'z score']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 5/435\n</pre> In\u00a0[45]: Copied! <pre># # year\nkey = 'year'\nprint('%s: '%key)\nfor ii, vv in enumerate(df[key][:20]):\n    try: print('%-30s: %s -- %s'%(df['dataSetName'].iloc[ii][:30], str(np.nanmin(vv)), str(np.nanmax(vv))))\n    except: print('NaNs detected.', vv)\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n</pre> # # year key = 'year' print('%s: '%key) for ii, vv in enumerate(df[key][:20]):     try: print('%-30s: %s -- %s'%(df['dataSetName'].iloc[ii][:30], str(np.nanmin(vv)), str(np.nanmax(vv))))     except: print('NaNs detected.', vv) print(np.unique([str(type(dd)) for dd in df[key]])) <pre>year: \nCO14WUCL                      : 1874.08 -- 1956.79\nIC00OEKC                      : 1881.0 -- 1997.0\nMS09TCIS                      : 1.43 -- 1975.13\nCO95TUNG                      : 1922.542 -- 1991.292\nCO17MUMA                      : 1986.67 -- 2010.92\nCO17MUMA                      : 1926.95 -- 2011.94\nIC16WE30                      : 1242.0 -- 1988.0\nLS10WRNA                      : 1456.0 -- 2000.0\nIC10VICR                      : 552.0 -- 1973.0\nIC10VICR                      : 552.0 -- 1973.0\nCO98SWPR                      : 1939.0 -- 1992.0\nCO04BAFI                      : 1940.54 -- 1997.11\nCO04BAFI                      : 1939.92 -- 2001.96\nCO04BAFI                      : 1940.0 -- 2000.0\nSP11STBR                      : 10.0 -- 660.0\nCO17DESC03A                   : 1628.0 -- 1657.0\nCO17DESC03A                   : 1628.0 -- 1657.0\nIC13STSP                      : 1801.0 -- 1998.0\nLS13WASU                      : 293.0 -- 2007.0\nLS13WASU                      : 293.0 -- 2007.0\n[\"&lt;class 'numpy.ndarray'&gt;\"]\n</pre> In\u00a0[46]: Copied! <pre># yearUnits\nkey = 'yearUnits'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # yearUnits key = 'yearUnits' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>yearUnits: \n['CE']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 1/435\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/load_iso2k/#load-iso-2k","title":"Load Iso 2k\u00b6","text":""},{"location":"notebooks/load_iso2k/#set-up-working-environment","title":"Set up working environment\u00b6","text":""},{"location":"notebooks/load_iso2k/#load-source-data","title":"Load source data\u00b6","text":""},{"location":"notebooks/load_iso2k/#create-compact-dataframe","title":"Create compact dataframe\u00b6","text":""},{"location":"notebooks/load_iso2k/#dataframe-construction","title":"Dataframe construction\u00b6","text":""},{"location":"notebooks/load_iso2k/#populate-dataframe","title":"Populate dataframe\u00b6","text":""},{"location":"notebooks/load_iso2k/#standardise-dataframe","title":"Standardise dataframe\u00b6","text":""},{"location":"notebooks/load_iso2k/#save-compact-dataframe","title":"save compact dataframe\u00b6","text":""},{"location":"notebooks/load_iso2k/#save-pickle","title":"save pickle\u00b6","text":""},{"location":"notebooks/load_iso2k/#save-csv","title":"save csv\u00b6","text":""},{"location":"notebooks/load_iso2k/#visualise-dataframe","title":"Visualise dataframe\u00b6","text":""},{"location":"notebooks/load_iso2k/#display-dataframe","title":"Display dataframe\u00b6","text":""},{"location":"notebooks/load_iso2k/#display-identification-metadata-datasetname-datasetid-originaldataurl-originaldatabase","title":"Display identification metadata: dataSetName, datasetId, originalDataURL, originalDatabase\u00b6","text":""},{"location":"notebooks/load_iso2k/#index","title":"index\u00b6","text":""},{"location":"notebooks/load_iso2k/#datasetname-associated-with-each-record-may-not-be-unique","title":"dataSetName (associated with each record, may not be unique)\u00b6","text":""},{"location":"notebooks/load_iso2k/#datasetid-unique-identifier-as-given-by-original-authors-includes-original-database-token","title":"datasetId (unique identifier, as given by original authors, includes original database token)\u00b6","text":""},{"location":"notebooks/load_iso2k/#originaldataurl-urldoi-of-original-published-record-where-available","title":"originalDataURL (URL/DOI of original published record where available)\u00b6","text":""},{"location":"notebooks/load_iso2k/#originaldatabase-original-database-used-as-input-for-dataframe","title":"originalDatabase (original database used as input for dataframe)\u00b6","text":""},{"location":"notebooks/load_iso2k/#geographical-metadata-elevation-latitude-longitude-site-name","title":"geographical metadata: elevation, latitude, longitude, site name\u00b6","text":""},{"location":"notebooks/load_iso2k/#geo_meanelev-mean-elevation-in-m","title":"geo_meanElev (mean elevation in m)\u00b6","text":""},{"location":"notebooks/load_iso2k/#geo_meanlat-mean-latitude-in-degrees-n","title":"geo_meanLat (mean latitude in degrees N)\u00b6","text":""},{"location":"notebooks/load_iso2k/#geo_meanlon-mean-longitude","title":"geo_meanLon (mean longitude)\u00b6","text":""},{"location":"notebooks/load_iso2k/#geo_sitename-name-of-collection-site","title":"geo_siteName (name of collection site)\u00b6","text":""},{"location":"notebooks/load_iso2k/#proxy-metadata-archive-type-proxy-type-interpretation","title":"proxy metadata: archive type, proxy type, interpretation\u00b6","text":""},{"location":"notebooks/load_iso2k/#archivetype-archive-type","title":"archiveType (archive type)\u00b6","text":""},{"location":"notebooks/load_iso2k/#paleodata_proxy-proxy-type","title":"paleoData_proxy (proxy type)\u00b6","text":""},{"location":"notebooks/load_iso2k/#paleodata_sensorspecies-further-information-on-proxy-type-species","title":"paleoData_sensorSpecies (further information on proxy type: species)\u00b6","text":""},{"location":"notebooks/load_iso2k/#paleodata_notes-notes","title":"paleoData_notes (notes)\u00b6","text":""},{"location":"notebooks/load_iso2k/#climate-metadata-interpretation-variable-direction-seasonality","title":"climate metadata: interpretation variable, direction, seasonality\u00b6","text":""},{"location":"notebooks/load_iso2k/#interpretation_direction","title":"interpretation_direction\u00b6","text":""},{"location":"notebooks/load_iso2k/#interpretation_seasonality","title":"interpretation_seasonality\u00b6","text":""},{"location":"notebooks/load_iso2k/#interpretation_variable","title":"interpretation_variable\u00b6","text":""},{"location":"notebooks/load_iso2k/#interpretation_variabledetail","title":"interpretation_variableDetail\u00b6","text":""},{"location":"notebooks/load_iso2k/#data","title":"data\u00b6","text":""},{"location":"notebooks/load_iso2k/#paleodata_values","title":"paleoData_values\u00b6","text":""},{"location":"notebooks/load_iso2k/#paleodata_units","title":"paleoData_units\u00b6","text":""},{"location":"notebooks/load_iso2k/#year","title":"year\u00b6","text":""},{"location":"notebooks/load_iso2k/#yearunits","title":"yearUnits\u00b6","text":""},{"location":"notebooks/load_pages2k/","title":"Load PAGES 2k","text":"<p>load PAGES 2k data from LiPDverse (https://lipdverse.org/Pages2kTemperature/current_version/, downloaded 31/10/2025 by LL)</p> <p>Created 15/06/2023 by Lucie Luecke (LL)</p> <p>Last updated: 06/11/2025 by LL: Introduced filtering for specific <code>paleoData_proxy</code> and <code>paleoData_variableName</code> 29/10/2025 by LL: Tidied up and load most current PAGES 2k version (v2.2) 27/11/2024 by LL: Changed <code>d2H</code> to <code>dD</code> 21/11/2024 by LL: added option to csv saving of compact dataframe, removed redundant output. 10/07/2024 by LL: silenced certain sections, commented on code, added headers and plots</p> <p>Here we extract a dataframe with the following columns:</p> <ul> <li><code>archiveType</code></li> <li><code>dataSetName</code></li> <li><code>datasetId</code></li> <li><code>geo_meanElev</code></li> <li><code>geo_meanLat</code></li> <li><code>geo_meanLon</code></li> <li><code>geo_siteName</code></li> <li><code>interpretation_direction</code> (new in v2.0)</li> <li><code>interpretation_variable</code></li> <li><code>interpretation_variableDetail</code></li> <li><code>interpretation_seasonality</code> (new in v2.0)</li> <li><code>originalDataURL</code></li> <li><code>originalDatabase</code></li> <li><code>paleoData_notes</code></li> <li><code>paleoData_proxy</code></li> <li><code>paleoData_sensorSpecies</code></li> <li><code>paleoData_units</code></li> <li><code>paleoData_values</code></li> <li><code>paleoData_variableName</code></li> <li><code>year</code></li> <li><code>yearUnits</code></li> </ul> <p>We save a standardised compact dataframe for concatenation to DoD2k</p> <p>Make sure the repo_root is added correctly, it should be: <code>your_root_dir/dod2k</code> This should be the working directory throughout this notebook (and all other notebooks).</p> In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n\nimport sys\nimport os\nfrom pathlib import Path\n\n# Add parent directory to path (works from any notebook in notebooks/)\n# the repo_root should be the parent directory of the notebooks folder\ninit_dir = Path().resolve()\n# Determine repo root\nif init_dir.name == 'dod2k': repo_root = init_dir\nelif init_dir.parent.name == 'dod2k': repo_root = init_dir.parent\nelse: raise Exception('Please review the repo root structure (see first cell).')\n\n# Update cwd and path only if needed\nif os.getcwd() != str(repo_root):\n    os.chdir(repo_root)\nif str(repo_root) not in sys.path:\n    sys.path.insert(0, str(repo_root))\n\nprint(f\"Repo root: {repo_root}\")\nif str(os.getcwd())==str(repo_root):\n    print(f\"Working directory matches repo root. \")\n</pre> %load_ext autoreload %autoreload 2  import sys import os from pathlib import Path  # Add parent directory to path (works from any notebook in notebooks/) # the repo_root should be the parent directory of the notebooks folder init_dir = Path().resolve() # Determine repo root if init_dir.name == 'dod2k': repo_root = init_dir elif init_dir.parent.name == 'dod2k': repo_root = init_dir.parent else: raise Exception('Please review the repo root structure (see first cell).')  # Update cwd and path only if needed if os.getcwd() != str(repo_root):     os.chdir(repo_root) if str(repo_root) not in sys.path:     sys.path.insert(0, str(repo_root))  print(f\"Repo root: {repo_root}\") if str(os.getcwd())==str(repo_root):     print(f\"Working directory matches repo root. \") <pre>Repo root: /home/jupyter-lluecke/dod2k\nWorking directory matches repo root. \n</pre> In\u00a0[2]: Copied! <pre># Import packages\n# import lipd\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport lipd\n\nfrom dod2k_utilities import ut_functions as utf # contains utility functions\nfrom dod2k_utilities import ut_plot as uplt # contains plotting functions\n</pre> # Import packages # import lipd import pandas as pd import numpy as np import matplotlib.pyplot as plt import lipd  from dod2k_utilities import ut_functions as utf # contains utility functions from dod2k_utilities import ut_plot as uplt # contains plotting functions <p>In order to get the source data, run the cell below, which downloads the data directly from LiPDverse into the directory data/pages2k:</p> In\u00a0[3]: Copied! <pre># # # Download the file\n\n# !wget -O data/pages2k/Pages2kTemperature2_2_0.pkl https://lipdverse.org/Pages2kTemperature/current_version/Pages2kTemperature2_2_0.pkl\n</pre> # # # Download the file  # !wget -O data/pages2k/Pages2kTemperature2_2_0.pkl https://lipdverse.org/Pages2kTemperature/current_version/Pages2kTemperature2_2_0.pkl  In\u00a0[4]: Copied! <pre>read_PAGES = pd.read_pickle('data/pages2k/Pages2kTemperature2_2_0.pkl')\ndf_orig    = pd.DataFrame(read_PAGES['TS']) #converts metadata into a DataFrame\n# df_metadata   = pd.DataFrame(read_PAGES['D'])\n</pre> read_PAGES = pd.read_pickle('data/pages2k/Pages2kTemperature2_2_0.pkl') df_orig    = pd.DataFrame(read_PAGES['TS']) #converts metadata into a DataFrame # df_metadata   = pd.DataFrame(read_PAGES['D']) <p>Create empty dataframe with standardised columns and populate with data and metadata:</p> <p>mapping:</p> <ul> <li><code>archiveType</code>             (no change)</li> <li><code>interpretation_direction</code> (new in DT2k v1.2) &lt;-- (<code>paleoData_interpretation</code> --&gt; <code>direction</code>)</li> <li><code>interpretation_seasonality</code> (new in DT2k v1.2) &lt;-- (<code>paleoData_interpretation</code> --&gt; <code>seasonality</code>)</li> <li><code>interpretation_variable</code> &lt;-- (<code>paleoData_interpretation</code> --&gt; <code>variable</code>)</li> <li><code>interpretation_variableDetail</code> &lt;-- (<code>paleoData_interpretation</code> --&gt; <code>variableDetail</code>)</li> <li><code>dataSetName</code>             (no change)</li> <li><code>datasetId</code>               (no change)</li> <li><code>geo_meanElev</code>            (no change)</li> <li><code>geo_meanLat</code>             (no change)</li> <li><code>geo_meanLon</code>             (no change)</li> <li><code>geo_siteName</code>            (no change)</li> <li><code>originalDataURL</code>         &lt;-- (<code>paleoData_WDSPaleoUrl</code>)</li> <li><code>originalDatabase</code>        &lt;-- (<code>PAGES 2k v2.2.0</code>)</li> <li><code>paleoData_notes</code>         (no change)</li> <li><code>paleoData_proxy</code>         &lt;-- paleoData_variableName</li> <li><code>paleoData_sensorSpecies</code> (no change)</li> <li><code>paleoData_units</code>         (no change)</li> <li><code>paleoData_values</code>        (no change)</li> <li><code>year</code>                    (no change)</li> <li><code>yearUnits</code>               (no change)</li> </ul> In\u00a0[5]: Copied! <pre>df_compact = df_orig[['archiveType',  'dataSetName', 'datasetId',  'geo_meanElev', 'geo_meanLat',\n                      'geo_meanLon', 'geo_siteName', 'paleoData_notes', \n                      'paleoData_units', 'paleoData_values', \n                      'paleoData_sensorSpecies', 'year', 'yearUnits']]\n</pre> df_compact = df_orig[['archiveType',  'dataSetName', 'datasetId',  'geo_meanElev', 'geo_meanLat',                       'geo_meanLon', 'geo_siteName', 'paleoData_notes',                        'paleoData_units', 'paleoData_values',                        'paleoData_sensorSpecies', 'year', 'yearUnits']] <p>Create the remaining columns and populate with data</p> <ul> <li><code>paleoData_variableName</code> as extra column<code>paleoData_variableName</code></li> <li><code>paleoData_proxy</code> as <code>paleoData_proxy</code> except for missing data, then paleoData_variableName</li> <li><code>originalDatabase</code> is PAGES 2k v2.2.0</li> <li>extract <code>climateInterpretation</code> from <code>interpretation</code></li> </ul> In\u00a0[6]: Copied! <pre>df_compact.insert(8, \"originalDataURL\",  df_orig['paleoData_WDSPaleoUrl'].values)\ndf_compact.insert(8, \"paleoData_proxy\",  df_orig['paleoData_proxy'].values)\ndf_compact.insert(8, \"paleoData_variableName\",  df_orig['paleoData_variableName'].values)\ndf_compact.insert(9, \"originalDatabase\", [\"PAGES 2k v2.2.0\"]*len(df_orig))\ndf_compact.insert(1, \"interpretation_direction\", np.empty(len(df_orig), dtype=object))\ndf_compact.insert(2, \"interpretation_seasonality\", np.empty(len(df_orig), dtype=object))\ndf_compact.insert(3, \"interpretation_variable\", np.empty(len(df_orig), dtype=object))\ndf_compact.insert(4, \"interpretation_variableDetail\", np.empty(len(df_orig), dtype=object))\n\nprint(df_compact.info())\n</pre>  df_compact.insert(8, \"originalDataURL\",  df_orig['paleoData_WDSPaleoUrl'].values) df_compact.insert(8, \"paleoData_proxy\",  df_orig['paleoData_proxy'].values) df_compact.insert(8, \"paleoData_variableName\",  df_orig['paleoData_variableName'].values) df_compact.insert(9, \"originalDatabase\", [\"PAGES 2k v2.2.0\"]*len(df_orig)) df_compact.insert(1, \"interpretation_direction\", np.empty(len(df_orig), dtype=object)) df_compact.insert(2, \"interpretation_seasonality\", np.empty(len(df_orig), dtype=object)) df_compact.insert(3, \"interpretation_variable\", np.empty(len(df_orig), dtype=object)) df_compact.insert(4, \"interpretation_variableDetail\", np.empty(len(df_orig), dtype=object))  print(df_compact.info()) <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3699 entries, 0 to 3698\nData columns (total 21 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   archiveType                    3699 non-null   object \n 1   interpretation_direction       0 non-null      object \n 2   interpretation_seasonality     0 non-null      object \n 3   interpretation_variable        0 non-null      object \n 4   interpretation_variableDetail  0 non-null      object \n 5   dataSetName                    3699 non-null   object \n 6   datasetId                      3699 non-null   object \n 7   geo_meanElev                   3699 non-null   float64\n 8   geo_meanLat                    3699 non-null   float64\n 9   geo_meanLon                    3699 non-null   float64\n 10  geo_siteName                   3699 non-null   object \n 11  paleoData_notes                465 non-null    object \n 12  paleoData_variableName         3699 non-null   object \n 13  originalDatabase               3699 non-null   object \n 14  paleoData_proxy                787 non-null    object \n 15  originalDataURL                3697 non-null   object \n 16  paleoData_units                1614 non-null   object \n 17  paleoData_values               3693 non-null   object \n 18  paleoData_sensorSpecies        2202 non-null   object \n 19  year                           3693 non-null   object \n 20  yearUnits                      3699 non-null   object \ndtypes: float64(3), object(18)\nmemory usage: 607.0+ KB\nNone\n</pre> <ul> <li>Drop rows with missing data:<ul> <li>all NaNs</li> <li>NaNs for <code>year</code> or <code>paleoData_values</code> (no data records)</li> </ul> </li> <li>Populate missing <code>paleoData_proxy</code> entries with <code>paleoData_variableName</code></li> </ul> In\u00a0[7]: Copied! <pre># drop the rows with all NaNs (those not for global temperature analysis)\nprint(len(df_compact))\ndf_compact = df_compact.dropna(how='all')\nprint(len(df_compact))\nfor subset in ['year', 'paleoData_values']:\n    df_compact = df_compact.dropna(subset=subset, how='all')\n    print(len(df_compact))\n    \nfor ii in df_compact.index:\n    if not isinstance(df_compact.at[ii, 'paleoData_proxy'], str):\n        df_compact.at[ii, 'paleoData_proxy'] = df_compact.at[ii, 'paleoData_variableName']\n</pre> # drop the rows with all NaNs (those not for global temperature analysis) print(len(df_compact)) df_compact = df_compact.dropna(how='all') print(len(df_compact)) for subset in ['year', 'paleoData_values']:     df_compact = df_compact.dropna(subset=subset, how='all')     print(len(df_compact))      for ii in df_compact.index:     if not isinstance(df_compact.at[ii, 'paleoData_proxy'], str):         df_compact.at[ii, 'paleoData_proxy'] = df_compact.at[ii, 'paleoData_variableName'] <pre>3699\n3699\n3693\n3693\n</pre> In\u00a0[8]: Copied! <pre># drop all rows which do not have entries for paleoData_proxy\n# df_compact = df_compact.dropna(subset='paleoData_proxy')\n</pre> # drop all rows which do not have entries for paleoData_proxy # df_compact = df_compact.dropna(subset='paleoData_proxy') <p>Populate columns associated with <code>paleoData_interpretation</code> in original dataframe <code>df_orig</code> (extracted from dictionary)</p> In\u00a0[9]: Copied! <pre>keys = ['interpretation_direction', 'interpretation_seasonality',\n       'interpretation_variable', 'interpretation_variableDetail']\n\nfor ii in df_compact.index:\n    if type(df_orig.at[ii, 'paleoData_interpretation'])!=list: continue\n    for key in keys:\n        if key.split('_')[-1] in df_orig.at[ii, 'paleoData_interpretation'][0].keys():\n            df_compact.at[ii, key] = df_orig.at[ii, 'paleoData_interpretation'][0][key.split('_')[-1]]\n            \n</pre> keys = ['interpretation_direction', 'interpretation_seasonality',        'interpretation_variable', 'interpretation_variableDetail']  for ii in df_compact.index:     if type(df_orig.at[ii, 'paleoData_interpretation'])!=list: continue     for key in keys:         if key.split('_')[-1] in df_orig.at[ii, 'paleoData_interpretation'][0].keys():             df_compact.at[ii, key] = df_orig.at[ii, 'paleoData_interpretation'][0][key.split('_')[-1]]              <p>clean <code>df_compact</code> from</p> <ul> <li>non-numeric values for <code>year</code> or <code>paleodata_values</code></li> <li>empty paleoData_values</li> <li>fill empty values with 'N/A'</li> <li>include Common Era only</li> </ul> In\u00a0[10]: Copied! <pre>drop_inds = []\nfor ii in df_compact.index:\n    try:\n        year = np.array(df_compact.at[ii, 'year'], dtype=float)\n        vals = np.array(df_compact.at[ii, 'paleoData_values'], dtype=float)\n        df_compact.at[ii, 'year']             = year[year&gt;=1]\n        df_compact.at[ii, 'paleoData_values'] = vals[year&gt;=1]\n    except:\n        # print\n        df_compact.at[ii, 'paleoData_values'] = np.array([utf.convert_to_float(y) for y in df_compact.at[ii, 'paleoData_values']], dtype=float)\n        df_compact.at[ii, 'year'] = np.array([utf.convert_to_float(y) for y in df_compact.at[ii, 'year']], dtype=float)\n        \n        print(f'Converted values in paleoData_values and/or year for {ii}.')\n        # drop_inds.append(ii)\n# df_compact = df_compact.drop(drop_inds)\n</pre> drop_inds = [] for ii in df_compact.index:     try:         year = np.array(df_compact.at[ii, 'year'], dtype=float)         vals = np.array(df_compact.at[ii, 'paleoData_values'], dtype=float)         df_compact.at[ii, 'year']             = year[year&gt;=1]         df_compact.at[ii, 'paleoData_values'] = vals[year&gt;=1]     except:         # print         df_compact.at[ii, 'paleoData_values'] = np.array([utf.convert_to_float(y) for y in df_compact.at[ii, 'paleoData_values']], dtype=float)         df_compact.at[ii, 'year'] = np.array([utf.convert_to_float(y) for y in df_compact.at[ii, 'year']], dtype=float)                  print(f'Converted values in paleoData_values and/or year for {ii}.')         # drop_inds.append(ii) # df_compact = df_compact.drop(drop_inds) <pre>Converted values in paleoData_values and/or year for 308.\nConverted values in paleoData_values and/or year for 312.\nConverted values in paleoData_values and/or year for 510.\nConverted values in paleoData_values and/or year for 901.\nConverted values in paleoData_values and/or year for 1697.\nConverted values in paleoData_values and/or year for 1699.\nConverted values in paleoData_values and/or year for 1969.\nConverted values in paleoData_values and/or year for 1979.\nConverted values in paleoData_values and/or year for 1984.\nConverted values in paleoData_values and/or year for 2336.\nConverted values in paleoData_values and/or year for 2341.\nConverted values in paleoData_values and/or year for 2591.\nConverted values in paleoData_values and/or year for 2903.\nConverted values in paleoData_values and/or year for 3338.\nConverted values in paleoData_values and/or year for 3339.\nConverted values in paleoData_values and/or year for 3343.\nConverted values in paleoData_values and/or year for 3344.\n</pre> In\u00a0[11]: Copied! <pre># drop all missing values and exclude all-missing-values-rows\n\nfor ii in df_compact.index:\n    dd   = np.array(df_compact.at[ii, 'paleoData_values'])\n    mask = dd==-9999.99\n    df_compact.at[ii, 'paleoData_values']=dd[~mask]\n    df_compact.at[ii, 'year']=np.array(df_compact.at[ii, 'year'])[~mask]\n    \ndrop_inds = []\nfor ii, row in enumerate(df_compact.paleoData_values):\n    if len(row)==0:\n        print(ii, 'empty row for paleodata_values')\n    elif len(df_compact.iloc[ii]['year'])==0:\n        print(ii, 'empty row for year')\n    elif np.std(row)==0: \n        print(ii, 'std=0')\n    elif np.sum(np.diff(row)**2)==0: \n        print(ii, 'diff=0')\n    elif np.isnan(np.std(row)):\n        print(ii, 'std nan')\n    else:\n        continue\n    if df_compact.index[ii] not in drop_inds: \n        drop_inds += [df_compact.index[ii]]\n    \nprint(drop_inds)\ndf_compact = df_compact.drop(index=drop_inds)\n</pre> # drop all missing values and exclude all-missing-values-rows  for ii in df_compact.index:     dd   = np.array(df_compact.at[ii, 'paleoData_values'])     mask = dd==-9999.99     df_compact.at[ii, 'paleoData_values']=dd[~mask]     df_compact.at[ii, 'year']=np.array(df_compact.at[ii, 'year'])[~mask]      drop_inds = [] for ii, row in enumerate(df_compact.paleoData_values):     if len(row)==0:         print(ii, 'empty row for paleodata_values')     elif len(df_compact.iloc[ii]['year'])==0:         print(ii, 'empty row for year')     elif np.std(row)==0:          print(ii, 'std=0')     elif np.sum(np.diff(row)**2)==0:          print(ii, 'diff=0')     elif np.isnan(np.std(row)):         print(ii, 'std nan')     else:         continue     if df_compact.index[ii] not in drop_inds:          drop_inds += [df_compact.index[ii]]      print(drop_inds) df_compact = df_compact.drop(index=drop_inds) <pre>87 std nan\n89 std nan\n195 std nan\n199 std nan\n234 std nan\n246 std nan\n256 std nan\n308 std nan\n312 std nan\n398 std nan\n400 std nan\n441 std nan\n443 std nan\n510 empty row for paleodata_values\n545 std nan\n547 std nan\n573 std nan\n641 std nan\n675 std nan\n677 std nan\n678 std nan\n838 std nan\n883 std nan\n885 std nan\n886 std nan\n887 std nan\n888 std nan\n901 empty row for paleodata_values\n904 std nan\n908 std nan\n963 std nan\n966 std nan\n968 std nan\n969 std nan\n1079 std nan\n1081 std nan\n1366 std nan\n1369 std nan\n1372 std nan\n1472 std nan\n1476 std nan\n1499 std nan\n1500 std nan\n1501 std nan\n1502 std nan\n1503 std nan\n1504 std nan\n1505 std nan\n1506 std nan\n1507 std nan\n1508 std nan\n1509 std nan\n1531 std nan\n1533 std nan\n1631 std nan\n1676 std nan\n1697 empty row for paleodata_values\n1699 empty row for paleodata_values\n1702 std nan\n1761 std nan\n1762 std nan\n1764 std nan\n1765 std nan\n1766 std nan\n1767 std nan\n1768 std nan\n1779 std nan\n1780 std nan\n1830 std nan\n1832 std nan\n1924 std nan\n1927 std nan\n1969 empty row for paleodata_values\n1974 std nan\n1979 empty row for paleodata_values\n1984 empty row for paleodata_values\n2330 empty row for paleodata_values\n2335 empty row for paleodata_values\n2339 std nan\n2340 std nan\n2342 std nan\n2343 std nan\n2345 std nan\n2346 std nan\n2369 std nan\n2370 std nan\n2577 std nan\n2579 std nan\n2583 std nan\n2585 empty row for paleodata_values\n2797 std nan\n2799 std nan\n2897 empty row for paleodata_values\n2906 std nan\n3060 std nan\n3186 std nan\n3189 std nan\n3198 std nan\n3246 std nan\n3248 std nan\n3253 std nan\n3254 std nan\n3332 empty row for paleodata_values\n3333 std nan\n3337 empty row for paleodata_values\n3338 std nan\n3569 std nan\n3631 diff=0\n[87, 89, 195, 199, 234, 246, 256, 308, 312, 398, 400, 441, 443, 510, 545, 547, 573, 641, 675, 677, 678, 838, 883, 885, 886, 887, 888, 901, 904, 908, 963, 966, 968, 969, 1079, 1081, 1366, 1369, 1372, 1472, 1476, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1531, 1533, 1631, 1676, 1697, 1699, 1702, 1761, 1762, 1764, 1765, 1766, 1767, 1768, 1779, 1780, 1830, 1832, 1924, 1927, 1969, 1974, 1979, 1984, 2336, 2341, 2345, 2346, 2348, 2349, 2351, 2352, 2375, 2376, 2583, 2585, 2589, 2591, 2803, 2805, 2903, 2912, 3066, 3192, 3195, 3204, 3252, 3254, 3259, 3260, 3338, 3339, 3343, 3344, 3575, 3637]\n</pre> <p>Note that the datasetId is not unique for each record and thus we add an additional array of strings to make the datasetId unique.</p> In\u00a0[12]: Copied! <pre>#  check that the datasetId is unique \nprint('dataset length: ', len(df_compact))\nprint('unique datasetIds:', len(df_compact.datasetId.unique()))\n# make datasetId unique by simply adding index number\ndf_compact.datasetId=df_compact.apply(lambda x: 'pages2k_'+str(x.name), axis=1)\n# check uniqueness - problem solved.\nassert len(df_compact)== len(df_compact.datasetId.unique())\n</pre> #  check that the datasetId is unique  print('dataset length: ', len(df_compact)) print('unique datasetIds:', len(df_compact.datasetId.unique())) # make datasetId unique by simply adding index number df_compact.datasetId=df_compact.apply(lambda x: 'pages2k_'+str(x.name), axis=1) # check uniqueness - problem solved. assert len(df_compact)== len(df_compact.datasetId.unique()) <pre>dataset length:  3585\nunique datasetIds: 646\n</pre> In\u00a0[13]: Copied! <pre># df_compact = df_compact.fillna(value='N/A')\n</pre> # df_compact = df_compact.fillna(value='N/A') <p>Convert standardised values to comply with dod2k terminology:</p> <ul> <li><code>archiveType</code><ul> <li>['Borehole' 'Coral' 'Documents' 'GlacierIce' 'LakeSediment' 'MarineSediment' 'Other' 'Sclerosponge' 'Speleothem' 'Wood']</li> </ul> </li> <li><code>interpretation_variable</code><ul> <li>allow only <code>moisture</code> and <code>temperature</code> (save original variable in variableDetail)</li> </ul> </li> <li><code>yearUnits</code><ul> <li><code>yr AD</code> to <code>CE</code></li> </ul> </li> </ul> In\u00a0[14]: Copied! <pre># archiveType \n# df_compact['archiveType'] = df_compact['archiveType'].replace({\n#     'wood': 'tree', 'lakesediment': 'lake sediment',\n#     'marinesediment': 'marine sediment',  'glacierice': 'glacier ice'})\ndf_compact['interpretation_variable']=df_compact['interpretation_variable'].fillna('N/A')\ninterpretation_dict = {'precipitation': 'moisture', 'effectivePrecipitation': 'moisture', 'seaIce': 'N/A', 'None': 'N/A'}\n\n\nfor key in interpretation_dict.keys():\n    mask = df_compact['interpretation_variable'] == key\n\n    df_compact.loc[mask, 'interpretation_variableDetail'] = (\n        'Original interpretation_variable: '\n        + key\n        + ', interpretation_variableDetail: '\n        + df_compact.loc[mask, 'interpretation_variableDetail']\n            .fillna('N/A')\n    )\n\ndf_compact['interpretation_variable'] = df_compact['interpretation_variable'].replace(\n    interpretation_dict)\n\nupdate_proxy_terms = {'ring width': 'ring width', 'ringWidth': 'ring width', \n                      'maximum latewood density': 'maximum latewood density', \n                      'calcificationRate': 'calcification rate', 'density': 'maximum latewood density', \n                      'd2H': 'dD', 'MXD': 'maximum latewood density',\n                      'iceMelt': 'ice melt', 'residualChronology': 'residual chronology', \n                      'varveThickness': 'varve thickness',\n                      'effectivePrecipitation': 'effective precipitation',\n                      'humidificationIndex': 'humidification index',\n                     }\n\ndf_compact['paleoData_proxy'] = df_compact['paleoData_proxy'].replace(update_proxy_terms)\n\ndf_compact['paleoData_variableName'] = df_compact['paleoData_variableName'].replace(update_proxy_terms)\n\ndf_compact['yearUnits'] = df_compact['yearUnits'].replace({'yr AD': 'CE'})\n</pre> # archiveType  # df_compact['archiveType'] = df_compact['archiveType'].replace({ #     'wood': 'tree', 'lakesediment': 'lake sediment', #     'marinesediment': 'marine sediment',  'glacierice': 'glacier ice'}) df_compact['interpretation_variable']=df_compact['interpretation_variable'].fillna('N/A') interpretation_dict = {'precipitation': 'moisture', 'effectivePrecipitation': 'moisture', 'seaIce': 'N/A', 'None': 'N/A'}   for key in interpretation_dict.keys():     mask = df_compact['interpretation_variable'] == key      df_compact.loc[mask, 'interpretation_variableDetail'] = (         'Original interpretation_variable: '         + key         + ', interpretation_variableDetail: '         + df_compact.loc[mask, 'interpretation_variableDetail']             .fillna('N/A')     )  df_compact['interpretation_variable'] = df_compact['interpretation_variable'].replace(     interpretation_dict)  update_proxy_terms = {'ring width': 'ring width', 'ringWidth': 'ring width',                        'maximum latewood density': 'maximum latewood density',                        'calcificationRate': 'calcification rate', 'density': 'maximum latewood density',                        'd2H': 'dD', 'MXD': 'maximum latewood density',                       'iceMelt': 'ice melt', 'residualChronology': 'residual chronology',                        'varveThickness': 'varve thickness',                       'effectivePrecipitation': 'effective precipitation',                       'humidificationIndex': 'humidification index',                      }  df_compact['paleoData_proxy'] = df_compact['paleoData_proxy'].replace(update_proxy_terms)  df_compact['paleoData_variableName'] = df_compact['paleoData_variableName'].replace(update_proxy_terms)  df_compact['yearUnits'] = df_compact['yearUnits'].replace({'yr AD': 'CE'}) In\u00a0[15]: Copied! <pre>df_compact.interpretation_variable.unique()\n</pre> df_compact.interpretation_variable.unique() Out[15]: <pre>array(['temperature', 'N/A', 'moisture'], dtype=object)</pre> In\u00a0[16]: Copied! <pre>np.sort(df_compact.archiveType.unique())\n</pre> np.sort(df_compact.archiveType.unique()) Out[16]: <pre>array(['Borehole', 'Coral', 'Documents', 'GlacierIce', 'LakeSediment',\n       'MarineSediment', 'Other', 'Sclerosponge', 'Speleothem', 'Wood'],\n      dtype=object)</pre> In\u00a0[17]: Copied! <pre># KF: Type-checking\ndf_compact = df_compact.astype({'archiveType': str, 'dataSetName': str, 'datasetId': str, 'geo_meanElev': float, 'geo_meanLat': float, 'geo_meanLon': float, 'geo_siteName': str, \n                    'originalDatabase': str, 'originalDataURL': str, 'paleoData_notes': str, 'paleoData_proxy': str, 'paleoData_units': str, 'yearUnits': str,\n               'interpretation_direction': str, 'interpretation_seasonality': str, 'interpretation_variable': str,'interpretation_variableDetail': str })\ndf_compact['year'] = df_compact['year'].map(lambda x: np.array(x, dtype = float))\ndf_compact['paleoData_values'] = df_compact['paleoData_values'].map(lambda x: np.array(x, dtype = float))\n</pre> # KF: Type-checking df_compact = df_compact.astype({'archiveType': str, 'dataSetName': str, 'datasetId': str, 'geo_meanElev': float, 'geo_meanLat': float, 'geo_meanLon': float, 'geo_siteName': str,                      'originalDatabase': str, 'originalDataURL': str, 'paleoData_notes': str, 'paleoData_proxy': str, 'paleoData_units': str, 'yearUnits': str,                'interpretation_direction': str, 'interpretation_seasonality': str, 'interpretation_variable': str,'interpretation_variableDetail': str }) df_compact['year'] = df_compact['year'].map(lambda x: np.array(x, dtype = float)) df_compact['paleoData_values'] = df_compact['paleoData_values'].map(lambda x: np.array(x, dtype = float))  <p>A lot of entries in the dataset are metadata for the actual records, such as e.g. <code>year</code> which should have a place in the <code>year</code> column, not in <code>paleoData_values</code> as a separate series.</p> <p>All keys mentioned in the list below are dropped accordingly.</p> In\u00a0[18]: Copied! <pre>for key in ['needsToBeChanged', 'year', 'uncertainty1s', 'JulianDay', 'age', 'uncertaintyHigh', 'uncertaintyLow',\n            'uncertaintyHigh95', 'uncertaintyLow95', 'sampleID', 'sampleCount', 'segmentLength', 'correlationCoefficient', \n            'notes', 'uncertainty', 'BSi', 'EPS', 'RBAR', 'core', 'depth','depthBottom', 'depthTop', 'duration',\n           'residual chronology']:\n    df_compact = df_compact[df_compact.paleoData_proxy != key]\n    df_compact = df_compact[df_compact.paleoData_variableName != key]\n</pre> for key in ['needsToBeChanged', 'year', 'uncertainty1s', 'JulianDay', 'age', 'uncertaintyHigh', 'uncertaintyLow',             'uncertaintyHigh95', 'uncertaintyLow95', 'sampleID', 'sampleCount', 'segmentLength', 'correlationCoefficient',              'notes', 'uncertainty', 'BSi', 'EPS', 'RBAR', 'core', 'depth','depthBottom', 'depthTop', 'duration',            'residual chronology']:     df_compact = df_compact[df_compact.paleoData_proxy != key]     df_compact = df_compact[df_compact.paleoData_variableName != key] <p>Check that the remaining data in <code>paleoData_proxy</code> is sensible and points to relevant data values:</p> In\u00a0[19]: Copied! <pre># list the values for paleoData_proxy\nnp.sort(df_compact['paleoData_proxy'].unique())\n</pre> # list the values for paleoData_proxy np.sort(df_compact['paleoData_proxy'].unique()) Out[19]: <pre>array(['ARSTAN', 'Mg/Ca', 'Sr/Ca', 'TEX86', 'Uk37', 'accumulation rate',\n       'alkenone', 'borehole', 'calcification rate', 'chironomid',\n       'chloride', 'chrysophyte assemblage', 'concentration', 'count',\n       'd13C', 'd18O', 'dD', 'diatom', 'dinocyst', 'dust',\n       'effective precipitation', 'foraminifera', 'historical',\n       'humidification index', 'ice melt', 'maximum latewood density',\n       'multiproxy', 'nitrate', 'pollen', 'reflectance', 'ring width',\n       'sodium', 'sulfate', 'temperature', 'thickness', 'varve thickness'],\n      dtype=object)</pre> In\u00a0[20]: Copied! <pre># list the values for paleoData_variableName\nnp.sort(df_compact['paleoData_variableName'].unique())\n</pre> # list the values for paleoData_variableName np.sort(df_compact['paleoData_variableName'].unique()) Out[20]: <pre>array(['ARSTAN', 'MAR', 'Mg/Ca', 'R650/R700', 'RABD660670', 'Sr/Ca',\n       'TEX86', 'Uk37', 'calcification rate', 'chloride', 'composite',\n       'concentration', 'count', 'd13C', 'd18O', 'dD', 'dust',\n       'effective precipitation', 'humidification index', 'ice melt',\n       'maximum latewood density', 'nitrate', 'precipitation',\n       'reflectance', 'ring width', 'sodium', 'sulfate', 'temperature',\n       'thickness', 'varve thickness'], dtype=object)</pre> In\u00a0[21]: Copied! <pre>df_compact.info()\n</pre> df_compact.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1191 entries, 0 to 3697\nData columns (total 21 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   archiveType                    1191 non-null   object \n 1   interpretation_direction       1191 non-null   object \n 2   interpretation_seasonality     1191 non-null   object \n 3   interpretation_variable        1191 non-null   object \n 4   interpretation_variableDetail  1191 non-null   object \n 5   dataSetName                    1191 non-null   object \n 6   datasetId                      1191 non-null   object \n 7   geo_meanElev                   1191 non-null   float64\n 8   geo_meanLat                    1191 non-null   float64\n 9   geo_meanLon                    1191 non-null   float64\n 10  geo_siteName                   1191 non-null   object \n 11  paleoData_notes                1191 non-null   object \n 12  paleoData_variableName         1191 non-null   object \n 13  originalDatabase               1191 non-null   object \n 14  paleoData_proxy                1191 non-null   object \n 15  originalDataURL                1191 non-null   object \n 16  paleoData_units                1191 non-null   object \n 17  paleoData_values               1191 non-null   object \n 18  paleoData_sensorSpecies        627 non-null    object \n 19  year                           1191 non-null   object \n 20  yearUnits                      1191 non-null   object \ndtypes: float64(3), object(18)\nmemory usage: 204.7+ KB\n</pre> In\u00a0[22]: Copied! <pre>df_compact = df_compact[sorted(df_compact.columns)]\nprint(df_compact.info())\n</pre> df_compact = df_compact[sorted(df_compact.columns)] print(df_compact.info()) <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1191 entries, 0 to 3697\nData columns (total 21 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   archiveType                    1191 non-null   object \n 1   dataSetName                    1191 non-null   object \n 2   datasetId                      1191 non-null   object \n 3   geo_meanElev                   1191 non-null   float64\n 4   geo_meanLat                    1191 non-null   float64\n 5   geo_meanLon                    1191 non-null   float64\n 6   geo_siteName                   1191 non-null   object \n 7   interpretation_direction       1191 non-null   object \n 8   interpretation_seasonality     1191 non-null   object \n 9   interpretation_variable        1191 non-null   object \n 10  interpretation_variableDetail  1191 non-null   object \n 11  originalDataURL                1191 non-null   object \n 12  originalDatabase               1191 non-null   object \n 13  paleoData_notes                1191 non-null   object \n 14  paleoData_proxy                1191 non-null   object \n 15  paleoData_sensorSpecies        627 non-null    object \n 16  paleoData_units                1191 non-null   object \n 17  paleoData_values               1191 non-null   object \n 18  paleoData_variableName         1191 non-null   object \n 19  year                           1191 non-null   object \n 20  yearUnits                      1191 non-null   object \ndtypes: float64(3), object(18)\nmemory usage: 204.7+ KB\nNone\n</pre> In\u00a0[23]: Copied! <pre># save to a pickle file (security: is it better to save to csv?)\n# sort columns alphabetically (so they all appear in same order across the compact dataframes, for easy comparison)\n\ndf_compact.to_pickle('data/pages2k/pages2k_compact.pkl')\n</pre> # save to a pickle file (security: is it better to save to csv?) # sort columns alphabetically (so they all appear in same order across the compact dataframes, for easy comparison)  df_compact.to_pickle('data/pages2k/pages2k_compact.pkl') In\u00a0[24]: Copied! <pre># save to a list of csv files (metadata, data, year)\ndf_compact.name='pages2k'\nutf.write_compact_dataframe_to_csv(df_compact)\n</pre> # save to a list of csv files (metadata, data, year) df_compact.name='pages2k' utf.write_compact_dataframe_to_csv(df_compact) <pre>METADATA: datasetId, archiveType, dataSetName, geo_meanElev, geo_meanLat, geo_meanLon, geo_siteName, interpretation_direction, interpretation_seasonality, interpretation_variable, interpretation_variableDetail, originalDataURL, originalDatabase, paleoData_notes, paleoData_proxy, paleoData_sensorSpecies, paleoData_units, paleoData_variableName, yearUnits\nSaved to /home/jupyter-lluecke/dod2k/data/pages2k/pages2k_compact_%s.csv\n</pre> In\u00a0[25]: Copied! <pre># load dataframe\ndf = utf.load_compact_dataframe_from_csv('pages2k')\n</pre> # load dataframe df = utf.load_compact_dataframe_from_csv('pages2k') <p>Show spatial distribution of records, show archive and proxy types</p> In\u00a0[26]: Copied! <pre># count archive types\narchive_count = {}\nfor ii, at in enumerate(set(df['archiveType'])):\n    archive_count[at] = df.loc[df['archiveType']==at, 'archiveType'].count()\n\nsort = np.argsort([cc for cc in archive_count.values()])\narchives_sorted = np.array([cc for cc in archive_count.keys()])[sort][::-1]\n\n# Specify colour for each archive (smaller archives get grouped into the same colour)\narchive_colour, major_archives, other_archives = uplt.get_archive_colours(archives_sorted, archive_count)\n\nfig = uplt.plot_geo_archive_proxy(df, archive_colour)\nutf.save_fig(fig, f'geo_{df.name}', dir=df.name)\n</pre> # count archive types archive_count = {} for ii, at in enumerate(set(df['archiveType'])):     archive_count[at] = df.loc[df['archiveType']==at, 'archiveType'].count()  sort = np.argsort([cc for cc in archive_count.values()]) archives_sorted = np.array([cc for cc in archive_count.keys()])[sort][::-1]  # Specify colour for each archive (smaller archives get grouped into the same colour) archive_colour, major_archives, other_archives = uplt.get_archive_colours(archives_sorted, archive_count)  fig = uplt.plot_geo_archive_proxy(df, archive_colour) utf.save_fig(fig, f'geo_{df.name}', dir=df.name) <pre>0 Wood 797\n1 MarineSediment 145\n2 Coral 119\n3 GlacierIce 52\n4 LakeSediment 52\n5 Documents 13\n6 Sclerosponge 4\n7 Speleothem 4\n8 Borehole 3\n9 Other 2\nsaved figure in /home/jupyter-lluecke/dod2k/figs/pages2k/geo_pages2k.pdf\n</pre> <p>Now plot the coverage over the Common Era</p> In\u00a0[27]: Copied! <pre>fig = uplt.plot_coverage(df, archives_sorted, major_archives, other_archives, archive_colour)\nutf.save_fig(fig, f'time_{df.name}', dir=df.name)\n</pre> fig = uplt.plot_coverage(df, archives_sorted, major_archives, other_archives, archive_colour) utf.save_fig(fig, f'time_{df.name}', dir=df.name) <pre>saved figure in /home/jupyter-lluecke/dod2k/figs/pages2k/time_pages2k.pdf\n</pre> In\u00a0[28]: Copied! <pre># # check index\nprint(df.index)\n</pre> # # check index print(df.index) <pre>RangeIndex(start=0, stop=1191, step=1)\n</pre> In\u00a0[29]: Copied! <pre># # check dataSetName\nkey = 'dataSetName'\nprint('%s: '%key)\nprint(df[key].values)\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # # check dataSetName key = 'dataSetName' print('%s: '%key) print(df[key].values) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>dataSetName: \n['Ant-WDC05A.Steig.2013' 'NAm-MtLemon.Briffa.2002'\n 'NAm-MtLemon.Briffa.2002' ... 'Ocn-TongueoftheOcean.Rosenheim.2005'\n 'Ocn-TongueoftheOcean.Rosenheim.2005' 'Asi-CHIN066.Shao.2013']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 623/1191\n</pre> In\u00a0[30]: Copied! <pre># # check datasetId\n\nprint(len(df.datasetId.unique()))\nprint(len(df))\nkey = 'datasetId'\nprint('%s (starts with): '%key)\nprint(df[key].values)\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint('datasetId starts with: ', np.unique([str(dd.split('_')[0]) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # # check datasetId  print(len(df.datasetId.unique())) print(len(df)) key = 'datasetId' print('%s (starts with): '%key) print(df[key].values) print(np.unique([str(type(dd)) for dd in df[key]])) print('datasetId starts with: ', np.unique([str(dd.split('_')[0]) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>1191\n1191\ndatasetId (starts with): \n['pages2k_0' 'pages2k_5' 'pages2k_6' ... 'pages2k_3694' 'pages2k_3696'\n 'pages2k_3697']\n[\"&lt;class 'str'&gt;\"]\ndatasetId starts with:  ['pages2k']\nNo. of unique values: 1191/1191\n</pre> In\u00a0[31]: Copied! <pre># originalDataURL\nkey = 'originalDataURL'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([kk for kk in df[key] if 'this' in kk]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n# 'this study' should point to the correct URL (PAGES2k)\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # originalDataURL key = 'originalDataURL' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([kk for kk in df[key] if 'this' in kk])) print(np.unique([str(type(dd)) for dd in df[key]])) # 'this study' should point to the correct URL (PAGES2k) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>originalDataURL: \n['ftp://ftp.ncdc.noaa.gov/pub/data/paleo/paleolimnology/northamerica/canada/baffin/big-round2008.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Afr-ColdAirCave.Sundqvist.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Afr-LakeMalawi.Powers.2011.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Afr-LakeTanganyika.Tierney.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Afr-P178-15P.Tierney.2015-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Afr-P178-15P.Tierney.2015-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ant-CoastalDML.Thamban.2012.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ant-DSS.DahlJensen.1999.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ant-DSS.Moy.2012.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ant-DomeC.Jouzel.2001.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ant-DomeC.Stenni.2001.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ant-DomeF1993.Uemura.2014.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ant-Ferrigno.Thomas.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ant-JamesRossIsland.Mulvaney.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ant-MES.Rhodes.2012.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ant-PlateauRemote.Mosley-Thompson.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ant-SipleStation.Mosley-Thompson.1990.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ant-SiteDML05.Graf.2002.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ant-TALDICE.Stenni.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ant-TalosDome.Stenni.2002.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ant-VLG.Bertler.2011.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ant-Vostok.Ekaykin.2014-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ant-Vostok.Ekaykin.2014-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ant-WAIS-Divide.Severinghaus.2012.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ant-WDC05A.Steig.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-Agassiz.Vinther.2008.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-AkademiiNaukIceCap.Opel.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-Arjeplog.Bjorklund.2014.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-Armarnaes.Bjorklund.2012.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-Austfonna.Isaksson.2005.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-Avam-Taimyr.Briffa.2008.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-BigRoundLake.Thomas.2009.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-BlueLake.Bird.2008.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-BrayaS.DAndrea.2011.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-CampCentury.Fisher.1969.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-Crete.Vinther.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-DevonIceCap.Fisher.1983.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-Dye.Vinther.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-Forfjorddalen.McCarroll.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-GISP2.Grootes.1997.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-GRIP.Vinther.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-GulfofAlaska.Wilson.2014.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-HalletLake.McKay.2008.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-HudsonLake.Clegg.2011.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-Hvtrvatn.Larsen.2011.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-Iceland.Bergthorsson.1969.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-Indigirka.Hughes.1999.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-Jamtland.Wilson.2016.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-Kittelfjall.Bjorklund.2012.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-Kongressvatnet.DAndrea.2012.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-Laanila.Lindholm.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-Lake4.Rolland.2009.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-LakeC2.Lamoureux.1996.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-LakeDonardBaffinIsland.Moore.2001.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-LakeE.DAndrea.2011.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-LakeHamptrsk.Luoto.2009.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-LakeLehmilampi.Haltia-Hovi.2007.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-LakeNataujrvi.Ojala.2005.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-LakePieni-Kauro.Luoto.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-LenaRiver.McDonald.1998.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-Lomonosovfonna.Divine.2011.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-LowerMurrayLake.Cook.2008.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-MD992275.Jiang.2005.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-MackenzieDelta.Porter.2013-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-MackenzieDelta.Porter.2013-3.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-MooseLake.Clegg.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-NGRIP1.Vinther.2006.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-NGTB16.Schwager.1998.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-NGTB18.Schwager.1998.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-NGTB21.Schwager.1998.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-PennyIceCapP96.Fisher.1998.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-PennyIceCapP96.Okuyama.2003.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-PolarUrals.Wilson.2015.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-PrinceOfWales.Kinnard.2011.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-Renland.Vinther.2008.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-ScreamingLynxLake.Clegg.2011.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-SoperLake.Hughen.2000.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-StoreggaSlide.Sejrup.2011.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-Tjeggelvas.Bjorklund.2012.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-Tornetrask.Melvin.2012.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-VringPlateau.Berner.2011.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-WindyDome.Henderson.2011.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-Yamalia.Briffa.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Arc-Yukon.DArrigo.2006.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-AltaiAktru.Cook.2005.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-AltaiDjasator.Cook.2011.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-AltaiJablonsky.Cook.2000.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-AltaiKorumdu.Cook.2005.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-AltaiKuraisky.Cook.2011.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-AltaiKuraiskySteppe.Cook.2011.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-AltaiSamakhaSteppe.Cook.2011.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-AltaiTjute.Cook.2011.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-AltaiUlaganValley.Cook.2011.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-AltaiUstUlaganLake.Cook.2005.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-BARELC.PAGES2k.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-BHUTSP.PAGES2k.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-BHUTTD.PAGES2k.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-BT001.Cook.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-BT002.Cook.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-BT003.Cook.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-BT004.Cook.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-BT005.Cook.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-BT006.Cook.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-BT008.Cook.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-BT009.Cook.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-BT010.Cook.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-BT011.Cook.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-BT018.Cook.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-BURGPW.PAGES2k.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-CENTIB.PAGES2k.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-CHIN004.Wu.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-CHIN005.Sheppard.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-CHIN006.Sheppard.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-CHIN016.Li.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-CHIN017.Li.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-CHIN018.Li.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-CHIN019.Li.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-CHIN020.Li.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-CHIN021.Li.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-CHIN026.Wright.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-CHIN029.Krusic.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-CHIN037.Fan.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-CHIN038.Fan.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-CHIN039.Fan.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-CHIN040.Fan.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-CHIN041.Fan.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-CHIN042.Brauning.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-CHIN043.Brauning.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-CHIN044.Brauning.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-CHIN045.Brauning.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-CHIN046.Brauning.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-CHIN047.Brauning.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-CHIN048.Brauning.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-CHIN049.Brauning.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-CHIN050.Shao.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-CHIN051.Shao.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-CHIN052.Shao.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-CHIN053.Shao.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-CHIN054.Shao.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-CHIN055.Shao.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-CHIN056.Shao.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-CHIN057.Shao.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-CHIN058.Shao.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-CHIN059.Shao.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-CHIN060.Shao.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-CHIN061.Shao.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-CHIN062.Shao.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-CHIN063.Shao.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-CHIN064.Shao.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-CHIN065.Shao.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-CHIN066.Shao.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-CentralChina.Wang.1998.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-DEZQIN.PAGES2k.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-DQHZHO.PAGES2k.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-DUSHJP.PAGES2k.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-Dasuopu.Thompson.2000.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-ENEPAB.PAGES2k.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-ESPERG.PAGES2k.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-ESPPAK.PAGES2k.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-EastChina.Wang.1990.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-EastChina.Wang.1998.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-Fujian.Wang.1998.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-GANGCD.PAGES2k.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-GHEGAN.PAGES2k.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-GOUQIN.PAGES2k.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-Guangdong.Zheng.1982.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-GuangdongAndGuangxi.Zhang.1980.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-Guliya.Thompson.1997-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-HBHXJP.PAGES2k.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-HBLXJP.PAGES2k.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-HBMXJP.PAGES2k.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-HEBQIN.PAGES2k.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-HRPCSM.PAGES2k.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-HXBURU.PAGES2k.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-HYGJUP.PAGES2k.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-Hunan-Jiangsu.Zhang.1980.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-Hushre.PAGES2k.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-INDI015.Borgaonkar.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-INDI024.Borgaonkar.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-INDI025.Borgaonkar.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-INDO005.DArrigo.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-JAPA012.Yasue.1997.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-JAPA013.Yasue.1997.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-JAPA014.Yasue.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-JAPA015.Yasue.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-JAPA016.Yasue.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-JAPA017.Yasue.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-JAPA019.Sano.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-JAPA020.Kimura.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-KAZ001.Solomina.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-KERALA.PAGES2k.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-KYRG002.Esper.2007.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-KYRG003.Esper.2007.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-KYRG004.Esper.2007.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-KYRG005.Esper.2007.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-KYRG007.Esper.2007.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-KYRG008.Shiyatov.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-KYRG009.Shiyatov.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-KYRG010.Shiyatov.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-KYRG011.Shiyatov.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-KYRG012.Solomina.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-KYRG013.Solomina.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-KYRG014.Solomina.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-KYRG015.Solomina.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-KunashirIsland.Demezhko.2009.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-LAJQIN.PAGES2k.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-LowerYangtzeRiver.Zhang.1980.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-MCCHFH.Sano.2008.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-MONG002.Jacoby.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-MONG004.Jacoby.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-MONG006.Jacoby.2006.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-MONG007.Schweingruber.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-MONG009.Jacoby.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-MONG010.Jacoby.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-MONG011.Jacoby.2006.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-MONG012.Jacoby.2006.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-MONG014.Jacoby.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-MONG015.Jacoby.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-MONG016.Jacoby.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-MONG017.Jacoby.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-MONG018.Jacoby.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-MONG019.Jacoby.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-MONG020.Jacoby.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-MONG021.Jacoby.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-MONG024.Jacoby.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-MONG025.Jacoby.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-MONG026.Jacoby.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-MONG027.Jacoby.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-MONG028.Jacoby.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-MONG029.Jacoby.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-MONG030.Jacoby.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-MONG031.Jacoby.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-MONG032.Jacoby.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-MONG033.Jacoby.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-MQACJP.PAGES2k.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-MQAXJP.PAGES2k.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-MQBXJP.PAGES2k.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-MQDXJP.PAGES2k.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-MQFXJP.PAGES2k.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-MQRXJP.PAGES2k.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-MTASMD.Davi.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-MTASSP.Davi.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-MiddleYangtzeRiver.Zhang.1980.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-NEPA003.Krusic.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-NEPA010.Krusic.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-NEPA014.Krusic.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-NEPA015.Krusic.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-NEPA018.Krusic.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-NEPA019.Krusic.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-NEPA021.Krusic.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-NEPA025.Krusic.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-NEPA027.Krusic.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-NEPA029.Krusic.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-NEPA030.Krusic.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-NEPA032.Krusic.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-NEPA036.Krusic.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-NEPA042.Krusic.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-NOGSAK.PAGES2k.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-PAKI001.Esper.2007.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-PAKI002.Esper.2007.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-PAKI003.Esper.2007.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-PAKI004.Esper.2007.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-PAKI006.Esper.2007.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-PAKI007.Esper.2007.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-PAKI009.Esper.2007.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-PAKI010.Esper.2007.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-PAKI011.Esper.2007.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-PAKI012.Esper.2007.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-PAKI014.Esper.2007.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-PAKI015.Esper.2007.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-PAKI016.Esper.2007.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-PAKI017.Cook.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-PAKI018.Cook.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-PAKI020.Cook.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-PAKI021.Cook.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-PAKI022.Cook.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-PAKI023.Cook.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-PAKI024.Cook.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-PAKI025.Cook.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-PAKI027.Cook.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-PAKI028.Cook.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-PAKI029.Cook.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-PAKI030.Cook.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-PAKI031.Cook.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-PAKI033.Cook.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-PAKI035.Cook.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-PAKI036.Cook.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-PAKI039.Cook.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-PAKI040.Cook.2011.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-PTCYUN.PAGES2k.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-Puruogangri.Thompson.2006.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-QUMAJP.PAGES2k.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-RANGCD.PAGES2k.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-RUSS219.Jacoby.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-RUSS236.Sano.2009.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-SANGTS.PAGES2k.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-SHIESP.PAGES2k.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-SHIYAT.PAGES2k.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-SO90-39KGSO130-275KL.Munz.2015.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-SODAPS.PAGES2k.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-SourthAndMiddleUrals.Demezhko.2007.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-SouthChina.Wang.1998.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-TDAXJP.PAGES2k.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-TDBXJP.PAGES2k.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-TH001.Buckley.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-TIANMU.PAGES2k.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-TW001.Wright.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-UKHEWD.Schweingruber.2002.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-UKHEWW.Schweingruber.2002.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-UKHLWD.Schweingruber.2002.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-UKHLWW.Schweingruber.2002.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-UKHMND.Schweingruber.2002.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-UKHMXD.Schweingruber.2002.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-UKHTRW.Schweingruber.2002.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-UULEWD.Schweingruber.2002.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-UULEWW.Schweingruber.2002.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-UULLWD.Schweingruber.2002.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-UULLWW.Schweingruber.2002.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-UULMND.Schweingruber.2002.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-UULMXD.Schweingruber.2002.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-UULTRW.Schweingruber.2002.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-VIET001.Buckley.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-WEXYUN.Wright.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-WULANJ.PAGES2k.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-WXIYUN.Wright.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-YKC.Yasue.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-YKCOM.Yasue.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-ZHANGX.PAGES2k.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-ZHIDJP.Shao.2003.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Asi-ZhejiangAndFujian.Zhang.1980.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Aus-BuckleysChance.Buckley.1997.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Aus-CTPEastTasmania.Allen.2001.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Aus-CTPWestTasmania.Allen.2001.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Aus-DuckholeLake.Saunders.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Aus-MtRead.Cook.2006.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Aus-Oroko.Cook.2002.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Aus-PinkPine.Duncan.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Aus-StewartIsland.DArrigo.1996.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Aus-TakapariCedar.Xiong.2000.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Eur-CentralEurope.Dobrovoln.2009.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Eur-CentralandEasternPyrenees.Pla.2004.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Eur-CoastofPortugal.Abrantes.2011.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Eur-EasternCarpathianMountains.Popa.2008.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Eur-EuropeanAlps.Bntgen.2011.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Eur-FinnishLakelands.Helama.2014.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Eur-LakeSilvaplana.Larocque-Tobler.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Eur-LakeSilvaplana.Trachsel.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Eur-Ltschental.Bntgen.2006.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Eur-MaritimeFrenchAlps.Bntgen.2012.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Eur-NorthernScandinavia.Esper.2012.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Eur-NorthernSpain.Martn-Chivelet.2011.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Eur-RAPiD-17-5P.Moffa-Sanchez.2014.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Eur-Seebergsee.Larocque-Tobler.2012.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Eur-SpanishPyrenees.Dorado-Linan.2012.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Eur-SpannagelCave.Mangini.2005.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Eur-TatraMountains.Bntgen.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-AlmondButterLower.DArrigo.2005.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-AlmondButterUpper.DArrigo.2005.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-Arapahoe.Kienast.1996-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-Arapahoe.Kienast.1996-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-ArrowsmithMountain.Briffa.1996-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-ArrowsmithMountain.Briffa.1996-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-Athabasca.Schweingruber.1996-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-Athabasca.Schweingruber.1996-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-AthabascaGlacier2.Luckman.2005-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-AthabascaGlacier2.Luckman.2005-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-BakerLake.Hughes.2005-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-BakerLake.Hughes.2005-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-BarlowPassamMtHood.Briffa.1996-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-BarlowPassamMtHood.Briffa.1996-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-BasinPond.Gajewski.1988.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-BellMountain.Schweingruber.1996-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-BellMountain.Schweingruber.1996-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-Bennington.Luckman.2001.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-Bennington.Luckman.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-BigBendLake.Jacoby.2003-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-BigBendLake.Jacoby.2003-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-BigWhite.Parish.2000.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-BigWhite2.Wilson.2005-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-BigWhite2.Wilson.2005-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-BigWhite2.Wilson.2005-3.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-BlanchardRiver.Luckman.2013-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-BlanchardRiver.Luckman.2013-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-Bonif.Schweingruber.1996-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-Bonif.Schweingruber.1996-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-BorealPlateau.Graumlich.2005.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-BowSummit-PeytoLake.Luckman.2005.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-BurntOver.DArrigo.2005.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-CameronPass.Bigler.2007.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-CanyonCreek.Lloyd.2002.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-CardinalDivide.Luckman.2001.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-CarltonRidge.Kipfmuller.2008.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-CeaderBreaks.Briffa.1996-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-CeaderBreaks.Briffa.1996-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-CienegadeNuestraSenoradeGuadalupe.Fule.2009.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-CirquePeak.Graybill.1995.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-ClearPond.Gajewski.1988.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-ConroyLake.Gajewski.1988.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-CoppermineRiver.Jacoby.1989.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-CornwallHills.Wilson.2005-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-CornwallHills.Wilson.2005-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-CottonwoodPass.Briffa.1996-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-CottonwoodPass.Briffa.1996-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-CraterLakeNE.Briffa.2002-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-CraterLakeNE.Briffa.2002-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-DanaPlateauInyoNationalForest.King.2000.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-DarkLake.Gajewski.1988.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-DeerMountain.Wiles.2011.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-DenaliNationalPark.Schweingruber.1996-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-DenaliNationalPark.Schweingruber.1996-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-DonJekRiverBridge.Schweingruber.1996-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-DonJekRiverBridge.Schweingruber.1996-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-EagleFecalPtarmiganMerge.Jacoby.2011-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-EagleFecalPtarmiganMerge.Jacoby.2011-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-EagleFecalPtarmiganMerge.Jacoby.2011-3.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-ElephantMountain.Conkey.1994-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-ElephantMountain.Conkey.1994-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-EurekaSummit.Schweingruber.1996-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-EurekaSummit.Schweingruber.1996-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-FirthRiver.Anchukaitis.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-FishCreekTrail.Biondi.2001.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-FlintCreekRange.Hughes.2005.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-FlowerLake.Graybill.1995.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-FoolCreek.Brown.2005-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-FoolCreek.Brown.2005-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-FortChimo.Fritts.1981.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-FremontGlacier.Hill.0.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-FrenchGlacier.Colenutt.1995.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-GalenaPassSawtooth.Briffa.1996-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-GalenaPassSawtooth.Briffa.1996-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-GeraldineLakes.Luckman.2001.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-GraniteMountain.Graumlich.2003.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-GranitePassHuntMountain.Briffa.1996-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-GranitePassHuntMountain.Briffa.1996-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-GreatBasin.Salzer.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-GreenLake.Menounos.2006.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-HartsPassN2.Peterson.1994.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-HellsKitchenLake.Gajewski.1988.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-HerringAlpine.Fritts.1995.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-HighlandFireOutlook.Briffa.1996-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-HighlandFireOutlook.Briffa.1996-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-HighwoodPass.Luckman.2001.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-Hilda.Kavanagh.2000.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-HohLakeHigh.Peterson.1994.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-HornbyCabin.Jacoby.1989.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-IttyhaukBay.DArrigo.2011.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-KobukNoatak.King.2003.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-LaTasajera.Biondi.2001.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-LacNoir.Paquette.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-LacRomanel.Schweingruber.1996-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-LacRomanel.Schweingruber.1996-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-LakeMina.St.Jacques.2008.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-LakeoftheClouds.Gajewski.1988.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-Landslide.Luckman.2006.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-LittlePineLake.Gajewski.1988.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-Manitoba.DArrigo.2006.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-McGinnisTrail.Wiles.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-MeadowMountain.Wilson.2005-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-MeadowMountain.Wilson.2005-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-MedicineBowPeak.Briffa.1996-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-MedicineBowPeak.Briffa.1996-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-MedusaBay.Buckley.2003.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-MinersWell.Wiles.2000.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-MountAdamsLow.Peterson.1994.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-MountWashington.Graybill.1994.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-MtLemon.Briffa.2002-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-MtLemon.Briffa.2002-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-MtStHelens.Briffa.1996-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-MtStHelens.Briffa.1996-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-NabesnaMine.Jacoby.2003-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-NabesnaMine.Jacoby.2003-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-Nadahini.Luckman.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-Nakiska.Luckman.2001.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-NorthernAlaskaComposite.DArrigo.2006.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-OregonCaves.Ersek.2012.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-ParkMountain.Wilson.2005-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-ParkMountain.Wilson.2005-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-PearlPeak.Graybill.1994.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-PethaiPeninsula.Schweingruber.1996-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-PethaiPeninsula.Schweingruber.1996-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-PeytoLake.Schweingruber.1996-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-PeytoLake.Schweingruber.1996-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-PikePeaks.Harlan.1996-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-PikePeaks.Harlan.1996-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-Pintlers.Littell.2011-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-Pintlers.Littell.2011-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-PintlersTwo.Littell.2011.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-PowderRiverPass.Briffa.1996-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-PowderRiverPass.Briffa.1996-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-PrinceWilliamSound.Barclay.1999.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-PyramidMountain.Luckman.2001.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-RedMountainPassSilverton.Graybill.1994-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-RedMountainPassSilverton.Graybill.1994-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-RockGlacierYukon.Kenigsberg.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-RoweLakes.Colenutt.2011.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-RubyLake.Gajewski.1988.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-STREC.Gennaretti.2014-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-SanFrancisoPeaksUpdate.Graybill.2005.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-ScatterLake.Graumlich.2003.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-SettlementPointAfognakIsland.Harlan.1998.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-SewardComposite.DArrigo.2006-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-SewardComposite.DArrigo.2006-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-SheepMountain.Graybill.1995.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-SheepTrail.Brown.2005-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-SheepTrail.Brown.2005-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-ShermanCreekPass.Briffa.1996-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-ShermanCreekPass.Briffa.1996-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-SiberianOutpostView.Bunn.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-SignalMountain.Luckman.2001.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-SleepingDeerRoad.Hughes.2005-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-SleepingDeerRoad.Hughes.2005-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-SmallRiver.Luckman.2001.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-SmithersSkiArea.Schweingruber.1996-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-SmithersSkiArea.Schweingruber.1996-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-SnakeCreek.Kenigsberg.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-SnowBowlSanFranciscoPeak.Briffa.2002-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-SnowBowlSanFranciscoPeak.Briffa.2002-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-SouthernAlaskacomposite.DArrigo.2006.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-SpillwayLakeYosemiteNationalPark.King.2000.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-SpruceCreek.Church.1981.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-StarrigawanOldSitka.Kaiser.1996.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-Sugarloaf.Kenigsberg.2013-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-Sugarloaf.Kenigsberg.2013-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-SunshineMeadows.Colenutt.1995-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-SunshineMeadows.Colenutt.1995-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-SunwaptaPass.Schweingruber.1996-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-SunwaptaPass.Schweingruber.1996-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-SurpriseValley.Luckman.2001.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-SylvanPassbeiCody.Briffa.1996-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-SylvanPassbeiCody.Briffa.1996-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-TamarackBowl.Sauchyn.2016.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-TimberGapUpper.Graybill.1995.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-TogwateePass.Briffa.1996-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-TogwateePass.Briffa.1996-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-TwistedTreeHeartrotHill.Jacoby.1989-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-TwistedTreeHeartrotHill.Jacoby.1989-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-UpperWrightLakes.Graumlich.2005.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-VancouverCyprusProvincialPark.Briffa.1996-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-VancouverCyprusProvincialPark.Briffa.1996-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-VicaryMine.Sauchyn.2016.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-WindyRidgeAlaska.DArrigo.2005.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-WrangellsComposite.DArrigo.2006.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-YellowMountainRidge.King.2002.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-Ymir.Wilson.2005-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-Ymir.Wilson.2005-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-YosemiteParkEEingang.Briffa.1996-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-YosemiteParkEEingang.Briffa.1996-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-wa069.Peterson.1994.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/NAm-wa071.Peterson.1994.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-AbrahamReef.Druffel.1999.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-AlboranSea384B.Nieto-Moreno.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-AlboranSea436B.Nieto-Moreno.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-AmedeeIsland.DeLong.2012.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-AqabaJordanAQ18.Heiss.1999.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-AqabaJordanAQ19.Heiss.1999.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-ArabianSea.Doose-Rolinski.2001-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-ArabianSea.Doose-Rolinski.2001-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Bermuda.DraschbaA.2000.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Bermuda.DraschbaB.2000.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-BermudaSouthShore.Goodkin.2008-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-BermudaSouthShore.Goodkin.2008-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-BiscayneBay.Swart.1996.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-BuccooReefTobagoMontastrea.Moses.2006.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-BuccooReefTobagoSidereastrea.Moses.2006-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-BuccooReefTobagoSidereastrea.Moses.2006-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-BunakenIsland.Charles.2003.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-CH07-98-MC-22.Saenger.2011-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-CH07-98-MC-22.Saenger.2011-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-CapeGhir.Doose-Rolinski.2007-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-CapeGhir.Doose-Rolinski.2007-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-CapeGhir.McGregor.2007-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-CapeGhir.McGregor.2007-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-CapeHatteras.Cleroux.2008.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-CariacoBasin.Black.2007-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-CariacoBasin.Black.2007-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-CariacoBasin.Lea.2003-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-CariacoBasin.Lea.2003-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-ChileanMargin.Lamy.2002-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-ChileanMargin.Lamy.2002-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-ClerkeReefRowleyShoals.Cooper.2012.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Clipperton1b.Wu.2014.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Clipperton2b.Wu.2014.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Clipperton4b.Wu.2014.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Clipperton6a.Wu.2014.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-ClippertonAtoll.Linsley.2000.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-CoralBayNingaloo.Cooper.2012.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-CoralSea.Calvo.2007.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-DoubleReef.Asami.2005.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-DryTortugas.DeLong.2014.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-DryTortugas.Lund.2006-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-DryTortugas.Lund.2006-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-DryTortugasA.Lund.2006-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-DryTortugasA.Lund.2006-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-EasternTropicalNorthAtlantic.Kuhnert.2011-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-EasternTropicalNorthAtlantic.Kuhnert.2011-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-EmeraldBasin.Keigwin.2003-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-EmeraldBasin.Keigwin.2003-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-EmeraldBasin.Keigwin.2007-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-EmeraldBasin.Keigwin.2007-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-FeniDrift.Richter.2009-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-FeniDrift.Richter.2009-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-FiskBasin.Richey.2009-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-FiskBasin.Richey.2009-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-FloridaBay.Swart.1996.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-GBR.Wei.2009.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-GarrisonBasin.Richey.2009-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-GarrisonBasin.Richey.2009-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-GingerbreadsBahamas.Saenger.2009.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-GreatBahamaBank.Lund.2006-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-GreatBahamaBank.Lund.2006-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-GreatBahamaBank.Richter.2006-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-GreatBahamaBank.Richter.2006-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-GreatBarrierReef.Hendy.2002-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Guadeloupe.Steinhilber.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-GulfofGuinea.Weldeab.2007-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-HaferaIsland.Wu.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-HoutmanAbrolhos.Kuhnert.1999.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-HoutmanAbrolhosIslands.Cooper.2012.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Ifaty1.Zinke.2014.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Ifaty4.Zinke.2004.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-IfatyTul3.Zinke.2014.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-ImperieuseRowleyShoals.Cooper.2012.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-JacafFjord.Seplveda.2009-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-JacafFjord.Seplveda.2009-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Kiritimati.Evans.1998.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-KuroshioCurrent.Isono.2009-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-KuroshioCurrent.Isono.2009-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-LaingIslandPapuaNewGuinea.Tudhope.2001.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-LaurentianFan.Keigwin.2005-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-LaurentianFan.Keigwin.2005-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Lombok.Charles.2003.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-LosRoques.Hetzinger.2008.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-MD95-2011.Grimalt.2002-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-MD95-2011.Grimalt.2002-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-MD98-2160.Newton.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-MD98-2177.Newton.2010.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-MadangLagoonPapuaNewGuinea.Kuhnert.2001.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-MadangLagoonPapuaNewGuinea.Tudhope.1995.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Mafia.Damassa.2006.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Mahe.Charles.1997.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Maiana.Urban.2000.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-MakassarStrait.Linsley.2010-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-MakassarStrait.Linsley.2010-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-MakassarStrait.Oppo.2009-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-MakassarStrait.Oppo.2009-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Malindi.Cole.2000.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Malindi.Nakamura.2009.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-MaloChannelEspirituSantoIsland.Kilbourne.2004.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Mayotte.Zinke.2008-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Mayotte.Zinke.2008-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Mayotte.Zinke.2008-3.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-MinorcaContourite.Moreno.2012-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-MinorcaContourite.Moreno.2012-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Miyanohama.Felis.2009.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Moorea.Boiseau.1998.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Ningaloo.Kuhnert.2000.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-NorthEastBreakersBermuda.Kuhnert.2002-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-NorthEastBreakersBermuda.Kuhnert.2005-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-NorthEastBreakersBermuda.Kuhnert.2005-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-NorthIceland.Sicre.2011-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-NorthwestPacificOcean.Harada.2004-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-NorthwestPacificOcean.Harada.2004-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-ODP984.Came.2007-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-ODP984.Came.2007-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-OkinawaTrough.Wu.2012-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-OkinawaTrough.Wu.2012-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-PalauRockIslands.Osborne.2014.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-PalauUlongChannel.Osborne.2014.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Palmyra.Cobb.2003.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Palmyra.Nurhati.2011-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Palmyra.Nurhati.2011-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-PedradeLume-CapeVerdeIslands.Moses.2006.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Philippines.Stott.2007.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-PigmyBasin.Richey.2015-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-PuntaMaroma.Vsquez-Bedoya.2012.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-RAPiD-12-1K.Thornalley.2009-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Rabaul.Quinn.2006.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-RarotongaSrCa2r.Linsley.2006.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Rarotongad18O2R.Linsley.2006.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Rarotongad18O3R.Linsley.2006.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Rarotongad18O99.Linsley.2006.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-RedSea.Felis.2000.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Reunion.Pfeiffer.2004.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-SWCoastOfIndia.Saraswat.2013-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-SWCoastOfIndia.Saraswat.2013-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-SantaBarbaraBasin.Zhao.2000-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-SavusavuBay1F.Linsley.2006.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-SavusavuBayAB.Linsley.2006.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-SavusavuBayFiji.Bagnato.2005.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-SecasIslandPanama.Linsley.1994.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-SinaiPeninsulaRedSea.Moustafa.2000.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-SouthAtlanticWestAfrica.Leduc.2010-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-SouthAtlanticWestAfrica.Leduc.2010-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-SouthChinaSea.Zhao.2006-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-SouthChinaSea.Zhao.2006-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-SouthIceland.Sicre.2011-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-SouthIceland.Sicre.2011-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-SouthernChileMargin.Mohtadi.2007-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-SouthernChileMargin.Mohtadi.2007-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-SubTropicalEasternNorthAtlantic.deMenocal.2000-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-SubTropicalEasternNorthAtlantic.deMenocal.2000-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-TagusMudPatch.Abrantes.2005-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-TagusMudPatch.Abrantes.2005-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-TantabiddiReefNingaloo.Cooper.2012.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Tarawa.Cole.1990.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-TongueoftheOcean.Rosenheim.2005.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-TongueoftheOceanExuma.Rosenheim.2005.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Vanuatu.Gorman.2012.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-Vanuatu.Quinn.1996.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-WEqPacific.Stott.2007.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-WestSpitzberg.Bonnet.2010-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-WestSpitzberg.Bonnet.2010-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-WestSpitzberg.Bonnet.2010-3.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-WesternAntarcticPeninsula.Shevenell.2011-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-WesternAntarcticPeninsula.Shevenell.2011-2.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-WesternSvalbard.Spielhagen.2011-1.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/Ocn-XishaIslandSouthChinaSea.Sun.2004.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/SAm-CentralAndes11.PAGES2k.2011.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/SAm-CentralAndes15.Neukom.2014.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/SAm-CentralAndes6.Villalba.2014.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/SAm-CentralAndes9.Mundo.2014.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/SAm-LagunaAculeo.vonGunten.2009.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/SAm-LagunaChepical.deJong.2013.txt'\n 'https://www1.ncdc.noaa.gov/pub/data/paleo/pages2k/pages2k-temperature-v2-2017/data-version-2.0.0/SAm-QuelccayaIceCap.Thompson.2013.txt']\n[]\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 724/1191\n</pre> In\u00a0[32]: Copied! <pre># # originalDataSet\nkey = 'originalDatabase'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n# Note: the last two records have missing URLs\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # # originalDataSet key = 'originalDatabase' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) # Note: the last two records have missing URLs print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>originalDatabase: \n['PAGES 2k v2.2.0']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 1/1191\n</pre> In\u00a0[33]: Copied! <pre># check Elevation\nkey = 'geo_meanElev'\nprint('%s: '%key)\nprint(df[key])\nprint(np.unique(['%d'%kk for kk in df[key] if np.isfinite(kk)]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # check Elevation key = 'geo_meanElev' print('%s: '%key) print(df[key]) print(np.unique(['%d'%kk for kk in df[key] if np.isfinite(kk)])) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>geo_meanElev: \n0       1806.0\n1       2700.0\n2       2700.0\n3       2700.0\n4       2700.0\n         ...  \n1186    3200.0\n1187    3100.0\n1188    -143.0\n1189    -143.0\n1190    3500.0\nName: geo_meanElev, Length: 1191, dtype: float32\n['-1' '-10' '-1011' '-1022' '-1048' '-11' '-1108' '-1169' '-1185' '-12'\n '-1245' '-1274' '-1295' '-13' '-136' '-143' '-1491' '-1497' '-1570'\n '-1598' '-16' '-1648' '-18' '-1895' '-1938' '-2' '-20' '-2114' '-2224'\n '-2259' '-2263' '-2303' '-2382' '-2394' '-25' '-250' '-2543' '-2630'\n '-2793' '-3' '-330' '-355' '-3975' '-4' '-450' '-470' '-5' '-503' '-510'\n '-530' '-531' '-547' '-590' '-594' '-6' '-620' '-694' '-695' '-7' '-790'\n '-8' '-80' '-817' '-852' '-869' '-875' '-9' '-900' '-96' '-968' '-97' '0'\n '1' '10' '100' '1000' '1020' '103' '1040' '1054' '106' '110' '1100'\n '1110' '1128' '1167' '1200' '124' '1250' '1275' '130' '1300' '1330'\n '1350' '1354' '1370' '14' '140' '1400' '1450' '1465' '1495' '150' '1500'\n '1530' '1542' '1550' '160' '1600' '1650' '168' '170' '1700' '1750' '176'\n '1766' '1791' '180' '1800' '1806' '1810' '1820' '1830' '1838' '1850'\n '188' '1880' '1900' '1909' '1925' '1950' '1980' '200' '2000' '2030'\n '2050' '2060' '2100' '2120' '213' '2135' '2150' '2185' '2200' '223'\n '2250' '2280' '230' '2300' '2315' '2316' '2347' '2350' '2400' '2480' '25'\n '250' '2500' '2508' '251' '2580' '259' '2600' '2614' '2645' '265' '2670'\n '2680' '2700' '2730' '275' '2750' '2760' '2800' '2810' '2820' '2827'\n '2850' '2880' '2890' '2892' '29' '2900' '2917' '2950' '30' '300' '3000'\n '3010' '3033' '3040' '3050' '3100' '3120' '3150' '3170' '3172' '320'\n '3200' '3233' '3235' '3238' '3240' '3261' '3276' '3291' '3292' '330'\n '3300' '3320' '3330' '334' '335' '3370' '3400' '3415' '3420' '3450'\n '3475' '3488' '3500' '3505' '3519' '3535' '3539' '355' '3600' '3630'\n '3700' '3720' '3730' '3740' '3750' '3780' '3800' '381' '3810' '3900'\n '3920' '3950' '3980' '4000' '4050' '4060' '4100' '415' '4150' '4200'\n '422' '4300' '437' '4400' '450' '4500' '46' '4600' '462' '477' '488'\n '4900' '5' '50' '500' '5000' '509' '513' '520' '550' '5600' '5670' '5900'\n '6070' '6200' '625' '640' '6400' '650' '657' '6700' '69' '700' '7200'\n '750' '757' '790' '80' '800' '815' '820' '880' '884' '900' '905' '915'\n '930' '931' '94' '95' '960']\n[\"&lt;class 'float'&gt;\"]\nNo. of unique values: 311/1191\n</pre> In\u00a0[34]: Copied! <pre># # Latitude\nkey = 'geo_meanLat'\nprint('%s: '%key)\nprint(np.unique(['%d'%kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # # Latitude key = 'geo_meanLat' print('%s: '%key) print(np.unique(['%d'%kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>geo_meanLat: \n['-1' '-10' '-12' '-13' '-15' '-16' '-17' '-18' '-19' '-21' '-22' '-23'\n '-24' '-28' '-29' '-3' '-32' '-33' '-38' '-39' '-4' '-40' '-41' '-42'\n '-43' '-44' '-47' '-5' '-6' '-64' '-66' '-7' '-70' '-72' '-74' '-75'\n '-77' '-78' '-79' '-8' '-84' '1' '10' '11' '12' '13' '16' '19' '2' '20'\n '21' '23' '24' '25' '26' '27' '28' '29' '30' '31' '32' '33' '34' '35'\n '36' '37' '38' '39' '40' '41' '42' '43' '44' '45' '46' '47' '48' '49' '5'\n '50' '51' '52' '53' '54' '55' '56' '57' '58' '59' '6' '60' '61' '62' '63'\n '64' '65' '66' '67' '68' '69' '7' '70' '71' '72' '73' '75' '76' '77' '78'\n '79' '8' '80' '81' '82']\n[\"&lt;class 'float'&gt;\"]\nNo. of unique values: 467/1191\n</pre> In\u00a0[35]: Copied! <pre># # Longitude \nkey = 'geo_meanLon'\nprint('%s: '%key)\nprint(np.unique(['%d'%kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # # Longitude  key = 'geo_meanLon' print('%s: '%key) print(np.unique(['%d'%kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>geo_meanLon: \n['-10' '-103' '-105' '-106' '-107' '-109' '-110' '-111' '-112' '-113'\n '-114' '-115' '-116' '-117' '-118' '-119' '-120' '-121' '-122' '-123'\n '-125' '-127' '-128' '-13' '-131' '-132' '-133' '-134' '-135' '-136'\n '-138' '-139' '-140' '-141' '-142' '-143' '-145' '-146' '-147' '-148'\n '-149' '-150' '-152' '-157' '-159' '-16' '-162' '-17' '-174' '-18' '-19'\n '-22' '-24' '-26' '-27' '-3' '-36' '-37' '-38' '-4' '-41' '-42' '-43'\n '-50' '-54' '-57' '-60' '-61' '-62' '-64' '-65' '-66' '-67' '-68' '-69'\n '-70' '-71' '-72' '-73' '-74' '-75' '-76' '-77' '-78' '-79' '-8' '-80'\n '-82' '-83' '-84' '-86' '-89' '-9' '-91' '-93' '-94' '-95' '0' '1' '10'\n '100' '101' '103' '104' '107' '108' '109' '11' '110' '111' '112' '113'\n '114' '115' '116' '117' '118' '119' '120' '121' '122' '123' '124' '125'\n '13' '130' '133' '134' '138' '141' '142' '143' '144' '145' '146' '147'\n '148' '15' '151' '152' '153' '159' '16' '160' '162' '166' '167' '17'\n '170' '171' '172' '173' '175' '179' '18' '19' '20' '24' '25' '27' '28'\n '29' '30' '34' '39' '4' '40' '43' '44' '45' '5' '55' '59' '6' '65' '68'\n '7' '70' '71' '72' '74' '75' '76' '77' '78' '79' '81' '82' '83' '85' '86'\n '87' '88' '89' '9' '90' '91' '92' '93' '94' '95' '96' '97' '98' '99']\n[\"&lt;class 'float'&gt;\"]\nNo. of unique values: 503/1191\n</pre> In\u00a0[36]: Copied! <pre># Site Name \nkey = 'geo_siteName'\nprint('%s: '%key)\nprint(df[key].values)\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # Site Name  key = 'geo_siteName' print('%s: '%key) print(df[key].values) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>geo_siteName: \n['WDC05A' 'Mt. Lemon' 'Mt. Lemon' ...\n 'Tongue of the Ocean, Exuma Sound Bahamas'\n 'Tongue of the Ocean, Exuma Sound Bahamas' 'CHIN066']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 575/1191\n</pre> In\u00a0[37]: Copied! <pre># archiveType\nkey = 'archiveType'\nprint('%s: '%key)\nprint(np.unique(df[key]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # archiveType key = 'archiveType' print('%s: '%key) print(np.unique(df[key])) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>archiveType: \n['Borehole' 'Coral' 'Documents' 'GlacierIce' 'LakeSediment'\n 'MarineSediment' 'Other' 'Sclerosponge' 'Speleothem' 'Wood']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 10/1191\n</pre> In\u00a0[38]: Copied! <pre># paleoData_proxy\nkey = 'paleoData_proxy'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # paleoData_proxy key = 'paleoData_proxy' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>paleoData_proxy: \n['ARSTAN' 'Mg/Ca' 'Sr/Ca' 'TEX86' 'Uk37' 'accumulation rate' 'alkenone'\n 'borehole' 'calcification rate' 'chironomid' 'chloride'\n 'chrysophyte assemblage' 'concentration' 'count' 'd13C' 'd18O' 'dD'\n 'diatom' 'dinocyst' 'dust' 'effective precipitation' 'foraminifera'\n 'historical' 'humidification index' 'ice melt' 'maximum latewood density'\n 'multiproxy' 'nitrate' 'pollen' 'reflectance' 'ring width' 'sodium'\n 'sulfate' 'temperature' 'thickness' 'varve thickness']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 36/1191\n</pre> In\u00a0[39]: Copied! <pre># climate_interpretation\nkey = 'paleoData_sensorSpecies'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # climate_interpretation key = 'paleoData_sensorSpecies' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')  <pre>paleoData_sensorSpecies: \n['ABAM' 'ABLA' 'Ceratoporella nicholsoni' 'Diploria labyrinthiformis'\n 'Diploria strigosa' 'Hydnophora microconos, Porites lobata' 'LALA' 'LALY'\n 'LAOC' 'Montastraea faveolata' 'NA' 'NaN'\n 'P. australiensis, possibly P. lobata' 'PCEN' 'PCGL' 'PCMA' 'PCRU' 'PCSI'\n 'PCSP' 'PIAL' 'PIAR' 'PIBA' 'PICO' 'PIFL' 'PIJE' 'PILO' 'PIPO' 'PSME'\n 'Porites' 'Porites austraiensis' 'Porites lobata' 'Porites lutea'\n 'Porites sp.' 'Siderastrea radians' 'Siderastrea siderea' 'TSHE' 'TSME'\n 'bournoni' 'faveolata' 'heliopora' 'labyrinthiformis' 'lamellina'\n 'lobata' 'lutea' 'nan' 'siderea']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 46/1191\n</pre> In\u00a0[40]: Copied! <pre>key = 'paleoData_variableName'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> key = 'paleoData_variableName' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')  <pre>paleoData_variableName: \n['ARSTAN' 'MAR' 'Mg/Ca' 'R650/R700' 'RABD660670' 'Sr/Ca' 'TEX86' 'Uk37'\n 'calcification rate' 'chloride' 'composite' 'concentration' 'count'\n 'd13C' 'd18O' 'dD' 'dust' 'effective precipitation'\n 'humidification index' 'ice melt' 'maximum latewood density' 'nitrate'\n 'precipitation' 'reflectance' 'ring width' 'sodium' 'sulfate'\n 'temperature' 'thickness' 'varve thickness']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 30/1191\n</pre> In\u00a0[41]: Copied! <pre># # paleoData_notes\nkey = 'paleoData_notes'\nprint('%s: '%key)\nprint(df[key].values)\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # # paleoData_notes key = 'paleoData_notes' print('%s: '%key) print(df[key].values) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>paleoData_notes: \n['; climateInterpretation_seasonality changed - was originally Mean annual values; archiveType changed - was originally ice core'\n 'nan' 'nan' ...\n '; climateInterpretation_seasonality changed - was originally annual'\n '; paleoData_variableName changed - was originally Sr_Ca; climateInterpretation_seasonality changed - was originally annual; paleoData_variableName changed - was originally Sr_Ca; paleoData_proxyObservationType changed - was originally Sr_Ca'\n 'nan']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 151/1191\n</pre> In\u00a0[42]: Copied! <pre># climate_interpretation\nkey = 'interpretation_direction'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df_compact[key]]))\nprint(f'No. of unique values: {len(np.unique(df_compact[key]))}/{len(df_compact)}')\n</pre> # climate_interpretation key = 'interpretation_direction' print('%s: '%key) print(np.unique([kk for kk in df_compact[key]])) print(f'No. of unique values: {len(np.unique(df_compact[key]))}/{len(df_compact)}') <pre>interpretation_direction: \n['None' 'negative' 'positive']\nNo. of unique values: 3/1191\n</pre> In\u00a0[43]: Copied! <pre># climate_interpretation\nkey = 'interpretation_seasonality'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df_compact[key]]))\nprint(f'No. of unique values: {len(np.unique(df_compact[key]))}/{len(df_compact)}')\n</pre> # climate_interpretation key = 'interpretation_seasonality' print('%s: '%key) print(np.unique([kk for kk in df_compact[key]])) print(f'No. of unique values: {len(np.unique(df_compact[key]))}/{len(df_compact)}') <pre>interpretation_seasonality: \n['Annual' 'Apr' 'Apr-Jul' 'Apr-Jun' 'Apr-Sep' 'Aug' 'Aug-Jul' 'Dec-Feb'\n 'Dec-Mar' 'Feb' 'Feb-Aug' 'Jan-Mar' 'Jul' 'Jul-Dec' 'Jul-Sep' 'Jun'\n 'Jun-Aug' 'Jun-Jul' 'Jun-Sep' 'Mar-Aug' 'Mar-May' 'Mar-Oct' 'May'\n 'May-Apr' 'May-Jul' 'None' 'Nov-Apr' 'Nov-Feb' 'Nov-Oct' 'Oct-Apr'\n 'Sep-Apr' 'Sep-Aug' 'Sep-Nov' 'Sep-Oct' 'Summer' 'Winter' 'subannual']\nNo. of unique values: 37/1191\n</pre> In\u00a0[44]: Copied! <pre># climate_interpretation\nkey = 'interpretation_variable'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df_compact[key]]))\nprint(f'No. of unique values: {len(np.unique(df_compact[key]))}/{len(df_compact)}')\n</pre> # climate_interpretation key = 'interpretation_variable' print('%s: '%key) print(np.unique([kk for kk in df_compact[key]])) print(f'No. of unique values: {len(np.unique(df_compact[key]))}/{len(df_compact)}') <pre>interpretation_variable: \n['N/A' 'moisture' 'temperature']\nNo. of unique values: 3/1191\n</pre> In\u00a0[45]: Copied! <pre># climate_interpretation\nkey = 'interpretation_variableDetail'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df_compact[key]]))\nprint(f'No. of unique values: {len(np.unique(df_compact[key]))}/{len(df_compact)}')\n</pre> # climate_interpretation key = 'interpretation_variableDetail' print('%s: '%key) print(np.unique([kk for kk in df_compact[key]])) print(f'No. of unique values: {len(np.unique(df_compact[key]))}/{len(df_compact)}') <pre>interpretation_variableDetail: \n['None'\n 'Original interpretation_variable: effectivePrecipitation, interpretation_variableDetail: eff'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: N/A'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: surface'\n 'Original interpretation_variable: seaIce, interpretation_variableDetail: N/A'\n 'air' 'air-surface' 'air@600m' 'air@condensationLevel' 'air@surface'\n 'ground@surface' 'ice@surface' 'lake surface' 'lake@surface'\n 'sea@surface' 'sea_surface' 'surface']\nNo. of unique values: 17/1191\n</pre> In\u00a0[46]: Copied! <pre># # paleoData_values\nkey = 'paleoData_values'\n\nprint('%s: '%key)\nfor ii, vv in enumerate(df[key][:20]):\n    try: \n        print('%-30s: %s -- %s'%(df['dataSetName'].iloc[ii][:30], str(np.nanmin(vv)), str(np.nanmax(vv))))\n        print(type(vv))\n    except: print(df['dataSetName'].iloc[ii], 'NaNs detected.')\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n</pre> # # paleoData_values key = 'paleoData_values'  print('%s: '%key) for ii, vv in enumerate(df[key][:20]):     try:          print('%-30s: %s -- %s'%(df['dataSetName'].iloc[ii][:30], str(np.nanmin(vv)), str(np.nanmax(vv))))         print(type(vv))     except: print(df['dataSetName'].iloc[ii], 'NaNs detected.') print(np.unique([str(type(dd)) for dd in df[key]])) <pre>paleoData_values: \nAnt-WDC05A.Steig.2013         : -37.1463 -- -30.6851\n&lt;class 'numpy.ndarray'&gt;\nNAm-MtLemon.Briffa.2002       : 0.154 -- 2.91\n&lt;class 'numpy.ndarray'&gt;\nNAm-MtLemon.Briffa.2002       : 0.205 -- 1.813\n&lt;class 'numpy.ndarray'&gt;\nNAm-MtLemon.Briffa.2002       : 0.283 -- 1.666\n&lt;class 'numpy.ndarray'&gt;\nNAm-MtLemon.Briffa.2002       : 0.574 -- 0.951\n&lt;class 'numpy.ndarray'&gt;\nNAm-MtLemon.Briffa.2002       : 0.707 -- 1.118\n&lt;class 'numpy.ndarray'&gt;\nNAm-MtLemon.Briffa.2002       : 0.757 -- 1.114\n&lt;class 'numpy.ndarray'&gt;\nArc-Arjeplog.Bjorklund.2014   : -3.532171 -- 2.5670047\n&lt;class 'numpy.ndarray'&gt;\nArc-Arjeplog.Bjorklund.2014   : -4.1141653 -- 2.6139\n&lt;class 'numpy.ndarray'&gt;\nAsi-CHIN019.Li.2010           : 0.298 -- 1.664\n&lt;class 'numpy.ndarray'&gt;\nNAm-Landslide.Luckman.2006    : 0.057 -- 0.76\n&lt;class 'numpy.ndarray'&gt;\nNAm-Landslide.Luckman.2006    : 0.164 -- 1.781\n&lt;class 'numpy.ndarray'&gt;\nNAm-Landslide.Luckman.2006    : 0.116 -- 1.889\n&lt;class 'numpy.ndarray'&gt;\nNAm-SmithersSkiArea.Schweingru: 0.319 -- 1.73\n&lt;class 'numpy.ndarray'&gt;\nNAm-SmithersSkiArea.Schweingru: 0.448 -- 1.741\n&lt;class 'numpy.ndarray'&gt;\nNAm-SmithersSkiArea.Schweingru: 0.472 -- 1.576\n&lt;class 'numpy.ndarray'&gt;\nNAm-SmithersSkiArea.Schweingru: 0.44 -- 0.83\n&lt;class 'numpy.ndarray'&gt;\nNAm-SmithersSkiArea.Schweingru: 0.626 -- 1.19\n&lt;class 'numpy.ndarray'&gt;\nNAm-SmithersSkiArea.Schweingru: 0.636 -- 1.179\n&lt;class 'numpy.ndarray'&gt;\nAsi-GANGCD.PAGES2k.2013       : 0.102 -- 2.109\n&lt;class 'numpy.ndarray'&gt;\n[\"&lt;class 'numpy.ndarray'&gt;\"]\n</pre> In\u00a0[47]: Copied! <pre># paleoData_units\nkey = 'paleoData_units'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # paleoData_units key = 'paleoData_units' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>paleoData_units: \n['cm' 'cm/yr' 'count' 'count/mL' 'degC' 'g/cm/yr' 'g/cm2/yr' 'g/cm3' 'mm'\n 'mm/yr' 'mmol/mol' 'nan' 'needsToBeChanged' 'ng/g' 'percent' 'permil'\n 'ppb' 'unitless']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 18/1191\n</pre> In\u00a0[48]: Copied! <pre># # year\nkey = 'year'\nprint('%s: '%key)\nfor ii, vv in enumerate(df[key][:20]):\n    try: print('%-30s: %s -- %s'%(df['dataSetName'].iloc[ii][:30], str(np.nanmin(vv)), str(np.nanmax(vv))))\n    except: print('NaNs detected.', vv)\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n</pre> # # year key = 'year' print('%s: '%key) for ii, vv in enumerate(df[key][:20]):     try: print('%-30s: %s -- %s'%(df['dataSetName'].iloc[ii][:30], str(np.nanmin(vv)), str(np.nanmax(vv))))     except: print('NaNs detected.', vv) print(np.unique([str(type(dd)) for dd in df[key]])) <pre>year: \nAnt-WDC05A.Steig.2013         : 786.0 -- 2005.0\nNAm-MtLemon.Briffa.2002       : 1568.0 -- 1983.0\nNAm-MtLemon.Briffa.2002       : 1568.0 -- 1983.0\nNAm-MtLemon.Briffa.2002       : 1568.0 -- 1983.0\nNAm-MtLemon.Briffa.2002       : 1568.0 -- 1983.0\nNAm-MtLemon.Briffa.2002       : 1568.0 -- 1983.0\nNAm-MtLemon.Briffa.2002       : 1568.0 -- 1983.0\nArc-Arjeplog.Bjorklund.2014   : 1200.0 -- 2010.0\nArc-Arjeplog.Bjorklund.2014   : 1200.0 -- 2010.0\nAsi-CHIN019.Li.2010           : 1509.0 -- 2006.0\nNAm-Landslide.Luckman.2006    : 913.0 -- 2001.0\nNAm-Landslide.Luckman.2006    : 913.0 -- 2001.0\nNAm-Landslide.Luckman.2006    : 913.0 -- 2001.0\nNAm-SmithersSkiArea.Schweingru: 1680.0 -- 1983.0\nNAm-SmithersSkiArea.Schweingru: 1680.0 -- 1983.0\nNAm-SmithersSkiArea.Schweingru: 1680.0 -- 1983.0\nNAm-SmithersSkiArea.Schweingru: 1680.0 -- 1983.0\nNAm-SmithersSkiArea.Schweingru: 1680.0 -- 1983.0\nNAm-SmithersSkiArea.Schweingru: 1680.0 -- 1983.0\nAsi-GANGCD.PAGES2k.2013       : 1567.0 -- 1999.0\n[\"&lt;class 'numpy.ndarray'&gt;\"]\n</pre> In\u00a0[49]: Copied! <pre># yearUnits\nkey = 'yearUnits'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # yearUnits key = 'yearUnits' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>yearUnits: \n['CE']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 1/1191\n</pre> In\u00a0[50]: Copied! <pre># df[df['paleoData_proxy']=='ARSTAN']\n</pre> # df[df['paleoData_proxy']=='ARSTAN'] In\u00a0[51]: Copied! <pre># df[(df['paleoData_proxy']=='ARSTAN')|(df['paleoData_proxy']=='residual chronology')][['paleoData_proxy', 'datasetId', 'originalDataURL']].to_csv('ARSTAN_residualchronology.csv')\n</pre> # df[(df['paleoData_proxy']=='ARSTAN')|(df['paleoData_proxy']=='residual chronology')][['paleoData_proxy', 'datasetId', 'originalDataURL']].to_csv('ARSTAN_residualchronology.csv') In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/load_pages2k/#load-pages-2k","title":"Load PAGES 2k\u00b6","text":""},{"location":"notebooks/load_pages2k/#set-up-working-environment","title":"Set up working environment\u00b6","text":""},{"location":"notebooks/load_pages2k/#load-source-data","title":"Load source data\u00b6","text":""},{"location":"notebooks/load_pages2k/#create-and-populate-compact-dataframe","title":"create and populate compact dataframe\u00b6","text":""},{"location":"notebooks/load_pages2k/#apply-standard-terminology","title":"Apply standard terminology\u00b6","text":""},{"location":"notebooks/load_pages2k/#ensure-correct-datatype","title":"ensure correct datatype\u00b6","text":""},{"location":"notebooks/load_pages2k/#select-for-paleodata_proxy-andor-paleodata_variablename","title":"select for <code>paleoData_proxy</code> and/or <code>paleoData_variableName</code>\u00b6","text":""},{"location":"notebooks/load_pages2k/#save-compact-dataframe","title":"save compact dataframe\u00b6","text":""},{"location":"notebooks/load_pages2k/#save-pickle","title":"save pickle\u00b6","text":""},{"location":"notebooks/load_pages2k/#save-csv","title":"save csv\u00b6","text":""},{"location":"notebooks/load_pages2k/#visualise-dataframe","title":"Visualise dataframe\u00b6","text":""},{"location":"notebooks/load_pages2k/#display-dataframe","title":"Display dataframe\u00b6","text":""},{"location":"notebooks/load_pages2k/#display-identification-metadata-datasetname-datasetid-originaldataurl-originaldatabase","title":"Display identification metadata: dataSetName, datasetId, originalDataURL, originalDatabase\u00b6","text":""},{"location":"notebooks/load_pages2k/#index","title":"index\u00b6","text":""},{"location":"notebooks/load_pages2k/#datasetname-associated-with-each-record-may-not-be-unique","title":"dataSetName (associated with each record, may not be unique)\u00b6","text":""},{"location":"notebooks/load_pages2k/#datasetid-unique-identifier-as-given-by-original-authors-includes-original-database-token","title":"datasetId (unique identifier, as given by original authors, includes original database token)\u00b6","text":""},{"location":"notebooks/load_pages2k/#originaldataurl-urldoi-of-original-published-record-where-available","title":"originalDataURL (URL/DOI of original published record where available)\u00b6","text":""},{"location":"notebooks/load_pages2k/#originaldatabase-original-database-used-as-input-for-dataframe","title":"originalDatabase (original database used as input for dataframe)\u00b6","text":""},{"location":"notebooks/load_pages2k/#geographical-metadata-elevation-latitude-longitude-site-name","title":"geographical metadata: elevation, latitude, longitude, site name\u00b6","text":""},{"location":"notebooks/load_pages2k/#geo_meanelev-mean-elevation-in-m","title":"geo_meanElev (mean elevation in m)\u00b6","text":""},{"location":"notebooks/load_pages2k/#geo_meanlat-mean-latitude-in-degrees-n","title":"geo_meanLat (mean latitude in degrees N)\u00b6","text":""},{"location":"notebooks/load_pages2k/#geo_meanlon-mean-longitude","title":"geo_meanLon (mean longitude)\u00b6","text":""},{"location":"notebooks/load_pages2k/#geo_sitename-name-of-collection-site","title":"geo_siteName (name of collection site)\u00b6","text":""},{"location":"notebooks/load_pages2k/#proxy-metadata-archive-type-proxy-type-interpretation","title":"proxy metadata: archive type, proxy type, interpretation\u00b6","text":""},{"location":"notebooks/load_pages2k/#archivetype-archive-type","title":"archiveType (archive type)\u00b6","text":""},{"location":"notebooks/load_pages2k/#paleodata_proxy-proxy-type","title":"paleoData_proxy (proxy type)\u00b6","text":""},{"location":"notebooks/load_pages2k/#paleodata_sensorspecies-further-information-on-proxy-type-species","title":"paleoData_sensorSpecies (further information on proxy type: species)\u00b6","text":""},{"location":"notebooks/load_pages2k/#paleodata_variablename","title":"paleoData_variableName\u00b6","text":""},{"location":"notebooks/load_pages2k/#paleodata_notes-notes","title":"paleoData_notes (notes)\u00b6","text":""},{"location":"notebooks/load_pages2k/#climate-metadata-interpretation-variable-direction-seasonality","title":"climate metadata: interpretation variable, direction, seasonality\u00b6","text":""},{"location":"notebooks/load_pages2k/#interpretation_direction","title":"interpretation_direction\u00b6","text":""},{"location":"notebooks/load_pages2k/#interpretation_seasonality","title":"interpretation_seasonality\u00b6","text":""},{"location":"notebooks/load_pages2k/#interpretation_variable","title":"interpretation_variable\u00b6","text":""},{"location":"notebooks/load_pages2k/#interpretation_variabledetail","title":"interpretation_variableDetail\u00b6","text":""},{"location":"notebooks/load_pages2k/#data","title":"data\u00b6","text":""},{"location":"notebooks/load_pages2k/#paleodata_values","title":"paleoData_values\u00b6","text":""},{"location":"notebooks/load_pages2k/#paleodata_units","title":"paleoData_units\u00b6","text":""},{"location":"notebooks/load_pages2k/#year","title":"year\u00b6","text":""},{"location":"notebooks/load_pages2k/#yearunits","title":"yearUnits\u00b6","text":""},{"location":"notebooks/load_sisal/","title":"Load SISAL","text":"<p>load SISAL data (https://ora.ox.ac.uk/objects/uuid:1e91e2ac-ca9f-46e5-85f3-8d82d4d3cfd4 | MNE 2024/06/03)</p> <p>loads the SISALv3 database (2024) downloaded on June 3rd 2024 from https://ora.ox.ac.uk/objects/uuid:1e91e2ac-ca9f-46e5-85f3-8d82d4d3cfd4 | MNE 2024/06/03</p> <p>Created by Kevin Fan and Lucie Luecke. Based on the code from sisal3_extractCSVdata.py (by Jens Fohlmeister)</p> <p>Updates: 06/11/2025 by LL: Overhauled, commented and tidied code with markdown. 29/11/2024 by KF: Changes have been made to filtering entityIds by date and how the dataframe is constructed. Mostly cleaning and checking work 21/11/2024 by LL : added option to save as csv 30/10/2024 by LL : added check for empty paleoData_values row</p> <p>Here we extract a dataframe with the following columns:</p> <ul> <li><code>archiveType</code></li> <li><code>dataSetName</code></li> <li><code>datasetId</code></li> <li><code>geo_meanElev</code></li> <li><code>geo_meanLat</code></li> <li><code>geo_meanLon</code></li> <li><code>geo_siteName</code></li> <li><code>interpretation_direction</code> (new in v2.0)</li> <li><code>interpretation_variable</code></li> <li><code>interpretation_variableDetail</code></li> <li><code>interpretation_seasonality</code> (new in v2.0)</li> <li><code>originalDataURL</code></li> <li><code>originalDatabase</code></li> <li><code>paleoData_notes</code></li> <li><code>paleoData_proxy</code></li> <li><code>paleoData_sensorSpecies</code></li> <li><code>paleoData_units</code></li> <li><code>paleoData_values</code></li> <li><code>paleoData_variableName</code></li> <li><code>year</code></li> <li><code>yearUnits</code> `</li> </ul> <p>We save a standardised compact dataframe for concatenation to DoD2k</p> <p>This python script reads SISALv3 csv-data in a directory './SISALv3_csv' relative to the path of this file (unpack all your downloaded csv-files there!) and extracts stable isotope, Mg/Ca and growth rate data for all entities, which cover a to be specified period of interest (change lines 70 and 71 according to your needs).</p> <p>Only records with more than 'number_of_dating_points' U-Th dated depths (line 78) will be accounted for. attention: it may happen that there are enough dated depth available in the requested period, but proxies might not be provided this will result in an empty output, but output nevertheless</p> <p>The individual data will be plotted and the plots can be saved (comment/uncomment line 225).</p> <p>The mean and standard deviation of all proxies within your specified period will be determined and saved in a csv file.</p> <p>There will also be a raw plot for illustrative purposes available.</p> <p>Feel free to change the code as you see fit.</p> <p>@original author: Jens Fohlmeister</p> <p>Make sure the repo_root is added correctly, it should be: <code>your_root_dir/dod2k</code> This should be the working directory throughout this notebook (and all other notebooks).</p> In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n\nimport sys\nimport os\nfrom pathlib import Path\n\n# Add parent directory to path (works from any notebook in notebooks/)\n# the repo_root should be the parent directory of the notebooks folder\ninit_dir = Path().resolve()\n# Determine repo root\nif init_dir.name == 'dod2k': repo_root = init_dir\nelif init_dir.parent.name == 'dod2k': repo_root = init_dir.parent\nelse: raise Exception('Please review the repo root structure (see first cell).')\n\n# Update cwd and path only if needed\nif os.getcwd() != str(repo_root):\n    os.chdir(repo_root)\nif str(repo_root) not in sys.path:\n    sys.path.insert(0, str(repo_root))\n\nprint(f\"Repo root: {repo_root}\")\nif str(os.getcwd())==str(repo_root):\n    print(f\"Working directory matches repo root. \")\n</pre> %load_ext autoreload %autoreload 2  import sys import os from pathlib import Path  # Add parent directory to path (works from any notebook in notebooks/) # the repo_root should be the parent directory of the notebooks folder init_dir = Path().resolve() # Determine repo root if init_dir.name == 'dod2k': repo_root = init_dir elif init_dir.parent.name == 'dod2k': repo_root = init_dir.parent else: raise Exception('Please review the repo root structure (see first cell).')  # Update cwd and path only if needed if os.getcwd() != str(repo_root):     os.chdir(repo_root) if str(repo_root) not in sys.path:     sys.path.insert(0, str(repo_root))  print(f\"Repo root: {repo_root}\") if str(os.getcwd())==str(repo_root):     print(f\"Working directory matches repo root. \") <pre>Repo root: /home/jupyter-lluecke/dod2k\nWorking directory matches repo root. \n</pre> In\u00a0[2]: Copied! <pre># Import packages\nimport numpy as np\nimport pandas as pd\nimport os\n\nfrom dod2k_utilities import ut_functions as utf # contains utility functions\nfrom dod2k_utilities import ut_plot as uplt # contains plotting functions\n</pre> # Import packages import numpy as np import pandas as pd import os  from dod2k_utilities import ut_functions as utf # contains utility functions from dod2k_utilities import ut_plot as uplt # contains plotting functions <p>In order to get the source data, run the cell below, which downloads the data directly</p> In\u00a0[3]: Copied! <pre># Download the file (use -O to specify output filename)\n!wget -O data/sisal/sisalv3_database_mysql_csv.zip https://ora.ox.ac.uk/objects/uuid:1e91e2ac-ca9f-46e5-85f3-8d82d4d3cfd4\n\n# Unzip to the correct destination\n# !unzip -d data/sisal/ data/sisal/sisalv3_database_mysql_csv.zip\n!unzip data/sisal/sisalv3_database_mysql_csv.zip -d data/sisal/sisalv3_csv\n</pre> # Download the file (use -O to specify output filename) !wget -O data/sisal/sisalv3_database_mysql_csv.zip https://ora.ox.ac.uk/objects/uuid:1e91e2ac-ca9f-46e5-85f3-8d82d4d3cfd4  # Unzip to the correct destination # !unzip -d data/sisal/ data/sisal/sisalv3_database_mysql_csv.zip !unzip data/sisal/sisalv3_database_mysql_csv.zip -d data/sisal/sisalv3_csv  <pre>--2025-12-17 09:49:37--  https://ora.ox.ac.uk/objects/uuid:1e91e2ac-ca9f-46e5-85f3-8d82d4d3cfd4\nResolving ora.ox.ac.uk (ora.ox.ac.uk)... 172.66.159.143, 104.20.30.6, 2606:4700:10::6814:1e06, ...\nConnecting to ora.ox.ac.uk (ora.ox.ac.uk)|172.66.159.143|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [text/html]\nSaving to: \u2018data/sisal/sisalv3_database_mysql_csv.zip\u2019\n\ndata/sisal/sisalv3_     [ &lt;=&gt;                ]  71.69K  --.-KB/s    in 0.1s    \n\n2025-12-17 09:49:38 (484 KB/s) - \u2018data/sisal/sisalv3_database_mysql_csv.zip\u2019 saved [73408]\n\nArchive:  data/sisal/sisalv3_database_mysql_csv.zip\n  End-of-central-directory signature not found.  Either this file is not\n  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n  latter case the central directory and zipfile comment will be found on\n  the last disk(s) of this archive.\nunzip:  cannot find zipfile directory in one of data/sisal/sisalv3_database_mysql_csv.zip or\n        data/sisal/sisalv3_database_mysql_csv.zip.zip, and cannot find data/sisal/sisalv3_database_mysql_csv.zip.ZIP, period.\n</pre> In\u00a0[4]: Copied! <pre># read the sisalv3 csv files\n\nentity = pd.read_csv('data/sisal/sisalv3_csv/entity.csv')\nd13C   = pd.read_csv('data/sisal/sisalv3_csv/d13C.csv')\nd18O   = pd.read_csv('data/sisal/sisalv3_csv/d18O.csv')\nMgCa   = pd.read_csv('data/sisal/sisalv3_csv/Mg_Ca.csv')\ndating = pd.read_csv('data/sisal/sisalv3_csv/dating.csv')\ndating.rename(columns = {'238U_content':'c238U_content','238U_uncertainty':'c238U_uncertainty',\n    '232Th_content':'c232Th_content','c232Th_uncertainty':'c232Th_uncertainty',\n    '230Th_content':'c230Th_content','c230Th_uncertainty':'c230Th_uncertainty',\n    '230Th_232Th_ratio':'a230Th_232Th_ratio','230Th_232Th_ratio_uncertainty':'a230Th_232Th_ratio_uncertainty',\n    '230Th_238U_activity':'a230Th_238U_activity','230Th_238U_activity_uncertainty':'a230Th_238U_activity_uncertainty',\n    '234U_238U_activity':'a234U_238U_activity','234U_238U_activity_uncertainty':'a234U_238U_activity_uncertainty'},\n    inplace = True)\n    # it is necessary to rename those columns with a number on first position\n\nentity_link_reference = pd.read_csv('data/sisal/sisalv3_csv/entity_link_reference.csv')\noriginal_chronology   = pd.read_csv('data/sisal/sisalv3_csv/original_chronology.csv')\nreference             = pd.read_csv('data/sisal/sisalv3_csv/reference.csv')\nsample                = pd.read_csv('data/sisal/sisalv3_csv/sample.csv')\nsisal_chronology      = pd.read_csv('data/sisal/sisalv3_csv/sisal_chronology.csv')\nsite                  = pd.read_csv('data/sisal/sisalv3_csv/site.csv')\n# os.chdir('..')\n</pre> # read the sisalv3 csv files  entity = pd.read_csv('data/sisal/sisalv3_csv/entity.csv') d13C   = pd.read_csv('data/sisal/sisalv3_csv/d13C.csv') d18O   = pd.read_csv('data/sisal/sisalv3_csv/d18O.csv') MgCa   = pd.read_csv('data/sisal/sisalv3_csv/Mg_Ca.csv') dating = pd.read_csv('data/sisal/sisalv3_csv/dating.csv') dating.rename(columns = {'238U_content':'c238U_content','238U_uncertainty':'c238U_uncertainty',     '232Th_content':'c232Th_content','c232Th_uncertainty':'c232Th_uncertainty',     '230Th_content':'c230Th_content','c230Th_uncertainty':'c230Th_uncertainty',     '230Th_232Th_ratio':'a230Th_232Th_ratio','230Th_232Th_ratio_uncertainty':'a230Th_232Th_ratio_uncertainty',     '230Th_238U_activity':'a230Th_238U_activity','230Th_238U_activity_uncertainty':'a230Th_238U_activity_uncertainty',     '234U_238U_activity':'a234U_238U_activity','234U_238U_activity_uncertainty':'a234U_238U_activity_uncertainty'},     inplace = True)     # it is necessary to rename those columns with a number on first position  entity_link_reference = pd.read_csv('data/sisal/sisalv3_csv/entity_link_reference.csv') original_chronology   = pd.read_csv('data/sisal/sisalv3_csv/original_chronology.csv') reference             = pd.read_csv('data/sisal/sisalv3_csv/reference.csv') sample                = pd.read_csv('data/sisal/sisalv3_csv/sample.csv') sisal_chronology      = pd.read_csv('data/sisal/sisalv3_csv/sisal_chronology.csv') site                  = pd.read_csv('data/sisal/sisalv3_csv/site.csv') # os.chdir('..')  In\u00a0[5]: Copied! <pre>###########################################################################\n# extract required data from speleothems covering the period of interest\n#   + provides all entities, which include non-14C ages and non-events  \n#     during the time period\n###########################################################################\nlow = 1950         # defines minimum age [a]\n\n# KF: Filtering data indicies that don't match age and variable requirements\ni0 = dating.loc[(dating['corr_age'] &lt;= low) &amp;\n                (dating['date_type']!='C14') &amp; (dating['date_type'].str.find('Event')!=0)]\ni1 = i0['entity_id'].to_numpy() \ni3 = np.unique(i1) \n\n### remove all entities with less than 'number_of_dating_points' dated depths\nnumber_of_dating_points = 3\nfor i in np.arange(0,len(i3)):\n    i_dummy = i0.entity_id[i0.entity_id==i3[i]].count()\n    if i_dummy &lt; number_of_dating_points:\n        i0 = i0[i0.entity_id!=i3[i]]\ni1 = i0['entity_id'].to_numpy()\ni2 = np.unique(i1)  # provides all entities, which include &gt;= 'number_of_dating_points' \n                    # dated depths during the required time period\n\n# You could speed up the above process by forming a frequency dictionary to begin with and just referencing those as you go intead of remeasuring frequencies.\n# However, this only becomes more efficient when i0 gets realy big with a lot of repeated entity IDs - Kevin\n\n###########################################################################\n</pre> ########################################################################### # extract required data from speleothems covering the period of interest #   + provides all entities, which include non-14C ages and non-events   #     during the time period ########################################################################### low = 1950         # defines minimum age [a]  # KF: Filtering data indicies that don't match age and variable requirements i0 = dating.loc[(dating['corr_age'] &lt;= low) &amp;                 (dating['date_type']!='C14') &amp; (dating['date_type'].str.find('Event')!=0)] i1 = i0['entity_id'].to_numpy()  i3 = np.unique(i1)   ### remove all entities with less than 'number_of_dating_points' dated depths number_of_dating_points = 3 for i in np.arange(0,len(i3)):     i_dummy = i0.entity_id[i0.entity_id==i3[i]].count()     if i_dummy &lt; number_of_dating_points:         i0 = i0[i0.entity_id!=i3[i]] i1 = i0['entity_id'].to_numpy() i2 = np.unique(i1)  # provides all entities, which include &gt;= 'number_of_dating_points'                      # dated depths during the required time period  # You could speed up the above process by forming a frequency dictionary to begin with and just referencing those as you go intead of remeasuring frequencies. # However, this only becomes more efficient when i0 gets realy big with a lot of repeated entity IDs - Kevin  ###########################################################################  In\u00a0[6]: Copied! <pre>### define parameters (all of those will be saved in a final file)\nsite1_id     = np.zeros(len(i2))\nsite_name1   = ['0']*len(i2)\nrock_age1    = ['0']*len(i2)\nmaterial1    = ['0']*len(i2)\nentity_name1 = ['0']*len(i2)\nlon          = np.zeros(len(i2))\nlat          = np.zeros(len(i2))\nelev         = np.zeros(len(i2))\nentity1_id   = np.zeros(len(i2))\nmean_C       = np.zeros(len(i2))\nmean_O       = np.zeros(len(i2))\nmean_GR      = np.zeros(len(i2))\nmean_MgCa    = np.zeros(len(i2))\nstd_C        = np.zeros(len(i2))\nstd_O        = np.zeros(len(i2))\nstd_GR       = np.zeros(len(i2))\nstd_MgCa     = np.zeros(len(i2))\n\n#we need to initialize a publication_DOI array with the length set by the number of publications meeting the selection criteria.\npublication_DOI1 = np.zeros(len(i2),dtype='object')\n\n#Check size of daa lists\nlen(i2)\n</pre> ### define parameters (all of those will be saved in a final file) site1_id     = np.zeros(len(i2)) site_name1   = ['0']*len(i2) rock_age1    = ['0']*len(i2) material1    = ['0']*len(i2) entity_name1 = ['0']*len(i2) lon          = np.zeros(len(i2)) lat          = np.zeros(len(i2)) elev         = np.zeros(len(i2)) entity1_id   = np.zeros(len(i2)) mean_C       = np.zeros(len(i2)) mean_O       = np.zeros(len(i2)) mean_GR      = np.zeros(len(i2)) mean_MgCa    = np.zeros(len(i2)) std_C        = np.zeros(len(i2)) std_O        = np.zeros(len(i2)) std_GR       = np.zeros(len(i2)) std_MgCa     = np.zeros(len(i2))  #we need to initialize a publication_DOI array with the length set by the number of publications meeting the selection criteria. publication_DOI1 = np.zeros(len(i2),dtype='object')  #Check size of daa lists len(i2)  Out[6]: <pre>211</pre> In\u00a0[7]: Copied! <pre># KF:common dataframe.\ndf = pd.DataFrame(columns=['archiveType', 'dataSetName', 'datasetId', 'geo_meanElev', \n                           'geo_meanLat', 'geo_meanLon', 'originalDataURL', \n                           'paleoData_notes', 'paleoData_proxy', 'paleoData_units',\n                           'paleoData_values', 'year', 'yearUnits'])\n\n# KF: Populating common dataframe\nfor n in np.arange(0,len(i2)): #for every valid unique entity\n\n    dummy = dating.loc[(dating['entity_id'] == i2[n])] # Row associated with valid entity\n\n    ### already some metadata for individual speleothems\n    site1_id[n] = entity.site_id[(entity['entity_id'] == i2[n])].to_numpy()[0] \n    entity1_id[n] = entity.entity_id[(entity['entity_id'] == i2[n])].to_numpy()[0]\n    entity_name1[n] = entity.entity_name[(entity['entity_id'] == i2[n])].to_list()\n    site_name1[n] = site.site_name[(site['site_id'] == site1_id[n])].to_list()\n    refID = entity_link_reference.ref_id[(entity_link_reference['entity_id'] == i2[n])].to_list()\n    publication_DOI1[n] = reference.publication_DOI[(reference['ref_id'] == refID[0])].to_list()\n    if len(publication_DOI1[n])==1: publication_DOI1[n]=publication_DOI1[n]\n    lon[n] = site.longitude[(site['site_id'] == site1_id[n]).to_numpy()].iloc[0]\n    lat[n] = site.latitude[(site['site_id'] == site1_id[n]).to_numpy()].iloc[0]\n    elev[n] = site.elevation[(site['site_id'] == site1_id[n]).to_numpy()].iloc[0]\n    if dummy.material_dated.dropna().eq('calcite').all():\n        material1[n] = 'calcite'\n    elif dummy.material_dated.dropna().eq('aragonite').all():\n        material1[n] = 'aragonite'\n    else:\n        material1[n] = 'mixed'\n    ### extract isotope data (d18O and d13C) and elements #####################\n\n    idx1 = sample.sample_id[(sample['entity_id']==i2[n])].to_numpy() # sample ids for current entity id\n    age = original_chronology.interp_age[original_chronology['sample_id'].isin(idx1)].to_numpy() # interpretation ages from orig. chron. based on current sample ids\n    \n# Oxygen\n    idx2 = original_chronology.sample_id[original_chronology['sample_id'].isin(idx1)].to_numpy() # orig. chron. sample ids based on idx sample ids\n    d18O_1 = d18O.d18O_measurement[d18O['sample_id'].isin(idx2)].to_numpy() # d18O measurements corresponding to idx2 sample ids \n    idx3 = d18O.sample_id[d18O['sample_id'].isin(idx2)].to_numpy() # d18O sample ids corresponding to idx2 sample ids\n    age18 = original_chronology.interp_age[original_chronology['sample_id'].isin(idx3)].to_numpy() #orig. chron. interpretation ages corresponding to idx3 sample ids\n    # KF: Filter dates too low and too high\n    filter = list(map((lambda x: True if (x &lt;= low) else False), age18))\n    age18 = age18[filter]\n    d18O_1 = d18O_1[filter]\n    if (len(idx3) &lt; len(idx2) and len(idx3) &gt; 0):\n        idx2 = idx3 # KF: if a discrepancy exists, brute force idx3 into idx2 for whatever reason, doing this for the other variables causes an index bounds error for growth rate.\n        # probably something to do with the age list manipulation.\n        age = original_chronology.interp_age[original_chronology['sample_id'].isin(idx2)].to_numpy() # then set age to the interp. age pertaining to the new idx2 sample ids\n    if (len(d18O_1) &gt; 0):\n        # Follows the common dictionary format.\n        df.loc[len(df)] = ['speleothem', site_name1[n][0], entity1_id[n], elev[n], lat[n], lon[n], publication_DOI1[n], material1[n], 'd18O', 'permil', d18O_1, age18, 'BP']  \n       \n# Carbon  \n    d13C_1 = d13C.d13C_measurement[d13C['sample_id'].isin(idx2)].to_numpy() # d13C measurements for idx sample ids\n    idx4 = d13C.sample_id[d13C['sample_id'].isin(idx2)].to_numpy() # d13C sample ids corresponding to idx2 sample ids\n    age13 = original_chronology.interp_age[original_chronology['sample_id'].isin(idx4)].to_numpy() # interp ages of d13C sample ids from orig. chron.\n    # KF: Filter dates too low and too high\n    filter = list(map((lambda x: True if (x&lt;= low) else False), age13))\n    age13 = age13[filter]\n    d13C_1 = d13C_1[filter]\n    if (len(d13C_1) &gt; 0):\n        df.loc[len(df)] = ['speleothem', site_name1[n][0], entity1_id[n], elev[n], lat[n], lon[n], publication_DOI1[n], material1[n], 'd13C', 'permil', d13C_1, age13, 'BP']   \n      \n# Magnesium Calcium\n    MgCa_1 = MgCa.Mg_Ca_measurement[MgCa['sample_id'].isin(idx2)].to_numpy() # MgCa measurements corresponding to idx2 sample ids\n    idx5 = MgCa.sample_id[MgCa['sample_id'].isin(idx2)].to_numpy() # MgCa sample ids based on idx2 sample ids\n    ageMgCa = original_chronology.interp_age[original_chronology['sample_id'].isin(idx5)].to_numpy() # interp ages of MgC samples from orig. chron.\n    # KF: Filter dates too low and too high\n    filter = list(map((lambda x: True if (x&lt;= low) else False), ageMgCa))\n    ageMgCa = ageMgCa[filter]\n    MgCa_1 = MgCa_1[filter]\n    if (len(MgCa_1) &gt; 0):\n        df.loc[len(df)] = ['speleothem', site_name1[n][0], entity1_id[n], elev[n], lat[n], lon[n], publication_DOI1[n], material1[n], \n                           'MgCa', 'mmol/mol', MgCa_1, ageMgCa, 'BP']\n    \n    ### also growth rate (gr) could be important ##############################\n    if len(idx2) != 0:\n        isotopeDepth = sample.depth_sample[sample['sample_id'].isin(idx2)].to_numpy()\n\n        # Estalish placeholder arrays\n        gr = np.zeros(len(isotopeDepth))\n        ageGR = np.zeros(len(isotopeDepth))\n        fage_err_gr = np.zeros(len(isotopeDepth))\n\n        # Array population\n        for i in np.arange(0,len(gr)-1):\n            if entity.depth_ref[(entity['entity_id'] == i2[n])].to_numpy() == 'from top':\n                gr[i] = (isotopeDepth[i] - isotopeDepth[i+1]) / (age[i] - age[i+1])\n            else:\n                gr[i] = -(isotopeDepth[i] - isotopeDepth[i+1]) / (age[i] - age[i+1])\n            ageGR[i] = age[i]\n            # KF: error checking\n            fage_err = (dating['corr_age_uncert_pos'][i] + dating['corr_age_uncert_neg'][i])/dating['corr_age'][i]\n            fage_err1 = (dating['corr_age_uncert_pos'][i + 1] + dating['corr_age_uncert_neg'][i + 1])/dating['corr_age'][i + 1]\n            fage_err_gr[i] = np.sqrt((fage_err ** 2) + (fage_err1 **2))\n                \n        gr[-1] = gr[-2]\n        if len(np.argwhere(np.isinf(gr))&gt;0): \n            if (np.argwhere(np.isinf(gr))[-1]==len(gr)-1): # if the last value is 'inf'\n                gr[np.argwhere(np.isinf(gr))[-1]] = gr[np.argwhere(np.isinf(gr))[-1]-2]\n            else:\n                gr[np.argwhere(np.isinf(gr))]=gr[np.argwhere(np.isinf(gr))+1] # replace 'inf' values by neighboring values for gr\n            while len(np.argwhere(np.isinf(gr))&gt;0): # second iteration for cases where there is very fast growth and initially two successive 'inf' values\n                gr[np.argwhere(np.isinf(gr))]=gr[np.argwhere(np.isinf(gr))+1] # replace 'inf' values by neighboring values for gr\n\n        # GR calculation\n        for i in np.arange(1,len(gr)-1):\n            if gr[i]&gt;1: \n                gr[i]=(gr[i-1]+gr[i+1])/2\n\n        # KF: error masking\n        gr[fage_err_gr&gt;0.1]=-9999.99\n        \n         # KF: Adding growth rate to common frame\n        filter = list(map((lambda x: True if (x&lt;= low) else False), ageGR))\n        df.loc[len(df)] = ['speleothem', site_name1[n][0], entity1_id[n], elev[n], lat[n], lon[n], publication_DOI1[n], material1[n], \"growth rate\", \"mm/year\", gr[filter], ageGR[filter], \"BP\"]    \n</pre> # KF:common dataframe. df = pd.DataFrame(columns=['archiveType', 'dataSetName', 'datasetId', 'geo_meanElev',                             'geo_meanLat', 'geo_meanLon', 'originalDataURL',                             'paleoData_notes', 'paleoData_proxy', 'paleoData_units',                            'paleoData_values', 'year', 'yearUnits'])  # KF: Populating common dataframe for n in np.arange(0,len(i2)): #for every valid unique entity      dummy = dating.loc[(dating['entity_id'] == i2[n])] # Row associated with valid entity      ### already some metadata for individual speleothems     site1_id[n] = entity.site_id[(entity['entity_id'] == i2[n])].to_numpy()[0]      entity1_id[n] = entity.entity_id[(entity['entity_id'] == i2[n])].to_numpy()[0]     entity_name1[n] = entity.entity_name[(entity['entity_id'] == i2[n])].to_list()     site_name1[n] = site.site_name[(site['site_id'] == site1_id[n])].to_list()     refID = entity_link_reference.ref_id[(entity_link_reference['entity_id'] == i2[n])].to_list()     publication_DOI1[n] = reference.publication_DOI[(reference['ref_id'] == refID[0])].to_list()     if len(publication_DOI1[n])==1: publication_DOI1[n]=publication_DOI1[n]     lon[n] = site.longitude[(site['site_id'] == site1_id[n]).to_numpy()].iloc[0]     lat[n] = site.latitude[(site['site_id'] == site1_id[n]).to_numpy()].iloc[0]     elev[n] = site.elevation[(site['site_id'] == site1_id[n]).to_numpy()].iloc[0]     if dummy.material_dated.dropna().eq('calcite').all():         material1[n] = 'calcite'     elif dummy.material_dated.dropna().eq('aragonite').all():         material1[n] = 'aragonite'     else:         material1[n] = 'mixed'     ### extract isotope data (d18O and d13C) and elements #####################      idx1 = sample.sample_id[(sample['entity_id']==i2[n])].to_numpy() # sample ids for current entity id     age = original_chronology.interp_age[original_chronology['sample_id'].isin(idx1)].to_numpy() # interpretation ages from orig. chron. based on current sample ids      # Oxygen     idx2 = original_chronology.sample_id[original_chronology['sample_id'].isin(idx1)].to_numpy() # orig. chron. sample ids based on idx sample ids     d18O_1 = d18O.d18O_measurement[d18O['sample_id'].isin(idx2)].to_numpy() # d18O measurements corresponding to idx2 sample ids      idx3 = d18O.sample_id[d18O['sample_id'].isin(idx2)].to_numpy() # d18O sample ids corresponding to idx2 sample ids     age18 = original_chronology.interp_age[original_chronology['sample_id'].isin(idx3)].to_numpy() #orig. chron. interpretation ages corresponding to idx3 sample ids     # KF: Filter dates too low and too high     filter = list(map((lambda x: True if (x &lt;= low) else False), age18))     age18 = age18[filter]     d18O_1 = d18O_1[filter]     if (len(idx3) &lt; len(idx2) and len(idx3) &gt; 0):         idx2 = idx3 # KF: if a discrepancy exists, brute force idx3 into idx2 for whatever reason, doing this for the other variables causes an index bounds error for growth rate.         # probably something to do with the age list manipulation.         age = original_chronology.interp_age[original_chronology['sample_id'].isin(idx2)].to_numpy() # then set age to the interp. age pertaining to the new idx2 sample ids     if (len(d18O_1) &gt; 0):         # Follows the common dictionary format.         df.loc[len(df)] = ['speleothem', site_name1[n][0], entity1_id[n], elev[n], lat[n], lon[n], publication_DOI1[n], material1[n], 'd18O', 'permil', d18O_1, age18, 'BP']           # Carbon       d13C_1 = d13C.d13C_measurement[d13C['sample_id'].isin(idx2)].to_numpy() # d13C measurements for idx sample ids     idx4 = d13C.sample_id[d13C['sample_id'].isin(idx2)].to_numpy() # d13C sample ids corresponding to idx2 sample ids     age13 = original_chronology.interp_age[original_chronology['sample_id'].isin(idx4)].to_numpy() # interp ages of d13C sample ids from orig. chron.     # KF: Filter dates too low and too high     filter = list(map((lambda x: True if (x&lt;= low) else False), age13))     age13 = age13[filter]     d13C_1 = d13C_1[filter]     if (len(d13C_1) &gt; 0):         df.loc[len(df)] = ['speleothem', site_name1[n][0], entity1_id[n], elev[n], lat[n], lon[n], publication_DOI1[n], material1[n], 'd13C', 'permil', d13C_1, age13, 'BP']           # Magnesium Calcium     MgCa_1 = MgCa.Mg_Ca_measurement[MgCa['sample_id'].isin(idx2)].to_numpy() # MgCa measurements corresponding to idx2 sample ids     idx5 = MgCa.sample_id[MgCa['sample_id'].isin(idx2)].to_numpy() # MgCa sample ids based on idx2 sample ids     ageMgCa = original_chronology.interp_age[original_chronology['sample_id'].isin(idx5)].to_numpy() # interp ages of MgC samples from orig. chron.     # KF: Filter dates too low and too high     filter = list(map((lambda x: True if (x&lt;= low) else False), ageMgCa))     ageMgCa = ageMgCa[filter]     MgCa_1 = MgCa_1[filter]     if (len(MgCa_1) &gt; 0):         df.loc[len(df)] = ['speleothem', site_name1[n][0], entity1_id[n], elev[n], lat[n], lon[n], publication_DOI1[n], material1[n],                             'MgCa', 'mmol/mol', MgCa_1, ageMgCa, 'BP']          ### also growth rate (gr) could be important ##############################     if len(idx2) != 0:         isotopeDepth = sample.depth_sample[sample['sample_id'].isin(idx2)].to_numpy()          # Estalish placeholder arrays         gr = np.zeros(len(isotopeDepth))         ageGR = np.zeros(len(isotopeDepth))         fage_err_gr = np.zeros(len(isotopeDepth))          # Array population         for i in np.arange(0,len(gr)-1):             if entity.depth_ref[(entity['entity_id'] == i2[n])].to_numpy() == 'from top':                 gr[i] = (isotopeDepth[i] - isotopeDepth[i+1]) / (age[i] - age[i+1])             else:                 gr[i] = -(isotopeDepth[i] - isotopeDepth[i+1]) / (age[i] - age[i+1])             ageGR[i] = age[i]             # KF: error checking             fage_err = (dating['corr_age_uncert_pos'][i] + dating['corr_age_uncert_neg'][i])/dating['corr_age'][i]             fage_err1 = (dating['corr_age_uncert_pos'][i + 1] + dating['corr_age_uncert_neg'][i + 1])/dating['corr_age'][i + 1]             fage_err_gr[i] = np.sqrt((fage_err ** 2) + (fage_err1 **2))                          gr[-1] = gr[-2]         if len(np.argwhere(np.isinf(gr))&gt;0):              if (np.argwhere(np.isinf(gr))[-1]==len(gr)-1): # if the last value is 'inf'                 gr[np.argwhere(np.isinf(gr))[-1]] = gr[np.argwhere(np.isinf(gr))[-1]-2]             else:                 gr[np.argwhere(np.isinf(gr))]=gr[np.argwhere(np.isinf(gr))+1] # replace 'inf' values by neighboring values for gr             while len(np.argwhere(np.isinf(gr))&gt;0): # second iteration for cases where there is very fast growth and initially two successive 'inf' values                 gr[np.argwhere(np.isinf(gr))]=gr[np.argwhere(np.isinf(gr))+1] # replace 'inf' values by neighboring values for gr          # GR calculation         for i in np.arange(1,len(gr)-1):             if gr[i]&gt;1:                  gr[i]=(gr[i-1]+gr[i+1])/2          # KF: error masking         gr[fage_err_gr&gt;0.1]=-9999.99                   # KF: Adding growth rate to common frame         filter = list(map((lambda x: True if (x&lt;= low) else False), ageGR))         df.loc[len(df)] = ['speleothem', site_name1[n][0], entity1_id[n], elev[n], lat[n], lon[n], publication_DOI1[n], material1[n], \"growth rate\", \"mm/year\", gr[filter], ageGR[filter], \"BP\"]      <pre>/tmp/ipykernel_2405199/2886697801.py:86: RuntimeWarning: invalid value encountered in scalar divide\n  gr[i] = (isotopeDepth[i] - isotopeDepth[i+1]) / (age[i] - age[i+1])\n/tmp/ipykernel_2405199/2886697801.py:86: RuntimeWarning: divide by zero encountered in scalar divide\n  gr[i] = (isotopeDepth[i] - isotopeDepth[i+1]) / (age[i] - age[i+1])\n/tmp/ipykernel_2405199/2886697801.py:86: RuntimeWarning: divide by zero encountered in scalar divide\n  gr[i] = (isotopeDepth[i] - isotopeDepth[i+1]) / (age[i] - age[i+1])\n/tmp/ipykernel_2405199/2886697801.py:86: RuntimeWarning: divide by zero encountered in scalar divide\n  gr[i] = (isotopeDepth[i] - isotopeDepth[i+1]) / (age[i] - age[i+1])\n/tmp/ipykernel_2405199/2886697801.py:86: RuntimeWarning: divide by zero encountered in scalar divide\n  gr[i] = (isotopeDepth[i] - isotopeDepth[i+1]) / (age[i] - age[i+1])\n/tmp/ipykernel_2405199/2886697801.py:86: RuntimeWarning: divide by zero encountered in scalar divide\n  gr[i] = (isotopeDepth[i] - isotopeDepth[i+1]) / (age[i] - age[i+1])\n/tmp/ipykernel_2405199/2886697801.py:86: RuntimeWarning: divide by zero encountered in scalar divide\n  gr[i] = (isotopeDepth[i] - isotopeDepth[i+1]) / (age[i] - age[i+1])\n/tmp/ipykernel_2405199/2886697801.py:86: RuntimeWarning: divide by zero encountered in scalar divide\n  gr[i] = (isotopeDepth[i] - isotopeDepth[i+1]) / (age[i] - age[i+1])\n/tmp/ipykernel_2405199/2886697801.py:86: RuntimeWarning: divide by zero encountered in scalar divide\n  gr[i] = (isotopeDepth[i] - isotopeDepth[i+1]) / (age[i] - age[i+1])\n/tmp/ipykernel_2405199/2886697801.py:86: RuntimeWarning: divide by zero encountered in scalar divide\n  gr[i] = (isotopeDepth[i] - isotopeDepth[i+1]) / (age[i] - age[i+1])\n/tmp/ipykernel_2405199/2886697801.py:86: RuntimeWarning: divide by zero encountered in scalar divide\n  gr[i] = (isotopeDepth[i] - isotopeDepth[i+1]) / (age[i] - age[i+1])\n/tmp/ipykernel_2405199/2886697801.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n  gr[i] = -(isotopeDepth[i] - isotopeDepth[i+1]) / (age[i] - age[i+1])\n/tmp/ipykernel_2405199/2886697801.py:86: RuntimeWarning: divide by zero encountered in scalar divide\n  gr[i] = (isotopeDepth[i] - isotopeDepth[i+1]) / (age[i] - age[i+1])\n/tmp/ipykernel_2405199/2886697801.py:86: RuntimeWarning: divide by zero encountered in scalar divide\n  gr[i] = (isotopeDepth[i] - isotopeDepth[i+1]) / (age[i] - age[i+1])\n/tmp/ipykernel_2405199/2886697801.py:86: RuntimeWarning: divide by zero encountered in scalar divide\n  gr[i] = (isotopeDepth[i] - isotopeDepth[i+1]) / (age[i] - age[i+1])\n/tmp/ipykernel_2405199/2886697801.py:86: RuntimeWarning: divide by zero encountered in scalar divide\n  gr[i] = (isotopeDepth[i] - isotopeDepth[i+1]) / (age[i] - age[i+1])\n</pre> In\u00a0[8]: Copied! <pre># KF: adding og dataset name\ndf.insert(6, 'originalDatabase', ['SISAL v3']*len(df))\ndf.insert(6, 'geo_siteName', df['dataSetName'])\ndf.insert(1, 'interpretation_variable', ['N/A']*len(df))\ndf.insert(1, 'interpretation_variableDetail', ['N/A']*len(df))\ndf.insert(1, 'interpretation_direction', ['N/A']*len(df))\ndf.insert(1, 'interpretation_seasonality', ['N/A']*len(df))\ndf.insert(12, 'paleoData_sensorSpecies', ['N/A']*len(df))\ndf.loc[df['paleoData_proxy']=='MgCa', 'paleoData_proxy']='Mg/Ca'\n\n# KF: Temp cleaning rows with NAN in year\n# There are thirteen of them, hopefully this does not skew data too much. \nlength = len(df['year'])\ndf = df[df['year'].notna()]\ndf = df[df['year'].map(lambda x: len(x) &gt; 1)]\ndf = df[df['paleoData_values'].map(lambda x: len(x) &gt; 1)]\ndf = df[df['paleoData_values'].map(lambda x: not any(pd.isnull(x)))]\nprint('Number of rows discarded: ', (length - len(df['year'])))\n</pre> # KF: adding og dataset name df.insert(6, 'originalDatabase', ['SISAL v3']*len(df)) df.insert(6, 'geo_siteName', df['dataSetName']) df.insert(1, 'interpretation_variable', ['N/A']*len(df)) df.insert(1, 'interpretation_variableDetail', ['N/A']*len(df)) df.insert(1, 'interpretation_direction', ['N/A']*len(df)) df.insert(1, 'interpretation_seasonality', ['N/A']*len(df)) df.insert(12, 'paleoData_sensorSpecies', ['N/A']*len(df)) df.loc[df['paleoData_proxy']=='MgCa', 'paleoData_proxy']='Mg/Ca'  # KF: Temp cleaning rows with NAN in year # There are thirteen of them, hopefully this does not skew data too much.  length = len(df['year']) df = df[df['year'].notna()] df = df[df['year'].map(lambda x: len(x) &gt; 1)] df = df[df['paleoData_values'].map(lambda x: len(x) &gt; 1)] df = df[df['paleoData_values'].map(lambda x: not any(pd.isnull(x)))] print('Number of rows discarded: ', (length - len(df['year']))) <pre>Number of rows discarded:  13\n</pre> <p>Check proxy types included in <code>paleoData_proxy</code></p> In\u00a0[9]: Copied! <pre>df['archiveType'] = df['archiveType'].replace({'speleothem': 'Speleothem'})\n</pre> df['archiveType'] = df['archiveType'].replace({'speleothem': 'Speleothem'}) In\u00a0[10]: Copied! <pre>set(df['paleoData_proxy'])\n</pre> set(df['paleoData_proxy']) Out[10]: <pre>{'Mg/Ca', 'd13C', 'd18O', 'growth rate'}</pre> <p>assign <code>climateInterpretation_variable</code>:</p> <ul> <li>d18O is temperature and moisture</li> <li>Mg/Ca is temperature</li> </ul> In\u00a0[11]: Copied! <pre># d18O is temperature and moisture\ndf.loc[df['paleoData_proxy']=='d18O', 'interpretation_variable']='temperature+moisture' \ndf.loc[df['paleoData_proxy']=='d18O', 'interpretation_variableDetail']='temperature+moisture - manually assigned by DoD2k authors for paleoData_proxy = d18O.' \n\n# Mg/Ca is temperature\ndf.loc[df['paleoData_proxy']=='Mg/Ca', 'interpretation_variable']='temperature' \ndf.loc[df['paleoData_proxy']=='Mg/Ca', 'interpretation_variableDetail']='temperature - manually assigned by DoD2k authors for paleoData_proxy = Mg/Ca' \n\ndf['paleoData_variableName'] = df['paleoData_proxy']\n</pre> # d18O is temperature and moisture df.loc[df['paleoData_proxy']=='d18O', 'interpretation_variable']='temperature+moisture'  df.loc[df['paleoData_proxy']=='d18O', 'interpretation_variableDetail']='temperature+moisture - manually assigned by DoD2k authors for paleoData_proxy = d18O.'   # Mg/Ca is temperature df.loc[df['paleoData_proxy']=='Mg/Ca', 'interpretation_variable']='temperature'  df.loc[df['paleoData_proxy']=='Mg/Ca', 'interpretation_variableDetail']='temperature - manually assigned by DoD2k authors for paleoData_proxy = Mg/Ca'   df['paleoData_variableName'] = df['paleoData_proxy'] <p>convert years before present to year Common Era</p> In\u00a0[12]: Copied! <pre># BP 0 Adjustment\n\ndef BP2CE(year):\n    year = 1950 - year\n    if year &lt;= 0:\n        year = year - 1\n    return year\ndf['year'] = df['year'].apply(lambda x: (np.array(list(map(lambda y: BP2CE(y), x)))))\ndf['yearUnits'] = ['CE']*len(df)\n</pre> # BP 0 Adjustment  def BP2CE(year):     year = 1950 - year     if year &lt;= 0:         year = year - 1     return year df['year'] = df['year'].apply(lambda x: (np.array(list(map(lambda y: BP2CE(y), x))))) df['yearUnits'] = ['CE']*len(df)  In\u00a0[13]: Copied! <pre># KF: Type-checking\n\ndf = df.astype({'archiveType': str, 'dataSetName': str, 'datasetId': str, 'geo_meanElev': float, 'geo_meanLat': float, 'geo_meanLon': float, 'geo_siteName': str, \n                    'originalDatabase': str, 'originalDataURL': str, 'paleoData_notes': str, 'paleoData_proxy': str, 'paleoData_units': str, 'yearUnits': str,\n               'interpretation_direction': str, 'interpretation_seasonality': str, 'interpretation_variable': str,'interpretation_variableDetail': str })\ndf['year'] = df['year'].map(lambda x: np.array(x, dtype = float))\ndf['paleoData_values'] = df['paleoData_values'].map(lambda x: np.array(x, dtype = float))\n</pre> # KF: Type-checking  df = df.astype({'archiveType': str, 'dataSetName': str, 'datasetId': str, 'geo_meanElev': float, 'geo_meanLat': float, 'geo_meanLon': float, 'geo_siteName': str,                      'originalDatabase': str, 'originalDataURL': str, 'paleoData_notes': str, 'paleoData_proxy': str, 'paleoData_units': str, 'yearUnits': str,                'interpretation_direction': str, 'interpretation_seasonality': str, 'interpretation_variable': str,'interpretation_variableDetail': str }) df['year'] = df['year'].map(lambda x: np.array(x, dtype = float)) df['paleoData_values'] = df['paleoData_values'].map(lambda x: np.array(x, dtype = float))  In\u00a0[14]: Copied! <pre># KF: display the dataframe\ndf.reset_index(drop= True, inplace= True)\ndf = df[sorted(df.columns)]\nprint(df.info())\n</pre> # KF: display the dataframe df.reset_index(drop= True, inplace= True) df = df[sorted(df.columns)] print(df.info()) <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 546 entries, 0 to 545\nData columns (total 21 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   archiveType                    546 non-null    object \n 1   dataSetName                    546 non-null    object \n 2   datasetId                      546 non-null    object \n 3   geo_meanElev                   546 non-null    float64\n 4   geo_meanLat                    546 non-null    float64\n 5   geo_meanLon                    546 non-null    float64\n 6   geo_siteName                   546 non-null    object \n 7   interpretation_direction       546 non-null    object \n 8   interpretation_seasonality     546 non-null    object \n 9   interpretation_variable        546 non-null    object \n 10  interpretation_variableDetail  546 non-null    object \n 11  originalDataURL                546 non-null    object \n 12  originalDatabase               546 non-null    object \n 13  paleoData_notes                546 non-null    object \n 14  paleoData_proxy                546 non-null    object \n 15  paleoData_sensorSpecies        546 non-null    object \n 16  paleoData_units                546 non-null    object \n 17  paleoData_values               546 non-null    object \n 18  paleoData_variableName         546 non-null    object \n 19  year                           546 non-null    object \n 20  yearUnits                      546 non-null    object \ndtypes: float64(3), object(18)\nmemory usage: 89.7+ KB\nNone\n</pre> In\u00a0[15]: Copied! <pre>#  check that the datasetId is unique - it currently is not (df has 546 records).\nprint(len(df.datasetId.unique()))\n# make datasetId unique by simply adding index number\ndf.datasetId=df.apply(lambda x: 'sisal_'+x.datasetId+'_'+str(x.name), axis=1)\n# check uniqueness - problem solved.\nprint(len(df.datasetId.unique()))\n</pre> #  check that the datasetId is unique - it currently is not (df has 546 records). print(len(df.datasetId.unique())) # make datasetId unique by simply adding index number df.datasetId=df.apply(lambda x: 'sisal_'+x.datasetId+'_'+str(x.name), axis=1) # check uniqueness - problem solved. print(len(df.datasetId.unique())) <pre>200\n546\n</pre> In\u00a0[16]: Copied! <pre>drop_inds = []\nfor ii in df.index:\n    try:\n        year = np.array(df.at[ii, 'year'], dtype=float)\n        vals = np.array(df.at[ii, 'paleoData_values'], dtype=float)\n        df.at[ii, 'year']             = year[year&gt;=1]\n        df.at[ii, 'paleoData_values'] = vals[year&gt;=1]\n    except:\n        # print\n        df.at[ii, 'paleoData_values'] = np.array([utf.convert_to_float(y) for y in df.at[ii, 'paleoData_values']], dtype=float)\n        df.at[ii, 'year'] = np.array([utf.convert_to_float(y) for y in df.at[ii, 'year']], dtype=float)\n        \n        print(f'Converted values in paleoData_values and/or year for {ii}.')\n        # drop_inds.append(ii)\n# df_compact = df_compact.drop(drop_inds)\n\n# drop all missing values and exclude all-missing-values-rows\n\nfor ii in df.index:\n    dd   = np.array(df.at[ii, 'paleoData_values'])\n    mask = dd==-9999.99\n    df.at[ii, 'paleoData_values']=dd[~mask]\n    df.at[ii, 'year']=np.array(df.at[ii, 'year'])[~mask]\n    \ndrop_inds = []\nfor ii, row in enumerate(df.paleoData_values):\n    try:\n        if len(row)==0:\n            print(ii, 'empty row for paleodata_values')\n        elif len(df.iloc[ii]['year'])==0:\n            print(ii, 'empty row for year')\n        elif np.std(row)==0: \n            print(ii, 'std=0')\n        elif np.sum(np.diff(row)**2)==0: \n            print(ii, 'diff=0')\n        elif np.isnan(np.std(row)):\n            print(ii, 'std nan')\n        else:\n            continue\n        if df.index[ii] not in drop_inds: \n            drop_inds += [df.index[ii]]\n    except:\n        drop_inds+=[df.index[ii]]\n    \nprint(drop_inds)\ndf = df.drop(index=drop_inds)\n</pre> drop_inds = [] for ii in df.index:     try:         year = np.array(df.at[ii, 'year'], dtype=float)         vals = np.array(df.at[ii, 'paleoData_values'], dtype=float)         df.at[ii, 'year']             = year[year&gt;=1]         df.at[ii, 'paleoData_values'] = vals[year&gt;=1]     except:         # print         df.at[ii, 'paleoData_values'] = np.array([utf.convert_to_float(y) for y in df.at[ii, 'paleoData_values']], dtype=float)         df.at[ii, 'year'] = np.array([utf.convert_to_float(y) for y in df.at[ii, 'year']], dtype=float)                  print(f'Converted values in paleoData_values and/or year for {ii}.')         # drop_inds.append(ii) # df_compact = df_compact.drop(drop_inds)  # drop all missing values and exclude all-missing-values-rows  for ii in df.index:     dd   = np.array(df.at[ii, 'paleoData_values'])     mask = dd==-9999.99     df.at[ii, 'paleoData_values']=dd[~mask]     df.at[ii, 'year']=np.array(df.at[ii, 'year'])[~mask]      drop_inds = [] for ii, row in enumerate(df.paleoData_values):     try:         if len(row)==0:             print(ii, 'empty row for paleodata_values')         elif len(df.iloc[ii]['year'])==0:             print(ii, 'empty row for year')         elif np.std(row)==0:              print(ii, 'std=0')         elif np.sum(np.diff(row)**2)==0:              print(ii, 'diff=0')         elif np.isnan(np.std(row)):             print(ii, 'std nan')         else:             continue         if df.index[ii] not in drop_inds:              drop_inds += [df.index[ii]]     except:         drop_inds+=[df.index[ii]]      print(drop_inds) df = df.drop(index=drop_inds) <pre>[]\n</pre> In\u00a0[17]: Copied! <pre>df = df[sorted(df.columns)]\nprint(df.info())\n</pre> df = df[sorted(df.columns)] print(df.info()) <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 546 entries, 0 to 545\nData columns (total 21 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   archiveType                    546 non-null    object \n 1   dataSetName                    546 non-null    object \n 2   datasetId                      546 non-null    object \n 3   geo_meanElev                   546 non-null    float64\n 4   geo_meanLat                    546 non-null    float64\n 5   geo_meanLon                    546 non-null    float64\n 6   geo_siteName                   546 non-null    object \n 7   interpretation_direction       546 non-null    object \n 8   interpretation_seasonality     546 non-null    object \n 9   interpretation_variable        546 non-null    object \n 10  interpretation_variableDetail  546 non-null    object \n 11  originalDataURL                546 non-null    object \n 12  originalDatabase               546 non-null    object \n 13  paleoData_notes                546 non-null    object \n 14  paleoData_proxy                546 non-null    object \n 15  paleoData_sensorSpecies        546 non-null    object \n 16  paleoData_units                546 non-null    object \n 17  paleoData_values               546 non-null    object \n 18  paleoData_variableName         546 non-null    object \n 19  year                           546 non-null    object \n 20  yearUnits                      546 non-null    object \ndtypes: float64(3), object(18)\nmemory usage: 89.7+ KB\nNone\n</pre> In\u00a0[18]: Copied! <pre># KF: Save to pickle\ndf.to_pickle('data/sisal/sisal_compact.pkl')\n</pre> # KF: Save to pickle df.to_pickle('data/sisal/sisal_compact.pkl')  In\u00a0[19]: Copied! <pre># save to a list of csv files (metadata, data, year)\ndf.name='sisal'\nutf.write_compact_dataframe_to_csv(df)\n</pre> # save to a list of csv files (metadata, data, year) df.name='sisal' utf.write_compact_dataframe_to_csv(df) <pre>METADATA: datasetId, archiveType, dataSetName, geo_meanElev, geo_meanLat, geo_meanLon, geo_siteName, interpretation_direction, interpretation_seasonality, interpretation_variable, interpretation_variableDetail, originalDataURL, originalDatabase, paleoData_notes, paleoData_proxy, paleoData_sensorSpecies, paleoData_units, paleoData_variableName, yearUnits\nSaved to /home/jupyter-lluecke/dod2k/data/sisal/sisal_compact_%s.csv\n</pre> In\u00a0[20]: Copied! <pre># load dataframe\ndf = utf.load_compact_dataframe_from_csv('sisal')\nprint(df.info)\n</pre> # load dataframe df = utf.load_compact_dataframe_from_csv('sisal') print(df.info) <pre>&lt;bound method DataFrame.info of     archiveType  dataSetName        datasetId  geo_meanElev  geo_meanLat  \\\n0    Speleothem  Bittoo cave      sisal_9.0_0        3000.0    30.790300   \n1    Speleothem  Bittoo cave      sisal_9.0_1        3000.0    30.790300   \n2    Speleothem  Bittoo cave      sisal_9.0_2        3000.0    30.790300   \n3    Speleothem  Kesang cave     sisal_19.0_3        2000.0    42.869999   \n4    Speleothem  Kesang cave     sisal_19.0_4        2000.0    42.869999   \n..          ...          ...              ...           ...          ...   \n541  Speleothem  Sahiya cave  sisal_900.0_541        1190.0    30.600000   \n542  Speleothem  Sahiya cave  sisal_900.0_542        1190.0    30.600000   \n543  Speleothem  Sahiya cave  sisal_901.0_543        1190.0    30.600000   \n544  Speleothem  Sahiya cave  sisal_901.0_544        1190.0    30.600000   \n545  Speleothem  Sahiya cave  sisal_901.0_545        1190.0    30.600000   \n\n     geo_meanLon geo_siteName interpretation_direction  \\\n0      77.776398  Bittoo cave                      N/A   \n1      77.776398  Bittoo cave                      N/A   \n2      77.776398  Bittoo cave                      N/A   \n3      81.750000  Kesang cave                      N/A   \n4      81.750000  Kesang cave                      N/A   \n..           ...          ...                      ...   \n541    77.866699  Sahiya cave                      N/A   \n542    77.866699  Sahiya cave                      N/A   \n543    77.866699  Sahiya cave                      N/A   \n544    77.866699  Sahiya cave                      N/A   \n545    77.866699  Sahiya cave                      N/A   \n\n    interpretation_seasonality interpretation_variable  ...  \\\n0                          N/A    temperature+moisture  ...   \n1                          N/A                     N/A  ...   \n2                          N/A                     N/A  ...   \n3                          N/A    temperature+moisture  ...   \n4                          N/A                     N/A  ...   \n..                         ...                     ...  ...   \n541                        N/A                     N/A  ...   \n542                        N/A                     N/A  ...   \n543                        N/A    temperature+moisture  ...   \n544                        N/A                     N/A  ...   \n545                        N/A                     N/A  ...   \n\n            originalDataURL originalDatabase paleoData_notes paleoData_proxy  \\\n0     ['10.1038/srep24374']         SISAL v3         calcite            d18O   \n1     ['10.1038/srep24374']         SISAL v3         calcite            d13C   \n2     ['10.1038/srep24374']         SISAL v3         calcite     growth rate   \n3     ['10.1038/srep36975']         SISAL v3         calcite            d18O   \n4     ['10.1038/srep36975']         SISAL v3         calcite            d13C   \n..                      ...              ...             ...             ...   \n541  ['10.1038/ncomms7309']         SISAL v3         calcite            d13C   \n542  ['10.1038/ncomms7309']         SISAL v3         calcite     growth rate   \n543  ['10.1038/ncomms7309']         SISAL v3         calcite            d18O   \n544  ['10.1038/ncomms7309']         SISAL v3         calcite            d13C   \n545  ['10.1038/ncomms7309']         SISAL v3         calcite     growth rate   \n\n    paleoData_sensorSpecies paleoData_units  \\\n0                       N/A          permil   \n1                       N/A          permil   \n2                       N/A         mm/year   \n3                       N/A          permil   \n4                       N/A          permil   \n..                      ...             ...   \n541                     N/A          permil   \n542                     N/A         mm/year   \n543                     N/A          permil   \n544                     N/A          permil   \n545                     N/A         mm/year   \n\n                                      paleoData_values paleoData_variableName  \\\n0    [-7.194, -5.274, -7.206, -7.624, -6.122, -6.57...                   d18O   \n1    [-0.848, 2.907, -1.927, -3.213, -3.958, -4.64,...                   d13C   \n2    [0.025, 0.023809524, 0.025, 0.023809524, 0.023...            growth rate   \n3    [-7.15, -7.49, -7.59, -7.98, -7.69, -7.95, -7....                   d18O   \n4    [-1.85, -3.48, -4.34, -4.9, -4.51, -4.6, -4.69...                   d13C   \n..                                                 ...                    ...   \n541  [-0.02, -0.23, -0.11, -0.18, -0.11, -0.13, -0....                   d13C   \n542  [0.14251104, 0.14144272, 0.1398895, 0.13837, 0...            growth rate   \n543  [-8.83, -9.12, -9.11, -9.15, -8.98, -9.07, -8....                   d18O   \n544  [0.863, 1.244, 0.668, 1.17, 1.262, 1.109, 1.18...                   d13C   \n545  [0.35714287, 0.35714287, 0.3539823, 0.3508772,...            growth rate   \n\n                                                  year yearUnits  \n0    [1076.0, 1056.0, 1035.0, 1015.0, 994.0, 973.0,...        CE  \n1    [1076.0, 1056.0, 1035.0, 1015.0, 994.0, 973.0,...        CE  \n2    [1076.0, 1056.0, 1035.0, 1015.0, 994.0, 973.0,...        CE  \n3    [1098.0, 1086.0, 1074.0, 1061.0, 1049.0, 1037....        CE  \n4    [1098.0, 1086.0, 1074.0, 1061.0, 1049.0, 1037....        CE  \n..                                                 ...       ...  \n541  [2006.3071, 2005.6053, 2004.8984, 2003.4688, 2...        CE  \n542  [2006.3071, 2005.6053, 2004.8984, 2003.4688, 2...        CE  \n543  [1357.23, 1356.67, 1356.11, 1354.98, 1354.41, ...        CE  \n544  [1357.23, 1356.67, 1356.11, 1354.98, 1354.41, ...        CE  \n545  [1357.23, 1356.67, 1356.11, 1354.98, 1354.41, ...        CE  \n\n[546 rows x 21 columns]&gt;\n</pre> <p>Show spatial distribution of records, show archive and proxy types</p> In\u00a0[21]: Copied! <pre># count archive types\narchive_count = {}\nfor ii, at in enumerate(set(df['archiveType'])):\n    archive_count[at] = df.loc[df['archiveType']==at, 'archiveType'].count()\n\nsort = np.argsort([cc for cc in archive_count.values()])\narchives_sorted = np.array([cc for cc in archive_count.keys()])[sort][::-1]\n\n# Specify colour for each archive (smaller archives get grouped into the same colour)\narchive_colour, major_archives, other_archives = uplt.get_archive_colours(archives_sorted, archive_count)\n\nfig = uplt.plot_geo_archive_proxy(df, archive_colour)\nutf.save_fig(fig, f'geo_{df.name}', dir=df.name)\n</pre> # count archive types archive_count = {} for ii, at in enumerate(set(df['archiveType'])):     archive_count[at] = df.loc[df['archiveType']==at, 'archiveType'].count()  sort = np.argsort([cc for cc in archive_count.values()]) archives_sorted = np.array([cc for cc in archive_count.keys()])[sort][::-1]  # Specify colour for each archive (smaller archives get grouped into the same colour) archive_colour, major_archives, other_archives = uplt.get_archive_colours(archives_sorted, archive_count)  fig = uplt.plot_geo_archive_proxy(df, archive_colour) utf.save_fig(fig, f'geo_{df.name}', dir=df.name) <pre>0 Speleothem 546\nsaved figure in /home/jupyter-lluecke/dod2k/figs/sisal/geo_sisal.pdf\n</pre> <p>Now plot the coverage over the Common Era</p> In\u00a0[22]: Copied! <pre>fig = uplt.plot_coverage(df, archives_sorted, major_archives, other_archives, archive_colour)\nutf.save_fig(fig, f'time_{df.name}', dir=df.name)\n</pre> fig = uplt.plot_coverage(df, archives_sorted, major_archives, other_archives, archive_colour) utf.save_fig(fig, f'time_{df.name}', dir=df.name) <pre>saved figure in /home/jupyter-lluecke/dod2k/figs/sisal/time_sisal.pdf\n</pre> In\u00a0[23]: Copied! <pre># # check index\nprint(df.index)\n</pre> # # check index print(df.index) <pre>RangeIndex(start=0, stop=546, step=1)\n</pre> In\u00a0[24]: Copied! <pre># # check dataSetName\nkey = 'dataSetName'\nprint('%s: '%key)\nprint(df[key].values)\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # # check dataSetName key = 'dataSetName' print('%s: '%key) print(df[key].values) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>dataSetName: \n['Bittoo cave' 'Bittoo cave' 'Bittoo cave' 'Kesang cave' 'Kesang cave'\n 'Kesang cave' 'Paraiso cave' 'Paraiso cave' 'Paraiso cave' 'Paraiso cave'\n 'Paraiso cave' 'Paraiso cave' 'Villars cave' 'Villars cave'\n 'Villars cave' 'Cold Air cave' 'Cold Air cave' 'Cold Air cave'\n 'Cold Air cave' 'Cold Air cave' 'Cold Air cave' 'Cold Air cave'\n 'Cold Air cave' 'Cold Air cave' 'Cold Air cave' 'Cold Air cave'\n 'Cold Air cave' 'Cold Air cave' 'Cold Air cave' 'Lancaster Hole'\n 'Lancaster Hole' 'Jeita cave' 'Jeita cave' 'Jeita cave' 'Jeita cave'\n 'Jeita cave' 'Jeita cave' 'Huangye cave' 'Huangye cave' 'Huangye cave'\n 'Huangye cave' 'Huangye cave' 'Huangye cave' 'Lapa grande cave'\n 'Lapa grande cave' 'Lapa grande cave' 'Palestina cave' 'Palestina cave'\n 'Palestina cave' 'Palestina cave' 'Palestina cave' 'Palestina cave'\n 'Okshola cave' 'Okshola cave' 'Okshola cave' 'Tamboril cave'\n 'Tamboril cave' 'Tamboril cave' 'Anjokipoty' 'Anjokipoty' 'Anjokipoty'\n 'Curupira cave' 'Curupira cave' 'Curupira cave' 'Dayu cave' 'Dayu cave'\n 'Diva cave' 'Diva cave' 'Diva cave' 'Dongge cave' 'Dongge cave'\n 'Ifoulki cave' 'Ifoulki cave' 'Ifoulki cave' 'Kapsia cave' 'Kapsia cave'\n 'Kapsia cave' 'Larshullet cave' 'Larshullet cave' 'Larshullet cave'\n 'Leviathan cave' 'Leviathan cave' 'Leviathan cave'\n 'Natural Bridge caverns' 'Natural Bridge caverns'\n 'Natural Bridge caverns' \"Pau d'Alho cave\" \"Pau d'Alho cave\"\n \"Pau d'Alho cave\" 'Skala Marion cave' 'Skala Marion cave'\n 'Skala Marion cave' 'Soylegrotta cave' 'Soylegrotta cave'\n 'Soylegrotta cave' 'Taurius cave' 'Taurius cave' 'Taurius cave'\n 'Torrinha cave' 'Torrinha cave' 'Torrinha cave' 'Tzabnah cave'\n 'Tzabnah cave' 'Wah Shikhar cave' 'Wah Shikhar cave' 'Wah Shikhar cave'\n 'Chilibrillo cave' 'Chilibrillo cave' 'Chilibrillo cave' 'Furong cave'\n 'Furong cave' 'Furong cave' 'Macal Chasm' 'Macal Chasm' 'Macal Chasm'\n 'Anjohibe' 'Anjohibe' 'Anjohibe' 'Anjohibe' 'Anjohibe' 'Anjohibe'\n 'Anjohibe' 'Anjohibe' 'Anjohibe' 'Anjohibe' 'Anjohibe' 'Anjohibe'\n 'Dante cave' 'Dante cave' 'Dante cave' 'Dante cave' 'Dante cave'\n 'Dante cave' 'Klapferloch cave' 'Klapferloch cave' 'Klapferloch cave'\n 'Lapa Doce cave' 'Lapa Doce cave' 'Lapa Doce cave' 'Liang Luar'\n 'Liang Luar' 'Liang Luar' 'Liang Luar' 'Yok Balum cave' 'Yok Balum cave'\n 'Yok Balum cave' 'Yok Balum cave' 'Yok Balum cave' 'Yok Balum cave'\n 'Liang Luar' 'Liang Luar' 'Liang Luar' 'Liang Luar' 'Liang Luar'\n 'Liang Luar' 'Liang Luar' 'Bukit Assam cave' 'Bukit Assam cave'\n 'Bukit Assam cave' 'Bukit Assam cave' 'Bukit Assam cave' 'Bunker cave'\n 'Bunker cave' 'Bunker cave' 'Bunker cave' 'Bunker cave' 'Bunker cave'\n 'Bunker cave' 'Bunker cave' 'Cueva de Asiul' 'Cueva de Asiul'\n 'Heshang cave' 'Heshang cave' 'Heshang cave' 'Buckeye creek'\n 'Buckeye creek' 'Buckeye creek' 'Buckeye creek' 'Buckeye creek'\n 'Buckeye creek' 'Buckeye creek' 'Grotta di Carburangeli'\n 'Grotta di Carburangeli' 'Grotta di Carburangeli' 'Dandak cave'\n 'Dandak cave' 'Dandak cave' 'Juxtlahuaca cave' 'Juxtlahuaca cave'\n 'Juxtlahuaca cave' 'Juxtlahuaca cave' 'Kinderlinskaya cave'\n 'Kinderlinskaya cave' 'Kinderlinskaya cave'\n 'Oregon caves national monument' 'Oregon caves national monument'\n 'Oregon caves national monument' 'Sanbao cave' 'Sanbao cave'\n 'Sofular cave' 'Sofular cave' 'Sofular cave' 'Cova da Arcoia'\n 'Cova da Arcoia' 'Cova da Arcoia' 'Botuver\u00e1 cave' 'Botuver\u00e1 cave'\n 'Botuver\u00e1 cave' 'Gunung-buda cave (snail shell cave)'\n 'Gunung-buda cave (snail shell cave)' 'Jhumar cave' 'Jhumar cave'\n 'Jhumar cave' 'Jiuxian cave' 'Jiuxian cave' 'Jiuxian cave' 'Jiuxian cave'\n 'KNI-51' 'KNI-51' 'KNI-51' 'KNI-51' 'KNI-51' 'KNI-51' 'KNI-51' 'KNI-51'\n 'KNI-51' 'KNI-51' 'Mavri Trypa cave' 'Mavri Trypa cave'\n 'Mavri Trypa cave' 'Munagamanu cave' 'Munagamanu cave' 'Munagamanu cave'\n 'Munagamanu cave' 'Munagamanu cave' 'Munagamanu cave' 'Soreq cave'\n 'Soreq cave' 'Te Reinga cave' 'Te Reinga cave' 'Te Reinga cave'\n 'Te Reinga cave' 'Te Reinga cave' 'Te Reinga cave' 'Liang Luar'\n 'Liang Luar' 'Perdida cave' 'Perdida cave' 'Perdida cave' 'Closani cave'\n 'Closani cave' 'Closani cave' 'Closani cave' 'Forestry cave'\n 'Forestry cave' 'Forestry cave' 'Forestry cave' 'Forestry cave'\n 'Forestry cave' 'Bribin cave' 'Bribin cave' 'Bribin cave' 'KNI-51'\n 'KNI-51' 'Lianhua cave, Hunan' 'Lianhua cave, Hunan'\n 'Lianhua cave, Hunan' 'Minnetonka cave' 'Minnetonka cave'\n 'Minnetonka cave' 'S\u00e3o Bernardo cave' 'S\u00e3o Bernardo cave'\n 'S\u00e3o Matheus cave' 'S\u00e3o Matheus cave' 'Shatuca cave' 'Shatuca cave'\n 'Shatuca cave' 'Shatuca cave' 'Tangga cave' 'Tangga cave' 'Tangga cave'\n 'Uluu-Too cave' 'Uluu-Too cave' 'Uluu-Too cave' 'Xibalba cave'\n 'Xibalba cave' 'Xibalba cave' 'Dongge cave' 'Dongge cave' 'Dos Anas cave'\n 'Dos Anas cave' 'Dos Anas cave' 'Dongge cave' 'Dongge cave'\n 'Jaragu\u00e1 cave' 'Jaragu\u00e1 cave' 'Jaragu\u00e1 cave' 'Jaragu\u00e1 cave'\n 'Jaragu\u00e1 cave' 'Jaragu\u00e1 cave' 'Mawmluh cave' 'Mawmluh cave'\n 'Mawmluh cave' 'Chaara cave' 'Chaara cave' 'Dark cave' 'Dark cave'\n \"E'mei cave\" \"E'mei cave\" \"E'mei cave\" 'Grotte de Piste'\n 'Grotte de Piste' 'Baeg-nyong cave' 'Baeg-nyong cave' 'Wanxiang cave'\n 'Wanxiang cave' 'Xianglong cave' 'Xianglong cave' 'Xianglong cave'\n 'Lianhua cave, Hunan' 'Lianhua cave, Hunan' 'Chiflonkhakha cave'\n 'Chiflonkhakha cave' 'Chiflonkhakha cave' 'Chiflonkhakha cave' 'Bat cave'\n 'Bat cave' 'Bat cave' 'Cueva del Tigre Perdido' 'Cueva del Tigre Perdido'\n 'Umajalanta cave' 'Umajalanta cave' 'Lianhua cave, Shanxi'\n 'Lianhua cave, Shanxi' 'Shenqi cave' 'Shenqi cave' 'Shenqi cave'\n 'Shenqi cave' 'Wuya cave' 'Wuya cave' 'Wuya cave' 'Wuya cave' 'Hoq cave'\n 'Hoq cave' 'Hoq cave' 'Hoq cave' 'Hoq cave' 'Hoq cave' 'Hoq cave'\n 'Hoq cave' 'Hoq cave' 'Akcakale cave' 'Akcakale cave' 'Akcakale cave'\n 'Ble\u00dfberg cave' 'Ble\u00dfberg cave' 'Ble\u00dfberg cave' 'Gejkar cave'\n 'Gejkar cave' 'Gejkar cave' 'Gejkar cave' 'Crystal cave' 'Crystal cave'\n 'Chaara cave' 'Chaara cave' 'Chaara cave' 'Chaara cave' 'El Condor cave'\n 'El Condor cave' 'El Condor cave' 'Tamboril cave' 'Tamboril cave'\n 'Tamboril cave' 'Huagapo cave' 'Huagapo cave' 'Huagapo cave'\n 'Huagapo cave' 'Huagapo cave' 'Huagapo cave' 'Pink Panther cave'\n 'Pink Panther cave' 'Kesang cave' 'Kesang cave' 'Kesang cave'\n 'La Garma cave' 'La Garma cave' 'Tham Doun Mai' 'Tham Doun Mai'\n 'Tham Doun Mai' 'Hollywood cave' 'Hollywood cave' 'Hollywood cave'\n 'Jaragu\u00e1 cave' 'Jaragu\u00e1 cave' 'Jaragu\u00e1 cave' 'Shennong cave'\n 'Shennong cave' 'Shennong cave' 'Anjohibe' 'Anjohibe' 'Anjohibe'\n 'B\u00e0sura cave' 'B\u00e0sura cave' 'B\u00e0sura cave' 'Careys cave' 'Careys cave'\n 'Careys cave' 'Careys cave' 'Cathedral cave' 'Cathedral cave'\n 'Cathedral cave' 'Cu\u00edca cave' 'Cu\u00edca cave' 'Cu\u00edca cave' 'Cu\u00edca cave'\n 'Cu\u00edca cave' 'Cu\u00edca cave' 'Golgotha cave' 'Golgotha cave' 'Golgotha cave'\n 'Golgotha cave' 'Golgotha cave' 'Golgotha cave' 'Golgotha cave'\n 'Golgotha cave' 'Golgotha cave' 'Golgotha cave' 'Golgotha cave'\n 'Golgotha cave' 'Harrie Wood cave' 'Harrie Wood cave' 'Harrie Wood cave'\n 'Harrie Wood cave' 'Harrie Wood cave' 'Harrie Wood cave'\n 'Harrie Wood cave' 'Harrie Wood cave' 'Harrie Wood cave' 'Heifeng cave'\n 'Heifeng cave' 'Herbstlabyrinth cave' 'Herbstlabyrinth cave'\n 'Herbstlabyrinth cave' 'Herbstlabyrinth cave' 'Huangchao cave'\n 'Huangchao cave' 'Huangchao cave' 'Huangchao cave' 'Ifoulki cave'\n 'Ifoulki cave' 'Ifoulki cave' 'Jinfo cave' 'Jinfo cave' 'Jiulong cave'\n 'Jiulong cave' 'Jiulong cave' 'Kuna Ba' 'Kuna Ba' 'Kuna Ba' 'Kuna Ba'\n 'La Vierge cave' 'La Vierge cave' 'La Vierge cave' 'Mata Virgem cave'\n 'Mata Virgem cave' 'Mata Virgem cave' 'Nova\\xa0Grgosova\\xa0cave'\n 'Nova\\xa0Grgosova\\xa0cave' 'Nova\\xa0Grgosova\\xa0cave' 'Coves del pirata'\n 'Coves del pirata' 'Coves del pirata' 'Coves del pirata' 'Qujia cave'\n 'Qujia cave' 'Rey Marcos' 'Rey Marcos' 'Rey Marcos'\n 'Sa balma des quart\u00f3 cave' 'Sa balma des quart\u00f3 cave'\n 'Sa balma des quart\u00f3 cave' 'Sa balma des quart\u00f3 cave'\n 'Sa balma des quart\u00f3 cave' 'Sa balma des quart\u00f3 cave'\n 'Sa balma des quart\u00f3 cave' 'Sa balma des quart\u00f3 cave'\n 'Sa balma des quart\u00f3 cave' 'Sa balma des quart\u00f3 cave'\n 'Sa balma des quart\u00f3 cave' 'Sa balma des quart\u00f3 cave' 'Shijiangjun cave'\n 'Shijiangjun cave' 'Shijiangjun cave' 'Shizi cave' 'Shizi cave'\n 'Tham Doun Mai' 'Tham Doun Mai' 'Tham Doun Mai' 'Trapi\u00e1 cave'\n 'Trapi\u00e1 cave' 'Trapi\u00e1 cave' 'Wuya cave' 'Wuya cave' 'Wuya cave'\n 'Wintimdouine' 'Wintimdouine' 'Wintimdouine' 'Wintimdouine' 'Wulu cave'\n 'Wulu cave' 'Wulu cave' 'Xiniu cave' 'Xiniu cave' 'Xiniu cave'\n 'Xiniu cave' 'Xiniu cave' 'Xiniu cave' 'Yonderup cave' 'Yonderup cave'\n 'Yonderup cave' 'Yonderup cave' 'Bunker cave' 'Bunker cave' 'Bunker cave'\n 'Bunker cave' 'Bunker cave' 'Bunker cave' 'Kocain cave' 'Kocain cave'\n 'Kocain cave' 'Kocain cave' 'S\u00e3o Bernardo cave' 'S\u00e3o Bernardo cave'\n 'S\u00e3o Bernardo cave' 'Chiflonkhakha cave' 'Chiflonkhakha cave'\n 'Chiflonkhakha cave' 'Chiflonkhakha cave' 'Chiflonkhakha cave'\n 'Chiflonkhakha cave' 'Sahiya cave' 'Sahiya cave' 'Sahiya cave'\n 'Sahiya cave' 'Sahiya cave' 'Sahiya cave']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 129/546\n</pre> In\u00a0[25]: Copied! <pre># # check datasetId\n\nprint(len(df.datasetId.unique()))\nprint(len(df))\nkey = 'datasetId'\nprint('%s (starts with): '%key)\nprint(df[key].values)\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint('datasetId starts with: ', np.unique([str(dd.split('_')[0]) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # # check datasetId  print(len(df.datasetId.unique())) print(len(df)) key = 'datasetId' print('%s (starts with): '%key) print(df[key].values) print(np.unique([str(type(dd)) for dd in df[key]])) print('datasetId starts with: ', np.unique([str(dd.split('_')[0]) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>546\n546\ndatasetId (starts with): \n['sisal_9.0_0' 'sisal_9.0_1' 'sisal_9.0_2' 'sisal_19.0_3' 'sisal_19.0_4'\n 'sisal_19.0_5' 'sisal_20.0_6' 'sisal_20.0_7' 'sisal_20.0_8'\n 'sisal_21.0_9' 'sisal_21.0_10' 'sisal_21.0_11' 'sisal_33.0_12'\n 'sisal_33.0_13' 'sisal_33.0_14' 'sisal_45.0_15' 'sisal_45.0_16'\n 'sisal_45.0_17' 'sisal_46.0_18' 'sisal_46.0_19' 'sisal_46.0_20'\n 'sisal_47.0_21' 'sisal_47.0_22' 'sisal_47.0_23' 'sisal_48.0_24'\n 'sisal_48.0_25' 'sisal_49.0_26' 'sisal_49.0_27' 'sisal_49.0_28'\n 'sisal_51.0_29' 'sisal_51.0_30' 'sisal_58.0_31' 'sisal_58.0_32'\n 'sisal_58.0_33' 'sisal_60.0_34' 'sisal_60.0_35' 'sisal_60.0_36'\n 'sisal_76.0_37' 'sisal_76.0_38' 'sisal_77.0_39' 'sisal_77.0_40'\n 'sisal_78.0_41' 'sisal_78.0_42' 'sisal_90.0_43' 'sisal_90.0_44'\n 'sisal_90.0_45' 'sisal_93.0_46' 'sisal_93.0_47' 'sisal_93.0_48'\n 'sisal_94.0_49' 'sisal_94.0_50' 'sisal_94.0_51' 'sisal_95.0_52'\n 'sisal_95.0_53' 'sisal_95.0_54' 'sisal_97.0_55' 'sisal_97.0_56'\n 'sisal_97.0_57' 'sisal_107.0_58' 'sisal_107.0_59' 'sisal_107.0_60'\n 'sisal_110.0_61' 'sisal_110.0_62' 'sisal_110.0_63' 'sisal_111.0_64'\n 'sisal_111.0_65' 'sisal_113.0_66' 'sisal_113.0_67' 'sisal_113.0_68'\n 'sisal_115.0_69' 'sisal_115.0_70' 'sisal_118.0_71' 'sisal_118.0_72'\n 'sisal_118.0_73' 'sisal_120.0_74' 'sisal_120.0_75' 'sisal_120.0_76'\n 'sisal_123.0_77' 'sisal_123.0_78' 'sisal_123.0_79' 'sisal_124.0_80'\n 'sisal_124.0_81' 'sisal_124.0_82' 'sisal_127.0_83' 'sisal_127.0_84'\n 'sisal_127.0_85' 'sisal_128.0_86' 'sisal_128.0_87' 'sisal_128.0_88'\n 'sisal_136.0_89' 'sisal_136.0_90' 'sisal_136.0_91' 'sisal_137.0_92'\n 'sisal_137.0_93' 'sisal_137.0_94' 'sisal_144.0_95' 'sisal_144.0_96'\n 'sisal_144.0_97' 'sisal_146.0_98' 'sisal_146.0_99' 'sisal_146.0_100'\n 'sisal_147.0_101' 'sisal_147.0_102' 'sisal_148.0_103' 'sisal_148.0_104'\n 'sisal_148.0_105' 'sisal_167.0_106' 'sisal_167.0_107' 'sisal_167.0_108'\n 'sisal_172.0_109' 'sisal_172.0_110' 'sisal_172.0_111' 'sisal_178.0_112'\n 'sisal_178.0_113' 'sisal_178.0_114' 'sisal_187.0_115' 'sisal_187.0_116'\n 'sisal_187.0_117' 'sisal_188.0_118' 'sisal_188.0_119' 'sisal_188.0_120'\n 'sisal_189.0_121' 'sisal_189.0_122' 'sisal_189.0_123' 'sisal_190.0_124'\n 'sisal_190.0_125' 'sisal_190.0_126' 'sisal_197.0_127' 'sisal_197.0_128'\n 'sisal_197.0_129' 'sisal_198.0_130' 'sisal_198.0_131' 'sisal_198.0_132'\n 'sisal_201.0_133' 'sisal_201.0_134' 'sisal_201.0_135' 'sisal_203.0_136'\n 'sisal_203.0_137' 'sisal_203.0_138' 'sisal_204.0_139' 'sisal_204.0_140'\n 'sisal_205.0_141' 'sisal_205.0_142' 'sisal_209.0_143' 'sisal_209.0_144'\n 'sisal_209.0_145' 'sisal_210.0_146' 'sisal_210.0_147' 'sisal_210.0_148'\n 'sisal_226.0_149' 'sisal_226.0_150' 'sisal_226.0_151' 'sisal_226.0_152'\n 'sisal_227.0_153' 'sisal_227.0_154' 'sisal_227.0_155' 'sisal_237.0_156'\n 'sisal_237.0_157' 'sisal_238.0_158' 'sisal_238.0_159' 'sisal_238.0_160'\n 'sisal_240.0_161' 'sisal_240.0_162' 'sisal_240.0_163' 'sisal_240.0_164'\n 'sisal_242.0_165' 'sisal_242.0_166' 'sisal_242.0_167' 'sisal_242.0_168'\n 'sisal_249.0_169' 'sisal_249.0_170' 'sisal_253.0_171' 'sisal_253.0_172'\n 'sisal_253.0_173' 'sisal_271.0_174' 'sisal_271.0_175' 'sisal_271.0_176'\n 'sisal_272.0_177' 'sisal_272.0_178' 'sisal_273.0_179' 'sisal_273.0_180'\n 'sisal_277.0_181' 'sisal_277.0_182' 'sisal_277.0_183' 'sisal_278.0_184'\n 'sisal_278.0_185' 'sisal_278.0_186' 'sisal_286.0_187' 'sisal_286.0_188'\n 'sisal_289.0_189' 'sisal_289.0_190' 'sisal_291.0_191' 'sisal_291.0_192'\n 'sisal_291.0_193' 'sisal_294.0_194' 'sisal_294.0_195' 'sisal_294.0_196'\n 'sisal_298.0_197' 'sisal_298.0_198' 'sisal_305.0_199' 'sisal_305.0_200'\n 'sisal_305.0_201' 'sisal_310.0_202' 'sisal_310.0_203' 'sisal_310.0_204'\n 'sisal_311.0_205' 'sisal_311.0_206' 'sisal_311.0_207' 'sisal_319.0_208'\n 'sisal_319.0_209' 'sisal_328.0_210' 'sisal_328.0_211' 'sisal_328.0_212'\n 'sisal_329.0_213' 'sisal_329.0_214' 'sisal_330.0_215' 'sisal_330.0_216'\n 'sisal_335.0_217' 'sisal_335.0_218' 'sisal_336.0_219' 'sisal_336.0_220'\n 'sisal_340.0_221' 'sisal_340.0_222' 'sisal_341.0_223' 'sisal_341.0_224'\n 'sisal_346.0_225' 'sisal_346.0_226' 'sisal_347.0_227' 'sisal_347.0_228'\n 'sisal_347.0_229' 'sisal_348.0_230' 'sisal_348.0_231' 'sisal_348.0_232'\n 'sisal_349.0_233' 'sisal_349.0_234' 'sisal_349.0_235' 'sisal_352.0_236'\n 'sisal_352.0_237' 'sisal_361.0_238' 'sisal_361.0_239' 'sisal_361.0_240'\n 'sisal_362.0_241' 'sisal_362.0_242' 'sisal_362.0_243' 'sisal_367.0_244'\n 'sisal_367.0_245' 'sisal_378.0_246' 'sisal_378.0_247' 'sisal_378.0_248'\n 'sisal_390.0_249' 'sisal_390.0_250' 'sisal_390.0_251' 'sisal_390.0_252'\n 'sisal_392.0_253' 'sisal_392.0_254' 'sisal_392.0_255' 'sisal_393.0_256'\n 'sisal_393.0_257' 'sisal_393.0_258' 'sisal_399.0_259' 'sisal_399.0_260'\n 'sisal_399.0_261' 'sisal_419.0_262' 'sisal_419.0_263' 'sisal_420.0_264'\n 'sisal_420.0_265' 'sisal_420.0_266' 'sisal_422.0_267' 'sisal_422.0_268'\n 'sisal_422.0_269' 'sisal_430.0_270' 'sisal_430.0_271' 'sisal_431.0_272'\n 'sisal_431.0_273' 'sisal_432.0_274' 'sisal_432.0_275' 'sisal_433.0_276'\n 'sisal_433.0_277' 'sisal_436.0_278' 'sisal_436.0_279' 'sisal_436.0_280'\n 'sisal_437.0_281' 'sisal_437.0_282' 'sisal_437.0_283' 'sisal_440.0_284'\n 'sisal_440.0_285' 'sisal_440.0_286' 'sisal_442.0_287' 'sisal_442.0_288'\n 'sisal_443.0_289' 'sisal_443.0_290' 'sisal_443.0_291' 'sisal_446.0_292'\n 'sisal_446.0_293' 'sisal_447.0_294' 'sisal_447.0_295' 'sisal_447.0_296'\n 'sisal_448.0_297' 'sisal_448.0_298' 'sisal_448.0_299' 'sisal_451.0_300'\n 'sisal_451.0_301' 'sisal_451.0_302' 'sisal_460.0_303' 'sisal_460.0_304'\n 'sisal_461.0_305' 'sisal_461.0_306' 'sisal_463.0_307' 'sisal_463.0_308'\n 'sisal_463.0_309' 'sisal_464.0_310' 'sisal_464.0_311' 'sisal_468.0_312'\n 'sisal_468.0_313' 'sisal_471.0_314' 'sisal_471.0_315' 'sisal_472.0_316'\n 'sisal_472.0_317' 'sisal_472.0_318' 'sisal_496.0_319' 'sisal_496.0_320'\n 'sisal_498.0_321' 'sisal_498.0_322' 'sisal_499.0_323' 'sisal_499.0_324'\n 'sisal_506.0_325' 'sisal_506.0_326' 'sisal_506.0_327' 'sisal_514.0_328'\n 'sisal_514.0_329' 'sisal_518.0_330' 'sisal_518.0_331' 'sisal_528.0_332'\n 'sisal_528.0_333' 'sisal_538.0_334' 'sisal_538.0_335' 'sisal_539.0_336'\n 'sisal_539.0_337' 'sisal_542.0_338' 'sisal_542.0_339' 'sisal_543.0_340'\n 'sisal_543.0_341' 'sisal_546.0_342' 'sisal_546.0_343' 'sisal_546.0_344'\n 'sisal_547.0_345' 'sisal_547.0_346' 'sisal_547.0_347' 'sisal_548.0_348'\n 'sisal_548.0_349' 'sisal_548.0_350' 'sisal_559.0_351' 'sisal_559.0_352'\n 'sisal_559.0_353' 'sisal_564.0_354' 'sisal_564.0_355' 'sisal_564.0_356'\n 'sisal_573.0_357' 'sisal_573.0_358' 'sisal_573.0_359' 'sisal_573.0_360'\n 'sisal_577.0_361' 'sisal_577.0_362' 'sisal_588.0_363' 'sisal_588.0_364'\n 'sisal_589.0_365' 'sisal_589.0_366' 'sisal_592.0_367' 'sisal_592.0_368'\n 'sisal_592.0_369' 'sisal_594.0_370' 'sisal_594.0_371' 'sisal_594.0_372'\n 'sisal_597.0_373' 'sisal_597.0_374' 'sisal_597.0_375' 'sisal_598.0_376'\n 'sisal_598.0_377' 'sisal_598.0_378' 'sisal_613.0_379' 'sisal_613.0_380'\n 'sisal_620.0_381' 'sisal_621.0_382' 'sisal_623.0_383' 'sisal_650.0_384'\n 'sisal_650.0_385' 'sisal_672.0_386' 'sisal_672.0_387' 'sisal_672.0_388'\n 'sisal_673.0_389' 'sisal_673.0_390' 'sisal_673.0_391' 'sisal_707.0_392'\n 'sisal_707.0_393' 'sisal_707.0_394' 'sisal_723.0_395' 'sisal_723.0_396'\n 'sisal_723.0_397' 'sisal_738.0_398' 'sisal_738.0_399' 'sisal_738.0_400'\n 'sisal_739.0_401' 'sisal_739.0_402' 'sisal_739.0_403' 'sisal_742.0_404'\n 'sisal_742.0_405' 'sisal_742.0_406' 'sisal_742.0_407' 'sisal_743.0_408'\n 'sisal_743.0_409' 'sisal_743.0_410' 'sisal_752.0_411' 'sisal_752.0_412'\n 'sisal_752.0_413' 'sisal_753.0_414' 'sisal_753.0_415' 'sisal_753.0_416'\n 'sisal_765.0_417' 'sisal_765.0_418' 'sisal_765.0_419' 'sisal_766.0_420'\n 'sisal_766.0_421' 'sisal_766.0_422' 'sisal_767.0_423' 'sisal_767.0_424'\n 'sisal_767.0_425' 'sisal_768.0_426' 'sisal_768.0_427' 'sisal_768.0_428'\n 'sisal_769.0_429' 'sisal_769.0_430' 'sisal_769.0_431' 'sisal_770.0_432'\n 'sisal_770.0_433' 'sisal_770.0_434' 'sisal_771.0_435' 'sisal_771.0_436'\n 'sisal_771.0_437' 'sisal_772.0_438' 'sisal_772.0_439' 'sisal_774.0_440'\n 'sisal_774.0_441' 'sisal_774.0_442' 'sisal_774.0_443' 'sisal_782.0_444'\n 'sisal_782.0_445' 'sisal_782.0_446' 'sisal_782.0_447' 'sisal_787.0_448'\n 'sisal_787.0_449' 'sisal_787.0_450' 'sisal_791.0_451' 'sisal_791.0_452'\n 'sisal_792.0_453' 'sisal_792.0_454' 'sisal_792.0_455' 'sisal_799.0_456'\n 'sisal_799.0_457' 'sisal_800.0_458' 'sisal_800.0_459' 'sisal_810.0_460'\n 'sisal_810.0_461' 'sisal_810.0_462' 'sisal_817.0_463' 'sisal_817.0_464'\n 'sisal_817.0_465' 'sisal_831.0_466' 'sisal_831.0_467' 'sisal_831.0_468'\n 'sisal_837.0_469' 'sisal_837.0_470' 'sisal_837.0_471' 'sisal_837.0_472'\n 'sisal_840.0_473' 'sisal_840.0_474' 'sisal_841.0_475' 'sisal_841.0_476'\n 'sisal_841.0_477' 'sisal_842.0_478' 'sisal_842.0_479' 'sisal_842.0_480'\n 'sisal_842.0_481' 'sisal_843.0_482' 'sisal_843.0_483' 'sisal_843.0_484'\n 'sisal_843.0_485' 'sisal_844.0_486' 'sisal_844.0_487' 'sisal_844.0_488'\n 'sisal_844.0_489' 'sisal_851.0_490' 'sisal_851.0_491' 'sisal_851.0_492'\n 'sisal_852.0_493' 'sisal_852.0_494' 'sisal_857.0_495' 'sisal_857.0_496'\n 'sisal_857.0_497' 'sisal_861.0_498' 'sisal_861.0_499' 'sisal_861.0_500'\n 'sisal_866.0_501' 'sisal_866.0_502' 'sisal_866.0_503' 'sisal_868.0_504'\n 'sisal_868.0_505' 'sisal_869.0_506' 'sisal_869.0_507' 'sisal_873.0_508'\n 'sisal_873.0_509' 'sisal_873.0_510' 'sisal_874.0_511' 'sisal_874.0_512'\n 'sisal_874.0_513' 'sisal_875.0_514' 'sisal_875.0_515' 'sisal_875.0_516'\n 'sisal_879.0_517' 'sisal_879.0_518' 'sisal_879.0_519' 'sisal_879.0_520'\n 'sisal_885.0_521' 'sisal_885.0_522' 'sisal_885.0_523' 'sisal_886.0_524'\n 'sisal_886.0_525' 'sisal_886.0_526' 'sisal_887.0_527' 'sisal_887.0_528'\n 'sisal_887.0_529' 'sisal_887.0_530' 'sisal_896.0_531' 'sisal_896.0_532'\n 'sisal_896.0_533' 'sisal_898.0_534' 'sisal_898.0_535' 'sisal_898.0_536'\n 'sisal_899.0_537' 'sisal_899.0_538' 'sisal_899.0_539' 'sisal_900.0_540'\n 'sisal_900.0_541' 'sisal_900.0_542' 'sisal_901.0_543' 'sisal_901.0_544'\n 'sisal_901.0_545']\n[\"&lt;class 'str'&gt;\"]\ndatasetId starts with:  ['sisal']\nNo. of unique values: 546/546\n</pre> In\u00a0[26]: Copied! <pre># originalDataURL\nkey = 'originalDataURL'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([kk for kk in df[key] if 'this' in kk]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n# 'this study' should point to the correct URL (PAGES2k)\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # originalDataURL key = 'originalDataURL' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([kk for kk in df[key] if 'this' in kk])) print(np.unique([str(type(dd)) for dd in df[key]])) # 'this study' should point to the correct URL (PAGES2k) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>originalDataURL: \n[\"['10.1002/2015GL063826']\" \"['10.1002/2015gl065397']\"\n \"['10.1002/2016GL071786']\" \"['10.1002/jqs.1490']\"\n \"['10.1007/s11430-019-9649-1']\" \"['10.1016/j.chemgeo.2013.08.026']\"\n \"['10.1016/j.epsl.2004.10.024']\" \"['10.1016/j.epsl.2005.01.036']\"\n \"['10.1016/j.epsl.2007.10.015']\" \"['10.1016/j.epsl.2008.07.060']\"\n \"['10.1016/j.epsl.2008.08.018']\" \"['10.1016/j.epsl.2009.12.017']\"\n \"['10.1016/j.epsl.2009.12.039']\" \"['10.1016/j.epsl.2010.04.002']\"\n \"['10.1016/j.epsl.2010.08.016']\" \"['10.1016/j.epsl.2011.05.028']\"\n \"['10.1016/j.epsl.2015.03.015']\" \"['10.1016/j.epsl.2016.02.050']\"\n \"['10.1016/j.epsl.2016.06.008']\" \"['10.1016/j.epsl.2017.01.034']\"\n \"['10.1016/j.epsl.2017.07.045']\" \"['10.1016/j.epsl.2017.11.044']\"\n \"['10.1016/j.epsl.2018.04.001']\" \"['10.1016/j.epsl.2018.04.048']\"\n \"['10.1016/j.epsl.2018.07.027']\" \"['10.1016/j.epsl.2019.115717']\"\n \"['10.1016/j.epsl.2019.115737']\" \"['10.1016/j.gca.2019.12.007']\"\n \"['10.1016/j.gca.2022.03.020']\" \"['10.1016/j.gloplacha.2019.03.007']\"\n \"['10.1016/j.gloplacha.2020.103266']\" \"['10.1016/j.jseaes.2010.06.011']\"\n \"['10.1016/j.jseaes.2013.04.015']\" \"['10.1016/j.jseaes.2017.10.016']\"\n \"['10.1016/j.palaeo.2009.06.030']\" \"['10.1016/j.palaeo.2011.02.030']\"\n \"['10.1016/j.palaeo.2013.02.030']\" \"['10.1016/j.palaeo.2016.07.007']\"\n \"['10.1016/j.palaeo.2017.01.003']\" \"['10.1016/j.palaeo.2017.10.021']\"\n \"['10.1016/j.quageo.2009.01.009']\" \"['10.1016/j.quaint.2007.09.039']\"\n \"['10.1016/j.quaint.2013.03.018']\" \"['10.1016/j.quascirev.2013.01.016']\"\n \"['10.1016/j.quascirev.2013.05.008']\"\n \"['10.1016/j.quascirev.2013.08.004']\"\n \"['10.1016/j.quascirev.2014.12.021']\"\n \"['10.1016/j.quascirev.2015.06.023']\"\n \"['10.1016/j.quascirev.2016.01.007']\"\n \"['10.1016/j.quascirev.2016.05.023']\"\n \"['10.1016/j.quascirev.2016.11.012']\"\n \"['10.1016/j.quascirev.2016.12.014']\"\n \"['10.1016/j.quascirev.2017.03.017']\"\n \"['10.1016/j.quascirev.2018.07.021']\"\n \"['10.1016/j.quascirev.2019.02.019']\"\n \"['10.1016/j.quascirev.2020.106191']\"\n \"['10.1016/j.quascirev.2020.106655']\"\n \"['10.1016/j.quascirev.2021.106822']\"\n \"['10.1016/j.quascirev.2021.106865']\"\n \"['10.1016/j.quascirev.2021.106911']\"\n \"['10.1016/j.quascirev.2021.107137']\"\n \"['10.1016/j.quascirev.2022.107383']\"\n \"['10.1016/j.quascirev.2022.107742']\" \"['10.1016/j.yqres.2006.05.003']\"\n \"['10.1016/j.yqres.2008.08.005']\" \"['10.1016/j.yqres.2011.01.005']\"\n \"['10.1016/j.yqres.2013.12.009']\" \"['10.1016/s0031-0182(00)00225-x']\"\n \"['10.1016/s0031-0182(98)00223-5']\" \"['10.1016/s0277-3791(03)00204-x']\"\n \"['10.1029/2000gl012728']\" \"['10.1029/2004jd004694']\"\n \"['10.1029/2009gl040050']\" \"['10.1029/2011gl047713']\"\n \"['10.1029/2012gl053936']\" \"['10.1029/2017GL076838']\"\n \"['10.1029/2019GL082405']\" \"['10.1029/2019GL084879']\"\n \"['10.1029/2020GL090273']\" \"['10.1029/2021GL093071']\"\n \"['10.1029/2021GL094232']\" \"['10.1029/2021GL094733']\"\n \"['10.1038/nature06164']\" \"['10.1038/nature20787']\"\n \"['10.1038/ncomms11719']\" \"['10.1038/ncomms2222']\"\n \"['10.1038/ncomms2415']\" \"['10.1038/ncomms3908']\"\n \"['10.1038/ncomms4805']\" \"['10.1038/ncomms7309']\"\n \"['10.1038/ncomms8627']\" \"['10.1038/ngeo1862']\" \"['10.1038/ngeo2353']\"\n \"['10.1038/ngeo2953']\" \"['10.1038/ngeo605']\"\n \"['10.1038/s41467-020-14490-y']\" \"['10.1038/s41467-020-17927-6']\"\n \"['10.1038/s41467-022-32654-w']\" \"['10.1038/s41598-017-15566-4']\"\n \"['10.1038/s41598-018-30112-6']\" \"['10.1038/s41598-018-35498-x']\"\n \"['10.1038/s41598-019-56852-7']\" \"['10.1038/s43247-022-00347-3']\"\n \"['10.1038/srep01767']\" \"['10.1038/srep06381']\" \"['10.1038/srep24374']\"\n \"['10.1038/srep24745']\" \"['10.1038/srep24762']\" \"['10.1038/srep36975']\"\n \"['10.1073/pnas.1214870110']\" \"['10.1073/pnas.1422270112']\"\n \"['10.1126/sciadv.aax6656']\" \"['10.1126/sciadv.abb2459']\"\n \"['10.1126/sciadv.abi9275']\" \"['10.1126/science.1091220']\"\n \"['10.1126/science.1106296']\" \"['10.1126/science.1163965']\"\n \"['10.1126/science.1226299']\" \"['10.1130/G22865A.1']\"\n \"['10.1130/g32098.1']\" \"['10.1130/g32471.1']\" \"['10.1130/g34718.1']\"\n \"['10.1177/0959683609350393']\" \"['10.1177/0959683610378880']\"\n \"['10.1177/0959683612449759']\" \"['10.1177/0959683612471986']\"\n \"['10.1177/0959683616652711']\" \"['10.1177/0959683616660170']\"\n \"['10.1177/0959683619831433']\" \"['10.1177/0959683620981717']\"\n \"['10.1177/09596836211019120']\" \"['10.1191/095968399672625464']\"\n \"['10.1371/journal.pone.0189447']\" \"['10.3390/geosciences11040166']\"\n \"['10.5194/cp-10-1319-2014']\" \"['10.5194/cp-10-1967-2014']\"\n \"['10.5194/cp-2016-137']\" \"['10.5194/cp-5-667-2009']\"\n \"['10.5194/cp-8-1751-2012']\" \"['unpublished']\"]\n[]\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 140/546\n</pre> In\u00a0[27]: Copied! <pre># # originalDataSet\nkey = 'originalDatabase'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n# Note: the last two records have missing URLs\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # # originalDataSet key = 'originalDatabase' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) # Note: the last two records have missing URLs print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>originalDatabase: \n['SISAL v3']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 1/546\n</pre> In\u00a0[28]: Copied! <pre># check Elevation\nkey = 'geo_meanElev'\nprint('%s: '%key)\nprint(df[key])\nprint(np.unique(['%d'%kk for kk in df[key] if np.isfinite(kk)]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # check Elevation key = 'geo_meanElev' print('%s: '%key) print(df[key]) print(np.unique(['%d'%kk for kk in df[key] if np.isfinite(kk)])) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>geo_meanElev: \n0      3000.0\n1      3000.0\n2      3000.0\n3      2000.0\n4      2000.0\n        ...  \n541    1190.0\n542    1190.0\n543    1190.0\n544    1190.0\n545    1190.0\nName: geo_meanElev, Length: 546, dtype: float32\n['10' '100' '1000' '1120' '1140' '1160' '1190' '120' '1200' '1240' '1250'\n '1260' '1290' '1300' '131' '1370' '1386' '1400' '1407' '1420' '1440'\n '1460' '1490' '1495' '150' '1530' '162' '165' '1650' '175' '180' '184'\n '1900' '1960' '20' '200' '2000' '2114' '2132' '22' '230' '2347' '239'\n '240' '2400' '250' '2660' '280' '2830' '285' '294' '300' '3000' '306'\n '310' '32' '335' '336' '340' '350' '352' '365' '383' '3850' '393' '400'\n '401' '41' '420' '43' '433' '435' '440' '455' '456' '475' '480' '500'\n '518' '53' '530' '550' '570' '590' '60' '600' '631' '650' '660' '680'\n '70' '700' '72' '730' '85' '860' '870' '934' '940' '965']\n[\"&lt;class 'float'&gt;\"]\nNo. of unique values: 100/546\n</pre> In\u00a0[29]: Copied! <pre># # Latitude\nkey = 'geo_meanLat'\nprint('%s: '%key)\nprint(np.unique(['%d'%kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # # Latitude key = 'geo_meanLat' print('%s: '%key) print(np.unique(['%d'%kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>geo_meanLat: \n['-11' '-12' '-13' '-14' '-15' '-16' '-18' '-19' '-21' '-24' '-27' '-31'\n '-32' '-34' '-35' '-38' '-4' '-41' '-5' '-8' '-9' '0' '12' '15' '16' '17'\n '18' '19' '20' '22' '25' '26' '27' '28' '29' '30' '31' '32' '33' '35'\n '36' '37' '38' '39' '4' '40' '41' '42' '43' '44' '45' '46' '50' '51' '54'\n '66' '67' '9']\n[\"&lt;class 'float'&gt;\"]\nNo. of unique values: 126/546\n</pre> In\u00a0[30]: Copied! <pre># # Longitude \nkey = 'geo_meanLon'\nprint('%s: '%key)\nprint(np.unique(['%d'%kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # # Longitude  key = 'geo_meanLon' print('%s: '%key) print(np.unique(['%d'%kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>geo_meanLon: \n['-104' '-105' '-111' '-115' '-118' '-123' '-2' '-3' '-37' '-4' '-41'\n '-44' '-46' '-47' '-49' '-55' '-56' '-60' '-65' '-67' '-7' '-75' '-77'\n '-79' '-80' '-83' '-89' '-9' '-90' '-98' '-99' '0' '10' '100' '102' '103'\n '105' '106' '107' '108' '109' '11' '110' '113' '114' '115' '117' '118'\n '120' '128' '13' '14' '148' '15' '159' '167' '17' '171' '177' '21' '22'\n '24' '29' '3' '30' '31' '35' '39' '45' '46' '54' '56' '63' '7' '72' '77'\n '8' '81' '82' '91']\n[\"&lt;class 'float'&gt;\"]\nNo. of unique values: 128/546\n</pre> In\u00a0[31]: Copied! <pre># Site Name \nkey = 'geo_siteName'\nprint('%s: '%key)\nprint(df[key].values)\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # Site Name  key = 'geo_siteName' print('%s: '%key) print(df[key].values) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>geo_siteName: \n['Bittoo cave' 'Bittoo cave' 'Bittoo cave' 'Kesang cave' 'Kesang cave'\n 'Kesang cave' 'Paraiso cave' 'Paraiso cave' 'Paraiso cave' 'Paraiso cave'\n 'Paraiso cave' 'Paraiso cave' 'Villars cave' 'Villars cave'\n 'Villars cave' 'Cold Air cave' 'Cold Air cave' 'Cold Air cave'\n 'Cold Air cave' 'Cold Air cave' 'Cold Air cave' 'Cold Air cave'\n 'Cold Air cave' 'Cold Air cave' 'Cold Air cave' 'Cold Air cave'\n 'Cold Air cave' 'Cold Air cave' 'Cold Air cave' 'Lancaster Hole'\n 'Lancaster Hole' 'Jeita cave' 'Jeita cave' 'Jeita cave' 'Jeita cave'\n 'Jeita cave' 'Jeita cave' 'Huangye cave' 'Huangye cave' 'Huangye cave'\n 'Huangye cave' 'Huangye cave' 'Huangye cave' 'Lapa grande cave'\n 'Lapa grande cave' 'Lapa grande cave' 'Palestina cave' 'Palestina cave'\n 'Palestina cave' 'Palestina cave' 'Palestina cave' 'Palestina cave'\n 'Okshola cave' 'Okshola cave' 'Okshola cave' 'Tamboril cave'\n 'Tamboril cave' 'Tamboril cave' 'Anjokipoty' 'Anjokipoty' 'Anjokipoty'\n 'Curupira cave' 'Curupira cave' 'Curupira cave' 'Dayu cave' 'Dayu cave'\n 'Diva cave' 'Diva cave' 'Diva cave' 'Dongge cave' 'Dongge cave'\n 'Ifoulki cave' 'Ifoulki cave' 'Ifoulki cave' 'Kapsia cave' 'Kapsia cave'\n 'Kapsia cave' 'Larshullet cave' 'Larshullet cave' 'Larshullet cave'\n 'Leviathan cave' 'Leviathan cave' 'Leviathan cave'\n 'Natural Bridge caverns' 'Natural Bridge caverns'\n 'Natural Bridge caverns' \"Pau d'Alho cave\" \"Pau d'Alho cave\"\n \"Pau d'Alho cave\" 'Skala Marion cave' 'Skala Marion cave'\n 'Skala Marion cave' 'Soylegrotta cave' 'Soylegrotta cave'\n 'Soylegrotta cave' 'Taurius cave' 'Taurius cave' 'Taurius cave'\n 'Torrinha cave' 'Torrinha cave' 'Torrinha cave' 'Tzabnah cave'\n 'Tzabnah cave' 'Wah Shikhar cave' 'Wah Shikhar cave' 'Wah Shikhar cave'\n 'Chilibrillo cave' 'Chilibrillo cave' 'Chilibrillo cave' 'Furong cave'\n 'Furong cave' 'Furong cave' 'Macal Chasm' 'Macal Chasm' 'Macal Chasm'\n 'Anjohibe' 'Anjohibe' 'Anjohibe' 'Anjohibe' 'Anjohibe' 'Anjohibe'\n 'Anjohibe' 'Anjohibe' 'Anjohibe' 'Anjohibe' 'Anjohibe' 'Anjohibe'\n 'Dante cave' 'Dante cave' 'Dante cave' 'Dante cave' 'Dante cave'\n 'Dante cave' 'Klapferloch cave' 'Klapferloch cave' 'Klapferloch cave'\n 'Lapa Doce cave' 'Lapa Doce cave' 'Lapa Doce cave' 'Liang Luar'\n 'Liang Luar' 'Liang Luar' 'Liang Luar' 'Yok Balum cave' 'Yok Balum cave'\n 'Yok Balum cave' 'Yok Balum cave' 'Yok Balum cave' 'Yok Balum cave'\n 'Liang Luar' 'Liang Luar' 'Liang Luar' 'Liang Luar' 'Liang Luar'\n 'Liang Luar' 'Liang Luar' 'Bukit Assam cave' 'Bukit Assam cave'\n 'Bukit Assam cave' 'Bukit Assam cave' 'Bukit Assam cave' 'Bunker cave'\n 'Bunker cave' 'Bunker cave' 'Bunker cave' 'Bunker cave' 'Bunker cave'\n 'Bunker cave' 'Bunker cave' 'Cueva de Asiul' 'Cueva de Asiul'\n 'Heshang cave' 'Heshang cave' 'Heshang cave' 'Buckeye creek'\n 'Buckeye creek' 'Buckeye creek' 'Buckeye creek' 'Buckeye creek'\n 'Buckeye creek' 'Buckeye creek' 'Grotta di Carburangeli'\n 'Grotta di Carburangeli' 'Grotta di Carburangeli' 'Dandak cave'\n 'Dandak cave' 'Dandak cave' 'Juxtlahuaca cave' 'Juxtlahuaca cave'\n 'Juxtlahuaca cave' 'Juxtlahuaca cave' 'Kinderlinskaya cave'\n 'Kinderlinskaya cave' 'Kinderlinskaya cave'\n 'Oregon caves national monument' 'Oregon caves national monument'\n 'Oregon caves national monument' 'Sanbao cave' 'Sanbao cave'\n 'Sofular cave' 'Sofular cave' 'Sofular cave' 'Cova da Arcoia'\n 'Cova da Arcoia' 'Cova da Arcoia' 'Botuver\u00e1 cave' 'Botuver\u00e1 cave'\n 'Botuver\u00e1 cave' 'Gunung-buda cave (snail shell cave)'\n 'Gunung-buda cave (snail shell cave)' 'Jhumar cave' 'Jhumar cave'\n 'Jhumar cave' 'Jiuxian cave' 'Jiuxian cave' 'Jiuxian cave' 'Jiuxian cave'\n 'KNI-51' 'KNI-51' 'KNI-51' 'KNI-51' 'KNI-51' 'KNI-51' 'KNI-51' 'KNI-51'\n 'KNI-51' 'KNI-51' 'Mavri Trypa cave' 'Mavri Trypa cave'\n 'Mavri Trypa cave' 'Munagamanu cave' 'Munagamanu cave' 'Munagamanu cave'\n 'Munagamanu cave' 'Munagamanu cave' 'Munagamanu cave' 'Soreq cave'\n 'Soreq cave' 'Te Reinga cave' 'Te Reinga cave' 'Te Reinga cave'\n 'Te Reinga cave' 'Te Reinga cave' 'Te Reinga cave' 'Liang Luar'\n 'Liang Luar' 'Perdida cave' 'Perdida cave' 'Perdida cave' 'Closani cave'\n 'Closani cave' 'Closani cave' 'Closani cave' 'Forestry cave'\n 'Forestry cave' 'Forestry cave' 'Forestry cave' 'Forestry cave'\n 'Forestry cave' 'Bribin cave' 'Bribin cave' 'Bribin cave' 'KNI-51'\n 'KNI-51' 'Lianhua cave, Hunan' 'Lianhua cave, Hunan'\n 'Lianhua cave, Hunan' 'Minnetonka cave' 'Minnetonka cave'\n 'Minnetonka cave' 'S\u00e3o Bernardo cave' 'S\u00e3o Bernardo cave'\n 'S\u00e3o Matheus cave' 'S\u00e3o Matheus cave' 'Shatuca cave' 'Shatuca cave'\n 'Shatuca cave' 'Shatuca cave' 'Tangga cave' 'Tangga cave' 'Tangga cave'\n 'Uluu-Too cave' 'Uluu-Too cave' 'Uluu-Too cave' 'Xibalba cave'\n 'Xibalba cave' 'Xibalba cave' 'Dongge cave' 'Dongge cave' 'Dos Anas cave'\n 'Dos Anas cave' 'Dos Anas cave' 'Dongge cave' 'Dongge cave'\n 'Jaragu\u00e1 cave' 'Jaragu\u00e1 cave' 'Jaragu\u00e1 cave' 'Jaragu\u00e1 cave'\n 'Jaragu\u00e1 cave' 'Jaragu\u00e1 cave' 'Mawmluh cave' 'Mawmluh cave'\n 'Mawmluh cave' 'Chaara cave' 'Chaara cave' 'Dark cave' 'Dark cave'\n \"E'mei cave\" \"E'mei cave\" \"E'mei cave\" 'Grotte de Piste'\n 'Grotte de Piste' 'Baeg-nyong cave' 'Baeg-nyong cave' 'Wanxiang cave'\n 'Wanxiang cave' 'Xianglong cave' 'Xianglong cave' 'Xianglong cave'\n 'Lianhua cave, Hunan' 'Lianhua cave, Hunan' 'Chiflonkhakha cave'\n 'Chiflonkhakha cave' 'Chiflonkhakha cave' 'Chiflonkhakha cave' 'Bat cave'\n 'Bat cave' 'Bat cave' 'Cueva del Tigre Perdido' 'Cueva del Tigre Perdido'\n 'Umajalanta cave' 'Umajalanta cave' 'Lianhua cave, Shanxi'\n 'Lianhua cave, Shanxi' 'Shenqi cave' 'Shenqi cave' 'Shenqi cave'\n 'Shenqi cave' 'Wuya cave' 'Wuya cave' 'Wuya cave' 'Wuya cave' 'Hoq cave'\n 'Hoq cave' 'Hoq cave' 'Hoq cave' 'Hoq cave' 'Hoq cave' 'Hoq cave'\n 'Hoq cave' 'Hoq cave' 'Akcakale cave' 'Akcakale cave' 'Akcakale cave'\n 'Ble\u00dfberg cave' 'Ble\u00dfberg cave' 'Ble\u00dfberg cave' 'Gejkar cave'\n 'Gejkar cave' 'Gejkar cave' 'Gejkar cave' 'Crystal cave' 'Crystal cave'\n 'Chaara cave' 'Chaara cave' 'Chaara cave' 'Chaara cave' 'El Condor cave'\n 'El Condor cave' 'El Condor cave' 'Tamboril cave' 'Tamboril cave'\n 'Tamboril cave' 'Huagapo cave' 'Huagapo cave' 'Huagapo cave'\n 'Huagapo cave' 'Huagapo cave' 'Huagapo cave' 'Pink Panther cave'\n 'Pink Panther cave' 'Kesang cave' 'Kesang cave' 'Kesang cave'\n 'La Garma cave' 'La Garma cave' 'Tham Doun Mai' 'Tham Doun Mai'\n 'Tham Doun Mai' 'Hollywood cave' 'Hollywood cave' 'Hollywood cave'\n 'Jaragu\u00e1 cave' 'Jaragu\u00e1 cave' 'Jaragu\u00e1 cave' 'Shennong cave'\n 'Shennong cave' 'Shennong cave' 'Anjohibe' 'Anjohibe' 'Anjohibe'\n 'B\u00e0sura cave' 'B\u00e0sura cave' 'B\u00e0sura cave' 'Careys cave' 'Careys cave'\n 'Careys cave' 'Careys cave' 'Cathedral cave' 'Cathedral cave'\n 'Cathedral cave' 'Cu\u00edca cave' 'Cu\u00edca cave' 'Cu\u00edca cave' 'Cu\u00edca cave'\n 'Cu\u00edca cave' 'Cu\u00edca cave' 'Golgotha cave' 'Golgotha cave' 'Golgotha cave'\n 'Golgotha cave' 'Golgotha cave' 'Golgotha cave' 'Golgotha cave'\n 'Golgotha cave' 'Golgotha cave' 'Golgotha cave' 'Golgotha cave'\n 'Golgotha cave' 'Harrie Wood cave' 'Harrie Wood cave' 'Harrie Wood cave'\n 'Harrie Wood cave' 'Harrie Wood cave' 'Harrie Wood cave'\n 'Harrie Wood cave' 'Harrie Wood cave' 'Harrie Wood cave' 'Heifeng cave'\n 'Heifeng cave' 'Herbstlabyrinth cave' 'Herbstlabyrinth cave'\n 'Herbstlabyrinth cave' 'Herbstlabyrinth cave' 'Huangchao cave'\n 'Huangchao cave' 'Huangchao cave' 'Huangchao cave' 'Ifoulki cave'\n 'Ifoulki cave' 'Ifoulki cave' 'Jinfo cave' 'Jinfo cave' 'Jiulong cave'\n 'Jiulong cave' 'Jiulong cave' 'Kuna Ba' 'Kuna Ba' 'Kuna Ba' 'Kuna Ba'\n 'La Vierge cave' 'La Vierge cave' 'La Vierge cave' 'Mata Virgem cave'\n 'Mata Virgem cave' 'Mata Virgem cave' 'Nova\\xa0Grgosova\\xa0cave'\n 'Nova\\xa0Grgosova\\xa0cave' 'Nova\\xa0Grgosova\\xa0cave' 'Coves del pirata'\n 'Coves del pirata' 'Coves del pirata' 'Coves del pirata' 'Qujia cave'\n 'Qujia cave' 'Rey Marcos' 'Rey Marcos' 'Rey Marcos'\n 'Sa balma des quart\u00f3 cave' 'Sa balma des quart\u00f3 cave'\n 'Sa balma des quart\u00f3 cave' 'Sa balma des quart\u00f3 cave'\n 'Sa balma des quart\u00f3 cave' 'Sa balma des quart\u00f3 cave'\n 'Sa balma des quart\u00f3 cave' 'Sa balma des quart\u00f3 cave'\n 'Sa balma des quart\u00f3 cave' 'Sa balma des quart\u00f3 cave'\n 'Sa balma des quart\u00f3 cave' 'Sa balma des quart\u00f3 cave' 'Shijiangjun cave'\n 'Shijiangjun cave' 'Shijiangjun cave' 'Shizi cave' 'Shizi cave'\n 'Tham Doun Mai' 'Tham Doun Mai' 'Tham Doun Mai' 'Trapi\u00e1 cave'\n 'Trapi\u00e1 cave' 'Trapi\u00e1 cave' 'Wuya cave' 'Wuya cave' 'Wuya cave'\n 'Wintimdouine' 'Wintimdouine' 'Wintimdouine' 'Wintimdouine' 'Wulu cave'\n 'Wulu cave' 'Wulu cave' 'Xiniu cave' 'Xiniu cave' 'Xiniu cave'\n 'Xiniu cave' 'Xiniu cave' 'Xiniu cave' 'Yonderup cave' 'Yonderup cave'\n 'Yonderup cave' 'Yonderup cave' 'Bunker cave' 'Bunker cave' 'Bunker cave'\n 'Bunker cave' 'Bunker cave' 'Bunker cave' 'Kocain cave' 'Kocain cave'\n 'Kocain cave' 'Kocain cave' 'S\u00e3o Bernardo cave' 'S\u00e3o Bernardo cave'\n 'S\u00e3o Bernardo cave' 'Chiflonkhakha cave' 'Chiflonkhakha cave'\n 'Chiflonkhakha cave' 'Chiflonkhakha cave' 'Chiflonkhakha cave'\n 'Chiflonkhakha cave' 'Sahiya cave' 'Sahiya cave' 'Sahiya cave'\n 'Sahiya cave' 'Sahiya cave' 'Sahiya cave']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 129/546\n</pre> In\u00a0[32]: Copied! <pre># archiveType\nkey = 'archiveType'\nprint('%s: '%key)\nprint(np.unique(df[key]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # archiveType key = 'archiveType' print('%s: '%key) print(np.unique(df[key])) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>archiveType: \n['Speleothem']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 1/546\n</pre> In\u00a0[33]: Copied! <pre># paleoData_proxy\nkey = 'paleoData_proxy'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # paleoData_proxy key = 'paleoData_proxy' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>paleoData_proxy: \n['Mg/Ca' 'd13C' 'd18O' 'growth rate']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 4/546\n</pre> In\u00a0[34]: Copied! <pre># climate_interpretation\nkey = 'paleoData_sensorSpecies'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # climate_interpretation key = 'paleoData_sensorSpecies' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')  <pre>paleoData_sensorSpecies: \n['N/A']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 1/546\n</pre> In\u00a0[35]: Copied! <pre># # paleoData_notes\nkey = 'paleoData_notes'\nprint('%s: '%key)\nprint(df[key].values)\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # # paleoData_notes key = 'paleoData_notes' print('%s: '%key) print(df[key].values) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>paleoData_notes: \n['calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite'\n 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite'\n 'calcite' 'aragonite' 'aragonite' 'aragonite' 'aragonite' 'aragonite'\n 'aragonite' 'aragonite' 'aragonite' 'aragonite' 'aragonite' 'aragonite'\n 'aragonite' 'aragonite' 'aragonite' 'calcite' 'calcite' 'calcite'\n 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite'\n 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite'\n 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite'\n 'calcite' 'calcite' 'aragonite' 'aragonite' 'aragonite' 'calcite'\n 'calcite' 'calcite' 'aragonite' 'aragonite' 'aragonite' 'aragonite'\n 'aragonite' 'mixed' 'mixed' 'mixed' 'calcite' 'calcite' 'calcite'\n 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite'\n 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite'\n 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite'\n 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'mixed' 'mixed' 'mixed'\n 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite'\n 'calcite' 'aragonite' 'aragonite' 'aragonite' 'calcite' 'calcite'\n 'calcite' 'calcite' 'calcite' 'calcite' 'mixed' 'mixed' 'mixed' 'calcite'\n 'calcite' 'calcite' 'mixed' 'mixed' 'mixed' 'mixed' 'mixed' 'mixed'\n 'aragonite' 'aragonite' 'aragonite' 'calcite' 'calcite' 'calcite' 'mixed'\n 'mixed' 'mixed' 'calcite' 'calcite' 'calcite' 'calcite' 'aragonite'\n 'aragonite' 'aragonite' 'aragonite' 'aragonite' 'aragonite' 'calcite'\n 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite'\n 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite'\n 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite'\n 'calcite' 'calcite' 'calcite' 'mixed' 'mixed' 'mixed' 'mixed' 'mixed'\n 'mixed' 'mixed' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite'\n 'calcite' 'aragonite' 'aragonite' 'aragonite' 'aragonite' 'calcite'\n 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite'\n 'calcite' 'calcite' 'calcite' 'mixed' 'mixed' 'mixed' 'calcite' 'calcite'\n 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite'\n 'calcite' 'calcite' 'calcite' 'aragonite' 'aragonite' 'aragonite'\n 'aragonite' 'aragonite' 'aragonite' 'aragonite' 'aragonite' 'aragonite'\n 'aragonite' 'calcite' 'calcite' 'calcite' 'mixed' 'mixed' 'mixed' 'mixed'\n 'mixed' 'mixed' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite'\n 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite'\n 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite'\n 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite'\n 'aragonite' 'aragonite' 'aragonite' 'aragonite' 'aragonite' 'mixed'\n 'mixed' 'mixed' 'aragonite' 'aragonite' 'aragonite' 'aragonite'\n 'aragonite' 'aragonite' 'aragonite' 'aragonite' 'aragonite' 'aragonite'\n 'aragonite' 'calcite' 'calcite' 'calcite' 'aragonite' 'aragonite'\n 'aragonite' 'mixed' 'mixed' 'calcite' 'calcite' 'calcite' 'calcite'\n 'calcite' 'aragonite' 'aragonite' 'aragonite' 'aragonite' 'aragonite'\n 'aragonite' 'aragonite' 'aragonite' 'aragonite' 'aragonite' 'aragonite'\n 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'aragonite' 'aragonite'\n 'calcite' 'calcite' 'calcite' 'calcite' 'aragonite' 'aragonite'\n 'aragonite' 'aragonite' 'aragonite' 'calcite' 'calcite' 'calcite'\n 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite'\n 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite'\n 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite'\n 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite'\n 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite'\n 'calcite' 'calcite' 'calcite' 'calcite' 'aragonite' 'aragonite'\n 'aragonite' 'aragonite' 'calcite' 'calcite' 'calcite' 'aragonite'\n 'aragonite' 'aragonite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite'\n 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite'\n 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite'\n 'aragonite' 'aragonite' 'aragonite' 'calcite' 'calcite' 'calcite' 'mixed'\n 'mixed' 'mixed' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite'\n 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite'\n 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite'\n 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite'\n 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite'\n 'calcite' 'calcite' 'calcite' 'calcite' 'mixed' 'mixed' 'calcite'\n 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite'\n 'calcite' 'calcite' 'calcite' 'mixed' 'mixed' 'calcite' 'calcite'\n 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite'\n 'calcite' 'aragonite' 'aragonite' 'aragonite' 'calcite' 'calcite'\n 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite'\n 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite'\n 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite'\n 'calcite' 'mixed' 'mixed' 'mixed' 'mixed' 'mixed' 'calcite' 'calcite'\n 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite'\n 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite'\n 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite'\n 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite'\n 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'aragonite'\n 'aragonite' 'aragonite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite'\n 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite' 'calcite']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 3/546\n</pre> In\u00a0[36]: Copied! <pre># key = 'paleoData_variableName'\n# print('%s: '%key)\n# print(np.unique([kk for kk in df[key]]))\n# print(np.unique([str(type(dd)) for dd in df[key]]))\n# print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # key = 'paleoData_variableName' # print('%s: '%key) # print(np.unique([kk for kk in df[key]])) # print(np.unique([str(type(dd)) for dd in df[key]])) # print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')  In\u00a0[37]: Copied! <pre># climate_interpretation\nkey = 'interpretation_direction'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # climate_interpretation key = 'interpretation_direction' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>interpretation_direction: \n['N/A']\nNo. of unique values: 1/546\n</pre> In\u00a0[38]: Copied! <pre># climate_interpretation\nkey = 'interpretation_seasonality'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # climate_interpretation key = 'interpretation_seasonality' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>interpretation_seasonality: \n['N/A']\nNo. of unique values: 1/546\n</pre> In\u00a0[39]: Copied! <pre># climate_interpretation\nkey = 'interpretation_variable'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # climate_interpretation key = 'interpretation_variable' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>interpretation_variable: \n['N/A' 'temperature' 'temperature+moisture']\nNo. of unique values: 3/546\n</pre> In\u00a0[40]: Copied! <pre># climate_interpretation\nkey = 'interpretation_variableDetail'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # climate_interpretation key = 'interpretation_variableDetail' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>interpretation_variableDetail: \n['N/A'\n 'temperature - manually assigned by DoD2k authors for paleoData_proxy = Mg/Ca'\n 'temperature+moisture - manually assigned by DoD2k authors for paleoData_proxy = d18O.']\nNo. of unique values: 3/546\n</pre> In\u00a0[41]: Copied! <pre># # paleoData_values\nkey = 'paleoData_values'\n\nprint('%s: '%key)\nfor ii, vv in enumerate(df[key][:20]):\n    try: \n        print('%-30s: %s -- %s'%(df['dataSetName'].iloc[ii][:30], str(np.nanmin(vv)), str(np.nanmax(vv))))\n        print(type(vv))\n    except: print(df['dataSetName'].iloc[ii], 'NaNs detected.')\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n</pre> # # paleoData_values key = 'paleoData_values'  print('%s: '%key) for ii, vv in enumerate(df[key][:20]):     try:          print('%-30s: %s -- %s'%(df['dataSetName'].iloc[ii][:30], str(np.nanmin(vv)), str(np.nanmax(vv))))         print(type(vv))     except: print(df['dataSetName'].iloc[ii], 'NaNs detected.') print(np.unique([str(type(dd)) for dd in df[key]])) <pre>paleoData_values: \nBittoo cave                   : -10.871 -- -5.274\n&lt;class 'numpy.ndarray'&gt;\nBittoo cave                   : -8.426 -- 2.907\n&lt;class 'numpy.ndarray'&gt;\nBittoo cave                   : 0.023809524 -- 0.5\n&lt;class 'numpy.ndarray'&gt;\nKesang cave                   : -9.86 -- -7.15\n&lt;class 'numpy.ndarray'&gt;\nKesang cave                   : -7.34 -- -1.85\n&lt;class 'numpy.ndarray'&gt;\nKesang cave                   : 0.011538462 -- 0.02875\n&lt;class 'numpy.ndarray'&gt;\nParaiso cave                  : -7.432 -- -5.568\n&lt;class 'numpy.ndarray'&gt;\nParaiso cave                  : -10.426 -- -7.532\n&lt;class 'numpy.ndarray'&gt;\nParaiso cave                  : 0.07721781 -- 0.9637938\n&lt;class 'numpy.ndarray'&gt;\nParaiso cave                  : -6.82 -- -4.73\n&lt;class 'numpy.ndarray'&gt;\nParaiso cave                  : -10.03 -- -6.5\n&lt;class 'numpy.ndarray'&gt;\nParaiso cave                  : 0.1243238 -- 0.3016036\n&lt;class 'numpy.ndarray'&gt;\nVillars cave                  : -5.2911587 -- -3.3193688\n&lt;class 'numpy.ndarray'&gt;\nVillars cave                  : -11.889349 -- -7.2085137\n&lt;class 'numpy.ndarray'&gt;\nVillars cave                  : 0.09545446 -- 11.775341\n&lt;class 'numpy.ndarray'&gt;\nCold Air cave                 : -5.52 -- -2.395\n&lt;class 'numpy.ndarray'&gt;\nCold Air cave                 : -6.63 -- -2.32\n&lt;class 'numpy.ndarray'&gt;\nCold Air cave                 : 0.0016826758 -- 1.0569707\n&lt;class 'numpy.ndarray'&gt;\nCold Air cave                 : -5.928387 -- -2.3865893\n&lt;class 'numpy.ndarray'&gt;\nCold Air cave                 : -5.998736 -- -2.3895519\n&lt;class 'numpy.ndarray'&gt;\n[\"&lt;class 'numpy.ndarray'&gt;\"]\n</pre> In\u00a0[42]: Copied! <pre># paleoData_units\nkey = 'paleoData_units'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # paleoData_units key = 'paleoData_units' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>paleoData_units: \n['mm/year' 'mmol/mol' 'permil']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 3/546\n</pre> In\u00a0[43]: Copied! <pre># # year\nkey = 'year'\nprint('%s: '%key)\nfor ii, vv in enumerate(df[key][:20]):\n    try: print('%-30s: %s -- %s'%(df['dataSetName'].iloc[ii][:30], str(np.nanmin(vv)), str(np.nanmax(vv))))\n    except: print('NaNs detected.', vv)\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n</pre> # # year key = 'year' print('%s: '%key) for ii, vv in enumerate(df[key][:20]):     try: print('%-30s: %s -- %s'%(df['dataSetName'].iloc[ii][:30], str(np.nanmin(vv)), str(np.nanmax(vv))))     except: print('NaNs detected.', vv) print(np.unique([str(type(dd)) for dd in df[key]])) <pre>year: \nBittoo cave                   : 11.0 -- 1076.0\nBittoo cave                   : 11.0 -- 1076.0\nBittoo cave                   : 11.0 -- 1950.0\nKesang cave                   : 632.0 -- 1098.0\nKesang cave                   : 632.0 -- 1098.0\nKesang cave                   : 640.0 -- 1950.0\nParaiso cave                  : 7.2082186 -- 1235.592\nParaiso cave                  : 7.2082186 -- 1235.592\nParaiso cave                  : 7.2082186 -- 1950.0\nParaiso cave                  : 1181.5256 -- 1998.0459\nParaiso cave                  : 1181.5256 -- 1998.0459\nParaiso cave                  : 1188.324 -- 1998.0459\nVillars cave                  : 6.147 -- 1987.727\nVillars cave                  : 6.147 -- 1987.727\nVillars cave                  : 6.147 -- 1987.727\nCold Air cave                 : 1264.2115 -- 1484.1798\nCold Air cave                 : 1264.2115 -- 1484.1798\nCold Air cave                 : 1264.2115 -- 1950.0\nCold Air cave                 : 19.0 -- 1996.0\nCold Air cave                 : 19.0 -- 1996.0\n[\"&lt;class 'numpy.ndarray'&gt;\"]\n</pre> In\u00a0[44]: Copied! <pre># yearUnits\nkey = 'yearUnits'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # yearUnits key = 'yearUnits' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>yearUnits: \n['CE']\n[\"&lt;class 'str'&gt;\"]\nNo. of unique values: 1/546\n</pre>"},{"location":"notebooks/load_sisal/#load-sisal","title":"Load SISAL\u00b6","text":""},{"location":"notebooks/load_sisal/#set-up-working-environment","title":"Set up working environment\u00b6","text":""},{"location":"notebooks/load_sisal/#load-source-data","title":"Load source data\u00b6","text":""},{"location":"notebooks/load_sisal/#read-csvs","title":"Read CSVs\u00b6","text":""},{"location":"notebooks/load_sisal/#filter-data-out-of-wanted-bounds","title":"Filter data out of wanted bounds\u00b6","text":""},{"location":"notebooks/load_sisal/#create-compact-dataframe","title":"Create compact dataframe\u00b6","text":""},{"location":"notebooks/load_sisal/#parameter-definitions","title":"Parameter definitions\u00b6","text":""},{"location":"notebooks/load_sisal/#parametermetadata-population","title":"Parameter/metadata population\u00b6","text":""},{"location":"notebooks/load_sisal/#data-cleaning-and-format-conventions","title":"Data cleaning and format conventions\u00b6","text":""},{"location":"notebooks/load_sisal/#drop-missing-entries-and-standardize-missing-data-format","title":"Drop missing entries and standardize missing data format\u00b6","text":""},{"location":"notebooks/load_sisal/#save-and-output-dataframe","title":"Save and output dataframe\u00b6","text":""},{"location":"notebooks/load_sisal/#save-pickle","title":"save pickle\u00b6","text":""},{"location":"notebooks/load_sisal/#save-csv","title":"save csv\u00b6","text":""},{"location":"notebooks/load_sisal/#visualise-dataframe","title":"Visualise dataframe\u00b6","text":""},{"location":"notebooks/load_sisal/#display-dataframe","title":"Display dataframe\u00b6","text":""},{"location":"notebooks/load_sisal/#display-identification-metadata-datasetname-datasetid-originaldataurl-originaldatabase","title":"Display identification metadata: dataSetName, datasetId, originalDataURL, originalDatabase\u00b6","text":""},{"location":"notebooks/load_sisal/#index","title":"index\u00b6","text":""},{"location":"notebooks/load_sisal/#datasetname-associated-with-each-record-may-not-be-unique","title":"dataSetName (associated with each record, may not be unique)\u00b6","text":""},{"location":"notebooks/load_sisal/#datasetid-unique-identifier-as-given-by-original-authors-includes-original-database-token","title":"datasetId (unique identifier, as given by original authors, includes original database token)\u00b6","text":""},{"location":"notebooks/load_sisal/#originaldataurl-urldoi-of-original-published-record-where-available","title":"originalDataURL (URL/DOI of original published record where available)\u00b6","text":""},{"location":"notebooks/load_sisal/#originaldatabase-original-database-used-as-input-for-dataframe","title":"originalDatabase (original database used as input for dataframe)\u00b6","text":""},{"location":"notebooks/load_sisal/#geographical-metadata-elevation-latitude-longitude-site-name","title":"geographical metadata: elevation, latitude, longitude, site name\u00b6","text":""},{"location":"notebooks/load_sisal/#geo_meanelev-mean-elevation-in-m","title":"geo_meanElev (mean elevation in m)\u00b6","text":""},{"location":"notebooks/load_sisal/#geo_meanlat-mean-latitude-in-degrees-n","title":"geo_meanLat (mean latitude in degrees N)\u00b6","text":""},{"location":"notebooks/load_sisal/#geo_meanlon-mean-longitude","title":"geo_meanLon (mean longitude)\u00b6","text":""},{"location":"notebooks/load_sisal/#geo_sitename-name-of-collection-site","title":"geo_siteName (name of collection site)\u00b6","text":""},{"location":"notebooks/load_sisal/#proxy-metadata-archive-type-proxy-type-interpretation","title":"proxy metadata: archive type, proxy type, interpretation\u00b6","text":""},{"location":"notebooks/load_sisal/#archivetype-archive-type","title":"archiveType (archive type)\u00b6","text":""},{"location":"notebooks/load_sisal/#paleodata_proxy-proxy-type","title":"paleoData_proxy (proxy type)\u00b6","text":""},{"location":"notebooks/load_sisal/#paleodata_sensorspecies-further-information-on-proxy-type-species","title":"paleoData_sensorSpecies (further information on proxy type: species)\u00b6","text":""},{"location":"notebooks/load_sisal/#paleodata_notes-notes","title":"paleoData_notes (notes)\u00b6","text":""},{"location":"notebooks/load_sisal/#paleodata_variablename","title":"paleoData_variableName\u00b6","text":""},{"location":"notebooks/load_sisal/#climate-metadata-interpretation-variable-direction-seasonality","title":"climate metadata: interpretation variable, direction, seasonality\u00b6","text":""},{"location":"notebooks/load_sisal/#interpretation_direction","title":"Interpretation_direction\u00b6","text":""},{"location":"notebooks/load_sisal/#interpretation_seasonality","title":"Interpretation_seasonality\u00b6","text":""},{"location":"notebooks/load_sisal/#interpretation_variable","title":"Interpretation_variable\u00b6","text":""},{"location":"notebooks/load_sisal/#interpretation_variabledetail","title":"Interpretation_variableDetail\u00b6","text":""},{"location":"notebooks/load_sisal/#data","title":"data\u00b6","text":""},{"location":"notebooks/load_sisal/#paleodata_values","title":"paleoData_values\u00b6","text":""},{"location":"notebooks/load_sisal/#paleodata_units","title":"paleoData_units\u00b6","text":""},{"location":"notebooks/load_sisal/#year","title":"year\u00b6","text":""},{"location":"notebooks/load_sisal/#yearunits","title":"yearUnits\u00b6","text":""},{"location":"notebooks/merge_databases/","title":"Merge original databases into a common dataframe, ready for duplicate detection.","text":"<p>Created on Mon Jul  3 12:53:15 2023</p> <p>Author: Lucie Luecke</p> <p>Update 06/11/2025 by LL: Tidied up and commented code for documentation tutorial.</p> <p>Create a common database from multiple standardised databases, based on the load notebooks:</p> <ul> <li>PAGES2k (load_pages2k.ipynb)</li> <li>FE23 (Breitenmoser 14) (load_fe23.ipynb)</li> <li>SISAL v3 (load_sisal.ipynb)</li> <li>CH2k (load_ch2k.ipynb)</li> <li>Iso2k (load_iso2k.ipynb)</li> </ul> <p>This database is subject to duplicates, so please run the duplicate detection files on the output.</p> <p>The dataframe has the data:</p> <ul> <li><code>archiveType</code></li> <li><code>dataSetName</code></li> <li><code>datasetId</code></li> <li><code>geo_meanElev</code></li> <li><code>geo_meanLat</code></li> <li><code>geo_meanLon</code></li> <li><code>geo_siteName</code></li> <li><code>interpretation_direction</code> (new in v2.0)</li> <li><code>interpretation_variable</code></li> <li><code>interpretation_variableDetail</code></li> <li><code>interpretation_seasonality</code> (new in v2.0)</li> <li><code>originalDataURL</code></li> <li><code>originalDatabase</code></li> <li><code>paleoData_notes</code></li> <li><code>paleoData_proxy</code></li> <li><code>paleoData_sensorSpecies</code></li> <li><code>paleoData_units</code></li> <li><code>paleoData_values</code></li> <li><code>paleoData_variableName</code></li> <li><code>year</code></li> <li><code>yearUnits</code></li> </ul> <p>Make sure the repo_root is added correctly, it should be: your_root_dir/dod2k This should be the working directory throughout this notebook (and all other notebooks).</p> In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n\nimport sys\nimport os\nfrom pathlib import Path\n\n# Add parent directory to path (works from any notebook in notebooks/)\n# the repo_root should be the parent directory of the notebooks folder\ninit_dir = Path().resolve()\n# Determine repo root\nif init_dir.name == 'dod2k': repo_root = init_dir\nelif init_dir.parent.name == 'dod2k': repo_root = init_dir.parent\nelse: raise Exception('Please review the repo root structure (see first cell).')\n\n# Update cwd and path only if needed\nif os.getcwd() != str(repo_root):\n    os.chdir(repo_root)\nif str(repo_root) not in sys.path:\n    sys.path.insert(0, str(repo_root))\n\nprint(f\"Repo root: {repo_root}\")\nif str(os.getcwd())==str(repo_root):\n    print(f\"Working directory matches repo root. \")\n</pre> %load_ext autoreload %autoreload 2  import sys import os from pathlib import Path  # Add parent directory to path (works from any notebook in notebooks/) # the repo_root should be the parent directory of the notebooks folder init_dir = Path().resolve() # Determine repo root if init_dir.name == 'dod2k': repo_root = init_dir elif init_dir.parent.name == 'dod2k': repo_root = init_dir.parent else: raise Exception('Please review the repo root structure (see first cell).')  # Update cwd and path only if needed if os.getcwd() != str(repo_root):     os.chdir(repo_root) if str(repo_root) not in sys.path:     sys.path.insert(0, str(repo_root))  print(f\"Repo root: {repo_root}\") if str(os.getcwd())==str(repo_root):     print(f\"Working directory matches repo root. \") <pre>Repo root: /home/jupyter-lluecke/dod2k\nWorking directory matches repo root. \n</pre> In\u00a0[2]: Copied! <pre># Import packages\nimport pandas as pd\nimport numpy as np\n\nfrom dod2k_utilities import ut_functions as utf # contains utility functions\nfrom dod2k_utilities import ut_plot as uplt # contains plotting functions\n</pre> # Import packages import pandas as pd import numpy as np  from dod2k_utilities import ut_functions as utf # contains utility functions from dod2k_utilities import ut_plot as uplt # contains plotting functions <p>Define which datasets should be loaded:</p> In\u00a0[3]: Copied! <pre>dataset_names = ['pages2k', 'fe23', 'ch2k', 'iso2k', 'sisal' ]\n</pre> dataset_names = ['pages2k', 'fe23', 'ch2k', 'iso2k', 'sisal' ] <p>Now load the dataframes and merge:</p> In\u00a0[4]: Copied! <pre># read compact dataframes from all the single databases\n\nprint(dataset_names[0])\ndf = utf.load_compact_dataframe_from_csv(dataset_names[0])\nprint('length: ', len(df))\n\nfor ii, dn in enumerate(dataset_names[1:]):\n    print(f'add {dn}')\n    new_df = utf.load_compact_dataframe_from_csv(dn)\n    df = pd.concat([df, new_df])\n    print('length: ', len(df))\n\nprint('---------------')\nprint('RESULT:')\ndf.index = range(len(df))\nprint(df.info())\n</pre> # read compact dataframes from all the single databases  print(dataset_names[0]) df = utf.load_compact_dataframe_from_csv(dataset_names[0]) print('length: ', len(df))  for ii, dn in enumerate(dataset_names[1:]):     print(f'add {dn}')     new_df = utf.load_compact_dataframe_from_csv(dn)     df = pd.concat([df, new_df])     print('length: ', len(df))  print('---------------') print('RESULT:') df.index = range(len(df)) print(df.info()) <pre>pages2k\nlength:  1191\nadd fe23\nlength:  3945\nadd ch2k\nlength:  4166\nadd iso2k\nlength:  4601\nadd sisal\nlength:  5147\n---------------\nRESULT:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5147 entries, 0 to 5146\nData columns (total 21 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   archiveType                    5147 non-null   object \n 1   dataSetName                    5147 non-null   object \n 2   datasetId                      5147 non-null   object \n 3   geo_meanElev                   5048 non-null   float32\n 4   geo_meanLat                    5147 non-null   float32\n 5   geo_meanLon                    5147 non-null   float32\n 6   geo_siteName                   5147 non-null   object \n 7   interpretation_direction       5147 non-null   object \n 8   interpretation_seasonality     5147 non-null   object \n 9   interpretation_variable        5147 non-null   object \n 10  interpretation_variableDetail  5147 non-null   object \n 11  originalDataURL                5147 non-null   object \n 12  originalDatabase               5147 non-null   object \n 13  paleoData_notes                5147 non-null   object \n 14  paleoData_proxy                5147 non-null   object \n 15  paleoData_sensorSpecies        5147 non-null   object \n 16  paleoData_units                5147 non-null   object \n 17  paleoData_values               5147 non-null   object \n 18  paleoData_variableName         5147 non-null   object \n 19  year                           5147 non-null   object \n 20  yearUnits                      5147 non-null   object \ndtypes: float32(3), object(18)\nmemory usage: 784.2+ KB\nNone\n</pre> <ol> <li>when writing OriginalDataURL, convert any ftp://ftp.ncdc to https://ncei :  e.g.</li> </ol> <p>URL 1: ftp://ftp.ncdc.noaa.gov/pub/data/paleo/contributions_by_author/keigwin2005</p> <p>write to URL 1: https://ncei.noaa.gov/pub/data/paleo/contributions_by_author/keigwin2005</p> In\u00a0[5]: Copied! <pre>for ii in df[[df['originalDataURL'].iloc[ii].startswith('ftp://ftp.ncdc') for ii in df.index]].index:\n    print('before: ', df['originalDataURL'].iloc[ii])\n    df.at[ii, 'originalDataURL'] = df.at[ii, 'originalDataURL'].replace('ftp://ftp.ncdc', 'https://ncei')\n    print('after: ', df['originalDataURL'].iloc[ii])\n    \n</pre> for ii in df[[df['originalDataURL'].iloc[ii].startswith('ftp://ftp.ncdc') for ii in df.index]].index:     print('before: ', df['originalDataURL'].iloc[ii])     df.at[ii, 'originalDataURL'] = df.at[ii, 'originalDataURL'].replace('ftp://ftp.ncdc', 'https://ncei')     print('after: ', df['originalDataURL'].iloc[ii])      <pre>before:  ftp://ftp.ncdc.noaa.gov/pub/data/paleo/paleolimnology/northamerica/canada/baffin/big-round2008.txt\nafter:  https://ncei.noaa.gov/pub/data/paleo/paleolimnology/northamerica/canada/baffin/big-round2008.txt\n</pre> <ol> <li>When writing OriginalDataURLs like this one:</li> </ol> <p>https://www.ncdc.noaa.gov/cdo/f?p=519:1:784943880673501::::P1_STUDY_ID:10492</p> <p>rewrite to a URL that is valid, e.g.</p> <p>https://www.ncei.noaa.gov/access/paleo-search/study/10492</p> In\u00a0[6]: Copied! <pre>for ii in df[[df['originalDataURL'].iloc[ii].startswith('https://www.ncdc.noaa.gov/cdo') for ii in df.index]].index:\n    print('before: ', df['originalDataURL'].iloc[ii])\n    study_id=df.at[ii, 'originalDataURL'].split('STUDY_ID:')[-1]\n    print('study_ID: ', study_id)\n    df.at[ii, 'originalDataURL'] = f'https://www.ncei.noaa.gov/access/paleo-search/study/{study_id}'\n    print('after: ', df['originalDataURL'].iloc[ii])\n    \n</pre> for ii in df[[df['originalDataURL'].iloc[ii].startswith('https://www.ncdc.noaa.gov/cdo') for ii in df.index]].index:     print('before: ', df['originalDataURL'].iloc[ii])     study_id=df.at[ii, 'originalDataURL'].split('STUDY_ID:')[-1]     print('study_ID: ', study_id)     df.at[ii, 'originalDataURL'] = f'https://www.ncei.noaa.gov/access/paleo-search/study/{study_id}'     print('after: ', df['originalDataURL'].iloc[ii])      <pre>before:  https://www.ncdc.noaa.gov/cdo/f?p=519:1:::::P1_STUDY_ID:5472\nstudy_ID:  5472\nafter:  https://www.ncei.noaa.gov/access/paleo-search/study/5472\nbefore:  https://www.ncdc.noaa.gov/cdo/f?p=519:1:0::::P1_STUDY_ID:13174\nstudy_ID:  13174\nafter:  https://www.ncei.noaa.gov/access/paleo-search/study/13174\nbefore:  https://www.ncdc.noaa.gov/cdo/f?p=519:1:::::P1_STUDY_ID:8647\nstudy_ID:  8647\nafter:  https://www.ncei.noaa.gov/access/paleo-search/study/8647\nbefore:  https://www.ncdc.noaa.gov/cdo/f?p=519:1:0::::P1_STUDY_ID:13174\nstudy_ID:  13174\nafter:  https://www.ncei.noaa.gov/access/paleo-search/study/13174\n</pre> In\u00a0[7]: Copied! <pre>db_name='all_merged'\ndf.name=db_name\nos.makedirs(f'data/{db_name}/', exist_ok=True)\n</pre> db_name='all_merged' df.name=db_name os.makedirs(f'data/{db_name}/', exist_ok=True) In\u00a0[8]: Copied! <pre># save concatenate dataframe as db_merged\ndf.to_pickle(f'data/{db_name}/{db_name}_compact.pkl')\n</pre> # save concatenate dataframe as db_merged df.to_pickle(f'data/{db_name}/{db_name}_compact.pkl') In\u00a0[9]: Copied! <pre>print(df.info())\n</pre> print(df.info()) <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5147 entries, 0 to 5146\nData columns (total 21 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   archiveType                    5147 non-null   object \n 1   dataSetName                    5147 non-null   object \n 2   datasetId                      5147 non-null   object \n 3   geo_meanElev                   5048 non-null   float32\n 4   geo_meanLat                    5147 non-null   float32\n 5   geo_meanLon                    5147 non-null   float32\n 6   geo_siteName                   5147 non-null   object \n 7   interpretation_direction       5147 non-null   object \n 8   interpretation_seasonality     5147 non-null   object \n 9   interpretation_variable        5147 non-null   object \n 10  interpretation_variableDetail  5147 non-null   object \n 11  originalDataURL                5147 non-null   object \n 12  originalDatabase               5147 non-null   object \n 13  paleoData_notes                5147 non-null   object \n 14  paleoData_proxy                5147 non-null   object \n 15  paleoData_sensorSpecies        5147 non-null   object \n 16  paleoData_units                5147 non-null   object \n 17  paleoData_values               5147 non-null   object \n 18  paleoData_variableName         5147 non-null   object \n 19  year                           5147 non-null   object \n 20  yearUnits                      5147 non-null   object \ndtypes: float32(3), object(18)\nmemory usage: 784.2+ KB\nNone\n</pre> In\u00a0[10]: Copied! <pre># save to a list of csv files (metadata, data, year)\nutf.write_compact_dataframe_to_csv(df)\n</pre> # save to a list of csv files (metadata, data, year) utf.write_compact_dataframe_to_csv(df) <pre>METADATA: datasetId, archiveType, dataSetName, geo_meanElev, geo_meanLat, geo_meanLon, geo_siteName, interpretation_direction, interpretation_seasonality, interpretation_variable, interpretation_variableDetail, originalDataURL, originalDatabase, paleoData_notes, paleoData_proxy, paleoData_sensorSpecies, paleoData_units, paleoData_variableName, yearUnits\nSaved to /home/jupyter-lluecke/dod2k/data/all_merged/all_merged_compact_%s.csv\n</pre> In\u00a0[11]: Copied! <pre># load dataframe\nprint(utf.load_compact_dataframe_from_csv(db_name).info())\nprint(df.info())\n</pre> # load dataframe print(utf.load_compact_dataframe_from_csv(db_name).info()) print(df.info()) <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5147 entries, 0 to 5146\nData columns (total 21 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   archiveType                    5147 non-null   object \n 1   dataSetName                    5147 non-null   object \n 2   datasetId                      5147 non-null   object \n 3   geo_meanElev                   5048 non-null   float32\n 4   geo_meanLat                    5147 non-null   float32\n 5   geo_meanLon                    5147 non-null   float32\n 6   geo_siteName                   5147 non-null   object \n 7   interpretation_direction       5147 non-null   object \n 8   interpretation_seasonality     5147 non-null   object \n 9   interpretation_variable        5147 non-null   object \n 10  interpretation_variableDetail  5147 non-null   object \n 11  originalDataURL                5147 non-null   object \n 12  originalDatabase               5147 non-null   object \n 13  paleoData_notes                5147 non-null   object \n 14  paleoData_proxy                5147 non-null   object \n 15  paleoData_sensorSpecies        5147 non-null   object \n 16  paleoData_units                5147 non-null   object \n 17  paleoData_values               5147 non-null   object \n 18  paleoData_variableName         5147 non-null   object \n 19  year                           5147 non-null   object \n 20  yearUnits                      5147 non-null   object \ndtypes: float32(3), object(18)\nmemory usage: 784.2+ KB\nNone\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5147 entries, 0 to 5146\nData columns (total 21 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   archiveType                    5147 non-null   object \n 1   dataSetName                    5147 non-null   object \n 2   datasetId                      5147 non-null   object \n 3   geo_meanElev                   5048 non-null   float32\n 4   geo_meanLat                    5147 non-null   float32\n 5   geo_meanLon                    5147 non-null   float32\n 6   geo_siteName                   5147 non-null   object \n 7   interpretation_direction       5147 non-null   object \n 8   interpretation_seasonality     5147 non-null   object \n 9   interpretation_variable        5147 non-null   object \n 10  interpretation_variableDetail  5147 non-null   object \n 11  originalDataURL                5147 non-null   object \n 12  originalDatabase               5147 non-null   object \n 13  paleoData_notes                5147 non-null   object \n 14  paleoData_proxy                5147 non-null   object \n 15  paleoData_sensorSpecies        5147 non-null   object \n 16  paleoData_units                5147 non-null   object \n 17  paleoData_values               5147 non-null   object \n 18  paleoData_variableName         5147 non-null   object \n 19  year                           5147 non-null   object \n 20  yearUnits                      5147 non-null   object \ndtypes: float32(3), object(18)\nmemory usage: 784.2+ KB\nNone\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>Show spatial distribution of records, show archive and proxy types</p> In\u00a0[12]: Copied! <pre># count archive types\narchive_count = {}\nfor ii, at in enumerate(set(df['archiveType'])):\n    archive_count[at] = df.loc[df['archiveType']==at, 'archiveType'].count()\n\nsort = np.argsort([cc for cc in archive_count.values()])\narchives_sorted = np.array([cc for cc in archive_count.keys()])[sort][::-1]\n\n# Specify colour for each archive (smaller archives get grouped into the same colour)\narchive_colour, major_archives, other_archives = uplt.get_archive_colours(archives_sorted, archive_count)\n\nfig = uplt.plot_geo_archive_proxy(df, archive_colour)\n</pre> # count archive types archive_count = {} for ii, at in enumerate(set(df['archiveType'])):     archive_count[at] = df.loc[df['archiveType']==at, 'archiveType'].count()  sort = np.argsort([cc for cc in archive_count.values()]) archives_sorted = np.array([cc for cc in archive_count.keys()])[sort][::-1]  # Specify colour for each archive (smaller archives get grouped into the same colour) archive_colour, major_archives, other_archives = uplt.get_archive_colours(archives_sorted, archive_count)  fig = uplt.plot_geo_archive_proxy(df, archive_colour) <pre>0 Wood 3584\n1 Speleothem 591\n2 Coral 455\n3 GlacierIce 198\n4 MarineSediment 169\n5 LakeSediment 119\n6 Documents 13\n7 GroundIce 6\n8 Sclerosponge 6\n9 Borehole 3\n10 Other 2\n11 MolluskShell 1\n</pre> <p>Now plot the coverage over the Common Era</p> In\u00a0[13]: Copied! <pre>fig = uplt.plot_coverage(df, archives_sorted, major_archives, other_archives, archive_colour)\n</pre> fig = uplt.plot_coverage(df, archives_sorted, major_archives, other_archives, archive_colour) In\u00a0[14]: Copied! <pre># # check index\nprint(df.index)\n</pre> # # check index print(df.index) <pre>RangeIndex(start=0, stop=5147, step=1)\n</pre> In\u00a0[15]: Copied! <pre># # check dataSetName\nkey = 'dataSetName'\nprint('%s: '%key)\nprint(df[key].values)\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n</pre> # # check dataSetName key = 'dataSetName' print('%s: '%key) print(df[key].values) print(np.unique([str(type(dd)) for dd in df[key]])) <pre>dataSetName: \n['Ant-WDC05A.Steig.2013' 'NAm-MtLemon.Briffa.2002'\n 'NAm-MtLemon.Briffa.2002' ... 'Sahiya cave' 'Sahiya cave' 'Sahiya cave']\n[\"&lt;class 'str'&gt;\"]\n</pre> In\u00a0[16]: Copied! <pre># # check datasetId\n\nprint(len(df.datasetId.unique()))\nprint(len(df))\nkey = 'datasetId'\nprint('%s (starts with): '%key)\nprint(df[key].values)\nprint(np.unique([str(type(dd)) for dd in df[key]]))\nprint('datasetId starts with: ', np.unique([str(dd.split('_')[0]) for dd in df[key]]))\n</pre> # # check datasetId  print(len(df.datasetId.unique())) print(len(df)) key = 'datasetId' print('%s (starts with): '%key) print(df[key].values) print(np.unique([str(type(dd)) for dd in df[key]])) print('datasetId starts with: ', np.unique([str(dd.split('_')[0]) for dd in df[key]])) <pre>5147\n5147\ndatasetId (starts with): \n['pages2k_0' 'pages2k_5' 'pages2k_6' ... 'sisal_901.0_543'\n 'sisal_901.0_544' 'sisal_901.0_545']\n[\"&lt;class 'str'&gt;\"]\ndatasetId starts with:  ['FE23' 'ch2k' 'iso2k' 'pages2k' 'sisal']\n</pre> In\u00a0[17]: Copied! <pre># originalDataURL\nkey = 'originalDataURL'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([kk for kk in df[key] if 'this' in kk]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n# 'this study' should point to the correct URL (PAGES2k)\n</pre> # originalDataURL key = 'originalDataURL' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([kk for kk in df[key] if 'this' in kk])) print(np.unique([str(type(dd)) for dd in df[key]])) # 'this study' should point to the correct URL (PAGES2k) <pre>originalDataURL: \n['This compilation' \"['10.1002/2015GL063826']\" \"['10.1002/2015gl065397']\"\n ... 'this compilation' 'www.ncdc.noaa.gov/paleo-search/study/27330'\n 'www.ncdc.noaa.gov/paleo/study/2474']\n['this compilation']\n[\"&lt;class 'str'&gt;\"]\n</pre> In\u00a0[18]: Copied! <pre># # originalDataSet\nkey = 'originalDatabase'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n# Note: the last two records have missing URLs\n</pre> # # originalDataSet key = 'originalDatabase' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) # Note: the last two records have missing URLs <pre>originalDatabase: \n['CoralHydro2k v1.0.1' 'FE23 (Breitenmoser et al. (2014))' 'Iso2k v1.1.2'\n 'PAGES 2k v2.2.0' 'SISAL v3']\n[\"&lt;class 'str'&gt;\"]\n</pre> In\u00a0[19]: Copied! <pre># check Elevation\nkey = 'geo_meanElev'\nprint('%s: '%key)\nprint(df[key])\nprint(np.unique(['%d'%kk for kk in df[key] if np.isfinite(kk)]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n</pre> # check Elevation key = 'geo_meanElev' print('%s: '%key) print(df[key]) print(np.unique(['%d'%kk for kk in df[key] if np.isfinite(kk)])) print(np.unique([str(type(dd)) for dd in df[key]])) <pre>geo_meanElev: \n0       1806.0\n1       2700.0\n2       2700.0\n3       2700.0\n4       2700.0\n         ...  \n5142    1190.0\n5143    1190.0\n5144    1190.0\n5145    1190.0\n5146    1190.0\nName: geo_meanElev, Length: 5147, dtype: float32\n['-1' '-10' '-1011' ... '991' '994' '995']\n[\"&lt;class 'float'&gt;\"]\n</pre> In\u00a0[20]: Copied! <pre># # Latitude\nkey = 'geo_meanLat'\nprint('%s: '%key)\nprint(np.unique(['%d'%kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n</pre> # # Latitude key = 'geo_meanLat' print('%s: '%key) print(np.unique(['%d'%kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) <pre>geo_meanLat: \n['-1' '-10' '-11' '-12' '-13' '-14' '-15' '-16' '-17' '-18' '-19' '-20'\n '-21' '-22' '-23' '-24' '-25' '-26' '-27' '-28' '-29' '-3' '-31' '-32'\n '-33' '-34' '-35' '-36' '-37' '-38' '-39' '-4' '-40' '-41' '-42' '-43'\n '-44' '-45' '-46' '-47' '-5' '-50' '-51' '-53' '-54' '-6' '-64' '-66'\n '-69' '-7' '-70' '-71' '-72' '-73' '-74' '-75' '-76' '-77' '-78' '-79'\n '-8' '-82' '-84' '-89' '-9' '0' '1' '10' '11' '12' '13' '15' '16' '17'\n '18' '19' '2' '20' '21' '22' '23' '24' '25' '26' '27' '28' '29' '3' '30'\n '31' '32' '33' '34' '35' '36' '37' '38' '39' '4' '40' '41' '42' '43' '44'\n '45' '46' '47' '48' '49' '5' '50' '51' '52' '53' '54' '55' '56' '57' '58'\n '59' '6' '60' '61' '62' '63' '64' '65' '66' '67' '68' '69' '7' '70' '71'\n '72' '73' '75' '76' '77' '78' '79' '8' '80' '81' '82' '9']\n[\"&lt;class 'float'&gt;\"]\n</pre> In\u00a0[21]: Copied! <pre># # Longitude \nkey = 'geo_meanLon'\nprint('%s: '%key)\nprint(np.unique(['%d'%kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n</pre> # # Longitude  key = 'geo_meanLon' print('%s: '%key) print(np.unique(['%d'%kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) <pre>geo_meanLon: \n['-1' '-10' '-100' '-101' '-102' '-103' '-104' '-105' '-106' '-107' '-108'\n '-109' '-110' '-111' '-112' '-113' '-114' '-115' '-116' '-117' '-118'\n '-119' '-12' '-120' '-121' '-122' '-123' '-124' '-125' '-126' '-127'\n '-128' '-129' '-13' '-130' '-131' '-132' '-133' '-134' '-135' '-136'\n '-137' '-138' '-139' '-140' '-141' '-142' '-143' '-144' '-145' '-146'\n '-147' '-148' '-149' '-150' '-151' '-152' '-153' '-154' '-157' '-159'\n '-16' '-160' '-161' '-162' '-163' '-169' '-17' '-174' '-18' '-19' '-2'\n '-22' '-24' '-26' '-27' '-3' '-33' '-35' '-36' '-37' '-38' '-39' '-4'\n '-41' '-42' '-43' '-44' '-45' '-46' '-47' '-49' '-5' '-50' '-51' '-54'\n '-55' '-56' '-57' '-58' '-6' '-60' '-61' '-62' '-63' '-64' '-65' '-66'\n '-67' '-68' '-69' '-7' '-70' '-71' '-72' '-73' '-74' '-75' '-76' '-77'\n '-78' '-79' '-8' '-80' '-81' '-82' '-83' '-84' '-85' '-86' '-87' '-88'\n '-89' '-9' '-90' '-91' '-92' '-93' '-94' '-95' '-96' '-97' '-98' '-99'\n '0' '1' '10' '100' '101' '102' '103' '104' '105' '106' '107' '108' '109'\n '11' '110' '111' '112' '113' '114' '115' '116' '117' '118' '119' '12'\n '120' '121' '122' '123' '124' '125' '126' '127' '128' '129' '13' '130'\n '132' '133' '134' '136' '137' '138' '14' '141' '142' '143' '144' '145'\n '146' '147' '148' '149' '15' '150' '151' '152' '153' '154' '155' '158'\n '159' '16' '160' '162' '163' '165' '166' '167' '168' '169' '17' '170'\n '171' '172' '173' '174' '175' '176' '177' '179' '18' '19' '2' '20' '21'\n '22' '23' '24' '25' '26' '27' '28' '29' '3' '30' '31' '32' '33' '34' '35'\n '36' '37' '38' '39' '4' '40' '41' '42' '43' '44' '45' '46' '49' '5' '50'\n '51' '53' '54' '55' '56' '57' '58' '59' '6' '60' '63' '64' '65' '68' '69'\n '7' '70' '71' '72' '74' '75' '76' '77' '78' '79' '8' '80' '81' '82' '83'\n '84' '85' '86' '87' '88' '89' '9' '90' '91' '92' '93' '94' '95' '96' '97'\n '98' '99']\n[\"&lt;class 'float'&gt;\"]\n</pre> In\u00a0[22]: Copied! <pre># Site Name \nkey = 'geo_siteName'\nprint('%s: '%key)\nprint(df[key].values)\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n</pre> # Site Name  key = 'geo_siteName' print('%s: '%key) print(df[key].values) print(np.unique([str(type(dd)) for dd in df[key]])) <pre>geo_siteName: \n['WDC05A' 'Mt. Lemon' 'Mt. Lemon' ... 'Sahiya cave' 'Sahiya cave'\n 'Sahiya cave']\n[\"&lt;class 'str'&gt;\"]\n</pre> In\u00a0[23]: Copied! <pre># archiveType\nkey = 'archiveType'\nprint('%s: '%key)\nprint(np.unique(df[key]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n</pre> # archiveType key = 'archiveType' print('%s: '%key) print(np.unique(df[key])) print(np.unique([str(type(dd)) for dd in df[key]])) <pre>archiveType: \n['Borehole' 'Coral' 'Documents' 'GlacierIce' 'GroundIce' 'LakeSediment'\n 'MarineSediment' 'MolluskShell' 'Other' 'Sclerosponge' 'Speleothem'\n 'Wood']\n[\"&lt;class 'str'&gt;\"]\n</pre> In\u00a0[24]: Copied! <pre># paleoData_proxy\nkey = 'paleoData_proxy'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n</pre> # paleoData_proxy key = 'paleoData_proxy' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) <pre>paleoData_proxy: \n['ARSTAN' 'Mg/Ca' 'Sr/Ca' 'TEX86' 'Uk37' 'accumulation rate' 'alkenone'\n 'borehole' 'calcification rate' 'chironomid' 'chloride'\n 'chrysophyte assemblage' 'concentration' 'count' 'd13C' 'd18O' 'dD'\n 'diatom' 'dinocyst' 'dust' 'effective precipitation' 'foraminifera'\n 'growth rate' 'historical' 'humidification index' 'ice melt'\n 'maximum latewood density' 'multiproxy' 'nitrate' 'pollen' 'reflectance'\n 'ring width' 'sodium' 'sulfate' 'temperature' 'thickness'\n 'varve thickness']\n[\"&lt;class 'str'&gt;\"]\n</pre> In\u00a0[25]: Copied! <pre># climate_interpretation\nkey = 'paleoData_sensorSpecies'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n</pre> # climate_interpretation key = 'paleoData_sensorSpecies' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]]))  <pre>paleoData_sensorSpecies: \n['ABAL' 'ABAM' 'ABBA' 'ABBO' 'ABCE' 'ABCI' 'ABCO' 'ABLA' 'ABMA' 'ABPI'\n 'ABPN' 'ABPR' 'ABSB' 'ABSP' 'ACRU' 'ACSH' 'ADHO' 'ADUS' 'AGAU' 'ARAR'\n 'ATCU' 'ATSE' 'AUCH' 'BEPU' 'CABU' 'CADE' 'CADN' 'CARO' 'CDAT' 'CDBR'\n 'CDDE' 'CDLI' 'CEAN' 'CESP' 'CHLA' 'CHNO' 'Ceratoporella nicholsoni'\n 'DABI' 'DACO' 'Diploastrea heliopora' 'Diploria labyrinthiformis'\n 'Diploria strigosa' 'FAGR' 'FASY' 'FICU' 'FRNI' 'Favia speciosa' 'HABI'\n 'Hydnophora microconos, Porites lobata' 'JGAU' 'JUEX' 'JUFO' 'JUOC'\n 'JUPH' 'JUPR' 'JURE' 'JUSC' 'JUSP' 'JUVI' 'LADE' 'LAGM' 'LALA' 'LALY'\n 'LAOC' 'LASI' 'LGFR' 'LIBI' 'LITU' 'Montastraea faveolata' 'N/A' 'NA'\n 'NOBE' 'NOGU' 'NOME' 'NOPU' 'NOSO' 'NaN' 'Orbicella faveolata'\n 'P. australiensis, possibly P. lobata' 'PCAB' 'PCEN' 'PCGL' 'PCGN' 'PCMA'\n 'PCOB' 'PCOM' 'PCPU' 'PCRU' 'PCSH' 'PCSI' 'PCSM' 'PCSP' 'PHAL' 'PHAS'\n 'PHGL' 'PHTR' 'PIAL' 'PIAM' 'PIAR' 'PIBA' 'PIBN' 'PIBR' 'PICE' 'PICL'\n 'PICO' 'PIEC' 'PIED' 'PIFL' 'PIHA' 'PIHR' 'PIJE' 'PIKO' 'PILA' 'PILE'\n 'PILO' 'PIMO' 'PIMU' 'PIMZ' 'PINI' 'PIPA' 'PIPE' 'PIPI' 'PIPN' 'PIPO'\n 'PIPU' 'PIRE' 'PIRI' 'PIRO' 'PISF' 'PISI' 'PISP' 'PIST' 'PISY' 'PITA'\n 'PITO' 'PIUN' 'PIVI' 'PIWA' 'PLRA' 'PLUV' 'PPDE' 'PPSP' 'PRMA' 'PSMA'\n 'PSME' 'PTAN' 'Pavona clavus' 'Platygyra lamellina' 'Porites'\n 'Porites austraiensis' 'Porites australiensis' 'Porites lobata'\n 'Porites lutea' 'Porites solida' 'Porites sp.' 'Pseudodiploria strigosa'\n 'QUAL' 'QUDG' 'QUFR' 'QUHA' 'QUKE' 'QULO' 'QULY' 'QUMA' 'QUMC' 'QUPE'\n 'QUPR' 'QURO' 'QURU' 'QUSP' 'QUST' 'QUVE' 'Siderastrea radians'\n 'Siderastrea siderea' 'Siderastrea sp.' 'Siderastrea stellata'\n 'Solenastrea bournoni' 'TABA' 'TADI' 'TAMU' 'TEGR' 'THOC' 'THPL' 'TSCA'\n 'TSCR' 'TSDU' 'TSHE' 'TSME' 'ULSP' 'VIKE' 'WICE' 'bournoni' 'faveolata'\n 'heliopora' 'labyrinthiformis' 'lamellina' 'lobata' 'lutea' 'nan'\n 'siderea']\n[\"&lt;class 'str'&gt;\"]\n</pre> In\u00a0[26]: Copied! <pre># # paleoData_notes\nkey = 'paleoData_notes'\nprint('%s: '%key)\nprint(df[key].values)\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n</pre> # # paleoData_notes key = 'paleoData_notes' print('%s: '%key) print(df[key].values) print(np.unique([str(type(dd)) for dd in df[key]])) <pre>paleoData_notes: \n['; climateInterpretation_seasonality changed - was originally Mean annual values; archiveType changed - was originally ice core'\n 'nan' 'nan' ... 'calcite' 'calcite' 'calcite']\n[\"&lt;class 'str'&gt;\"]\n</pre> In\u00a0[27]: Copied! <pre># paleoData_variableName\nkey = 'paleoData_variableName'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n</pre> # paleoData_variableName key = 'paleoData_variableName' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) <pre>paleoData_variableName: \n['ARSTAN' 'MAR' 'Mg/Ca' 'R650/R700' 'RABD660670' 'Sr/Ca' 'TEX86' 'Uk37'\n 'calcification rate' 'chloride' 'composite' 'concentration' 'count'\n 'd13C' 'd18O' 'd2H' 'dD' 'dust' 'effective precipitation' 'growth rate'\n 'humidification index' 'ice melt' 'maximum latewood density' 'nitrate'\n 'precipitation' 'reflectance' 'ring width' 'sodium' 'sulfate'\n 'temperature' 'thickness' 'varve thickness' 'year']\n[\"&lt;class 'str'&gt;\"]\n</pre> In\u00a0[28]: Copied! <pre># climate_interpretation\nkey = 'interpretation_direction'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # climate_interpretation key = 'interpretation_direction' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>interpretation_direction: \n['Increase' 'N/A' 'NaN' 'None' 'T_air (positive), P_amount (negative)'\n 'T_air (positive), P_amount (negative), SPEI (negative)' 'decrease'\n 'decrease/increase'\n 'depends (orbital timescale: More Indian Monsoon moisture--&gt;more enriched. Since 3ka: Indian source has been stable, so amount effect dominates: more rainfall, more intense hydrological cycle --&gt;More depleted)'\n 'increase' 'negaitive' 'negative' 'positive'\n 'positive for d18O-temperature relation, negative for d13C-precipiation amount']\nNo. of unique values: 14/5147\n</pre> In\u00a0[29]: Copied! <pre># climate_interpretation\nkey = 'interpretation_seasonality'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # climate_interpretation key = 'interpretation_seasonality' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>interpretation_seasonality: \n['Annual' 'Apr' 'Apr-Jul' 'Apr-Jun' 'Apr-Sep' 'Aug' 'Aug-Jul' 'Dec-Feb'\n 'Dec-Mar' 'Dec-May' 'Feb' 'Feb-Aug' 'Growing Season' 'Jan' 'Jan-Apr'\n 'Jan-Jun' 'Jan-Mar' 'Jul' 'Jul-Dec' 'Jul-Sep' 'Jun' 'Jun-Aug' 'Jun-Jul'\n 'Jun-Sep' 'Mar' 'Mar-Aug' 'Mar-May' 'Mar-Nov' 'Mar-Oct' 'May' 'May-Apr'\n 'May-Dec' 'May-Jul' 'May-Oct' 'May-Sep' 'N/A' 'None' 'Nov-Apr' 'Nov-Feb'\n 'Nov-Jan' 'Nov-Oct' 'Oct-Apr' 'Oct-Dec' 'Oct-Sep' 'Sep-Apr' 'Sep-Aug'\n 'Sep-Nov' 'Sep-Oct' 'Spr-Sum' 'Summer' 'Wet Season' 'Winter' 'deleteMe'\n 'subannual']\nNo. of unique values: 54/5147\n</pre> In\u00a0[30]: Copied! <pre># climate_interpretation\nkey = 'interpretation_variable'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # climate_interpretation key = 'interpretation_variable' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>interpretation_variable: \n['N/A' 'NOT temperature NOT moisture' 'moisture' 'temperature'\n 'temperature+moisture']\nNo. of unique values: 5/5147\n</pre> In\u00a0[31]: Copied! <pre># climate_interpretation\nkey = 'interpretation_variableDetail'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}')\n</pre> # climate_interpretation key = 'interpretation_variableDetail' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(f'No. of unique values: {len(np.unique(df[key]))}/{len(df)}') <pre>interpretation_variableDetail: \n['0.58 +- 0.11ppt/degrees C'\n 'Changes in annual temperature modulated by oceanic circulation and gradients'\n 'Maximum air temperature, seasonal' 'Maximum temperature' 'N/A' 'NaN'\n 'None'\n 'Original interpretation_variable: circulationIndex, interpretation_variableDetail: lake water'\n 'Original interpretation_variable: circulationVariable, interpretation_variableDetail: Indian monsoon'\n 'Original interpretation_variable: circulationVariable, interpretation_variableDetail: More negative d18O values correspond to stronger amount'\n 'Original interpretation_variable: circulationVariable, interpretation_variableDetail: N/A'\n 'Original interpretation_variable: circulationVariable, interpretation_variableDetail: tropical or North Pacific moisture'\n 'Original interpretation_variable: deleteMe, interpretation_variableDetail: N/A'\n 'Original interpretation_variable: deleteMe, interpretation_variableDetail: more positive values of d13C indicate a spread of C4 prairy grasses and decline of C3 forest plants, more positive d18O indicates evaporation of soil water which is stronger in the prairy environment than in the forsest'\n 'Original interpretation_variable: effectivePrecipitation, interpretation_variableDetail: E:P lake water'\n 'Original interpretation_variable: effectivePrecipitation, interpretation_variableDetail: LakeLevel@surface'\n 'Original interpretation_variable: effectivePrecipitation, interpretation_variableDetail: N/A'\n 'Original interpretation_variable: effectivePrecipitation, interpretation_variableDetail: Seasonal'\n 'Original interpretation_variable: effectivePrecipitation, interpretation_variableDetail: air@surface'\n 'Original interpretation_variable: effectivePrecipitation, interpretation_variableDetail: eff'\n 'Original interpretation_variable: effectivePrecipitation, interpretation_variableDetail: lake level'\n 'Original interpretation_variable: effectivePrecipitation, interpretation_variableDetail: lake water'\n 'Original interpretation_variable: effectivePrecipitation, interpretation_variableDetail: lake, winds in eastern Patagonia'\n 'Original interpretation_variable: effectivePrecipitation, interpretation_variableDetail: soil moisture'\n 'Original interpretation_variable: evaporation, interpretation_variableDetail: Aleutian Low/westerly storm trajectories'\n 'Original interpretation_variable: evaporation, interpretation_variableDetail: Indian Monsoon Strength'\n 'Original interpretation_variable: evaporation, interpretation_variableDetail: N/A'\n 'Original interpretation_variable: hydrologicBalance, interpretation_variableDetail: groundwater'\n 'Original interpretation_variable: hydrologicBalance, interpretation_variableDetail: lake water'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: Amount of rainfall change'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: Australian-Indonesian Summer monsoon; More negative d18O values correspond to stronger amount'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: Australian-Indonesian monsoon rainfall'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: Continental Sweden'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: ENSO/PDO'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: East Asian Monsoon Strength'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: East Asian Monsoon Strength; more negative values of d18O are interpreted as indicative of increased monsoon strength'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: Indian Summer Monsoon; more negative values of d18O are interpreted as indicative of increased monsoon strength'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: Lower precipitation produces higher d13C and Sr/Ca values'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: Monsoon strength'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: More negative d18O values correspond to stronger amount'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: N/A'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: Precipitation'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: SAM'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: Seasonal'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: Seasonal, annual'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: South China Sea'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: Southern Tibet'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: The interpretation is made for an older section of the sample. Last 2k data was not the focus of the manuscript'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: Variations in NAO (related to the amount of rainfall. Season not specified)'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: air@surface'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: amount of rainfall'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: d18O changes of speleothems reflect effects of temperature on raifnall d18O, rainfall amounts affect cave hydrology and biomass density above the cave, which is recorded in d13C of speleothems'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: higher values are related to less rainfall - this can be realted to less moisture influex from the Caribbean due to a southward shift of the ITCZ in phases when high amounts of meltwater enter the cooling north Atlantic Ocean; after ~4.3 ka the connection to the north Atalatic is lost and ENSO becomes more important with warm ENSO events (El Nino) causing higher d18O'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: in the southern tropical Andes'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: more negative values of d18O are interpreted as indicative of increased summer monsoon precipitation'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: more positive d18O values are interpreted to represent wetter conditions'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: precipitation'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: relative portion of summer (SAM) vs winter rainfall'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: surface'\n 'Original interpretation_variable: precipitation, interpretation_variableDetail: variations in paleoprecipitation amount on a multi-annual timescale (On longer timescales, however, the flowstone?s growth dynamics have to be considered)'\n 'Original interpretation_variable: precipitationIsotope, interpretation_variableDetail: Competing influence of polar and maritime airmasses'\n 'Original interpretation_variable: precipitationIsotope, interpretation_variableDetail: East Asian Monsoon rainfall'\n 'Original interpretation_variable: precipitationIsotope, interpretation_variableDetail: minimum temperature'\n 'Original interpretation_variable: precipitationIsotope, interpretation_variableDetail: moisture'\n 'Original interpretation_variable: precipitationIsotope, interpretation_variableDetail: of precipitation'\n 'Original interpretation_variable: precipitationIsotope, interpretation_variableDetail: precipitation'\n 'Original interpretation_variable: precipitationIsotope, interpretation_variableDetail: precipitation amount'\n 'Original interpretation_variable: precipitationIsotope, interpretation_variableDetail: rain'\n 'Original interpretation_variable: precipitationIsotope, interpretation_variableDetail: relative humidity'\n 'Original interpretation_variable: precipitationIsotope, interpretation_variableDetail: summer monsoon'\n 'Original interpretation_variable: salinity, interpretation_variableDetail: N/A'\n 'Original interpretation_variable: salinity, interpretation_variableDetail: sea surface'\n 'Original interpretation_variable: salinity, interpretation_variableDetail: surface'\n 'Original interpretation_variable: seaIce, interpretation_variableDetail: N/A'\n 'Original interpretation_variable: seasonality, interpretation_variableDetail: N/A'\n 'Original interpretation_variable: seasonality, interpretation_variableDetail: changes of d18O in speleothems reflect changes of the average d18O of rainfall in the region related to rainfall seasonality'\n 'Original interpretation_variable: seasonality, interpretation_variableDetail: relative amount of winter snowfall'\n 'Original interpretation_variable: streamflow, interpretation_variableDetail: N/A'\n 'Original interpretation_variable: streamflow, interpretation_variableDetail: lake water'\n 'Seasonal, annual' 'air' 'air-surface' 'air@600m' 'air@condensationLevel'\n 'air@surface' 'ground@surface' 'ice@surface' 'lake surface' 'lake water'\n 'lake@surface' 'near sea surface' 'regional and hemispheric temperature'\n 'sea surface' 'sea@surface' 'sea_surface' 'sub surface (30m)'\n 'sub surface (~50 m)' 'subsurface (60-80m)' 'subsurface, 136 m'\n 'subsurface, 143 m' 'surface'\n 'temperature - manually assigned by DoD2k authors for paleoData_proxy = Mg/Ca'\n 'temperature - manually assigned by DoD2k authors for paleoData_proxy = Sr/Ca'\n 'temperature+moisture - manually assigned by DoD2k authors for paleoData_proxy = d18O'\n 'temperature+moisture - manually assigned by DoD2k authors for paleoData_proxy = d18O.'\n 'variations in air temperature due to large-scale atmospheric patterns'\n 'variations in winter temperature in the Alps']\nNo. of unique values: 107/5147\n</pre> In\u00a0[32]: Copied! <pre># # paleoData_values\nkey = 'paleoData_values'\n\nprint('%s: '%key)\nfor ii, vv in enumerate(df[key][:20]):\n    try: \n        print('%-30s: %s -- %s'%(df['dataSetName'].iloc[ii][:30], str(np.nanmin(vv)), str(np.nanmax(vv))))\n        print(type(vv))\n    except: print(df['dataSetName'].iloc[ii], 'NaNs detected.')\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n</pre> # # paleoData_values key = 'paleoData_values'  print('%s: '%key) for ii, vv in enumerate(df[key][:20]):     try:          print('%-30s: %s -- %s'%(df['dataSetName'].iloc[ii][:30], str(np.nanmin(vv)), str(np.nanmax(vv))))         print(type(vv))     except: print(df['dataSetName'].iloc[ii], 'NaNs detected.') print(np.unique([str(type(dd)) for dd in df[key]])) <pre>paleoData_values: \nAnt-WDC05A.Steig.2013         : -37.1463 -- -30.6851\n&lt;class 'numpy.ndarray'&gt;\nNAm-MtLemon.Briffa.2002       : 0.154 -- 2.91\n&lt;class 'numpy.ndarray'&gt;\nNAm-MtLemon.Briffa.2002       : 0.205 -- 1.813\n&lt;class 'numpy.ndarray'&gt;\nNAm-MtLemon.Briffa.2002       : 0.283 -- 1.666\n&lt;class 'numpy.ndarray'&gt;\nNAm-MtLemon.Briffa.2002       : 0.574 -- 0.951\n&lt;class 'numpy.ndarray'&gt;\nNAm-MtLemon.Briffa.2002       : 0.707 -- 1.118\n&lt;class 'numpy.ndarray'&gt;\nNAm-MtLemon.Briffa.2002       : 0.757 -- 1.114\n&lt;class 'numpy.ndarray'&gt;\nArc-Arjeplog.Bjorklund.2014   : -3.532171 -- 2.5670047\n&lt;class 'numpy.ndarray'&gt;\nArc-Arjeplog.Bjorklund.2014   : -4.1141653 -- 2.6139\n&lt;class 'numpy.ndarray'&gt;\nAsi-CHIN019.Li.2010           : 0.298 -- 1.664\n&lt;class 'numpy.ndarray'&gt;\nNAm-Landslide.Luckman.2006    : 0.057 -- 0.76\n&lt;class 'numpy.ndarray'&gt;\nNAm-Landslide.Luckman.2006    : 0.164 -- 1.781\n&lt;class 'numpy.ndarray'&gt;\nNAm-Landslide.Luckman.2006    : 0.116 -- 1.889\n&lt;class 'numpy.ndarray'&gt;\nNAm-SmithersSkiArea.Schweingru: 0.319 -- 1.73\n&lt;class 'numpy.ndarray'&gt;\nNAm-SmithersSkiArea.Schweingru: 0.448 -- 1.741\n&lt;class 'numpy.ndarray'&gt;\nNAm-SmithersSkiArea.Schweingru: 0.472 -- 1.576\n&lt;class 'numpy.ndarray'&gt;\nNAm-SmithersSkiArea.Schweingru: 0.44 -- 0.83\n&lt;class 'numpy.ndarray'&gt;\nNAm-SmithersSkiArea.Schweingru: 0.626 -- 1.19\n&lt;class 'numpy.ndarray'&gt;\nNAm-SmithersSkiArea.Schweingru: 0.636 -- 1.179\n&lt;class 'numpy.ndarray'&gt;\nAsi-GANGCD.PAGES2k.2013       : 0.102 -- 2.109\n&lt;class 'numpy.ndarray'&gt;\n[\"&lt;class 'numpy.ndarray'&gt;\"]\n</pre> In\u00a0[33]: Copied! <pre># paleoData_units\nkey = 'paleoData_units'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n</pre> # paleoData_units key = 'paleoData_units' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) <pre>paleoData_units: \n['cm' 'cm/yr' 'count' 'count/mL' 'degC' 'g/cm/yr' 'g/cm2/yr' 'g/cm3' 'mm'\n 'mm/year' 'mm/yr' 'mmol/mol' 'nan' 'needsToBeChanged' 'ng/g' 'percent'\n 'permil' 'ppb' 'standardized_anomalies' 'unitless' 'yr AD' 'z score']\n[\"&lt;class 'str'&gt;\"]\n</pre> In\u00a0[34]: Copied! <pre># # year\nkey = 'year'\nprint('%s: '%key)\nfor ii, vv in enumerate(df[key][:20]):\n    try: print('%-30s: %s -- %s'%(df['dataSetName'].iloc[ii][:30], str(np.nanmin(vv)), str(np.nanmax(vv))))\n    except: print('NaNs detected.', vv)\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n</pre> # # year key = 'year' print('%s: '%key) for ii, vv in enumerate(df[key][:20]):     try: print('%-30s: %s -- %s'%(df['dataSetName'].iloc[ii][:30], str(np.nanmin(vv)), str(np.nanmax(vv))))     except: print('NaNs detected.', vv) print(np.unique([str(type(dd)) for dd in df[key]])) <pre>year: \nAnt-WDC05A.Steig.2013         : 786.0 -- 2005.0\nNAm-MtLemon.Briffa.2002       : 1568.0 -- 1983.0\nNAm-MtLemon.Briffa.2002       : 1568.0 -- 1983.0\nNAm-MtLemon.Briffa.2002       : 1568.0 -- 1983.0\nNAm-MtLemon.Briffa.2002       : 1568.0 -- 1983.0\nNAm-MtLemon.Briffa.2002       : 1568.0 -- 1983.0\nNAm-MtLemon.Briffa.2002       : 1568.0 -- 1983.0\nArc-Arjeplog.Bjorklund.2014   : 1200.0 -- 2010.0\nArc-Arjeplog.Bjorklund.2014   : 1200.0 -- 2010.0\nAsi-CHIN019.Li.2010           : 1509.0 -- 2006.0\nNAm-Landslide.Luckman.2006    : 913.0 -- 2001.0\nNAm-Landslide.Luckman.2006    : 913.0 -- 2001.0\nNAm-Landslide.Luckman.2006    : 913.0 -- 2001.0\nNAm-SmithersSkiArea.Schweingru: 1680.0 -- 1983.0\nNAm-SmithersSkiArea.Schweingru: 1680.0 -- 1983.0\nNAm-SmithersSkiArea.Schweingru: 1680.0 -- 1983.0\nNAm-SmithersSkiArea.Schweingru: 1680.0 -- 1983.0\nNAm-SmithersSkiArea.Schweingru: 1680.0 -- 1983.0\nNAm-SmithersSkiArea.Schweingru: 1680.0 -- 1983.0\nAsi-GANGCD.PAGES2k.2013       : 1567.0 -- 1999.0\n[\"&lt;class 'numpy.ndarray'&gt;\"]\n</pre> In\u00a0[35]: Copied! <pre># yearUnits\nkey = 'yearUnits'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n</pre> # yearUnits key = 'yearUnits' print('%s: '%key) print(np.unique([kk for kk in df[key]])) print(np.unique([str(type(dd)) for dd in df[key]])) <pre>yearUnits: \n['CE']\n[\"&lt;class 'str'&gt;\"]\n</pre> In\u00a0[36]: Copied! <pre>df[df['paleoData_proxy']=='ARSTAN']\n</pre> df[df['paleoData_proxy']=='ARSTAN'] Out[36]: archiveType dataSetName datasetId geo_meanElev geo_meanLat geo_meanLon geo_siteName interpretation_direction interpretation_seasonality interpretation_variable ... originalDataURL originalDatabase paleoData_notes paleoData_proxy paleoData_sensorSpecies paleoData_units paleoData_values paleoData_variableName year yearUnits 3 Wood NAm-MtLemon.Briffa.2002 pages2k_8 2700.0 32.500000 -110.800003 Mt. Lemon None None N/A ... https://www1.ncdc.noaa.gov/pub/data/paleo/page... PAGES 2k v2.2.0 nan ARSTAN PSME nan [1.143, 1.223, 0.876, 1.1, 1.126, 0.874, 0.679... ARSTAN [1568.0, 1569.0, 1570.0, 1571.0, 1572.0, 1573.... CE 6 Wood NAm-MtLemon.Briffa.2002 pages2k_21 2700.0 32.500000 -110.800003 Mt. Lemon None None N/A ... https://www1.ncdc.noaa.gov/pub/data/paleo/page... PAGES 2k v2.2.0 nan ARSTAN PSME nan [0.973, 0.967, 1.023, 0.946, 1.021, 0.944, 1.0... ARSTAN [1568.0, 1569.0, 1570.0, 1571.0, 1572.0, 1573.... CE 12 Wood NAm-Landslide.Luckman.2006 pages2k_39 800.0 60.200001 -138.500000 Landslide None None N/A ... https://www1.ncdc.noaa.gov/pub/data/paleo/page... PAGES 2k v2.2.0 nan ARSTAN PCGL nan [1.137, 0.858, 0.838, 1.203, 1.198, 1.131, 0.8... ARSTAN [913.0, 914.0, 915.0, 916.0, 917.0, 918.0, 919... CE 15 Wood NAm-SmithersSkiArea.Schweingruber.1996 pages2k_52 1200.0 54.900002 -127.300003 Smithers Ski Area None None N/A ... https://www1.ncdc.noaa.gov/pub/data/paleo/page... PAGES 2k v2.2.0 nan ARSTAN PCGL nan [0.97, 0.983, 0.936, 1.04, 1.088, 1.004, 1.054... ARSTAN [1680.0, 1681.0, 1682.0, 1683.0, 1684.0, 1685.... CE 18 Wood NAm-SmithersSkiArea.Schweingruber.1996 pages2k_65 1200.0 54.900002 -127.300003 Smithers Ski Area None None N/A ... https://www1.ncdc.noaa.gov/pub/data/paleo/page... PAGES 2k v2.2.0 nan ARSTAN PCGL nan [1.035, 1.06, 1.052, 0.993, 1.002, 0.948, 0.93... ARSTAN [1680.0, 1681.0, 1682.0, 1683.0, 1684.0, 1685.... CE ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 1170 Wood NAm-PethaiPeninsula.Schweingruber.1996 pages2k_3624 1400.0 62.700001 -111.000000 Pethai Peninsula None None N/A ... https://www1.ncdc.noaa.gov/pub/data/paleo/page... PAGES 2k v2.2.0 nan ARSTAN PCGL nan [0.996, 0.987, 1.01, 0.966, 0.959, 1.064, 1.03... ARSTAN [1610.0, 1611.0, 1612.0, 1613.0, 1614.0, 1615.... CE 1176 Wood NAm-TogwateePass.Briffa.1996 pages2k_3644 2820.0 43.700001 -110.099998 Togwatee Pass None None N/A ... https://www1.ncdc.noaa.gov/pub/data/paleo/page... PAGES 2k v2.2.0 nan ARSTAN PCEN nan [0.935, 0.983, 1.02, 0.992, 1.037, 0.913, 0.83... ARSTAN [1672.0, 1673.0, 1674.0, 1675.0, 1676.0, 1677.... CE 1179 Wood NAm-TogwateePass.Briffa.1996 pages2k_3657 2820.0 43.700001 -110.099998 Togwatee Pass None None N/A ... https://www1.ncdc.noaa.gov/pub/data/paleo/page... PAGES 2k v2.2.0 nan ARSTAN PCEN nan [0.955, 0.9, 0.867, 0.934, 0.935, 1.014, 0.924... ARSTAN [1672.0, 1673.0, 1674.0, 1675.0, 1676.0, 1677.... CE 1182 Wood NAm-Hilda.Kavanagh.2000 pages2k_3670 2200.0 52.200001 -117.199997 Hilda None None N/A ... https://www1.ncdc.noaa.gov/pub/data/paleo/page... PAGES 2k v2.2.0 nan ARSTAN PCEN nan [1.096, 1.175, 1.093, 1.013, 0.905, 1.049, 1.0... ARSTAN [1428.0, 1429.0, 1430.0, 1431.0, 1432.0, 1433.... CE 1185 Wood NAm-PyramidMountain.Luckman.2001 pages2k_3683 2000.0 53.000000 -118.199997 Pyramid Mountain None None N/A ... https://www1.ncdc.noaa.gov/pub/data/paleo/page... PAGES 2k v2.2.0 nan ARSTAN PCEN nan [0.843, 0.903, 0.927, 0.795, 0.898, 0.993, 0.8... ARSTAN [1625.0, 1626.0, 1627.0, 1628.0, 1629.0, 1630.... CE <p>173 rows \u00d7 21 columns</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/merge_databases/#merge-original-databases-into-a-common-dataframe-ready-for-duplicate-detection","title":"Merge original databases into a common dataframe, ready for duplicate detection.\u00b6","text":""},{"location":"notebooks/merge_databases/#set-up-working-environment","title":"Set up working environment\u00b6","text":""},{"location":"notebooks/merge_databases/#load-compact-dataframes","title":"Load compact dataframes\u00b6","text":""},{"location":"notebooks/merge_databases/#change-metadata","title":"change metadata\u00b6","text":""},{"location":"notebooks/merge_databases/#originaldataurl","title":"OriginalDataURL\u00b6","text":""},{"location":"notebooks/merge_databases/#save-merged-dataframe","title":"save merged dataframe\u00b6","text":""},{"location":"notebooks/merge_databases/#save-pickle","title":"save pickle\u00b6","text":""},{"location":"notebooks/merge_databases/#save-csv","title":"save csv\u00b6","text":""},{"location":"notebooks/merge_databases/#visualise-dataframe","title":"Visualise dataframe\u00b6","text":""},{"location":"notebooks/merge_databases/#display-dataframe","title":"Display dataframe\u00b6","text":""},{"location":"notebooks/merge_databases/#display-identification-metadata-datasetname-datasetid-originaldataurl-originaldatabase","title":"Display identification metadata: dataSetName, datasetId, originalDataURL, originalDatabase\u00b6","text":""},{"location":"notebooks/merge_databases/#index","title":"index\u00b6","text":""},{"location":"notebooks/merge_databases/#datasetname-associated-with-each-record-may-not-be-unique","title":"dataSetName (associated with each record, may not be unique)\u00b6","text":""},{"location":"notebooks/merge_databases/#datasetid-unique-identifier-as-given-by-original-authors-includes-original-database-token","title":"datasetId (unique identifier, as given by original authors, includes original database token)\u00b6","text":""},{"location":"notebooks/merge_databases/#originaldataurl-urldoi-of-original-published-record-where-available","title":"originalDataURL (URL/DOI of original published record where available)\u00b6","text":""},{"location":"notebooks/merge_databases/#originaldatabase-original-database-used-as-input-for-dataframe","title":"originalDatabase (original database used as input for dataframe)\u00b6","text":""},{"location":"notebooks/merge_databases/#geographical-metadata-elevation-latitude-longitude-site-name","title":"geographical metadata: elevation, latitude, longitude, site name\u00b6","text":""},{"location":"notebooks/merge_databases/#geo_meanelev-mean-elevation-in-m","title":"geo_meanElev (mean elevation in m)\u00b6","text":""},{"location":"notebooks/merge_databases/#geo_meanlat-mean-latitude-in-degrees-n","title":"geo_meanLat (mean latitude in degrees N)\u00b6","text":""},{"location":"notebooks/merge_databases/#geo_meanlon-mean-longitude","title":"geo_meanLon (mean longitude)\u00b6","text":""},{"location":"notebooks/merge_databases/#geo_sitename-name-of-collection-site","title":"geo_siteName (name of collection site)\u00b6","text":""},{"location":"notebooks/merge_databases/#proxy-metadata-archive-type-proxy-type-interpretation","title":"proxy metadata: archive type, proxy type, interpretation\u00b6","text":""},{"location":"notebooks/merge_databases/#archivetype-archive-type","title":"archiveType (archive type)\u00b6","text":""},{"location":"notebooks/merge_databases/#paleodata_proxy-proxy-type","title":"paleoData_proxy (proxy type)\u00b6","text":""},{"location":"notebooks/merge_databases/#paleodata_sensorspecies-further-information-on-proxy-type-species","title":"paleoData_sensorSpecies (further information on proxy type: species)\u00b6","text":""},{"location":"notebooks/merge_databases/#paleodata_notes-notes","title":"paleoData_notes (notes)\u00b6","text":""},{"location":"notebooks/merge_databases/#paleodata_variablename","title":"paleoData_variableName\u00b6","text":""},{"location":"notebooks/merge_databases/#climate-metadata-interpretation-variable-direction-seasonality","title":"climate metadata: interpretation variable, direction, seasonality\u00b6","text":""},{"location":"notebooks/merge_databases/#interpretation_direction","title":"interpretation_direction\u00b6","text":""},{"location":"notebooks/merge_databases/#interpretation_seasonality","title":"interpretation_seasonality\u00b6","text":""},{"location":"notebooks/merge_databases/#interpretation_variable","title":"interpretation_variable\u00b6","text":""},{"location":"notebooks/merge_databases/#interpretation_variabledetail","title":"interpretation_variableDetail\u00b6","text":""},{"location":"notebooks/merge_databases/#data","title":"data\u00b6","text":""},{"location":"notebooks/merge_databases/#paleodata_values","title":"paleoData_values\u00b6","text":""},{"location":"notebooks/merge_databases/#paleodata_units","title":"paleoData_units\u00b6","text":""},{"location":"notebooks/merge_databases/#year","title":"year\u00b6","text":""},{"location":"notebooks/merge_databases/#yearunits","title":"yearUnits\u00b6","text":""},{"location":"tutorial/","title":"Tutorials","text":"<p>Step-by-step guides for working with the DoD2k database, from basic usage to advanced workflows.</p> <p>New to DoD2k?</p> <p>Start with the Quickstart Guide for environment setup and first steps.</p>"},{"location":"tutorial/#core-workflows","title":"Core Workflows","text":"<ul> <li> <p> Loading &amp; Visualizing DoD2k</p> <p>Load the DoD2k database and create maps and plots of paleoclimate data</p> <p> Start tutorial</p> </li> <li> <p> Loading &amp; Merging Databases</p> <p>Load original databases from source and merge them with standardized metadata</p> <p> Start tutorial</p> </li> <li> <p> Duplicate Detection</p> <p>Run the complete duplicate detection workflow on merged databases</p> <p> Start tutorial</p> </li> <li> <p> Recreate DoD2k from Scratch</p> <p>Complete end-to-end workflow to rebuild the entire DoD2k database</p> <p> Start tutorial</p> </li> <li> <p> Applications</p> <p>Apply DoD2k to real-world paleoclimate research questions</p> <p> Start tutorial</p> </li> </ul>"},{"location":"tutorial/#learning-path","title":"Learning Path","text":"<p>Recommended progression:</p> <ol> <li>Start with Loading &amp; Visualizing to understand the database structure</li> <li>Explore Loading &amp; Merging to work with source databases</li> <li>Learn Duplicate Detection for data quality control</li> <li>Follow Recreate from Scratch for the complete pipeline</li> </ol>"},{"location":"tutorial/#prerequisites","title":"Prerequisites","text":"<p>Before starting these tutorials, ensure you have:</p> <ul> <li>\u2713 Python 3.8 or higher</li> <li>\u2713 Jupyter notebook environment</li> <li>\u2713 DoD2k repository cloned locally</li> <li>\u2713 Required dependencies installed</li> </ul> <p>See the Installation Guide for setup instructions.</p> <p>--&gt;</p>"},{"location":"tutorial/applications/","title":"Applications","text":"<p>These notebooks are tailored to analyse the database filtered for specific variables, such as <code>interpretation_variable</code>, to investigate the climatic properties of certain archive types. </p>"},{"location":"tutorial/applications/#pca-on-database-filtered-for-interpretation_variable","title":"PCA on database filtered for <code>interpretation_variable</code>","text":""},{"location":"tutorial/applications/#moisture-and-moisture-temperature-records-only","title":"Moisture and moisture-temperature records only","text":"<p>This notebook performs a principal component analysis (PCA) on moisture and moisture-temperature sensitive records in DoD 2k.</p> <p>For each subset, the following algorithm is being used:</p> <ol> <li> <p>Loads the filtered database for moisture and moisture-temperature records only</p> python3/Jupyter<pre><code>db_name = 'dod2k_v2.0_filtered_M_TM'\n\n# load dataframe\ndf = utf.load_compact_dataframe_from_csv(db_name)\nprint(df.info())\ndf.name = db_name\n</code></pre> </li> <li> <p>Filter <code>archive_type</code> and <code>paleoData_proxy</code> (defines subset) and produces summary plots of the data, in particular regarding: coverage, resolution and length of the records. This gives us information for the next step, in which we need to choose the parameters for the PCA</p> </li> </ol> <p>!!! example</p> <p>For <code>archiveType</code> Wood and <code>paleoData_proxy</code> ring width, do:</p> <pre><code>```python title='python3/Jupyter'\n# (1) filter for archiveType and/or paleoData_proxy: \n\nat = 'Wood'\npt = 'ring width'\nkey = '%s_%s'%(at, pt)\n\nkeys += [key]\n\ndf_proxy = df.copy().loc[(df['archiveType']==at)].loc[(df['paleoData_proxy']==pt)] # filter records for specific archive and proxy type\n\nn_recs = len(df_proxy) # number of records\nprint('n_records   : ', n_recs)\n\nprint('archive type: ', set(df_proxy['archiveType']))\nprint('proxy type:   ', set(df_proxy['paleoData_proxy']))\n\n# (2) plot the spatial distribution of records\ngeo_fig, col = uplt.geo_plot(df_proxy, return_col=True)\n\n# (3) plot the coverage for proxy types and plot resolution\n\nuta.convert_subannual_to_annual_res(df_proxy)\n\ndf_proxy = uta.add_auxvars_plot_summary(df_proxy, key, col=col[at])\n```\n</code></pre> <ol> <li> <p>Define proxy specific parameters for the PCA:</p> <ul> <li>period (start and end year): choose a period of sufficient data density (all records chosen for the analysis need to at least overlap during this period)</li> <li>minimum resolution: records exceeding this resolution are being excluded from the analysis. Records with higher resolution will be subsampled to create homogeneous resolution across all the records.</li> <li>record length: records shorter than the record length are being excluded from the analysis.</li> <li>The choice of parameters will determine the success of the PCA. There is a trade-off between the number of records included and the quality (i.e. period/record length/resolution).</li> <li>Summary figures are being produced for the filtered data</li> <li>z-scores added to dataframe (mean=0 and std=1 over the entire record) as 'paleoData_zscores'</li> <li>note: z-scores may be biased if records are only partly overlapping in time, or increase in availability over time, or both.</li> </ul> <p>Example</p> <p>For <code>archiveType</code> Wood and <code>paleoData_proxy</code> ring width, do:</p> python3/Jupyter<pre><code>#========================= PROXY SPECIFIC: Wood ring width =========================\nminres    = 1                         # homogenised resolution\nmny       = 1000                      # start year of homogenised time coord\nmxy       = 2000                      # end year of homogenised time coord\nnyears    = np.min([600, mxy-mny])    # minimum length of each record\n#====================================================================\n\n# filter for record length during target period\ndf_proxy = uta.filter_record_length(df_proxy, nyears, mny, mxy)\n\n# filter for resolution\ndf_proxy = uta.filter_resolution(df_proxy, minres)\n\n# plot coverage and resolution\nuplt.plot_coverage_analysis(df_proxy, np.arange(mny, mxy+minres, minres), key, col[at])\nuplt.plot_resolution(df_proxy, key, col=col[at])\nuplt.plot_length(df_proxy, key, col=col[at])\n\nn_recs = len(df_proxy) # final number of records\n\n\nprint(df_proxy[['miny', 'maxy', 'originalDatabase']])\n\npca_rec[key] = df_proxy['datasetId']\n\n# add 'z-scores' to dataframe and plot z-scores and values\ndf_proxy = uta.add_zscores_plot(df_proxy, key, plot_output=True)\n</code></pre> </li> <li> <p>Homogenise data dimensions across the records</p> <ul> <li>defines a homogenised time variable over the target period and with the target resolution (as defined in the last step), which is saved as a new column in the dataframe named 'years_hom'</li> <li>creates a data matrix with dimensions n_records x n_time which is saved as a new column in df, named <code>paleoData_values_hom</code> and <code>paleoData_zscores_hom</code>.</li> <li>Note that this data is formatted as a np.ma.masked_array, where missing data is set to zero and masked out.</li> </ul> <p>```python title='python3/Jupyter'</p> </li> <li> <p>PCA</p> <ul> <li>obtains covariance matrix of <code>paleoData_zscores_hom</code> (note that for every two records the covariance is calculated over their intersect of data availability)</li> <li>obtains eigenvectors and eigenvalues via SVD composition</li> <li>obtains and plots fraction of explained variance, first two PCs and load for first two EOFs vs ordering in the data frame.</li> </ul> </li> </ol> <p>See the notebook analysis_moisttemp.ipynb</p>"},{"location":"tutorial/applications/#define-new-homogenised-time-coordinate","title":"define new homogenised time coordinate","text":"<p>df_proxy, years_hom = uta.homogenise_time(df_proxy, mny, mxy, minres) time[key] = years_hom</p>"},{"location":"tutorial/applications/#psm-on-database-filtered-for-archivetype-and-paleodata_proxy","title":"PSM on database filtered for <code>archiveType</code> and <code>paleoData_proxy</code>","text":""},{"location":"tutorial/applications/#speleothem-delta18o-calcite-records","title":"Speleothem $\\delta^{18}O$ calcite records","text":"<p>See the notebook analysis_speleothem.ipynb</p>"},{"location":"tutorial/duplicate/","title":"Run the duplicate detection workflow to generate a duplicate free dataframe","text":"<p>This workflow runs a duplicate detection, decision and removal algorithm to generate a duplicate free dataframe. </p>"},{"location":"tutorial/duplicate/#required-columns","title":"Required columns","text":"<p>The input dataframe must have the following columns:</p> <ul> <li><code>archiveType</code>       (used for duplicate detection algorithm)</li> <li><code>dataSetName</code></li> <li><code>datasetId</code></li> <li><code>geo_meanElev</code>      (used for duplicate detection algorithm)</li> <li><code>geo_meanLat</code>       (used for duplicate detection algorithm)</li> <li><code>geo_meanLon</code>       (used for duplicate detection algorithm)</li> <li><code>geo_siteName</code>      (used for duplicate detection algorithm)</li> <li><code>interpretation_direction</code></li> <li><code>interpretation_seasonality</code></li> <li><code>interpretation_variable</code></li> <li><code>interpretation_variableDetails</code></li> <li><code>originalDataURL</code></li> <li><code>originalDatabase</code></li> <li><code>paleoData_notes</code></li> <li><code>paleoData_proxy</code>   (used for duplicate detection algorithm)</li> <li><code>paleoData_units</code></li> <li><code>paleoData_values</code>  (used for duplicate detection algorithm, test for correlation, RMSE, correlation of 1st difference, RMSE of 1st difference)</li> <li><code>paleoData_variableName</code></li> <li><code>year</code>              (used for duplicate detection algorithm)</li> <li><code>yearUnits</code></li> </ul> <p>Output Location</p> <p>All outputs are saved as <code>csv</code> in the directory <code>data/DATABASENAME/dup_detection</code>.</p>"},{"location":"tutorial/duplicate/#step-1-duplicate-detection-dup_detectionipynb","title":"Step 1: Duplicate detection (<code>dup_detection.ipynb</code>)","text":"<p>Notebook: <code>dup_detection.ipynb</code></p> <p>This interactive notebook (<code>dup_detection.ipynb</code>) runs a duplicate detection algorithm for a specific database. </p>"},{"location":"tutorial/duplicate/#11-set-up-working-environment","title":"1.1 Set up working environment","text":"<p>Make sure the repo_root is added correctly: <code>your_root_dir/dod2k</code> This should be the working directory throughout this notebook (and all other notebooks). The following libraries are required to run this notebook</p> python3/Jupyter<pre><code>import pandas as pd\nimport numpy as np\n\nfrom dod2k_utilities import ut_functions as utf # contains utility functions\nfrom dod2k_utilities import ut_duplicate_search as dup # contains utility functions\n</code></pre>"},{"location":"tutorial/duplicate/#12-load-the-compact-dataframe","title":"1.2 Load the compact dataframe","text":"<p>Define the dataset which needs to be screened for duplicates. Input files for the duplicate detection mechanism need to be compact dataframes (<code>pandas</code> dataframes with standardised columns and entry formatting). </p> <p>The function <code>load_compact_dataframe_from_csv</code> loads the dataframe from a <code>csv</code> file from <code>data\\DB\\</code>, with <code>DB</code> the name of the database. The database name (<code>db_name</code>) can be  - <code>pages2k</code> - <code>ch2k</code> - <code>iso2k</code> - <code>sisal</code> - <code>fe23</code></p> <p>for the individual databases, or </p> <ul> <li><code>all_merged</code></li> </ul> <p>to load the merged database of all individual databases, or can be any user defined compact dataframe.</p> <p>Load the dataframe using python3/Jupyter<pre><code>db_name='all_merged' \ndf = utf.load_compact_dataframe_from_csv(db_name)\n</code></pre></p>"},{"location":"tutorial/duplicate/#13-run-the-duplicate-detection-algorithm","title":"1.3 Run the duplicate detection algorithm","text":"<p>Now run the first part of the duplicate detection algorithm, which goes through each candidate pair and evaluates the pairs according to a defined set of criteria.</p> python3/Jupyter<pre><code>dup.find_duplicates_optimized(df, n_points_thresh=10)\n</code></pre> <p>Output: <code>data/DB/dup_detection/dup_detection_candidates_DB.csv</code></p> <p>Detection Criteria</p> <ul> <li>metadata criteria:</li> <li>archive types (<code>archiveType</code>) must be identical</li> <li>proxy types (<code>paleoData_proxy</code>) must be identical</li> <li>geographical criteria:</li> <li>elevation (<code>geo_meanElev</code>) similar, within defined tolerance (use kwarg <code>elevation_tolerance</code>, defaults to 0)</li> <li>latitude and longtitude (<code>geo_meanLat</code> and <code>geo_meanLon</code>) similar, within defined tolerance in km (use kwarg <code>dist_tolerance_km</code>, defaults to 8 km)</li> <li>overlap criterion:</li> <li>time must overlap for at least $n$ points (use kwarg <code>n_points_thresh</code> to modify, defaults to $n=10$) unless at least one of the record is shorter than <code>n_points_thresh</code> </li> <li>site criterion:</li> <li>there must be some overlap in the site name (<code>geo_siteName</code>)</li> <li>correlation criteria:</li> <li>correlation between the overlapping period must be greater than defined threshold (use <code>corr_thresh</code> to modify, defaults to 0.9) or correlation of first difference must be greater than defined threshold (use <code>corr_diff_thresh</code> to modify, defaults to 0.9)</li> <li>RMSE of overlapping period must be smaller than defined threshold (use <code>rmse_thresh</code> to modify, defaults to 0.1) or RMSE of first difference must be smaller than defined threshold (use <code>rmse_diff_thresh</code> to modify, defaults to 0.1)</li> <li>URL criterion:</li> <li>URLs (<code>originalDataURL</code>) must be identical if both records originate from the same database (<code>originalDatabase</code> must be identical)</li> </ul> <p>Flagging Logic</p> <p>A potential duplicate candidate pair is flagged, if all of these criteria are satisfied OR the correlation between the candidates is particularly high (&gt;0.98), while there is sufficient overlap (as defined by the overlap criterion).</p> Tip for large databases <p>The duplicate detection algorithm can take a while to run, especially for large databases (such as the merged database with over 5000 records).  Instead of running this notebook interactively, it might therefore be better to execute it as a python script via the command line.</p> <p>In order to do this, run</p> bash<pre><code>cd ~/dod2k_v2.0/dod2k\nmkdir -p scripts\njupyter nbconvert --to python notebooks/dup_detection.ipynb --stdout | \\\n  sed 's/^get_ipython()/# get_ipython()/' | \\\n  sed 's/^\\([[:space:]]*\\)%/\\1# %/' &gt; scripts/dup_detection.py\n</code></pre> <p>This generates a script <code>dup_detection.py</code> from the command line. Make sure you have modified this file to load the correct database before executing. Then run  bash<pre><code>python scripts/dup_detection.py\n</code></pre></p> <p>Optional: Plot flagged candidate pairs. Figures are saved to <code>figs/DB/dup_detection/</code>.</p> python3/Jupyter<pre><code>dup.plot_duplicates(df, save_figures=True)\n</code></pre> <p>Note</p> <p>These same figures are used in the duplicate decision process.</p>"},{"location":"tutorial/duplicate/#step-2-duplicate-decisions-dup_decisionipynb","title":"Step 2: Duplicate decisions (<code>dup_decision.ipynb</code>)","text":"<p>Notebook: <code>dup_decision.ipynb</code></p> <p>This interactive notebook (<code>dup_decision.ipynb</code>) runs a duplicate decision algorithm for a specific database, following the identification of the potential duplicate candidate pairs. The algorithm walks the operator through each of the detected duplicate candidate pairs from <code>dup_detection.ipynb</code> and runs a decision process to decide whether to keep or reject the identified records.  </p>"},{"location":"tutorial/duplicate/#21-initialisation","title":"2.1 Initialisation","text":"<p>To set up the working directory and load the compact dataframe, please follow the instructions detailed in  steps 1.1 (set up working directory) and 1.2 (load compact dataframe). </p> <p>In addition, the operator is asked to provide their credentials along with the decision process. Please fill in your details:</p> python3/Jupyter<pre><code>initials = 'FN'\nfullname = 'Full Name'\nemail    = 'name@email.ac.uk'\noperator_details = [initials, fullname, email]\n</code></pre> <p>Why Credentials?</p> <ul> <li>Initials label intermediate output files</li> <li>Name and email ensure transparency and traceability</li> </ul>"},{"location":"tutorial/duplicate/#22-hierarchy-for-duplicate-removal-for-identical-duplicates","title":"2.2 Hierarchy for duplicate removal for identical duplicates","text":"<p>For automated decisions, which apply to identical duplicates, we have defined a hierarchy (importance level) to the databases, which automatically decides which record should be kept in case of identical data and metadata.</p> <p>The hierarchy is assigned to the original databases, from 1 the highest value (should always be kept) to the lowest value $n$ (the number of original databases). The hierarchy is added to the dataframe as an additional column (<code>Hierarchy</code>) for the decision process. </p> <p>The hierarchy is added to the dataframe </p> <p>python3/Jupyter<pre><code># implement hierarchy for automated decisions for identical records\n\ndf = dup.define_hierarchy(df, hierarchy='default')\n</code></pre> By default the hierarchy uses the novelty of the databases for determining the importance level: </p> <p><code>PAGES 2k v2.2.0</code> &gt; <code>SISAL v3</code> &gt; <code>CoralHydro2k v1.0.1</code> &gt; <code>Iso2k v1.1.2</code> &gt; <code>FE23 (Breitenmoser et al. (2014))</code></p> <p>Info</p> <p>The hierarchy can be changed by providing a dictionary to the <code>hierarchy</code> kwarg: python3/Jupyter<pre><code>df = define_hierarchy(df)  # Use default hierarchy\ncustom = {'PAGES 2k v2.2.0', 'Hierarchy': 2, 'SISAL v3': 1, 'FE23 (Breitenmoser et al. (2014))': 3, 'CoralHydro2k v1.0.': 4, 'Iso2k v1.1.2': 5}\ndf = define_hierarchy(df, hierarchy=custom)  # Custom hierarchy\n</code></pre></p> <p>Note</p> <p>The hierarchy is not saved in the final duplicate-free database.</p> <p>In order to reduce the operator workload, you also have the option to implement an automatic choice for specific database combinations. Please also specify a reason when doing so!</p> <p>This is meant to be for any records which do not satisfy the hierarchy criterion, i.e. records with different data but identical metadata, such as updated records. </p> <p>If you do not wish to do this, delete <code>automate_db_choice</code> from kwargs or set to <code>False</code> (default).</p> <p>For example we have set </p> python3/Jupyter<pre><code>automate_db_choice = {'preferred_db': 'FE23 (Breitenmoser et al. (2014))', \n                      'rejected_db': 'PAGES 2k v2.2.0', \n                      'reason': 'conservative replication requirement'}\n</code></pre>"},{"location":"tutorial/duplicate/#23-duplicate-decision-process","title":"2.3 Duplicate decision process","text":"<p>Run the decision algorithm: python3/Jupyter<pre><code>dup.duplicate_decisions_multiple(df, operator_details=operator_details, choose_recollection=True, \n                                 remove_identicals=True, backup=True, comment=True, automate_db_choice=automate_db_choice)\n</code></pre> Decision options for each pair:</p> <ul> <li>keep both records</li> <li>keep just one record</li> <li>delete both records</li> <li>create composite of both records.</li> </ul> <p>Automated Decisions</p> <ul> <li>Recollections/updates: automatically selected</li> <li>Identical duplicates: highest hierarchy record kept automatically</li> <li>Automate db choice: as described previously</li> </ul> <p>Example prompts:</p> Figure 1: Summary figure of a potential duplicate candidate pair, for which the operator is asked to make a decision. <p><pre><code>**Decision required for this duplicate pair (see figure above).**\nBefore inputting your decision.\nWould you like to leave a comment on your decision process?\n**COMMENT** Please type your comment here and/or press enter.\n</code></pre> <pre><code> **DECISION** Keep record 1 (pages2k_50, blue circles) [1],\nrecord 2 (FE23_northamerica_canada_cana091, red crosses) [2],\nkeep both [b], keep none [n] or create a composite of both records [c]?\nNote: only overlapping timesteps are being composited. [Type 1/2/b/n/c]:\n</code></pre></p> <p>Output: <code>data/DB/dup_detection/dup_decisions_dod2k_dupfree_INITIALS_DATE.csv</code></p> <p>Figures: <code>figs/dup_detection/DB/</code> (linked in output CSV)</p> <p>Backup &amp; Resume</p> <p>The process creates backup files in <code>data/DB/dup_detection/</code>. If interrupted, you can resume from the backup.</p> <p>Handling of multiple duplicates</p> <p>The decision process is currently not optimised for handling of multiple duplicates (i.e. records which have more than one potential duplicate candidate), going through the duplicates on a pair-by-pair basis. However, <code>dup.duplicate_decisions_multiple</code> includes improved handling of multiple duplicates. For any records which are associated with multiple duplicates, all the other duplicate candidates are shown alongside the summary figure for the duplicate candidate pair. Any previous decisions, when available, are shown besides the <code>datasetId</code>, <code>archiveType</code>, <code>paleoData_proxy</code> etc.:</p> <pre><code>***ATTENTION*** THIS RECORD IS ASSOCIATED WITH MULTIPLE DUPLICATES! \nPLEASE PAY SPECIAL ATTENTION WHEN MAKING DECISIONS FOR THIS RECORD!\nThe potential duplicates also associated with this record are:\n Dataset ID          : iso2k_786\n     - URL                 : https://www.ncdc.noaa.gov/paleo/study/1856\n</code></pre> <p> Figure 2: Summary figure for multiple duplicates. </p> <p>The operator can then make an informed decision for each candidate pair.</p> Can I Reverse a Decision? <p>There is currently no option to reverse a decision while running the duplicate decisions.  However should the operator want to revise a previous decision they have two options: </p> <ol> <li> <p>Most recent decision: Interrupt the process, remove the last line from the backup file (<code>data/DB/dup_detection/dup_decisions_DB_INITIALS_BACKUP.csv</code>), then restart.</p> </li> <li> <p>Any decision: Interrupt and directly edit the backup file columns 'Decision 1' and 'Decision 2'. Use only: <code>KEEP</code>, <code>REMOVE</code>, or <code>COMPOSITE</code>.</p> </li> <li> <p>After completion: Manually edit the final output file with correct terminology.</p> </li> </ol>"},{"location":"tutorial/duplicate/#step-3-duplicate-removal-dup_removalipynb","title":"Step 3: Duplicate removal (<code>dup_removal.ipynb</code>)","text":"<p>Notebook: <code>dup_removal.ipynb</code></p> <p>This notebook removes duplicates based on the operator's previous decisions (see Step 2).</p>"},{"location":"tutorial/duplicate/#31-initialisation","title":"3.1 Initialisation","text":"<p>To set up the working directory and load the compact dataframe, please follow the instructions detailed in steps 1.1 (set up working directory), 1.2 (load compact dataframe) and 2.1 (provide operator credentials).</p> <p>In addition, <code>datasetId</code> is set as dataframe index to reliably identify the duplicates later on: python3/Jupyter<pre><code>df.set_index('datasetId', inplace = True)\ndf['datasetId']=df.index\n</code></pre></p>"},{"location":"tutorial/duplicate/#32-load-duplicate-decisions-from-csv","title":"3.2 Load duplicate decisions from csv","text":"<p>In order to load the duplicate decisions from csv, the operator initials and the date need to be specified, to match the desired decision output file.</p> <p>Accordingly, the decision output file is loaded from <code>data/DBNAME/dup_detection/dup_decisions_DBNAME_INITIALS_DATE.csv</code>:</p> python3/Jupyter<pre><code>filename      = f'data/{df.name}/dup_detection/dup_decisions_{df.name}_{initials}_{date}'\ndata, header  = dup.read_csv(filename, header=True)\ndf_decisions  = pd.read_csv(filename+'.csv', header=5)\n</code></pre> <p><code>dup.read_csv</code> reads the <code>header</code>, which provides the operator's details as saved in the decision file, along with any comments on the general decision process. Later in the notebook, <code>header</code> is written into a metadata file which should be provided alongside the duplicate free dataset. <code>df_decisions</code> is a <code>pandas</code> dataframe which is populated with the decision data, record by record, and will be used to implement the decisions to create a duplicate free dataset.</p>"},{"location":"tutorial/duplicate/#33-implement-duplicate-decisions","title":"3.3 Implement duplicate decisions","text":"<p>From <code>df_decisions</code> we extract a dictionary which includes all decisions for each individual record (instead of pairwise decisions as in <code>df_decisions</code>):</p> <p>python3/Jupyter<pre><code># Collect decisions for each record\ndecisions = dup.collect_record_decisions(df_decisions)\n</code></pre> This dictionary can be used to identify and track decisions for multiple duplicates (records which are associated with more than one duplicate candidate pair).</p> <p>Note</p> <p>Note that any one record can appear more than once and have multiple decisions associated with it (e.g. <code>'REMOVE'</code>, <code>'KEEP'</code> or <code>'COMPOSITE'</code>).</p> <p>In order to remove the duplicates we therefore implement the following steps:</p> <ol> <li>Remove all records from the dataframe which are associated with the decision <code>'REMOVE'</code> or <code>COMPOSITE</code> -&gt; <code>df_cleaned</code></li> <li>Create composites of the <code>COMPOSITE</code> records -&gt; <code>df_composite</code></li> <li>Check for records which have multiple decisions associated. These are potentially remaining duplicates.</li> </ol> <p>We also extract the details of each decisions, which will later be used to populate the field <code>duplicateDetails</code> in the final dataframe (the output of this notebook). The details provide information on the nature of the decision (automatically determined or manually, i.e. by the operator), as well as operator's comments. </p> python3/Jupyter<pre><code># Collect duplicate details for each record\ndup_details = dup.collect_dup_details(df_decisions, header)\n</code></pre>"},{"location":"tutorial/duplicate/#331-records-to-be-removed","title":"3.3.1. Records to be removed","text":"<p>First simply remove all the records to which the decision <code>REMOVE</code> or <code>COMPOSITE</code> applies to and store in <code>df_cleaned</code>, while all <code>'REMOVE'</code> or <code>'COMPOSITE'</code> type records are stored in <code>df_duplica_rmv</code> (for later inspection).</p> python3/Jupyter<pre><code># load the records TO BE REMOVED OR COMPOSITED\nremove_IDs  = list(df_decisions['datasetId 1'][np.isin(df_decisions['Decision 1'],['REMOVE', 'COMPOSITE'])])\nremove_IDs += list(df_decisions['datasetId 2'][np.isin(df_decisions['Decision 2'],['REMOVE', 'COMPOSITE'])])\nremove_IDs  = np.unique(remove_IDs)\n\ndf_duplica =  df.loc[remove_IDs, 'datasetId'] # df containing only records which were removed\ndf_cleaned =  df.drop(remove_IDs) # df freed from 'REMOVE' type duplicates\n\nprint(f'Removed {len(df_duplica)} REMOVE or COMPOSITE type records.')\nprint(f'REMOVE type duplicate free dataset contains {len(df_cleaned)} records.')\nprint('Removed the following IDs:', remove_IDs)\n</code></pre> <p><code>df_cleaned</code> then contains all data apart from records which are marked as <code>REMOVE</code> or <code>COMPOSITE</code>. Thus, it only keeps the records which either were never marked as duplicates or where the operator had decided to keep a duplicate. </p> <p>Note that the <code>duplicateDetails</code> need to be added to <code>df_cleaned</code> via</p> python3/Jupyter<pre><code>df_cleaned['duplicateDetails']='N/A'\nfor ID in dup_details:\n    if ID in df_cleaned.index: \n        if df_cleaned.at[ID, 'duplicateDetails']=='N/A': \n            df_cleaned.at[ID, 'duplicateDetails']=dup_details[ID]\n        else: df_cleaned.at[ID, 'duplicateDetails']+=dup_details[ID]\n</code></pre>"},{"location":"tutorial/duplicate/#332-records-to-be-composited","title":"3.3.2. Records to be composited","text":"<p>Now identify all the records to which the decision <code>'COMPOSITE'</code> applies to, create composites and store in <code>df_composite</code>.  For differences in the numerical metadata we use the average (e.g. <code>geo_meanLat</code>, <code>geo_meanLon</code>, ...), while for string types we merge the strings to form a composite. The <code>datasetId</code> is created from both original values to <code>'f{df.name}_composite_z_{ID_1}_{ID_2}'</code>, with <code>ID_1</code> and <code>ID_2</code> the original <code>datasetId</code> for each record. The data is being composited by averaging the z-scores of the original data. </p> <p>python3/Jupyter<pre><code># add the column 'duplicateDetails' to df, in case it does not exist\nif 'duplicateDetails' not in df.columns: df['duplicateDetails']='N/A'\n\n# load the records to be composited\ncomp_ID_pairs = df_decisions[(df_decisions['Decision 1']=='COMPOSITE')&amp;(df_decisions['Decision 2']=='COMPOSITE')]\n\n# create new composite data and metadata from the pairs\n# loop through the composite pairs and check metadata\ndf_composite = dup.join_composites_metadata(df, comp_ID_pairs, df_decisions, header)\n</code></pre> The function <code>join_composites_metadata</code> also creates summary figures of the composites in order to supervise the composition process. </p>"},{"location":"tutorial/duplicate/#333-check-for-multiple-duplicate-records-with-different-decisions","title":"3.3.3. Check for multiple duplicate records with different decisions","text":"<p>In order to obtain the duplicate free dataframe we merge <code>df_cleaned</code> and <code>df_composite</code>:</p> python3/Jupyter<pre><code>tmp_df_dupfree = pd.concat([df_cleaned, df_composite])\ntmp_df_dupfree.index = tmp_df_dupfree['datasetId']\ntmp_decisions = decisions.copy()\n</code></pre> <p>This dataframe initiates a loop in which the records which are associated with multiple decisions are fed into another round of duplicate detection, decisions and removal. This is necessary to ensure that no duplicates remain in the merged dataframe because of combined decisions.</p> <p>Example</p> <ul> <li> <p><code>REMOVE</code>/<code>KEEP</code> and <code>COMPOSITE</code>:</p> <ul> <li>duplicate pair <code>a</code> and <code>b</code> have had the decisions assigned: <code>a</code> \u2192 <code>REMOVE</code>, <code>b</code> \u2192 <code>KEEP</code></li> <li>duplicate pair <code>a</code> and <code>c</code> have had the decisions assigned: <code>a</code> \u2192 <code>COMPOSITE</code>, <code>c</code> \u2192 <code>COMPOSITE</code></li> <li>In this case, <code>b</code> and <code>ac</code> (the composite record of <code>a</code> and <code>c</code>) would be duplicates in the merged dataframe</li> </ul> </li> <li> <p><code>REMOVE</code>/<code>KEEP</code> &amp; <code>REMOVE</code>/<code>KEEP</code></p> <ul> <li>duplicate pair <code>a</code> and <code>b</code> have had the decisions assigned: <code>a</code> \u2192 <code>REMOVE</code>, <code>b</code> \u2192 <code>KEEP</code></li> <li>duplicate pair <code>a</code> and <code>c</code> have had the decisions assigned: <code>a</code> \u2192 <code>REMOVE</code>, <code>c</code> \u2192 <code>KEEP</code></li> <li>In this case, <code>a</code> would be removed, but <code>b</code> and <code>c</code> will be kept and would be duplicates in the merged dataframe</li> </ul> </li> <li> <p><code>COMPOSITE</code> \u00d7 2</p> <ul> <li>duplicate pair <code>a</code> and <code>b</code> have had the decisions assigned: <code>a</code> \u2192 <code>COMPOSITE</code>, <code>b</code> \u2192 <code>COMPOSITE</code></li> <li>duplicate pair <code>a</code> and <code>c</code> have had the decisions assigned: <code>a</code> \u2192 <code>COMPOSITE</code>, <code>c</code> \u2192 <code>COMPOSITE</code></li> <li>In this case, <code>ab</code> and <code>ac</code> would be duplicates in the merged dataframe</li> </ul> </li> </ul> <p>The loop iterates for a maximum of ten, but stops as soon as no duplicates are detected anymore in the dataframe subset. Note that this loop only checks among the records associated with more than one decision. In each iteration, the operator also has the opportunity to end the duplicate search. Note also that it is not advised to create multiple iterations of composites. </p> python3/Jupyter<pre><code># Simple composite tracking for debugging only\ncomposite_log = []\n\nfor ii in range(10): \n    tmp_df_dupfree.set_index('datasetId', inplace = True)\n    tmp_df_dupfree['datasetId']=tmp_df_dupfree.index\n\n    print('-'*20)\n    print(f'ITERATION # {ii}')\n\n    multiple_dups = []\n    for id in tmp_decisions.keys():\n        if len(tmp_decisions[id]) &gt; 1:\n            if id not in multiple_dups:\n                multiple_dups.append(id)\n\n    if len(multiple_dups) &gt; 0:\n        # Check which of the multiple duplicate IDs are still in the dataframe\n        multiple_dups_new = []\n        current_ids = set(tmp_df_dupfree.index)  # Get all current IDs as a set\n\n        for id in multiple_dups:\n            if id in current_ids:  # Simple membership check\n                multiple_dups_new.append(id)\n\n        if len(multiple_dups_new) &gt; 0:\n            print(f'WARNING! Decisions associated with {len(multiple_dups_new)} multiple duplicates in the new dataframe.')\n            print('Please review these records below and run through a further duplicate detection workflow until no more duplicates are found.')\n        else:\n            print('No more multiple duplicates found in current dataframe.')\n            print('SUCCESS!!')\n            break\n    else:\n        print('No more multiple duplicates.')\n        print('SUCCESS!!')\n        break\n\n    # Now we create a small dataframe which needs to be checked for duplicates.\n    df_check = tmp_df_dupfree.copy()[np.isin(tmp_df_dupfree['datasetId'], multiple_dups_new)]\n    print('Check dataframe: ')\n    df_check.name = 'tmp'\n    df_check.index = range(len(df_check))\n    print(df_check.info())\n    # We then run a brief duplicate detection algorithm on the dataframe. Note that by default the composited data has the highest value in the hierarchy.\n    pot_dup_IDs = dup.find_duplicates_optimized(df_check, n_points_thresh=10, return_data=True)\n    if len(pot_dup_IDs)==0:\n        print('SUCCESS!! NO MORE DUPLICATES DETECTED!!')\n        break\n    else:\n        yn=''\n        while yn not in ['y', 'n']:\n            yn = input('Do you want to continue with the decision process for duplicates? [y/n]')\n        if yn=='n': break\n\n    df_check = dup.define_hierarchy(df_check)\n    dup.duplicate_decisions_multiple(df_check, operator_details=operator_details, choose_recollection=True, \n                            remove_identicals=False, backup=False, comment=False)\n    # implement the decisions\n    tmp_df_decisions  = pd.read_csv(f'data/{df_check.name}/dup_detection/dup_decisions_{df_check.name}_{initials}_{date}'+'.csv', header=5)\n    tmp_dup_details   = dup.provide_dup_details(tmp_df_decisions, header)\n\n\n    # decisions\n    tmp_decisions = {}\n    for ind in tmp_df_decisions.index:\n        id1, id2   = tmp_df_decisions.loc[ind, ['datasetId 1', 'datasetId 2']]\n        dec1, dec2 = tmp_df_decisions.loc[ind, ['Decision 1', 'Decision 2']]\n        for id, dec in zip([id1, id2], [dec1, dec2]):\n            if id not in tmp_decisions: tmp_decisions[id] = []\n            tmp_decisions[id]+=[dec]\n\n    df_check.set_index('datasetId', inplace = True)\n    df_check['datasetId']=df_check.index\n\n    #drop all REMOVE or COMPOSITE types\n    tmp_remove_IDs  = list(tmp_df_decisions['datasetId 1'][np.isin(tmp_df_decisions['Decision 1'],['REMOVE', 'COMPOSITE'])])\n    tmp_remove_IDs += list(tmp_df_decisions['datasetId 2'][np.isin(tmp_df_decisions['Decision 2'],['REMOVE', 'COMPOSITE'])])\n    tmp_remove_IDs = np.unique(tmp_remove_IDs)#[id for id in np.unique(tmp_remove_IDs) if id not in tmp_remove_IDs]\n    tmp_df_cleaned = tmp_df_dupfree.drop(tmp_remove_IDs) # df freed from 'REMOVE' type duplicates\n\n    # # composite the \n    tmp_comp_ID_pairs = tmp_df_decisions[(tmp_df_decisions['Decision 1']=='COMPOSITE')&amp;(tmp_df_decisions['Decision 2']=='COMPOSITE')]\n\n    if len(tmp_comp_ID_pairs) &gt; 0:\n        for _, pair in tmp_comp_ID_pairs.iterrows():\n            id1, id2 = pair['datasetId 1'], pair['datasetId 2']\n            # Log what was composited\n            composite_log.append({\n                'iteration': ii,\n                'composited': [id1, id2],\n                'new_id': f\"{id1}_{id2}_composite\"  # or however you generate it\n            })\n    # # create new composite data and metadata from the pairs\n    # # loop through the composite pairs and check metadata\n    tmp_df_composite = dup.join_composites_metadata(df_check, tmp_comp_ID_pairs, tmp_df_decisions, header)\n\n    tmp_df_dupfree = pd.concat([tmp_df_cleaned, tmp_df_composite])\n    print('--'*20)\n    print('Finished iteration.')\n\n    print('NEW DATAFRAME:')\n    print(tmp_df_dupfree.info())\n\n    print('--'*20)\n    print('--'*20)\n    if ii==19: print('STILL DUPLICATES PRESENT AFTER MULTIPLE ITERATIONS! REVISE DECISION PROCESS!!')\n\n    print('--'*20)\n\nprint(f\"Created {len(composite_log)} composites across all iterations\")\n</code></pre> <p>As soon as no more duplicates are detected among the remaining candidates, the loop outputs:</p> <pre><code>No more multiple duplicates.\nSUCCESS!!\n</code></pre>"},{"location":"tutorial/duplicate/#34-check-entire-dataframe-for-remaining-duplicates","title":"3.4 Check entire dataframe for remaining duplicates","text":"<p>In order to check that all duplicates have definitely been removed from the dataframe, we run another round of duplicate detection, decisions and removal, using a similar workflow as in the previous step:</p> python3/Jupyter<pre><code>tmp_df_dupfree.set_index('datasetId', inplace = True)\ntmp_df_dupfree['datasetId']=tmp_df_dupfree.index\n\n# Now we create a  dataframe which needs to be checked for duplicates.\ndf_check = tmp_df_dupfree.copy()\ndf_check.name = 'tmp'\ndf_check.index = range(len(df_check))\n# We then run a brief duplicate detection algorithm on the dataframe. Note that by default the composited data has the highest value in the hierarchy.\npot_dup_IDs = dup.find_duplicates_optimized(df_check, n_points_thresh=10, return_data=True)\nif len(pot_dup_IDs)==0:\n    print('SUCCESS!! NO MORE DUPLICATES DETECTED!!')\nelse:\n    df_check = dup.define_hierarchy(df_check)\n    dup.duplicate_decisions_multiple(df_check, operator_details=operator_details, choose_recollection=True, \n                            remove_identicals=False, backup=False)\n    # implement the decisions\n    tmp_df_decisions  = pd.read_csv(f'data/{df_check.name}/dup_detection/dup_decisions_{df_check.name}_{initials}_{date}'+'.csv', header=5)\n    tmp_dup_details   = dup.provide_dup_details(tmp_df_decisions, header)\n\n\n    # decisions\n    tmp_decisions = {}\n    for ind in tmp_df_decisions.index:\n        id1, id2   = tmp_df_decisions.loc[ind, ['datasetId 1', 'datasetId 2']]\n        dec1, dec2 = tmp_df_decisions.loc[ind, ['Decision 1', 'Decision 2']]\n        for id, dec in zip([id1, id2], [dec1, dec2]):\n            if id not in tmp_decisions: tmp_decisions[id] = []\n            tmp_decisions[id]+=[dec]\n\n    df_check.set_index('datasetId', inplace = True)\n    df_check['datasetId']=df_check.index\n\n    #drop all REMOVE or COMPOSITE types\n    tmp_remove_IDs  = list(tmp_df_decisions['datasetId 1'][np.isin(tmp_df_decisions['Decision 1'],['REMOVE', 'COMPOSITE'])])\n    tmp_remove_IDs += list(tmp_df_decisions['datasetId 2'][np.isin(tmp_df_decisions['Decision 2'],['REMOVE', 'COMPOSITE'])])\n    tmp_remove_IDs = np.unique(tmp_remove_IDs)#[id for id in np.unique(tmp_remove_IDs) if id not in tmp_remove_IDs]\n    tmp_df_cleaned = tmp_df_dupfree.drop(tmp_remove_IDs) # df freed from 'REMOVE' type duplicates\n\n    # # composite the \n    tmp_comp_ID_pairs = tmp_df_decisions[(tmp_df_decisions['Decision 1']=='COMPOSITE')&amp;(tmp_df_decisions['Decision 2']=='COMPOSITE')]\n\n    # # create new composite data and metadata from the pairs\n    # # loop through the composite pairs and check metadata\n    tmp_df_composite = dup.join_composites_metadata(df_check, tmp_comp_ID_pairs, tmp_df_decisions, header)\n\n    tmp_df_dupfree = pd.concat([tmp_df_cleaned, tmp_df_composite])\n\n    print('Finished last round of duplicate removal.')\n    print('Potentially run through this cell again to check for remaining duplicates.')\n</code></pre> <p>Warning</p> <p>This step runs an entire duplicate detection and thus can take a substantial amount of time, as previously. Alternatively, you can skip this step, output the dataframe and feed it back into <code>dup_detection.ipynb</code> and repeat the duplicate workflow.</p>"},{"location":"tutorial/duplicate/#35-save-duplicate-free-dataframe","title":"3.5 Save duplicate free dataframe","text":"<p>Once the operator is satisfied that no more duplicates remain, the final dataframe can be created</p> python3/Jupyter<pre><code>df_dupfree = tmp_df_dupfree\nprint(df_dupfree.info())\n</code></pre> <p>and saved via </p> python3/Jupyter<pre><code>df_dupfree = df_dupfree[sorted(df_dupfree.columns)]\ndf_dupfree.name =f'{df.name}_{initials}_{date}_dupfree'\nos.makedirs(f'data/{df_dupfree.name}/', exist_ok=True)\n\n\nutf.write_compact_dataframe_to_csv(df_dupfree)\n</code></pre> <p>In order to provide the associated operator's information (such as details, date of creation and operator's comments), we also create the README file:</p> python3/Jupyter<pre><code># write header with operator information as README txt file\nfile = open(f'data/{df_dupfree.name}/{df_dupfree.name}_dupfree_README.txt', 'w')\nfor line in header:\n    file.write(line+'\\n')\nfile.close()\n</code></pre> <p>Workflow Complete</p> <p>The duplicate detection workflow is now finished!</p> <p>Info</p> <p>For more details on the interactive notebooks, see  1. dup_detection.ipynb 2. dup_decision.ipynb 3. dup_removal.ipynb</p>"},{"location":"tutorial/from_scratch/","title":"Generate DoD2k from scratch","text":"<p>This tutorial guides you through the complete end-to-end workflow to recreate the DoD2k database from original source data.</p>"},{"location":"tutorial/from_scratch/#complete-workflow-summary","title":"Complete Workflow Summary","text":"<pre><code>graph LR\n    A[Load&lt;br/&gt;Databases] --&gt; B[Merge]\n    B --&gt; C[Detect&lt;br/&gt;Duplicates]\n    C --&gt; D[Review]\n    D --&gt; E[Remove]\n    E --&gt; F{Residual Duplicates?}\n    F --&gt;|Yes| C\n    F --&gt;|No| G[Final&lt;br/&gt;DoD2k]</code></pre> <p>Time Investment</p> <p>Recreating DoD2k from scratch is a substantial undertaking. Consider using the pre-built DoD2k v2.0 for most applications.</p>"},{"location":"tutorial/from_scratch/#step-1-load-the-input-databases-from-source","title":"Step 1: Load the input databases from source","text":"<p>Load each database using the interactive notebooks in <code>dod2k/notebooks/</code>. Each notebook is named <code>load_DB.ipynb</code> for database <code>DB</code>.</p> Load PAGES 2k data from source <p>Notebook: load_pages2k.ipynb</p> <p>This notebook loads PAGES 2k data from LiPDverse (currently version 2.2.0) and creates a standardised compact dataframe.</p> <p>Download the data from source:</p> python3/Jupyter<pre><code># Download the file\n\n!wget -O data/pages2k/Pages2kTemperature2_2_0.pkl https://lipdverse.org/Pages2kTemperature/current_version/Pages2kTemperature2_2_0.pkl\n</code></pre> <p>This will download the pickle file <code>Pages2kTemperature2_2_0.pkl</code> into <code>data/pages2k</code>.</p> <p>Subsequently, run the interactive notebook to create a set of <code>csv</code> files containing the compact dataframe.</p> Load FE23 data from source <p>Notebook: load_fe23.ipynb</p> <p>This notebook loads FE23 from NCEI and creates a standardised compact dataframe.</p> <p>Download the data from source:</p> python3/Jupyter<pre><code># download and unzip FE23 \n!wget -O /data/fe23/franke2022-fe23.nc https://www.ncei.noaa.gov/pub/data/paleo/contributions_by_author/franke2022/franke2022-fe23.nc\nfe23_full  = xr.open_dataset('fe23/franke2022-fe23.nc')\n\n# save slice of FE23 with only relevant variables as netCDF (fe23_full is 25GB)\nfe23_slice = fe23_full[vars]\nfe23_slice.to_netcdf('data/fe23/franke2022-fe23_slice.nc')\n</code></pre> <p>This will download the netCDF file <code>franke2022-fe23.nc</code> into <code>data/fe23</code>. </p> <p>Large Dataset</p> <p><code>franke2022-fe23.nc</code> is a very large dataset (~25GB). Consider using the slice provided in data/fe23/franke2022-fe23_slice.nc which contains only the desired variables.</p> <p>Subsequently, run the interactive notebook to create a set of <code>csv</code> files containing the compact dataframe.</p> Load Iso2k data from source <p>Notebook: load_iso2k.ipynb</p> <p>This notebook loads Iso2k (v1.1.2) from LiPDverse and creates a standardised compact dataframe.</p> <p>In order to load the data from source please activate the following cell:</p> python3/Jupyter<pre><code># Download the file (use -O to specify output filename)\n!wget -O data/iso2k/iso2k1_1_2.zip https://lipdverse.org/iso2k/current_version/iso2k1_1_2.zip\n\n# Unzip to the correct destination\n!unzip data/iso2k/iso2k1_1_2.zip -d data/iso2k/iso2k1_1_2\n</code></pre> <p>This will download the zip file <code>iso2k1_1_2.zip</code> into <code>data/iso2k</code> and unzip into the directory <code>data/iso2k/iso2k1_1_2</code>.</p> <p>Subsequently, run the interactive notebook to create a set of <code>csv</code> files containing the compact dataframe.</p> Load SISAL data from source <p>Notebook: load_sisal.ipynb</p> <p>Download a set of CSV files from ORA and run the notebook to create a standardised compact dataframe.</p> Load CoralHydro2k data from source <p>Notebook: load_ch2k.ipynb</p> <p>This notebook loads CoralHydro2k from LiPDverse and creates a standardised compact dataframe.</p> <p>Download and extract the data:</p> python3/Jupyter<pre><code># Download the file (use -O to specify output filename)\n!wget -O data/ch2k/CoralHydro2k1_0_1.zip https://lipdverse.org/CoralHydro2k/current_version/CoralHydro2k1_0_1.zip\n\n# Unzip to the correct destination\n!unzip data/ch2k/CoralHydro2k1_0_1.zip -d data/ch2k/ch2k_101\n</code></pre> <p>This will download the zip file <code>CoralHydro2k1_0_1.zip</code> into <code>data/ch2k</code> and unzip into the directory <code>data/ch2k/ch2k_101</code>.</p> <p>Subsequently, run the interactive notebook to create a set of <code>csv</code> files containing the compact dataframe.</p> <p>See Also</p> <p>For detailed information on the loading process, see Loading &amp; Merging Databases.</p>"},{"location":"tutorial/from_scratch/#step-2-merge-the-databases","title":"Step 2: Merge the databases","text":"<p>Notebook: merge_databases.ipynb</p> <p>After running all the load notebooks, the next step is to merge the standardised compact dataframes into a single database.</p> <p>Before merging the databases, make sure that all the databases are included by defining</p> python3/Jupyter<pre><code>dataset_names = ['pages2k', 'fe23', 'ch2k', 'iso2k', 'sisal' ]\n</code></pre> <p>The merged compact dataframe is saved in <code>data/all_merged/</code>.</p> <p>See Also</p> <p>For detailed merging instructions, see Loading &amp; Merging Databases.</p>"},{"location":"tutorial/from_scratch/#step-3-run-the-duplicate-detection-workflow","title":"Step 3: Run the duplicate detection workflow","text":"<p>The merged dataset must go through the three-step duplicate workflow.</p> <p>In the duplicate workflow, potential duplicate candidates are first flagged. While some duplicates are obviously identical, in which case one of the records is automatically removed, a number of duplicates require expert decisions. The operator then has to go through these candidate pairs and manually inspect the potential duplicates and make a decision for each of those pairs.  Ultimately, the decisions are implemented and the identified duplicates are removed from the dataset. </p> <p>See Also</p> <p>For complete details on the duplicate detection workflow, see Duplicate Detection Tutorial.</p>"},{"location":"tutorial/from_scratch/#31-duplicate-detection","title":"3.1 Duplicate detection","text":"<p>Notebook: dup_detection.ipynb</p> <p>Set up the environment and load the merged dataset: python3/Jupter<pre><code>db_name='all_merged' \ndf = utf.load_compact_dataframe_from_csv(db_name)\ndf.name = db_name\n</code></pre></p> <p>Important</p> <p>Make sure to set <code>df.name = db_name</code> for proper output tagging.</p> <p>Run the duplicate detection algorithm: python3/Jupyter<pre><code>dup.find_duplicates_optimized(df, n_points_thresh=10)\n</code></pre></p> <p>Output: <code>data/all_merged/dup_detection/dup_detection_candidates_all_merged.csv</code></p> Run as Script for Large Databases <p>The duplicate detection algorithm does not require any user input. It can therefore be easier to convert the interactive notebook to a python script and run it from the command line. Do this by python3/Jupyter<pre><code>cd ~/dod2k_v2.0/dod2k\nmkdir -p scripts\njupyter nbconvert --to python notebooks/dup_detection.ipynb --stdout | \\\nsed 's/^get_ipython()/# get_ipython()/' | \\\nsed 's/^\\([[:space:]]*\\)%/\\1# %/' &gt; scripts/dup_detection.py\n</code></pre> and subsequently  python3/Jupyter<pre><code>python scripts/dup_detection.py\n</code></pre></p>"},{"location":"tutorial/from_scratch/#32-duplicate-decisions","title":"3.2 Duplicate decisions","text":"<p>Notebook: dup_decision.ipynb</p> <p>This step requires operator input to classify detected duplicate candidates.</p> <p>Initialization:</p> <ol> <li>Set up environment (see Section 1.1 of Duplicate Detection Tutorial)</li> <li>Load the merged dataframe (see Section 1.2)</li> <li>Provide operator credentials: python3/Jupyter<pre><code>initials = 'FN'\nfullname = 'Full Name'\nemail = 'name@email.ac.uk'\noperator_details = [initials, fullname, email]\n</code></pre></li> </ol> <p>Automate recurrent decisions, using the default hierarchy and an additional automated preference criterion for specific database combinations: python3/Jupyter<pre><code># implement hierarchy for automated decisions for identical records\n\ndf = dup.define_hierarchy(df, hierarchy='default')\n</code></pre></p> python3/Jupyter<pre><code># automate database choice for specific database cominations\nautomate_db_choice = {'preferred_db': 'FE23 (Breitenmoser et al. (2014))', \n                      'rejected_db': 'PAGES 2k v2.2.0', \n                      'reason': 'conservative replication requirement'}\n</code></pre> <p>Run the decision process: python3/Jupyter<pre><code>dup.duplicate_decisions_multiple(df, operator_details=operator_details, choose_recollection=True, \n                                 remove_identicals=True, backup=True, comment=True, automate_db_choice=automate_db_choice)\n</code></pre> Output: <code>data/all_merged/dup_detection/dup_decisions_all_merged_INITIALS_DATE.csv</code></p> <p>For each candidate pair, the operator decides to:</p> <ul> <li>Keep both records</li> <li>Keep one record only</li> <li>Delete both records</li> <li>Create a composite</li> </ul> <p>Automated vs Manual Decisions</p> <ul> <li>Automated: Identical duplicates are handled based on hierarchy</li> <li>Manual: Ambiguous cases require operator review with summary figures</li> </ul> <p>!! tip \"Backup &amp; Resume Functionality\"</p> <pre><code>The decision process automatically creates backup files in `data/all_merged/dup_detection/`. If your session is interrupted, you can restart the process and it will resume from where the backup left off. This is especially useful for large databases with hundreds of duplicate pairs.\n</code></pre>"},{"location":"tutorial/from_scratch/#33-remove-duplicates","title":"3.3 Remove Duplicates","text":"<p>Notebook: dup_removal.ipynb</p> <p>Implement the decisions to create the final duplicate-free database.</p> <p>Setup: python3/Jupyter<pre><code># Set index\ndf.set_index('datasetId', inplace = True)\ndf['datasetId']=df.index\n\n# Load decisions (specify your initials and date)\nfilename = f'data/{df.name}/dup_detection/dup_decisions_{df.name}_{initials}_{date}'\ndata, header = dup.read_csv(filename, header=True)\ndf_decisions = pd.read_csv(filename+'.csv', header=5)\n\n# Collect decisions for each record\ndecisions = dup.collect_record_decisions(df_decisions)\n\n# Collect duplicate details for each record\ndup_details = dup.collect_dup_details(df_decisions, header)\n</code></pre></p> <p>Implementation workflow:</p> <p>The notebook processes duplicates in four steps:</p> <ol> <li> <p>Remove duplicate record: All records flagged for removal or compositing are saved in <code>df_duplica</code> (for inspection) and dropped from the cleaned dataframe <code>df_cleaned</code></p> python3/Jupyter<pre><code># load the records TO BE REMOVED OR COMPOSITED\nremove_IDs  = list(df_decisions['datasetId 1'][np.isin(df_decisions['Decision 1'],['REMOVE', 'COMPOSITE'])])\nremove_IDs += list(df_decisions['datasetId 2'][np.isin(df_decisions['Decision 2'],['REMOVE', 'COMPOSITE'])])\nremove_IDs  = np.unique(remove_IDs)\n\ndf_duplica =  df.loc[remove_IDs, 'datasetId'] # df containing only records which were removed\ndf_cleaned =  df.drop(remove_IDs) # df freed from 'REMOVE' type duplicates\n\n# also add columns on decision process to df_cleaned:\ndf_cleaned['duplicateDetails']='N/A'\nfor ID in dup_details:\n    if ID in df_cleaned.index: \n        if df_cleaned.at[ID, 'duplicateDetails']=='N/A': \n            df_cleaned.at[ID, 'duplicateDetails']=dup_details[ID]\n        else: df_cleaned.at[ID, 'duplicateDetails']+=dup_details[ID]\n</code></pre> </li> <li> <p>Create composites: Records marked as <code>COMPOSITE</code> are averaged (z-scores for data values, means for coordinates) and given new composite IDs. Summary figures are generated for quality control.</p> python3/Jupyter<pre><code># load the records to be composited\ncomp_ID_pairs = df_decisions[(df_decisions['Decision 1']=='COMPOSITE')&amp;(df_decisions['Decision 2']=='COMPOSITE')]\n\n# create new composite data and metadata from the pairs\n# loop through the composite pairs and check metadata\ndf_composite = dup.join_composites_metadata(df, comp_ID_pairs, df_decisions, header)\n</code></pre> </li> <li> <p>Join and check for overlapping decisions: The duplicate free dataframe is obtained by joining </p> </li> <li><code>df_cleaned</code> (duplicate free as all records with decision <code>REMOVE</code> and/or <code>COMPOSITE</code> removed) and</li> <li><code>df_composite</code> (dupicate free as duplicates are composited)</li> </ol> python3/Jupyter<pre><code>    tmp_df_dupfree = pd.concat([df_cleaned, df_composite])\n    tmp_df_dupfree.index = tmp_df_dupfree['datasetId']\n</code></pre> <p>!!! info 'Remove remaining multiple duplicates' </p> <pre><code>There might still be duplicates between the two dataframes: when a record has been associated with more than 1 duplicate candidate pair. Therefore, we loop through the records in the joined dataframe which have been associated with multiple duplicates.\n\n```python title='python3/Jupyter'\n# initiate the loop\ntmp_df_dupfree = pd.concat([df_cleaned, df_composite])\ntmp_df_dupfree.index = tmp_df_dupfree['datasetId']\ntmp_decisions = decisions.copy()\n\ncomposite_log = []\nfor ii in range(10): \n    tmp_df_dupfree.set_index('datasetId', inplace = True)\n    tmp_df_dupfree['datasetId']=tmp_df_dupfree.index\n\n    print('-'*20)\n    print(f'ITERATION # {ii}')\n\n    multiple_dups = []\n    for id in tmp_decisions.keys():\n        if len(tmp_decisions[id]) &gt; 1:\n            if id not in multiple_dups:\n                multiple_dups.append(id)\n\n    if len(multiple_dups) &gt; 0:\n        # Check which of the multiple duplicate IDs are still in the dataframe\n        multiple_dups_new = []\n        current_ids = set(tmp_df_dupfree.index)  # Get all current IDs as a set\n\n        for id in multiple_dups:\n            if id in current_ids:  # Simple membership check\n                multiple_dups_new.append(id)\n\n        if len(multiple_dups_new) &gt; 0:\n            print(f'WARNING! Decisions associated with {len(multiple_dups_new)} multiple duplicates in the new dataframe.')\n            print('Please review these records below and run through a further duplicate detection workflow until no more duplicates are found.')\n        else:\n            print('No more multiple duplicates found in current dataframe.')\n            print('SUCCESS!!')\n            break\n    else:\n        print('No more multiple duplicates.')\n        print('SUCCESS!!')\n        break\n\n    # Now we create a small dataframe which needs to be checked for duplicates.\n    df_check = tmp_df_dupfree.copy()[np.isin(tmp_df_dupfree['datasetId'], multiple_dups_new)]\n    print('Check dataframe: ')\n    df_check.name = 'tmp'\n    df_check.index = range(len(df_check))\n    print(df_check.info())\n    # We then run a brief duplicate detection algorithm on the dataframe. Note that by default the composited data has the highest value in the hierarchy.\n    pot_dup_IDs = dup.find_duplicates_optimized(df_check, n_points_thresh=10, return_data=True)\n    if len(pot_dup_IDs)==0:\n        print('SUCCESS!! NO MORE DUPLICATES DETECTED!!')\n        break\n    else:\n        yn=''\n        while yn not in ['y', 'n']:\n            yn = input('Do you want to continue with the decision process for duplicates? [y/n]')\n        if yn=='n': break\n\n    df_check = dup.define_hierarchy(df_check)\n    dup.duplicate_decisions_multiple(df_check, operator_details=operator_details, choose_recollection=True, \n                            remove_identicals=False, backup=False, comment=False)\n    # implement the decisions\n    tmp_df_decisions  = pd.read_csv(f'data/{df_check.name}/dup_detection/dup_decisions_{df_check.name}_{initials}_{date}'+'.csv', header=5)\n    tmp_dup_details   = dup.provide_dup_details(tmp_df_decisions, header)\n\n\n    # decisions\n    tmp_decisions = {}\n    for ind in tmp_df_decisions.index:\n        id1, id2   = tmp_df_decisions.loc[ind, ['datasetId 1', 'datasetId 2']]\n        dec1, dec2 = tmp_df_decisions.loc[ind, ['Decision 1', 'Decision 2']]\n        for id, dec in zip([id1, id2], [dec1, dec2]):\n            if id not in tmp_decisions: tmp_decisions[id] = []\n            tmp_decisions[id]+=[dec]\n\n    df_check.set_index('datasetId', inplace = True)\n    df_check['datasetId']=df_check.index\n\n    #drop all REMOVE or COMPOSITE types\n    tmp_remove_IDs  = list(tmp_df_decisions['datasetId 1'][np.isin(tmp_df_decisions['Decision 1'],['REMOVE', 'COMPOSITE'])])\n    tmp_remove_IDs += list(tmp_df_decisions['datasetId 2'][np.isin(tmp_df_decisions['Decision 2'],['REMOVE', 'COMPOSITE'])])\n    tmp_remove_IDs = np.unique(tmp_remove_IDs)#[id for id in np.unique(tmp_remove_IDs) if id not in tmp_remove_IDs]\n    tmp_df_cleaned = tmp_df_dupfree.drop(tmp_remove_IDs) # df freed from 'REMOVE' type duplicates\n\n    # # composite the \n    tmp_comp_ID_pairs = tmp_df_decisions[(tmp_df_decisions['Decision 1']=='COMPOSITE')&amp;(tmp_df_decisions['Decision 2']=='COMPOSITE')]\n\n    if len(tmp_comp_ID_pairs) &gt; 0:\n        for _, pair in tmp_comp_ID_pairs.iterrows():\n            id1, id2 = pair['datasetId 1'], pair['datasetId 2']\n            # Log what was composited\n            composite_log.append({\n                'iteration': ii,\n                'composited': [id1, id2],\n                'new_id': f\"{id1}_{id2}_composite\"  # or however you generate it\n            })\n    # # create new composite data and metadata from the pairs\n    # # loop through the composite pairs and check metadata\n    tmp_df_composite = dup.join_composites_metadata(df_check, tmp_comp_ID_pairs, tmp_df_decisions, header)\n\n    tmp_df_dupfree = pd.concat([tmp_df_cleaned, tmp_df_composite])\n    print('--'*20)\n    print('Finished iteration.')\n\n    print('NEW DATAFRAME:')\n    print(tmp_df_dupfree.info())\n\n    print('--'*20)\n    print('--'*20)\n    if ii==19: print('STILL DUPLICATES PRESENT AFTER MULTIPLE ITERATIONS! REVISE DECISION PROCESS!!')\n\n    print('--'*20)\n\nprint(f\"Created {len(composite_log)} composites across all iterations\")\n\n```\n</code></pre> <ol> <li>Check again for remaining duplicates in the entire dataframe: The resulting dataframe should be checked once more for resulting duplicates. This can be dome by setting up a loop of the duplicate workflow until no more duplicates are found:</li> </ol> python3/Jupyter<pre><code>    tmp_df_dupfree.set_index('datasetId', inplace = True)\n    tmp_df_dupfree['datasetId']=tmp_df_dupfree.index\n\n    # Now we create a  dataframe which needs to be checked for duplicates.\n    df_check = tmp_df_dupfree.copy()\n    df_check.name = 'tmp'\n    df_check.index = range(len(df_check))\n    # We then run a brief duplicate detection algorithm on the dataframe. Note that by default the composited data has the highest value in the hierarchy.\n    pot_dup_IDs = dup.find_duplicates_optimized(df_check, n_points_thresh=10, return_data=True)\n    if len(pot_dup_IDs)==0:\n        print('SUCCESS!! NO MORE DUPLICATES DETECTED!!')\n    else:\n        df_check = dup.define_hierarchy(df_check)\n        dup.duplicate_decisions_multiple(df_check, operator_details=operator_details, choose_recollection=True, \n                                remove_identicals=False, backup=False)\n        # implement the decisions\n        tmp_df_decisions  = pd.read_csv(f'data/{df_check.name}/dup_detection/dup_decisions_{df_check.name}_{initials}_{date}'+'.csv', header=5)\n        tmp_dup_details   = dup.provide_dup_details(tmp_df_decisions, header)\n\n\n        # decisions\n        tmp_decisions = {}\n        for ind in tmp_df_decisions.index:\n            id1, id2   = tmp_df_decisions.loc[ind, ['datasetId 1', 'datasetId 2']]\n            dec1, dec2 = tmp_df_decisions.loc[ind, ['Decision 1', 'Decision 2']]\n            for id, dec in zip([id1, id2], [dec1, dec2]):\n                if id not in tmp_decisions: tmp_decisions[id] = []\n                tmp_decisions[id]+=[dec]\n\n        df_check.set_index('datasetId', inplace = True)\n        df_check['datasetId']=df_check.index\n\n        #drop all REMOVE or COMPOSITE types\n        tmp_remove_IDs  = list(tmp_df_decisions['datasetId 1'][np.isin(tmp_df_decisions['Decision 1'],['REMOVE', 'COMPOSITE'])])\n        tmp_remove_IDs += list(tmp_df_decisions['datasetId 2'][np.isin(tmp_df_decisions['Decision 2'],['REMOVE', 'COMPOSITE'])])\n        tmp_remove_IDs = np.unique(tmp_remove_IDs)#[id for id in np.unique(tmp_remove_IDs) if id not in tmp_remove_IDs]\n        tmp_df_cleaned = tmp_df_dupfree.drop(tmp_remove_IDs) # df freed from 'REMOVE' type duplicates\n\n        # # composite the \n        tmp_comp_ID_pairs = tmp_df_decisions[(tmp_df_decisions['Decision 1']=='COMPOSITE')&amp;(tmp_df_decisions['Decision 2']=='COMPOSITE')]\n\n        # # create new composite data and metadata from the pairs\n        # # loop through the composite pairs and check metadata\n        tmp_df_composite = dup.join_composites_metadata(df_check, tmp_comp_ID_pairs, tmp_df_decisions, header)\n\n        tmp_df_dupfree = pd.concat([tmp_df_cleaned, tmp_df_composite])\n\n        print('Finished last round of duplicate removal.')\n        print('Potentially run through this cell again to check for remaining duplicates.')\n</code></pre> <p>Warning</p> <p>This process once again goes, at least once, through the entire duplicate detection, decision and removal workflow and might therefore take a considerable amount of time. </p> <p>Detailed Instructions</p> <p>For complete step-by-step code and explanations, see Section 3.3 of the Duplicate Detection Tutorial.</p> <p>Save the duplicate-free database: python3/Jupyter<pre><code>    df_dupfree = df_dupfree[sorted(df_dupfree.columns)]\n    df_dupfree.name =f'{df.name}_{initials}_{date}_dupfree'\n\n    os.makedirs(f'data/{df_dupfree.name}/', exist_ok=True)\n\n    # save to a list of csv files (metadata, data, year)\n    utf.write_compact_dataframe_to_csv(df_dupfree)\n</code></pre></p> <p>Output: Duplicate-free database saved in <code>data/all_merged_INITIALS_DATE_dupfree/</code></p> <p>Duplicate Detection Complete</p> <p>The database is now free of duplicates and ready for analysis.</p>"},{"location":"tutorial/from_scratch/#related-tutorials","title":"Related Tutorials","text":"<ul> <li>Loading &amp; Merging Databases - Detailed instructions for Steps 1-2</li> <li>Duplicate Detection - Complete duplicate workflow documentation</li> <li>Loading &amp; Visualizing DoD2k - Working with the final database</li> </ul>"},{"location":"tutorial/load_merge/","title":"Load the original databases and merge to a common dataframe.","text":"<p>This tutorial shows you how to load the original databases from source and merge them to a common dataframe (ready for duplicate detection).</p>"},{"location":"tutorial/load_merge/#load-the-original-databases-from-source","title":"Load the original databases from source","text":"<p>The original databases are each loaded via their individual load notebooks (<code>load_ds.ipynb</code>, <code>ds</code> as below). These notebooks load each database from source and create a standardised 'compact' <code>pandas</code> dataframe, with the following columns:</p> <ul> <li><code>archiveType</code></li> <li><code>dataSetName</code></li> <li><code>datasetId</code></li> <li><code>geo_meanElev</code></li> <li><code>geo_meanLat</code></li> <li><code>geo_meanLon</code></li> <li><code>geo_siteName</code></li> <li><code>interpretation_direction</code> (new in v2.0)</li> <li><code>interpretation_variable</code></li> <li><code>interpretation_variableDetail</code></li> <li><code>interpretation_seasonality</code> (new in v2.0)</li> <li><code>originalDataURL</code></li> <li><code>originalDatabase</code></li> <li><code>paleoData_notes</code></li> <li><code>paleoData_proxy</code></li> <li><code>paleoData_sensorSpecies</code></li> <li><code>paleoData_units</code></li> <li><code>paleoData_values</code></li> <li><code>paleoData_variableName</code></li> <li><code>year</code></li> <li><code>yearUnits</code></li> </ul> <p>The intermediate data is saved in the <code>data</code> directory, where each database has its own subdirectory. For any database <code>db</code>, the output is saved in <code>data/db</code> under:</p> <ul> <li><code>db_compact.pkl</code>: <code>pickle</code> file for easy python processing, read with <code>pd.DataFrame(pd.read_pickle(db_compact.pkl))</code></li> <li><code>db_compact_metadata.csv</code>: <code>csv</code> file which includes all the metadata associated with each record</li> <li><code>db_compact_year.csv</code>: <code>csv</code> file which includes the time coordinates for each record</li> <li><code>db_compact_paleoData_values.csv</code>: <code>csv</code> file which includes the actual record data for each record</li> </ul> <p>Further information about each individual load notebook can be found here:</p> Load PAGES 2k data from source <p>The interactive notebook to load PAGES 2k can be found here: load_pages2k.ipynb.</p> <p>This notebook loads PAGES 2k data from LiPDverse, currently version 2.2.0, and creates a standardised 'compact dataframe'. </p> <p>In order to load the data from source please activate the following cell:</p> python3/Jupyter<pre><code># Download the file\n\n!wget -O data/pages2k/Pages2kTemperature2_2_0.pkl https://lipdverse.org/Pages2kTemperature/current_version/Pages2kTemperature2_2_0.pkl\n</code></pre> <p>This will download the pickle file <code>Pages2kTemperature2_2_0.pkl</code> into <code>data/pages2k</code>.</p> <p>Subsequently, run the interactive notebook to create a set of <code>csv</code> files containing the compact dataframe.</p> Load FE23 data from source <p>The interactive notebook to load FE23 can be found here: load_fe23.ipynb.</p> <p>This notebook loads FE23 from the NCEI and creates a standardised 'compact dataframe'. </p> <p>In order to load the data from source please activate the following cell:</p> python3/Jupyter<pre><code># download and unzip FE23 \n!wget -O /data/fe23/franke2022-fe23.nc https://www.ncei.noaa.gov/pub/data/paleo/contributions_by_author/franke2022/franke2022-fe23.nc\nfe23_full  = xr.open_dataset('fe23/franke2022-fe23.nc')\n\n# save slice of FE23 with only relevant variables as netCDF (fe23_full is 25GB)\nfe23_slice = fe23_full[vars]\nfe23_slice.to_netcdf('data/fe23/franke2022-fe23_slice.nc')\n</code></pre> <p>This will download the netCDF file <code>franke2022-fe23.nc</code> into <code>data/fe23</code>. Note that this is a very large dataset (around 25GB) so it might be useful for the operator to slice a small list of variables of the large file and save it instead.</p> <p>Subsequently, run the interactive notebook to create a set of <code>csv</code> files containing the compact dataframe.</p> Load Iso2k data from source <p>The interactive notebook to load Iso2k can be found here: load_iso2k.ipynb.</p> <p>This notebook loads Iso2k from the LiPDverse and creates a standardised 'compact dataframe'. </p> <p>In order to load the data from source please activate the following cell:</p> python3/Jupyter<pre><code># Download the file (use -O to specify output filename)\n!wget -O data/iso2k/iso2k1_1_2.zip https://lipdverse.org/iso2k/current_version/iso2k1_1_2.zip\n\n# Unzip to the correct destination\n!unzip data/iso2k/iso2k1_1_2.zip -d data/iso2k/iso2k1_1_2\n</code></pre> <p>This will download the zip file <code>iso2k1_1_2.zip</code> into <code>data/iso2k</code> and unzip into the directory <code>data/iso2k/iso2k1_1_2</code>.</p> <p>Subsequently, run the interactive notebook to create a set of <code>csv</code> files containing the compact dataframe.</p> Load SISAL data from source <p>The interactive notebook to load SISAL can be found here: load_sisal.ipynb.</p> <p>This notebook loads SISAL from a set of <code>csv</code> files. </p> <p>The current version (v3) was made available to the authorsd but is not currently uploaded on the LiPDverse homepage yet. We will update the notebook and documentation accordingly once there are updates on the public availability. </p> <p>Subsequently, run the interactive notebook to create a set of <code>csv</code> files containing the compact dataframe.</p> Load CoralHydro2k data from source <p>The interactive notebook to load CoralHydro2k can be found here: load_ch2k.ipynb.</p> <p>This notebook loads CoralHydro2k from the LiPDverse and creates a standardised 'compact dataframe'. </p> <p>In order to load the data from source please activate the following cell:</p> python3/Jupyter<pre><code># Download the file (use -O to specify output filename)\n!wget -O data/ch2k/CoralHydro2k1_0_1.zip https://lipdverse.org/CoralHydro2k/current_version/CoralHydro2k1_0_1.zip\n\n# Unzip to the correct destination\n!unzip data/ch2k/CoralHydro2k1_0_1.zip -d data/ch2k/ch2k_101\n</code></pre> <p>This will download the zip file <code>CoralHydro2k1_0_1.zip</code> into <code>data/ch2k</code> and unzip into the directory <code>data/ch2k/ch2k_101</code>.</p> <p>Subsequently, run the interactive notebook to create a set of <code>csv</code> files containing the compact dataframe.</p>"},{"location":"tutorial/load_merge/#merge-the-databases-to-a-common-dataframe","title":"Merge the databases to a common dataframe.","text":"<p>The individual databases can be merged into a common dataframe by loading all the compact dataframes, created in the load notebooks (see previous section). </p> <p>Simply run the notebook merge_databases.ipynb. This notebook loads the compact dataframes and concatenates them via </p> <pre><code># read compact dataframes from all the single databases\n\ndataset_names = ['pages2k', 'fe23', 'ch2k', 'iso2k', 'sisal' ]\n\nprint(dataset_names[0])\ndf = utf.load_compact_dataframe_from_csv(dataset_names[0])\nprint('length: ', len(df))\n\nfor ii, dn in enumerate(dataset_names[1:]):\n    print(f'add {dn}')\n    new_df = utf.load_compact_dataframe_from_csv(dn)\n    df = pd.concat([df, new_df])\n    print('length: ', len(df))\n\nprint('---------------')\nprint('RESULT:')\nprint(df.info())\n</code></pre> <p>which creates the following dataframe: Output<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 5879 entries, 0 to 545\nData columns (total 21 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   archiveType                    5879 non-null   object \n 1   dataSetName                    5879 non-null   object \n 2   datasetId                      5879 non-null   object \n 3   geo_meanElev                   5780 non-null   float32\n 4   geo_meanLat                    5879 non-null   float32\n 5   geo_meanLon                    5879 non-null   float32\n 6   geo_siteName                   5879 non-null   object \n 7   interpretation_direction       5879 non-null   object \n 8   interpretation_seasonality     5879 non-null   object \n 9   interpretation_variable        5879 non-null   object \n 10  interpretation_variableDetail  5879 non-null   object \n 11  originalDataURL                5879 non-null   object \n 12  originalDatabase               5879 non-null   object \n 13  paleoData_notes                5879 non-null   object \n 14  paleoData_proxy                5879 non-null   object \n 15  paleoData_sensorSpecies        5879 non-null   object \n 16  paleoData_units                5879 non-null   object \n 17  paleoData_values               5879 non-null   object \n 18  paleoData_variableName         5879 non-null   object \n 19  year                           5879 non-null   object \n 20  yearUnits                      5879 non-null   object \ndtypes: float32(3), object(18)\nmemory usage: 941.6+ KB\nNone\n</code></pre></p> <p>This dataframe is saved in the directory <code>data/all_merged</code>, and can be used as input for duplicate detection.</p>"},{"location":"tutorial/use_dod2k/","title":"Loading the DoD2k Database","text":"<p>This tutorial shows you how to load and explore the DoD2k paleoclimate database.</p>"},{"location":"tutorial/use_dod2k/#what-is-dod2k","title":"What is DoD2k?","text":"<p>DoD2k (Database of Databases 2k) integrates five major paleoclimate databases:</p> Database Version Data Reference Records Archives PAGES 2k v2.2.0 LiPDverse PAGES 2k Consortium 2017 1364 multi-proxy SISAL v3 ORA Kaushal et al. 2024 546 speleothems Iso2k v1.1.2 LiPDverse Konecky et al. 2020 435 multi-proxy CoralHydro2k v1.0.1 LiPDverse Walter et al. 2023 221 corals FE23 - NCEI Evans et al. 2022 2754 tree-rings <p>Since these databases may share a number of records, these databases were subject to a duplicate detection and removal process. The resulting output is DoD2k.</p>"},{"location":"tutorial/use_dod2k/#the-database","title":"The database","text":"<p>The database is saved in <code>root_dir/data/dod2k</code> and provided in two data formats:</p> <ul> <li>pickle format (fast, python only):   <pre><code>dod2k_compact.pkl\n</code></pre></li> <li>comma seperated value format (portable, readable)   <pre><code>dod2k_compact_year.csv\ndod2k_compact_metadata.csv\ndod2k_compact_paleoData_values.csv\n</code></pre></li> </ul> <p>In this directory we also provide a text file  <pre><code>dod2k_compact_README.txt\n</code></pre> which shows the details of the duplicate screening process (date, operator credentials, notes).</p> <p>Which Format Should I Use?</p> <ul> <li>CSV: Best for interoperability and inspecting data manually</li> <li>Pickle: Faster loading, preserves numpy arrays without conversion - python only!!!</li> </ul> <p>Here we explain how to load these files into a pandas dataframe using python.</p>"},{"location":"tutorial/use_dod2k/#loading-the-database-from-csv","title":"Loading the database from csv","text":""},{"location":"tutorial/use_dod2k/#step-1-set-up-your-environment","title":"Step 1: Set up your environment","text":"<p>Start in the repository root directory (<code>dod2k/</code>). From here import</p> <p>python3/Jupyter<pre><code>import sys\nfrom pathlib import Path\n\n# Add dod2k to path\ndod2k = Path().resolve().parent\nsys.path.insert(0, str(dod2k))\nprint(dod2k)\nfrom dod2k_utilities.ut_functions import load_compact_dataframe_from_csv\n</code></pre> The function <code>load_compact_dataframe_from_csv</code> imports the different csv files and stitches them together to form a dataframe. See <code>load_compact_dataframe_from_csv()</code> for details.</p>"},{"location":"tutorial/use_dod2k/#step-2-load-the-data-from-csv","title":"Step 2: Load the data from csv","text":"python3/Jupyter<pre><code># Load the duplicate-free database\ndf = load_compact_dataframe_from_csv('dod2k')\n\n# Check the shape\nprint(f\"Database contains {len(df)} records\")\nprint(f\"Columns: {', '.join(df.columns)}\")\n</code></pre>"},{"location":"tutorial/use_dod2k/#alternatively-load-the-database-from-the-pickle","title":"Alternatively: Load the database from the pickle","text":"<p>For faster loading and if you only need python access, use the pickle format. Make sure you start in the repository root directory (<code>dod2k</code>). From here import python3/Jupyter<pre><code>import pandas as pd\n\n# Load the duplicate-free database\ndf = pd.read_pickle('/data/dod2k_dupfree_dupfree/dod2k_compact.pkl')\n\nprint(f\"Database contains {len(df)} records\")\n</code></pre></p>"},{"location":"tutorial/use_dod2k/#explore-the-dataframe-and-visualise-the-data","title":"Explore the dataframe and visualise the data","text":""},{"location":"tutorial/use_dod2k/#step-3-explore-the-dataframe-column-by-column","title":"Step 3: Explore the dataframe column by column","text":"python3/Jupyter<pre><code>import pandas as pd\nimport numpy as np\n</code></pre> <p>Under <code>dod2k/notebooks</code> you can find the notebook <code>df_info.ipynb</code>. This notebook goes through the dataframe column by column and shows you a quick summary of the entries. </p> <p>The key features of this notebook are:</p> Input<pre><code>print(df.info())\n</code></pre> <p>Output<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4516 entries, 0 to 4515\nData columns (total 19 columns):\n #   Column                                Non-Null Count  Dtype  \n---  ------                                --------------  -----  \n 0   archiveType                           4516 non-null   object \n 1   climateInterpretation_variable        4516 non-null   object \n 2   climateInterpretation_variableDetail  4516 non-null   object \n 3   dataSetName                           4516 non-null   object \n 4   datasetId                             4516 non-null   object \n 5   duplicateDetails                      4516 non-null   object \n 6   geo_meanElev                          4433 non-null   float32\n 7   geo_meanLat                           4516 non-null   float32\n 8   geo_meanLon                           4516 non-null   float32\n 9   geo_siteName                          4516 non-null   object \n 10  originalDataURL                       4516 non-null   object \n 11  originalDatabase                      4516 non-null   object \n 12  paleoData_notes                       4516 non-null   object \n 13  paleoData_proxy                       4516 non-null   object \n 14  paleoData_sensorSpecies               4516 non-null   object \n 15  paleoData_units                       4516 non-null   object \n 16  paleoData_values                      4516 non-null   object \n 17  year                                  4516 non-null   object \n 18  yearUnits                             4516 non-null   object \ndtypes: float32(3), object(16)\nmemory usage: 617.6+ KB\nNone\n</code></pre> The interactive notebook then goes through each column and shows the entries, for example</p> Input<pre><code># archiveType\nkey = 'archiveType'\nprint('%s: '%key)\nprint(np.unique(df[key]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n</code></pre> Output<pre><code>archiveType: \n['bivalve' 'borehole' 'coral' 'documents' 'glacier ice' 'ground ice'\n 'hybrid' 'lake sediment' 'marine sediment' 'mollusk shells'\n 'sclerosponge' 'speleothem' 'terrestrial sediment' 'tree']\n[\"&lt;class 'str'&gt;\"]\n</code></pre> Input<pre><code># paleoData_proxy\nkey = 'paleoData_proxy'\nprint('%s: '%key)\nprint(np.unique([kk for kk in df[key]]))\nprint(np.unique([str(type(dd)) for dd in df[key]]))\n</code></pre> Output<pre><code>paleoData_proxy: \n['BSi' 'Documentary' 'MXD' 'Mg/Ca' 'Sr/Ca' 'TEX86' 'TRW' 'alkenone'\n 'borehole' 'calcification' 'calcification rate' 'chironomid'\n 'chrysophyte' 'd13C' 'd18O' 'd2H' 'diatom' 'dynocist MAT' 'foram Mg/Ca'\n 'foram d18O' 'foraminifera' 'growth rate' 'historic' 'hybrid' 'melt'\n 'midge' 'pollen' 'reflectance' 'sed accumulation' 'varve property'\n 'varve thickness']\n[\"&lt;class 'str'&gt;\"]\n</code></pre> <p>For further guidance see the interactive notebook.</p>"},{"location":"tutorial/use_dod2k/#step-4-visualise-the-dataframe","title":"Step 4: Visualise the dataframe","text":"<p>Under <code>dod2k/notebooks</code> you can find the notebook <code>df_plot_dod2k.ipynb</code>. This notebook visualises the dataframe and produces summary figures of the database. It also reproduces the manuscript figures. </p> <p>Import the python libraries python3/Jupyter<pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature \nfrom matplotlib.gridspec import GridSpec as GS\nfrom copy import deepcopy as dc\n\nfrom dod2k_utilities import ut_functions as utf # contains utility functions\nfrom dod2k_utilities import ut_plot as uplt # contains plotting functions\n</code></pre></p> <p>After loading the dataframe, start off by counting the number of records in each archive type:</p> python3/Jupyter<pre><code># count archive types\narchive_count = {}\nfor ii, at in enumerate(set(df['archiveType'])):\n    archive_count[at] = df.loc[df['archiveType']==at, 'archiveType'].count()\n</code></pre> <p>Now count the number of records for each proxy type, depending on the archive type: python3/Jupyter<pre><code>archive_proxy_count = {}\narchive_proxy_ticks = []\nfor ii, at in enumerate(set(df['archiveType'])):\n    proxy_types   = df['paleoData_proxy'][df['archiveType']==at].unique()\n    for pt in proxy_types:\n        cc = df['paleoData_proxy'][(df['paleoData_proxy']==pt)&amp;(df['archiveType']==at)].count()\n        archive_proxy_count['%s: %s'%(at, pt)] = cc\n        archive_proxy_ticks += [at+': '+pt]\n</code></pre></p> <p>For each archive type, specify colours for each archive, but also distinguish between major archives (which have the most records) and minor archives (rare ones, only including less than ten records): python3/Jupyter<pre><code>archive_colour = {'other': cols[-1]}\nother_archives = []\nmajor_archives = []\n\nsort = np.argsort([cc for cc in archive_count.values()])\narchives_sorted = np.array([cc for cc in archive_count.keys()])[sort][::-1]\n\nfor ii, at in enumerate(archives_sorted):\n    print(ii, at, archive_count[at])\n    if archive_count[at]&gt;10:\n        major_archives     +=[at]\n        archive_colour[at] = cols[ii]\n    else:\n        other_archives     +=[at]\n        archive_colour[at] = cols[-1]\n</code></pre> Now plot a bar chart of the major archives using <code>plot_count_proxy_by_archive_short()</code></p> python3/Jupyter<pre><code>uplt.plot_count_proxy_by_archive_short(df, archive_proxy_count, archive_proxy_ticks, archive_colour) \n</code></pre> Figure 1: Number of records for each proxy type, by archive. <p>Next plot a spatial plot of all the proxy records:</p> python3/Jupyter<pre><code>#%% plot the spatial distribution of all records\nproxy_lats = df['geo_meanLat'].values\nproxy_lons = df['geo_meanLon'].values\n\n# plots the map\nfig = plt.figure(figsize=(15, 12), dpi=350)\ngrid = GS(1, 3)\n\nax = plt.subplot(grid[:, :], projection=ccrs.Robinson()) # create axis with Robinson projection of globe\n\nax.add_feature(cfeature.LAND, alpha=0.5) # adds land features\nax.add_feature(cfeature.OCEAN, alpha=0.6, facecolor='#C5DEEA') # adds ocean features\nax.coastlines() # adds coastline features\n\nax.set_global()\n\n# loop through the data to generate a scatter plot of each data record:\n# 1st loop: go through archive types individually (determines marker type)\n# 2nd loop: through paleo proxy types attributed to the specific archive, which is colour coded\n\n\nmt = 'ov^s&lt;&gt;pP*XDdh'*10 # generates string of marker types\n\narchive_types = major_archives+other_archives\n# archive_types = [aa for aa in archive_types if aa!='other']\n\n\nijk=0\nfor jj, at in enumerate(archive_types):\n    arch_mask = df['archiveType']==at\n    arch_proxy_types = np.unique(df['paleoData_proxy'][arch_mask])\n    for ii, pt in enumerate(arch_proxy_types):\n        pt_mask = df['paleoData_proxy']==pt\n        at_mask = df['archiveType']==at\n        label = at+': '+pt+' ($n=%d$)'% df['paleoData_proxy'][(df['paleoData_proxy']==pt)&amp;(df['archiveType']==at)].count()\n        marker = mt[ii] if at in major_archives else mt[ijk]\n        plt.scatter(proxy_lons[pt_mask&amp;at_mask], proxy_lats[pt_mask&amp;at_mask], \n                    transform=ccrs.PlateCarree(), zorder=999,\n                    marker=marker, color=archive_colour[at], \n                    label=label,#.replace('marine sediment:', 'marine sediment:\\n'), \n                    lw=.3, ec='k', s=200)\n        if at not in major_archives: ijk+=1\n\nplt.legend(bbox_to_anchor=(-0.01,-0.01), loc='upper left', ncol=3, fontsize=13.5, framealpha=0)\ngrid.tight_layout(fig)\n\nutf.save_fig(fig, f'{df.name}_spatial_all', dir=df.name)\n</code></pre> <p>Which creates this plot</p> Figure 2: Spatial distribution of records by archive and proxy type. <p>For further guidance see the interactive notebook.</p>"},{"location":"tutorial/use_dod2k/#filter-the-dataframe","title":"Filter the dataframe","text":"<p>Under <code>dod2k/notebooks</code> you can find the notebook <code>df_filter.ipynb</code>. This notebook let's you filter the dataframe for a specific metadata type, e.g. for moisture sensitive records, or for tree/TRW type records, etc.</p> <p>This notebook then saves the filtered dataframe as a compact dataframe under <code>dod2k/data</code>, from which it can be loaded by other notebooks (e.g. <code>df_plot_dod2k.ipynb</code>).</p> <p>Start by loading the dataframe, then filter using e.g.  python3/Jupyter<pre><code># # filter for &gt;&gt;exclusively moisture&lt;&lt; sensitive records only (without t+m)\ndf_filter = df.loc[(df['climateInterpretation_variable']=='moisture')]\n</code></pre></p> <p>The resulting dataframe can then be saved and used as input for <code>df_info.ipynb</code> or <code>df_plot_dod2k.ipynb</code>, or you can add this line to another notebook if you prefer. </p> <p>For further guidance see the interactive notebook.</p>"}]}